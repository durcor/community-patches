diff --git a/src/gallium/drivers/zink/meson.build b/src/gallium/drivers/zink/meson.build
index a104c1841fd..8c61fdc3b24 100644
--- a/src/gallium/drivers/zink/meson.build
+++ b/src/gallium/drivers/zink/meson.build
@@ -23,6 +23,7 @@ files_libzink = files(
   'nir_to_spirv/spirv_builder.c',
   'zink_batch.c',
   'zink_blit.c',
+  'zink_clear.c',
   'zink_compiler.c',
   'zink_context.c',
   'zink_draw.c',
diff --git a/src/gallium/drivers/zink/nir_to_spirv/nir_to_spirv.c b/src/gallium/drivers/zink/nir_to_spirv/nir_to_spirv.c
index a1347f163fd..6c5281857f7 100644
--- a/src/gallium/drivers/zink/nir_to_spirv/nir_to_spirv.c
+++ b/src/gallium/drivers/zink/nir_to_spirv/nir_to_spirv.c
@@ -29,65 +29,7 @@
 #include "util/u_memory.h"
 #include "util/hash_table.h"
 
-/* this consistently maps slots to a zero-indexed value to avoid wasting slots */
-static unsigned slot_pack_map[] = {
-   /* Position is builtin */
-   [VARYING_SLOT_POS] = UINT_MAX,
-   [VARYING_SLOT_COL0] = 0, /* input/output */
-   [VARYING_SLOT_COL1] = 1, /* input/output */
-   [VARYING_SLOT_FOGC] = 2, /* input/output */
-   /* TEX0-7 are deprecated, so we put them at the end of the range and hope nobody uses them all */
-   [VARYING_SLOT_TEX0] = VARYING_SLOT_VAR0 - 1, /* input/output */
-   [VARYING_SLOT_TEX1] = VARYING_SLOT_VAR0 - 2,
-   [VARYING_SLOT_TEX2] = VARYING_SLOT_VAR0 - 3,
-   [VARYING_SLOT_TEX3] = VARYING_SLOT_VAR0 - 4,
-   [VARYING_SLOT_TEX4] = VARYING_SLOT_VAR0 - 5,
-   [VARYING_SLOT_TEX5] = VARYING_SLOT_VAR0 - 6,
-   [VARYING_SLOT_TEX6] = VARYING_SLOT_VAR0 - 7,
-   [VARYING_SLOT_TEX7] = VARYING_SLOT_VAR0 - 8,
-
-   /* PointSize is builtin */
-   [VARYING_SLOT_PSIZ] = UINT_MAX,
-
-   [VARYING_SLOT_BFC0] = 3, /* output only */
-   [VARYING_SLOT_BFC1] = 4, /* output only */
-   [VARYING_SLOT_EDGE] = 5, /* output only */
-   [VARYING_SLOT_CLIP_VERTEX] = 6, /* output only */
-
-   /* ClipDistance is builtin */
-   [VARYING_SLOT_CLIP_DIST0] = UINT_MAX,
-   [VARYING_SLOT_CLIP_DIST1] = UINT_MAX,
-
-   /* CullDistance is builtin */
-   [VARYING_SLOT_CULL_DIST0] = UINT_MAX, /* input/output */
-   [VARYING_SLOT_CULL_DIST1] = UINT_MAX, /* never actually used */
-
-   /* PrimitiveId is builtin */
-   [VARYING_SLOT_PRIMITIVE_ID] = UINT_MAX,
-
-   /* Layer is builtin */
-   [VARYING_SLOT_LAYER] = UINT_MAX, /* input/output */
-
-   /* ViewportIndex is builtin */
-   [VARYING_SLOT_VIEWPORT] =  UINT_MAX, /* input/output */
-
-   /* FrontFacing is builtin */
-   [VARYING_SLOT_FACE] = UINT_MAX,
-
-   /* PointCoord is builtin */
-   [VARYING_SLOT_PNTC] = UINT_MAX, /* input only */
-
-   /* TessLevelOuter is builtin */
-   [VARYING_SLOT_TESS_LEVEL_OUTER] = UINT_MAX,
-   /* TessLevelInner is builtin */
-   [VARYING_SLOT_TESS_LEVEL_INNER] = UINT_MAX,
-
-   [VARYING_SLOT_BOUNDING_BOX0] = 7, /* Only appears as TCS output. */
-   [VARYING_SLOT_BOUNDING_BOX1] = 8, /* Only appears as TCS output. */
-   [VARYING_SLOT_VIEW_INDEX] = 9, /* input/output */
-   [VARYING_SLOT_VIEWPORT_MASK] = 10, /* output only */
-};
-#define NTV_MIN_RESERVED_SLOTS 11
+#define SLOT_UNSET ((unsigned char) -1)
 
 struct ntv_context {
    void *mem_ctx;
@@ -97,11 +39,21 @@ struct ntv_context {
    SpvId GLSL_std_450;
 
    gl_shader_stage stage;
+   const struct zink_so_info *so_info;
+   const VkPhysicalDeviceFeatures *feats;
 
    SpvId ubos[128];
    size_t num_ubos;
+
+   SpvId ssbos[PIPE_MAX_SHADER_BUFFERS];
+   nir_variable *ssbo_vars[PIPE_MAX_SHADER_BUFFERS];
+   uint32_t ssbo_mask;
+   uint32_t num_ssbos;
    SpvId image_types[PIPE_MAX_SAMPLERS];
+   SpvId images[PIPE_MAX_SAMPLERS];
+   SpvId sampler_types[PIPE_MAX_SAMPLERS];
    SpvId samplers[PIPE_MAX_SAMPLERS];
+   unsigned char sampler_array_sizes[PIPE_MAX_SAMPLERS];
    unsigned samplers_used : PIPE_MAX_SAMPLERS;
    SpvId entry_ifaces[PIPE_MAX_SHADER_INPUTS * 4 + PIPE_MAX_SHADER_OUTPUTS * 4];
    size_t num_entry_ifaces;
@@ -113,33 +65,43 @@ struct ntv_context {
    size_t num_regs;
 
    struct hash_table *vars; /* nir_variable -> SpvId */
+   struct hash_table *image_vars; /* SpvId -> nir_variable */
    struct hash_table *so_outputs; /* pipe_stream_output -> SpvId */
-   unsigned outputs[VARYING_SLOT_MAX];
-   const struct glsl_type *so_output_gl_types[VARYING_SLOT_MAX];
-   SpvId so_output_types[VARYING_SLOT_MAX];
+   unsigned outputs[VARYING_SLOT_MAX * 4];
+   const struct glsl_type *so_output_gl_types[VARYING_SLOT_MAX * 4];
+   SpvId so_output_types[VARYING_SLOT_MAX * 4];
 
    const SpvId *block_ids;
    size_t num_blocks;
    bool block_started;
    SpvId loop_break, loop_cont;
 
-   SpvId front_face_var, instance_id_var, vertex_id_var;
-#ifndef NDEBUG
-   bool seen_texcoord[8]; //whether we've seen a VARYING_SLOT_TEX[n] this pass
-#endif
+   unsigned char *shader_slot_map;
+   unsigned char shader_slots_reserved;
+
+   SpvId front_face_var, instance_id_var, vertex_id_var,
+         primitive_id_var, invocation_id_var, // geometry
+         sample_mask_type, sample_id_var, sample_pos_var, sample_mask_in_var,
+         tess_patch_vertices_in, tess_coord_var, // tess
+         push_const_var,
+         workgroup_id_var, num_workgroups_var,
+         local_invocation_id_var, global_invocation_id_var,
+         local_invocation_index_var, helper_invocation_var,
+         shared_block_var,
+         base_vertex_var, base_instance_var, draw_id_var;
 };
 
 static SpvId
 get_fvec_constant(struct ntv_context *ctx, unsigned bit_size,
-                  unsigned num_components, float value);
+                  unsigned num_components, double value);
 
 static SpvId
 get_uvec_constant(struct ntv_context *ctx, unsigned bit_size,
-                  unsigned num_components, uint32_t value);
+                  unsigned num_components, uint64_t value);
 
 static SpvId
 get_ivec_constant(struct ntv_context *ctx, unsigned bit_size,
-                  unsigned num_components, int32_t value);
+                  unsigned num_components, int64_t value);
 
 static SpvId
 emit_unop(struct ntv_context *ctx, SpvOp op, SpvId type, SpvId src);
@@ -171,31 +133,103 @@ block_label(struct ntv_context *ctx, nir_block *block)
    return ctx->block_ids[block->index];
 }
 
+static void
+emit_access_decorations(struct ntv_context *ctx, nir_variable *var, SpvId var_id)
+{
+    enum gl_access_qualifier access = var->data.access;
+    while (access) {
+       unsigned bit = u_bit_scan(&access);
+       switch (1 << bit) {
+       case ACCESS_COHERENT:
+          //this can't be used with vulkan memory model
+          //spirv_builder_emit_decoration(&ctx->builder, var_id, SpvDecorationCoherent);
+          break;
+       case ACCESS_RESTRICT:
+          spirv_builder_emit_decoration(&ctx->builder, var_id, SpvDecorationRestrict);
+          break;
+       case ACCESS_VOLATILE:
+          //this can't be used with vulkan memory model
+          //spirv_builder_emit_decoration(&ctx->builder, var_id, SpvDecorationVolatile);
+          break;
+       case ACCESS_NON_READABLE:
+          spirv_builder_emit_decoration(&ctx->builder, var_id, SpvDecorationNonReadable);
+          break;
+       case ACCESS_NON_WRITEABLE:
+          spirv_builder_emit_decoration(&ctx->builder, var_id, SpvDecorationNonWritable);
+          break;
+       case ACCESS_NON_UNIFORM:
+          spirv_builder_emit_decoration(&ctx->builder, var_id, SpvDecorationNonUniform);
+          break;
+       case ACCESS_CAN_REORDER:
+       case ACCESS_STREAM_CACHE_POLICY:
+          /* no equivalent */
+          break;
+       default:
+          unreachable("unknown access bit");
+       }
+    }
+}
+
+static SpvOp
+get_atomic_op(nir_intrinsic_op op)
+{
+   switch (op) {
+#define CASE_ATOMIC_OP(type) \
+   case nir_intrinsic_ssbo_atomic_##type: \
+   case nir_intrinsic_image_deref_atomic_##type: \
+   case nir_intrinsic_shared_atomic_##type
+
+   CASE_ATOMIC_OP(add):
+      return SpvOpAtomicIAdd;
+   CASE_ATOMIC_OP(umin):
+      return SpvOpAtomicUMin;
+   CASE_ATOMIC_OP(imin):
+      return SpvOpAtomicSMin;
+   CASE_ATOMIC_OP(umax):
+      return SpvOpAtomicUMax;
+   CASE_ATOMIC_OP(imax):
+      return SpvOpAtomicSMax;
+   CASE_ATOMIC_OP(and):
+      return SpvOpAtomicAnd;
+   CASE_ATOMIC_OP(or):
+      return SpvOpAtomicOr;
+   CASE_ATOMIC_OP(xor):
+      return SpvOpAtomicXor;
+   CASE_ATOMIC_OP(exchange):
+      return SpvOpAtomicExchange;
+   CASE_ATOMIC_OP(comp_swap):
+      return SpvOpAtomicCompareExchange;
+   default:
+      unreachable("unhandled atomic op");
+   }
+   return 0;
+}
+#undef CASE_ATOMIC_OP
 static SpvId
-emit_float_const(struct ntv_context *ctx, int bit_size, float value)
+emit_float_const(struct ntv_context *ctx, int bit_size, double value)
 {
-   assert(bit_size == 32);
+   assert(bit_size == 32 || bit_size == 64);
    return spirv_builder_const_float(&ctx->builder, bit_size, value);
 }
 
 static SpvId
-emit_uint_const(struct ntv_context *ctx, int bit_size, uint32_t value)
+emit_uint_const(struct ntv_context *ctx, int bit_size, uint64_t value)
 {
-   assert(bit_size == 32);
+   assert(bit_size == 32 || bit_size == 64);
    return spirv_builder_const_uint(&ctx->builder, bit_size, value);
 }
 
 static SpvId
-emit_int_const(struct ntv_context *ctx, int bit_size, int32_t value)
+emit_int_const(struct ntv_context *ctx, int bit_size, int64_t value)
 {
-   assert(bit_size == 32);
+   assert(bit_size == 32 || bit_size == 64);
    return spirv_builder_const_int(&ctx->builder, bit_size, value);
 }
 
 static SpvId
 get_fvec_type(struct ntv_context *ctx, unsigned bit_size, unsigned num_components)
 {
-   assert(bit_size == 32); // only 32-bit floats supported so far
+   assert(bit_size == 32 || bit_size == 64);
 
    SpvId float_type = spirv_builder_type_float(&ctx->builder, bit_size);
    if (num_components > 1)
@@ -209,7 +243,7 @@ get_fvec_type(struct ntv_context *ctx, unsigned bit_size, unsigned num_component
 static SpvId
 get_ivec_type(struct ntv_context *ctx, unsigned bit_size, unsigned num_components)
 {
-   assert(bit_size == 32); // only 32-bit ints supported so far
+   assert(bit_size == 32 || bit_size == 64);
 
    SpvId int_type = spirv_builder_type_int(&ctx->builder, bit_size);
    if (num_components > 1)
@@ -223,7 +257,7 @@ get_ivec_type(struct ntv_context *ctx, unsigned bit_size, unsigned num_component
 static SpvId
 get_uvec_type(struct ntv_context *ctx, unsigned bit_size, unsigned num_components)
 {
-   assert(bit_size == 32); // only 32-bit uints supported so far
+   assert(bit_size == 32 || bit_size == 64);
 
    SpvId uint_type = spirv_builder_type_uint(&ctx->builder, bit_size);
    if (num_components > 1)
@@ -237,7 +271,7 @@ get_uvec_type(struct ntv_context *ctx, unsigned bit_size, unsigned num_component
 static SpvId
 get_dest_uvec_type(struct ntv_context *ctx, nir_dest *dest)
 {
-   unsigned bit_size = MAX2(nir_dest_bit_size(*dest), 32);
+   unsigned bit_size = nir_dest_bit_size(*dest);
    return get_uvec_type(ctx, bit_size, nir_dest_num_components(*dest));
 }
 
@@ -256,6 +290,15 @@ get_glsl_basetype(struct ntv_context *ctx, enum glsl_base_type type)
 
    case GLSL_TYPE_UINT:
       return spirv_builder_type_uint(&ctx->builder, 32);
+
+   case GLSL_TYPE_DOUBLE:
+      return spirv_builder_type_float(&ctx->builder, 64);
+
+   case GLSL_TYPE_INT64:
+      return spirv_builder_type_int(&ctx->builder, 64);
+
+   case GLSL_TYPE_UINT64:
+      return spirv_builder_type_uint(&ctx->builder, 64);
    /* TODO: handle more types */
 
    default:
@@ -264,11 +307,11 @@ get_glsl_basetype(struct ntv_context *ctx, enum glsl_base_type type)
 }
 
 static SpvId
-get_glsl_type(struct ntv_context *ctx, const struct glsl_type *type)
+get_glsl_type_element(struct ntv_context *ctx, const struct glsl_type *type, SpvId element)
 {
    assert(type);
    if (glsl_type_is_scalar(type))
-      return get_glsl_basetype(ctx, glsl_get_base_type(type));
+      return element ?: get_glsl_basetype(ctx, glsl_get_base_type(type));
 
    if (glsl_type_is_vector(type))
       return spirv_builder_type_vector(&ctx->builder,
@@ -277,37 +320,97 @@ get_glsl_type(struct ntv_context *ctx, const struct glsl_type *type)
 
    if (glsl_type_is_array(type)) {
       SpvId ret = spirv_builder_type_array(&ctx->builder,
-         get_glsl_type(ctx, glsl_get_array_element(type)),
-         emit_uint_const(ctx, 32, glsl_get_length(type)));
+                                           get_glsl_type_element(ctx, glsl_get_array_element(type), element),
+                                           emit_uint_const(ctx, 32, glsl_get_length(type)));
       uint32_t stride = glsl_get_explicit_stride(type);
+      if (!stride && glsl_type_is_scalar(glsl_get_array_element(type))) {
+         switch (glsl_get_bit_size(glsl_get_array_element(type))) {
+         case 1:
+         case 8:
+            stride = sizeof(uint8_t);
+            break;
+         case 16:
+            stride = sizeof(uint16_t);
+            break;
+         case 32:
+            stride = sizeof(uint32_t);
+            break;
+         case 64:
+            stride = sizeof(uint64_t);
+            break;
+         default:
+            unreachable("unknown array element size");
+         }
+      }
       if (stride)
          spirv_builder_emit_array_stride(&ctx->builder, ret, stride);
       return ret;
    }
 
+   if (glsl_type_is_struct(type)) {
+      static unsigned recursion;
+      recursion++;
+      SpvId types[glsl_get_length(type)];
+      for (unsigned i = 0; i < glsl_get_length(type); i++)
+         types[i] = get_glsl_type_element(ctx, glsl_get_struct_field(type, i), element);
+      SpvId ret = spirv_builder_type_struct(&ctx->builder,
+                                           types,
+                                           glsl_get_length(type));
+      /* only the outermost struct can have the Block decoration */
+      if (recursion == 1)
+         spirv_builder_emit_decoration(&ctx->builder, ret,
+                                       SpvDecorationBlock);
+      for (unsigned i = 0; i < glsl_get_length(type); i++)
+         spirv_builder_emit_member_offset(&ctx->builder, ret, i, glsl_get_struct_field_offset(type, i));
+      recursion--;
+      return ret;
+   }
+
+   if (glsl_type_is_matrix(type))
+      return spirv_builder_type_matrix(&ctx->builder,
+         spirv_builder_type_vector(&ctx->builder,
+            get_glsl_basetype(ctx, glsl_get_base_type(type)),
+            glsl_get_vector_elements(type)),
+      glsl_get_matrix_columns(type));
 
    unreachable("we shouldn't get here, I think...");
 }
 
+static SpvId
+get_glsl_type(struct ntv_context *ctx, const struct glsl_type *type)
+{
+   return get_glsl_type_element(ctx, type, 0);
+}
+
+static void
+create_shared_block(struct ntv_context *ctx, unsigned shared_size)
+{
+   SpvId type = spirv_builder_type_uint(&ctx->builder, 32);
+   SpvId array = spirv_builder_type_array(&ctx->builder, type, emit_uint_const(ctx, 32, shared_size / 4));
+   spirv_builder_emit_array_stride(&ctx->builder, array, 4);
+   SpvId ptr_type = spirv_builder_type_pointer(&ctx->builder,
+                                               SpvStorageClassWorkgroup,
+                                               array);
+   ctx->shared_block_var = spirv_builder_emit_var(&ctx->builder, ptr_type, SpvStorageClassWorkgroup);
+}
+
+static inline unsigned char
+reserve_slot(struct ntv_context *ctx)
+{
+   /* TODO: this should actually be clamped to the limits value as in the table in 14.1.4 of the vulkan spec,
+    * though there's not really any recourse other than aborting if we do hit it...
+    */
+   assert(ctx->shader_slots_reserved < MAX_VARYING);
+   return ctx->shader_slots_reserved++;
+}
+
 static inline unsigned
 handle_slot(struct ntv_context *ctx, unsigned slot)
 {
-   unsigned orig = slot;
-   if (slot < VARYING_SLOT_VAR0) {
-#ifndef NDEBUG
-      if (slot >= VARYING_SLOT_TEX0 && slot <= VARYING_SLOT_TEX7)
-         ctx->seen_texcoord[slot - VARYING_SLOT_TEX0] = true;
-#endif
-      slot = slot_pack_map[slot];
-      if (slot == UINT_MAX)
-         debug_printf("unhandled varying slot: %s\n", gl_varying_slot_name(orig));
-   } else {
-      slot -= VARYING_SLOT_VAR0 - NTV_MIN_RESERVED_SLOTS;
-      assert(slot <= VARYING_SLOT_VAR0 - 8 ||
-             !ctx->seen_texcoord[VARYING_SLOT_VAR0 - slot - 1]);
-
-   }
-   assert(slot < VARYING_SLOT_VAR0);
+   if (ctx->shader_slot_map[slot] == SLOT_UNSET)
+      ctx->shader_slot_map[slot] = reserve_slot(ctx);
+   slot = ctx->shader_slot_map[slot];
+   assert(slot < MAX_VARYING);
    return slot;
 }
 
@@ -322,16 +425,26 @@ emit_input(struct ntv_context *ctx, struct nir_variable *var)
 {
    SpvId var_type = get_glsl_type(ctx, var->type);
    SpvId pointer_type = spirv_builder_type_pointer(&ctx->builder,
+                                                   var->data.mode == nir_var_mem_push_const ?
+                                                   SpvStorageClassPushConstant :
                                                    SpvStorageClassInput,
                                                    var_type);
    SpvId var_id = spirv_builder_emit_var(&ctx->builder, pointer_type,
+                                         var->data.mode == nir_var_mem_push_const ?
+                                         SpvStorageClassPushConstant :
                                          SpvStorageClassInput);
 
    if (var->name)
       spirv_builder_emit_name(&ctx->builder, var_id, var->name);
-
-   if (ctx->stage == MESA_SHADER_FRAGMENT) {
-      unsigned slot = var->data.location;
+   if (var->data.mode == nir_var_mem_push_const) {
+      ctx->push_const_var = var_id;
+      return;
+   }
+   unsigned slot = var->data.location;
+   if (ctx->stage == MESA_SHADER_VERTEX)
+      spirv_builder_emit_location(&ctx->builder, var_id,
+                                  var->data.driver_location);
+   else if (ctx->stage == MESA_SHADER_FRAGMENT) {
       switch (slot) {
       HANDLE_EMIT_BUILTIN(POS, FragCoord);
       HANDLE_EMIT_BUILTIN(PNTC, PointCoord);
@@ -344,17 +457,48 @@ emit_input(struct ntv_context *ctx, struct nir_variable *var)
 
       default:
          slot = handle_slot(ctx, slot);
+         if (ctx->stage == MESA_SHADER_TESS_CTRL || ctx->stage == MESA_SHADER_TESS_EVAL)
+            printf("INPUT %s: %s:%u -> %u:\n", gl_shader_stage_name(ctx->stage), gl_varying_slot_name(var->data.location), var->data.location_frac, slot);
+         spirv_builder_emit_location(&ctx->builder, var_id, slot);
+      }
+      if (var->data.centroid)
+         spirv_builder_emit_decoration(&ctx->builder, var_id, SpvDecorationCentroid);
+      if (var->data.sample)
+         spirv_builder_emit_decoration(&ctx->builder, var_id, SpvDecorationSample);
+   } else if (ctx->stage < MESA_SHADER_FRAGMENT) {
+      switch (slot) {
+      HANDLE_EMIT_BUILTIN(POS, Position);
+      HANDLE_EMIT_BUILTIN(PSIZ, PointSize);
+      HANDLE_EMIT_BUILTIN(LAYER, Layer);
+      HANDLE_EMIT_BUILTIN(PRIMITIVE_ID, PrimitiveId);
+      HANDLE_EMIT_BUILTIN(CULL_DIST0, CullDistance);
+      HANDLE_EMIT_BUILTIN(VIEWPORT, ViewportIndex);
+      HANDLE_EMIT_BUILTIN(TESS_LEVEL_OUTER, TessLevelOuter);
+      HANDLE_EMIT_BUILTIN(TESS_LEVEL_INNER, TessLevelInner);
+
+      case VARYING_SLOT_CLIP_DIST0:
+         assert(glsl_type_is_array(var->type));
+         spirv_builder_emit_builtin(&ctx->builder, var_id, SpvBuiltInClipDistance);
+         break;
+
+      default:
+         if (var->data.patch)
+            slot -= VARYING_SLOT_PATCH0;
+         else if (ctx->stage == MESA_SHADER_TESS_EVAL)
+            slot -= VARYING_SLOT_VAR0;
+         else
+            slot = handle_slot(ctx, slot);
          spirv_builder_emit_location(&ctx->builder, var_id, slot);
       }
-   } else {
-      spirv_builder_emit_location(&ctx->builder, var_id,
-                                  var->data.driver_location);
    }
 
    if (var->data.location_frac)
       spirv_builder_emit_component(&ctx->builder, var_id,
                                    var->data.location_frac);
 
+   if (var->data.patch)
+      spirv_builder_emit_decoration(&ctx->builder, var_id, SpvDecorationPatch);
+
    if (var->data.interpolation == INTERP_MODE_FLAT)
       spirv_builder_emit_decoration(&ctx->builder, var_id, SpvDecorationFlat);
 
@@ -368,6 +512,10 @@ static void
 emit_output(struct ntv_context *ctx, struct nir_variable *var)
 {
    SpvId var_type = get_glsl_type(ctx, var->type);
+
+   /* SampleMask is always an array in spirv */
+   if (ctx->stage == MESA_SHADER_FRAGMENT && var->data.location == FRAG_RESULT_SAMPLE_MASK)
+      ctx->sample_mask_type = var_type = spirv_builder_type_array(&ctx->builder, var_type, emit_uint_const(ctx, 32, 1));
    SpvId pointer_type = spirv_builder_type_pointer(&ctx->builder,
                                                    SpvStorageClassOutput,
                                                    var_type);
@@ -376,9 +524,8 @@ emit_output(struct ntv_context *ctx, struct nir_variable *var)
    if (var->name)
       spirv_builder_emit_name(&ctx->builder, var_id, var->name);
 
-
-   if (ctx->stage == MESA_SHADER_VERTEX) {
-      unsigned slot = var->data.location;
+   unsigned slot = var->data.location;
+   if (ctx->stage != MESA_SHADER_FRAGMENT) {
       switch (slot) {
       HANDLE_EMIT_BUILTIN(POS, Position);
       HANDLE_EMIT_BUILTIN(PSIZ, PointSize);
@@ -399,13 +546,22 @@ emit_output(struct ntv_context *ctx, struct nir_variable *var)
          break;
 
       default:
-         slot = handle_slot(ctx, slot);
+         if (var->data.patch)
+            slot -= VARYING_SLOT_PATCH0;
+         else if (ctx->stage == MESA_SHADER_TESS_CTRL)
+            slot -= VARYING_SLOT_VAR0;
+         else
+            slot = handle_slot(ctx, slot);
          spirv_builder_emit_location(&ctx->builder, var_id, slot);
       }
-      ctx->outputs[var->data.location] = var_id;
-      ctx->so_output_gl_types[var->data.location] = var->type;
-      ctx->so_output_types[var->data.location] = var_type;
-   } else if (ctx->stage == MESA_SHADER_FRAGMENT) {
+      /* tcs can't do xfb */
+      if (ctx->stage != MESA_SHADER_TESS_CTRL) {
+         unsigned idx = var->data.location << 2 | var->data.location_frac;
+         ctx->outputs[idx] = var_id;
+         ctx->so_output_gl_types[idx] = var->type;
+         ctx->so_output_types[idx] = var_type;
+      }
+   } else {
       if (var->data.location >= FRAG_RESULT_DATA0) {
          spirv_builder_emit_location(&ctx->builder, var_id,
                                      var->data.location - FRAG_RESULT_DATA0);
@@ -419,12 +575,20 @@ emit_output(struct ntv_context *ctx, struct nir_variable *var)
             spirv_builder_emit_builtin(&ctx->builder, var_id, SpvBuiltInFragDepth);
             break;
 
+         case FRAG_RESULT_SAMPLE_MASK:
+            spirv_builder_emit_builtin(&ctx->builder, var_id, SpvBuiltInSampleMask);
+            break;
+
          default:
-            spirv_builder_emit_location(&ctx->builder, var_id,
-                                        var->data.driver_location);
+            slot = handle_slot(ctx, slot);
+            spirv_builder_emit_location(&ctx->builder, var_id, slot);
             spirv_builder_emit_index(&ctx->builder, var_id, var->data.index);
          }
       }
+      if (var->data.centroid)
+         spirv_builder_emit_decoration(&ctx->builder, var_id, SpvDecorationCentroid);
+      if (var->data.sample)
+         spirv_builder_emit_decoration(&ctx->builder, var_id, SpvDecorationSample);
    }
 
    if (var->data.location_frac)
@@ -448,6 +612,21 @@ emit_output(struct ntv_context *ctx, struct nir_variable *var)
       unreachable("unknown interpolation value");
    }
 
+   if (var->data.patch)
+      spirv_builder_emit_decoration(&ctx->builder, var_id, SpvDecorationPatch);
+
+   if (var->data.stream & ~NIR_STREAM_PACKED) {
+      unsigned stream = var->data.stream & ~NIR_STREAM_PACKED;
+      if (stream && stream < 4)
+         /* ordinary stream value */
+         spirv_builder_emit_stream(&ctx->builder, var_id, stream);
+      else if (stream) {
+         /* packed stream ughhhhhh */
+         stream = stream >> (var->data.location_frac / 2);
+         spirv_builder_emit_stream(&ctx->builder, var_id, stream);
+      }
+   }
+
    _mesa_hash_table_insert(ctx->vars, var, (void *)(intptr_t)var_id);
 
    assert(ctx->num_entry_ifaces < ARRAY_SIZE(ctx->entry_ifaces));
@@ -486,12 +665,13 @@ type_to_dim(enum glsl_sampler_dim gdim, bool *is_ms)
 uint32_t
 zink_binding(gl_shader_stage stage, VkDescriptorType type, int index)
 {
-   if (stage == MESA_SHADER_NONE ||
-       stage >= MESA_SHADER_COMPUTE) {
+   if (stage == MESA_SHADER_NONE) {
       unreachable("not supported");
    } else {
       uint32_t stage_offset = (uint32_t)stage * (PIPE_MAX_CONSTANT_BUFFERS +
-                                                 PIPE_MAX_SHADER_SAMPLER_VIEWS);
+                                                 PIPE_MAX_SHADER_SAMPLER_VIEWS +
+                                                 PIPE_MAX_SHADER_BUFFERS +
+                                                 PIPE_MAX_SHADER_IMAGES);
 
       switch (type) {
       case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER:
@@ -499,96 +679,213 @@ zink_binding(gl_shader_stage stage, VkDescriptorType type, int index)
          return stage_offset + index;
 
       case VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER:
+      case VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER:
          assert(index < PIPE_MAX_SHADER_SAMPLER_VIEWS);
          return stage_offset + PIPE_MAX_CONSTANT_BUFFERS + index;
 
+      case VK_DESCRIPTOR_TYPE_STORAGE_BUFFER:
+         assert(index < PIPE_MAX_SHADER_BUFFERS);
+         return stage_offset + PIPE_MAX_CONSTANT_BUFFERS + PIPE_MAX_SHADER_SAMPLER_VIEWS + index;
+
+      case VK_DESCRIPTOR_TYPE_STORAGE_IMAGE:
+      case VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER:
+         assert(index < PIPE_MAX_SHADER_IMAGES);
+         return stage_offset + PIPE_MAX_CONSTANT_BUFFERS + PIPE_MAX_SHADER_SAMPLER_VIEWS + PIPE_MAX_SHADER_IMAGES + index;
+
       default:
          unreachable("unexpected type");
       }
    }
 }
 
+static inline SpvImageFormat
+get_image_format(enum pipe_format format)
+{
+   switch (format) {
+   case PIPE_FORMAT_NONE:
+      return SpvImageFormatUnknown;
+   case PIPE_FORMAT_R32G32B32A32_FLOAT:
+      return SpvImageFormatRgba32f;
+   case PIPE_FORMAT_R16G16B16A16_FLOAT:
+      return SpvImageFormatRgba16f;
+   case PIPE_FORMAT_R32_FLOAT:
+      return SpvImageFormatR32f;
+   case PIPE_FORMAT_R8G8B8A8_UNORM:
+      return SpvImageFormatRgba8;
+   case PIPE_FORMAT_R8G8B8A8_SNORM:
+      return SpvImageFormatRgba8Snorm;
+   case PIPE_FORMAT_R32G32_FLOAT:
+      return SpvImageFormatRg32f;
+   case PIPE_FORMAT_R16G16_FLOAT:
+      return SpvImageFormatRg16f;
+   case PIPE_FORMAT_R11G11B10_FLOAT:
+      return SpvImageFormatR11fG11fB10f;
+   case PIPE_FORMAT_R16_FLOAT:
+      return SpvImageFormatR16f;
+   case PIPE_FORMAT_R16G16B16A16_UNORM:
+      return SpvImageFormatRgba16;
+   case PIPE_FORMAT_R10G10B10A2_UNORM:
+      return SpvImageFormatRgb10A2;
+   case PIPE_FORMAT_R16G16_UNORM:
+      return SpvImageFormatRg16;
+   case PIPE_FORMAT_R8G8_UNORM:
+      return SpvImageFormatRg8;
+   case PIPE_FORMAT_R16_UNORM:
+      return SpvImageFormatR16;
+   case PIPE_FORMAT_R8_UNORM:
+      return SpvImageFormatR8;
+   case PIPE_FORMAT_R16G16B16A16_SNORM:
+      return SpvImageFormatRgba16Snorm;
+   case PIPE_FORMAT_R16G16_SNORM:
+      return SpvImageFormatRg16Snorm;
+   case PIPE_FORMAT_R8G8_SNORM:
+      return SpvImageFormatRg8Snorm;
+   case PIPE_FORMAT_R16_SNORM:
+      return SpvImageFormatR16Snorm;
+   case PIPE_FORMAT_R8_SNORM:
+      return SpvImageFormatR8Snorm;
+   case PIPE_FORMAT_R32G32B32A32_SINT:
+      return SpvImageFormatRgba32i;
+   case PIPE_FORMAT_R16G16B16A16_SINT:
+      return SpvImageFormatRgba16i;
+   case PIPE_FORMAT_R8G8B8A8_SINT:
+      return SpvImageFormatRgba8i;
+   case PIPE_FORMAT_R32_SINT:
+      return SpvImageFormatR32i;
+   case PIPE_FORMAT_R32G32_SINT:
+      return SpvImageFormatRg32i;
+   case PIPE_FORMAT_R16G16_SINT:
+      return SpvImageFormatRg16i;
+   case PIPE_FORMAT_R8G8_SINT:
+      return SpvImageFormatRg8i;
+   case PIPE_FORMAT_R16_SINT:
+      return SpvImageFormatR16i;
+   case PIPE_FORMAT_R8_SINT:
+      return SpvImageFormatR8i;
+   case PIPE_FORMAT_R32G32B32A32_UINT:
+      return SpvImageFormatRgba32ui;
+   case PIPE_FORMAT_R16G16B16A16_UINT:
+      return SpvImageFormatRgba16ui;
+   case PIPE_FORMAT_R8G8B8A8_UINT:
+      return SpvImageFormatRgba8ui;
+   case PIPE_FORMAT_R32_UINT:
+      return SpvImageFormatR32ui;
+   case PIPE_FORMAT_R10G10B10A2_UINT:
+      return SpvImageFormatRgb10a2ui;
+   case PIPE_FORMAT_R32G32_UINT:
+      return SpvImageFormatRg32ui;
+   case PIPE_FORMAT_R16G16_UINT:
+      return SpvImageFormatRg16ui;
+   case PIPE_FORMAT_R8G8_UINT:
+      return SpvImageFormatRg8ui;
+   case PIPE_FORMAT_R16_UINT:
+      return SpvImageFormatR16ui;
+   case PIPE_FORMAT_R8_UINT:
+      return SpvImageFormatR8ui;
+   default:
+      break;
+   }
+   unreachable("unknown format");
+   return SpvImageFormatUnknown;
+}
+
 static void
-emit_sampler(struct ntv_context *ctx, struct nir_variable *var)
+emit_image(struct ntv_context *ctx, struct nir_variable *var)
 {
    const struct glsl_type *type = glsl_without_array(var->type);
 
    bool is_ms;
+   bool is_sampler = glsl_type_is_sampler(type);
    SpvDim dimension = type_to_dim(glsl_get_sampler_dim(type), &is_ms);
 
    SpvId result_type = get_glsl_basetype(ctx, glsl_get_sampler_result_type(type));
    SpvId image_type = spirv_builder_type_image(&ctx->builder, result_type,
                                                dimension, false,
                                                glsl_sampler_type_is_array(type),
-                                               is_ms, 1,
-                                               SpvImageFormatUnknown);
+                                               is_ms, 1 + !is_sampler,
+                                               get_image_format(var->data.image.format));
 
    SpvId sampled_type = spirv_builder_type_sampled_image(&ctx->builder,
                                                          image_type);
-   SpvId pointer_type = spirv_builder_type_pointer(&ctx->builder,
-                                                   SpvStorageClassUniformConstant,
-                                                   sampled_type);
+   SpvId var_type = is_sampler ? sampled_type : image_type;
+
+   int index = var->data.binding;
+   assert(!is_sampler || (!(ctx->samplers_used & (1 << index))));
+   assert(!is_sampler || !ctx->sampler_types[index]);
+   assert(is_sampler || !ctx->image_types[index]);
 
    if (glsl_type_is_array(var->type)) {
-      for (int i = 0; i < glsl_get_length(var->type); ++i) {
-         SpvId var_id = spirv_builder_emit_var(&ctx->builder, pointer_type,
-                                               SpvStorageClassUniformConstant);
-
-         if (var->name) {
-            char element_name[100];
-            snprintf(element_name, sizeof(element_name), "%s_%d", var->name, i);
-            spirv_builder_emit_name(&ctx->builder, var_id, var->name);
-         }
+      var_type = spirv_builder_type_array(&ctx->builder, var_type,
+                                              emit_uint_const(ctx, 32, glsl_get_aoa_size(var->type)));
+      spirv_builder_emit_array_stride(&ctx->builder, var_type, sizeof(void*));
+      ctx->sampler_array_sizes[index] = glsl_get_aoa_size(var->type);
+   }
+   SpvId pointer_type = spirv_builder_type_pointer(&ctx->builder,
+                                                   SpvStorageClassUniformConstant,
+                                                   var_type);
 
-         int index = var->data.binding + i;
-         assert(!(ctx->samplers_used & (1 << index)));
-         assert(!ctx->image_types[index]);
-         ctx->image_types[index] = image_type;
-         ctx->samplers[index] = var_id;
-         ctx->samplers_used |= 1 << index;
-
-         spirv_builder_emit_descriptor_set(&ctx->builder, var_id,
-                                           var->data.descriptor_set);
-         int binding = zink_binding(ctx->stage,
-                                    VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER,
-                                    var->data.binding + i);
-         spirv_builder_emit_binding(&ctx->builder, var_id, binding);
-      }
-   } else {
-      SpvId var_id = spirv_builder_emit_var(&ctx->builder, pointer_type,
-                                            SpvStorageClassUniformConstant);
+   SpvId var_id = spirv_builder_emit_var(&ctx->builder, pointer_type,
+                                         SpvStorageClassUniformConstant);
 
-      if (var->name)
-         spirv_builder_emit_name(&ctx->builder, var_id, var->name);
+   if (var->name)
+      spirv_builder_emit_name(&ctx->builder, var_id, var->name);
 
-      int index = var->data.binding;
-      assert(!(ctx->samplers_used & (1 << index)));
-      assert(!ctx->image_types[index]);
-      ctx->image_types[index] = image_type;
+   if (is_sampler) {
+      ctx->sampler_types[index] = image_type;
       ctx->samplers[index] = var_id;
       ctx->samplers_used |= 1 << index;
-
-      spirv_builder_emit_descriptor_set(&ctx->builder, var_id,
-                                        var->data.descriptor_set);
-      int binding = zink_binding(ctx->stage,
-                                 VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER,
-                                 var->data.binding);
-      spirv_builder_emit_binding(&ctx->builder, var_id, binding);
+   } else {
+      ctx->image_types[index] = image_type;
+      ctx->images[index] = var_id;
+      _mesa_hash_table_insert(ctx->vars, var, (void *)(intptr_t)var_id);
+      uint32_t *key = ralloc_size(ctx->mem_ctx, sizeof(uint32_t));
+      *key = var_id;
+      _mesa_hash_table_insert(ctx->image_vars, key, var);
+      emit_access_decorations(ctx, var, var_id);
    }
+
+   spirv_builder_emit_descriptor_set(&ctx->builder, var_id, 0);
+   int binding = zink_binding(ctx->stage,
+                              is_sampler ? zink_sampler_type(type) : zink_image_type(type),
+                              var->data.binding);
+   spirv_builder_emit_binding(&ctx->builder, var_id, binding);
 }
 
 static void
-emit_ubo(struct ntv_context *ctx, struct nir_variable *var)
+emit_bo(struct ntv_context *ctx, struct nir_variable *var)
 {
-   uint32_t size = glsl_count_attribute_slots(var->type, false);
+   bool is_ubo_array = glsl_type_is_array(var->type) && glsl_type_is_interface(glsl_without_array(var->type));
+   /* variables accessed inside a uniform block will get merged into a big
+    * memory blob and accessed by offset
+    */
+   if (var->data.location && !is_ubo_array && var->type != var->interface_type)
+      return;
+   bool ssbo = var->data.mode == nir_var_mem_ssbo;
+
+   SpvId array_type;
    SpvId vec4_type = get_uvec_type(ctx, 32, 4);
-   SpvId array_length = emit_uint_const(ctx, 32, size);
-   SpvId array_type = spirv_builder_type_array(&ctx->builder, vec4_type,
+   uint32_t array_size = glsl_count_attribute_slots(var->interface_type, false);
+   if (glsl_type_is_unsized_array(var->type))
+      array_type = spirv_builder_type_runtime_array(&ctx->builder, vec4_type);
+   else {
+      SpvId array_length = emit_uint_const(ctx, 32, array_size);
+      array_type = spirv_builder_type_array(&ctx->builder, vec4_type,
                                                array_length);
+   }
    spirv_builder_emit_array_stride(&ctx->builder, array_type, 16);
 
    // wrap UBO-array in a struct
-   SpvId struct_type = spirv_builder_type_struct(&ctx->builder, &array_type, 1);
+   SpvId runtime_array = 0;
+   if (ssbo) {
+      if (glsl_type_is_interface(var->interface_type) && !glsl_type_is_unsized_array(var->type)) {
+          if (glsl_type_is_unsized_array(glsl_get_struct_field(var->interface_type, glsl_get_length(var->interface_type) - 1))) {
+             runtime_array = spirv_builder_type_runtime_array(&ctx->builder, get_uvec_type(ctx, 32, 1));
+             spirv_builder_emit_array_stride(&ctx->builder, runtime_array, 4);
+          }
+      }
+   }
+   SpvId types[] = {array_type, runtime_array};
+   SpvId struct_type = spirv_builder_type_struct(&ctx->builder, types, 1 + !!runtime_array);
    if (var->name) {
       char struct_name[100];
       snprintf(struct_name, sizeof(struct_name), "struct_%s", var->name);
@@ -598,37 +895,88 @@ emit_ubo(struct ntv_context *ctx, struct nir_variable *var)
    spirv_builder_emit_decoration(&ctx->builder, struct_type,
                                  SpvDecorationBlock);
    spirv_builder_emit_member_offset(&ctx->builder, struct_type, 0, 0);
-
+   if (runtime_array)
+      spirv_builder_emit_member_offset(&ctx->builder, struct_type, 1, array_size * 16);
 
    SpvId pointer_type = spirv_builder_type_pointer(&ctx->builder,
-                                                   SpvStorageClassUniform,
+                                                   ssbo ? SpvStorageClassStorageBuffer : SpvStorageClassUniform,
                                                    struct_type);
 
-   SpvId var_id = spirv_builder_emit_var(&ctx->builder, pointer_type,
-                                         SpvStorageClassUniform);
-   if (var->name)
-      spirv_builder_emit_name(&ctx->builder, var_id, var->name);
+   /* if this is a ubo array, create a binding point for each array member:
+    * 
+      "For uniform blocks declared as arrays, each individual array element
+       corresponds to a separate buffer object backing one instance of the block."
+       - ARB_gpu_shader5
+
+      (also it's just easier)
+    */
+   unsigned size = is_ubo_array ? glsl_get_aoa_size(var->type) : 1;
+   int base = -1;
+   for (unsigned i = 0; i < size; i++) {
+      SpvId var_id = spirv_builder_emit_var(&ctx->builder, pointer_type,
+                                            ssbo ? SpvStorageClassStorageBuffer : SpvStorageClassUniform);
+      if (var->name) {
+         char struct_name[100];
+         snprintf(struct_name, sizeof(struct_name), "%s[%u]", var->name, i);
+         spirv_builder_emit_name(&ctx->builder, var_id, var->name);
+      }
 
-   assert(ctx->num_ubos < ARRAY_SIZE(ctx->ubos));
-   ctx->ubos[ctx->num_ubos++] = var_id;
+      if (ssbo) {
+         unsigned ssbo_idx;
+         if (!is_ubo_array && var->data.explicit_binding &&
+             (glsl_type_is_unsized_array(var->type) || glsl_get_length(var->interface_type) == 1)) {
+             /* - block ssbos get their binding broken in gl_nir_lower_buffers,
+              *   but also they're totally indistinguishable from lowered counter buffers which have valid bindings
+              *
+              * hopefully this is a counter or some other non-block variable, but if not then we're probably fucked
+              */
+             ssbo_idx = var->data.binding;
+         } else if (base >= 0)
+            /* we're indexing into a ssbo array and already have the base index */
+            ssbo_idx = base + i;
+         else {
+            if (ctx->ssbo_mask & 1) {
+               /* 0 index is used, iterate through the used blocks until we find the first unused one */
+               for (unsigned j = 1; j < ctx->num_ssbos; j++)
+                  if (!(ctx->ssbo_mask & (1 << j))) {
+                     /* we're iterating forward through the blocks, so the first available one should be
+                      * what we're looking for
+                      */
+                     base = ssbo_idx = j;
+                     break;
+                  }
+            } else
+               /* we're iterating forward through the ssbos, so always assign 0 first */
+               base = ssbo_idx = 0;
+            assert(ssbo_idx < ctx->num_ssbos);
+         }
+         assert(!ctx->ssbos[ssbo_idx]);
+         ctx->ssbos[ssbo_idx] = var_id;
+         ctx->ssbo_mask |= 1 << ssbo_idx;
+         ctx->ssbo_vars[ssbo_idx] = var;
+      } else {
+         assert(ctx->num_ubos < ARRAY_SIZE(ctx->ubos));
+         ctx->ubos[ctx->num_ubos++] = var_id;
+      }
 
-   spirv_builder_emit_descriptor_set(&ctx->builder, var_id,
-                                     var->data.descriptor_set);
-   int binding = zink_binding(ctx->stage,
-                              VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER,
-                              var->data.binding);
-   spirv_builder_emit_binding(&ctx->builder, var_id, binding);
+      spirv_builder_emit_descriptor_set(&ctx->builder, var_id, 0);
+      int binding = zink_binding(ctx->stage,
+                                 ssbo ? VK_DESCRIPTOR_TYPE_STORAGE_BUFFER : VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER,
+                                 var->data.binding + i);
+      spirv_builder_emit_binding(&ctx->builder, var_id, binding);
+   }
 }
 
 static void
 emit_uniform(struct ntv_context *ctx, struct nir_variable *var)
 {
-   if (var->data.mode == nir_var_mem_ubo)
-      emit_ubo(ctx, var);
+   if (var->data.mode == nir_var_mem_ubo || var->data.mode == nir_var_mem_ssbo)
+      emit_bo(ctx, var);
    else {
       assert(var->data.mode == nir_var_uniform);
-      if (glsl_type_is_sampler(glsl_without_array(var->type)))
-         emit_sampler(ctx, var);
+      const struct glsl_type *type = glsl_without_array(var->type);
+      if (glsl_type_is_sampler(type) || glsl_type_is_image(type))
+         emit_image(ctx, var);
    }
 }
 
@@ -637,7 +985,7 @@ get_vec_from_bit_size(struct ntv_context *ctx, uint32_t bit_size, uint32_t num_c
 {
    if (bit_size == 1)
       return get_bvec_type(ctx, num_components);
-   if (bit_size == 32)
+   if (bit_size == 32 || bit_size == 64)
       return get_uvec_type(ctx, bit_size, num_components);
    unreachable("unhandled register bit size");
    return 0;
@@ -709,7 +1057,7 @@ get_alu_src_raw(struct ntv_context *ctx, nir_alu_instr *alu, unsigned src)
       return def;
 
    int bit_size = nir_src_bit_size(alu->src[src].src);
-   assert(bit_size == 1 || bit_size == 32);
+   assert(bit_size == 1 || bit_size == 32 || bit_size == 64);
 
    SpvId raw_type = bit_size == 1 ? spirv_builder_type_bool(&ctx->builder) :
                                     spirv_builder_type_uint(&ctx->builder, bit_size);
@@ -860,7 +1208,10 @@ emit_unop(struct ntv_context *ctx, SpvOp op, SpvId type, SpvId src)
 static SpvId
 get_output_type(struct ntv_context *ctx, unsigned register_index, unsigned num_components)
 {
-   const struct glsl_type *out_type = ctx->so_output_gl_types[register_index];
+   const struct glsl_type *out_type = NULL;
+   /* index is based on component, so we might have to go back a few slots to get to the base */
+   while (!out_type)
+      out_type = ctx->so_output_gl_types[register_index--];
    enum glsl_base_type base_type = glsl_get_base_type(out_type);
    if (base_type == GLSL_TYPE_ARRAY)
       base_type = glsl_get_base_type(glsl_without_array(out_type));
@@ -888,12 +1239,11 @@ get_output_type(struct ntv_context *ctx, unsigned register_index, unsigned num_c
 /* for streamout create new outputs, as streamout can be done on individual components,
    from complete outputs, so we just can't use the created packed outputs */
 static void
-emit_so_info(struct ntv_context *ctx, unsigned max_output_location,
-             const struct zink_so_info *so_info)
+emit_so_info(struct ntv_context *ctx, const struct zink_so_info *so_info)
 {
    for (unsigned i = 0; i < so_info->so_info.num_outputs; i++) {
       struct pipe_stream_output so_output = so_info->so_info.output[i];
-      unsigned slot = so_info->so_info_slots[i];
+      unsigned slot = so_info->so_info_slots[i] << 2 | so_output.start_component;
       SpvId out_type = get_output_type(ctx, slot, so_output.num_components);
       SpvId pointer_type = spirv_builder_type_pointer(&ctx->builder,
                                                       SpvStorageClassOutput,
@@ -907,20 +1257,15 @@ emit_so_info(struct ntv_context *ctx, unsigned max_output_location,
       spirv_builder_emit_offset(&ctx->builder, var_id, (so_output.dst_offset * 4));
       spirv_builder_emit_xfb_buffer(&ctx->builder, var_id, so_output.output_buffer);
       spirv_builder_emit_xfb_stride(&ctx->builder, var_id, so_info->so_info.stride[so_output.output_buffer] * 4);
+      if (so_output.stream)
+         spirv_builder_emit_stream(&ctx->builder, var_id, so_output.stream);
 
       /* output location is incremented by VARYING_SLOT_VAR0 for non-builtins in vtn,
        * so we need to ensure that the new xfb location slot doesn't conflict with any previously-emitted
        * outputs.
-       *
-       * if there's no previous outputs that take up user slots (VAR0+) then we can start right after the
-       * glsl builtin reserved slots, otherwise we start just after the adjusted user output slot
        */
-      uint32_t location = NTV_MIN_RESERVED_SLOTS + i;
-      if (max_output_location >= VARYING_SLOT_VAR0)
-         location = max_output_location - VARYING_SLOT_VAR0 + 1 + i;
+      uint32_t location = reserve_slot(ctx);
       assert(location < VARYING_SLOT_VAR0);
-      assert(location <= VARYING_SLOT_VAR0 - 8 ||
-             !ctx->seen_texcoord[VARYING_SLOT_VAR0 - location - 1]);
       spirv_builder_emit_location(&ctx->builder, var_id, location);
 
       /* note: gl_ClipDistance[4] can the 0-indexed member of VARYING_SLOT_CLIP_DIST1 here,
@@ -942,31 +1287,33 @@ static void
 emit_so_outputs(struct ntv_context *ctx,
                 const struct zink_so_info *so_info)
 {
-   SpvId loaded_outputs[VARYING_SLOT_MAX] = {};
    for (unsigned i = 0; i < so_info->so_info.num_outputs; i++) {
       uint32_t components[NIR_MAX_VEC_COMPONENTS];
       unsigned slot = so_info->so_info_slots[i];
       struct pipe_stream_output so_output = so_info->so_info.output[i];
       uint32_t so_key = (uint32_t) so_output.register_index << 2 | so_output.start_component;
+      uint32_t location = (uint32_t) slot << 2 | so_output.start_component;
       struct hash_entry *he = _mesa_hash_table_search(ctx->so_outputs, &so_key);
       assert(he);
       SpvId so_output_var_id = (SpvId)(intptr_t)he->data;
 
-      SpvId type = get_output_type(ctx, slot, so_output.num_components);
-      SpvId output = ctx->outputs[slot];
-      SpvId output_type = ctx->so_output_types[slot];
-      const struct glsl_type *out_type = ctx->so_output_gl_types[slot];
+      SpvId type = get_output_type(ctx, location, so_output.num_components);
+      SpvId output = 0;
+      /* index is based on component, so we might have to go back a few slots to get to the base */
+      while (!output)
+         output = ctx->outputs[location--];
+      location++;
+      SpvId output_type = ctx->so_output_types[location];
+      const struct glsl_type *out_type = ctx->so_output_gl_types[location];
 
-      if (!loaded_outputs[slot])
-         loaded_outputs[slot] = spirv_builder_emit_load(&ctx->builder, output_type, output);
-      SpvId src = loaded_outputs[slot];
+      SpvId src = spirv_builder_emit_load(&ctx->builder, output_type, output);
 
       SpvId result;
 
       for (unsigned c = 0; c < so_output.num_components; c++) {
          components[c] = so_output.start_component + c;
          /* this is the second half of a 2 * vec4 array */
-         if (ctx->stage == MESA_SHADER_VERTEX && slot == VARYING_SLOT_CLIP_DIST1)
+         if (slot == VARYING_SLOT_CLIP_DIST1)
             components[c] += 4;
       }
 
@@ -993,7 +1340,7 @@ emit_so_outputs(struct ntv_context *ctx,
                 uint32_t member[] = { so_output.start_component + c };
                 SpvId base_type = get_glsl_type(ctx, glsl_without_array(out_type));
 
-                if (ctx->stage == MESA_SHADER_VERTEX && slot == VARYING_SLOT_CLIP_DIST1)
+                if (slot == VARYING_SLOT_CLIP_DIST1)
                    member[0] += 4;
                 components[c] = spirv_builder_emit_composite_extract(&ctx->builder, base_type, src, member, 1);
              }
@@ -1005,6 +1352,23 @@ emit_so_outputs(struct ntv_context *ctx,
    }
 }
 
+static SpvId
+emit_atomic(struct ntv_context *ctx, SpvId op, SpvId type, SpvId src0, SpvId src1, SpvId src2)
+{
+   if (op == SpvOpAtomicLoad)
+      return spirv_builder_emit_triop(&ctx->builder, op, type, src0, emit_uint_const(ctx, 32, SpvScopeWorkgroup),
+                                       emit_uint_const(ctx, 32, 0));
+   if (op == SpvOpAtomicCompareExchange)
+      return spirv_builder_emit_hexop(&ctx->builder, op, type, src0, emit_uint_const(ctx, 32, SpvScopeWorkgroup),
+                                       emit_uint_const(ctx, 32, 0),
+                                       emit_uint_const(ctx, 32, 0),
+                                       /* these params are intentionally swapped */
+                                       src2, src1);
+
+   return spirv_builder_emit_quadop(&ctx->builder, op, type, src0, emit_uint_const(ctx, 32, SpvScopeWorkgroup),
+                                    emit_uint_const(ctx, 32, 0), src1);
+}
+
 static SpvId
 emit_binop(struct ntv_context *ctx, SpvOp op, SpvId type,
            SpvId src0, SpvId src1)
@@ -1048,9 +1412,9 @@ emit_builtin_triop(struct ntv_context *ctx, enum GLSLstd450 op, SpvId type,
 
 static SpvId
 get_fvec_constant(struct ntv_context *ctx, unsigned bit_size,
-                  unsigned num_components, float value)
+                  unsigned num_components, double value)
 {
-   assert(bit_size == 32);
+   assert(bit_size == 32 || bit_size == 64);
 
    SpvId result = emit_float_const(ctx, bit_size, value);
    if (num_components == 1)
@@ -1068,9 +1432,9 @@ get_fvec_constant(struct ntv_context *ctx, unsigned bit_size,
 
 static SpvId
 get_uvec_constant(struct ntv_context *ctx, unsigned bit_size,
-                  unsigned num_components, uint32_t value)
+                  unsigned num_components, uint64_t value)
 {
-   assert(bit_size == 32);
+   assert(bit_size == 32 || bit_size == 64);
 
    SpvId result = emit_uint_const(ctx, bit_size, value);
    if (num_components == 1)
@@ -1088,9 +1452,9 @@ get_uvec_constant(struct ntv_context *ctx, unsigned bit_size,
 
 static SpvId
 get_ivec_constant(struct ntv_context *ctx, unsigned bit_size,
-                  unsigned num_components, int32_t value)
+                  unsigned num_components, int64_t value)
 {
-   assert(bit_size == 32);
+   assert(bit_size == 32 || bit_size == 64);
 
    SpvId result = emit_int_const(ctx, bit_size, value);
    if (num_components == 1)
@@ -1224,9 +1588,45 @@ emit_alu(struct ntv_context *ctx, nir_alu_instr *alu)
    UNOP(nir_op_f2u32, SpvOpConvertFToU)
    UNOP(nir_op_i2f32, SpvOpConvertSToF)
    UNOP(nir_op_u2f32, SpvOpConvertUToF)
+   UNOP(nir_op_f2f32, SpvOpFConvert)
+   UNOP(nir_op_u2f64, SpvOpConvertUToF)
+   UNOP(nir_op_i2f64, SpvOpConvertSToF)
+   UNOP(nir_op_f2f64, SpvOpFConvert)
    UNOP(nir_op_bitfield_reverse, SpvOpBitReverse)
+   UNOP(nir_op_bit_count, SpvOpBitCount)
 #undef UNOP
 
+   case nir_op_unpack_64_2x32_split_x:
+   case nir_op_unpack_64_2x32_split_y: {
+      assert(nir_op_infos[alu->op].num_inputs == 1);
+      assert(nir_src_bit_size(alu->src[0].src) == 64);
+      unsigned member = alu->op == nir_op_unpack_64_2x32_split_y;
+      src[0] = emit_bitcast(ctx, get_uvec_type(ctx, 32, num_components * 2), src[0]);
+      if (num_components > 1) {
+         uint32_t constituents[num_components];
+         for (unsigned i = 0; i < num_components; i++)
+            constituents[i] = i * 2 + member;
+         result = spirv_builder_emit_vector_shuffle(&ctx->builder, get_uvec_type(ctx, 32, num_components), src[0], src[0], constituents, num_components);
+      } else
+         result = spirv_builder_emit_vector_extract(&ctx->builder, get_uvec_type(ctx, 32, 1), src[0], member);
+      result = emit_bitcast(ctx, dest_type, result);
+      break;
+   }
+   case nir_op_pack_64_2x32_split:
+      assert(nir_src_bit_size(alu->src[0].src) == 32);
+      assert(nir_src_bit_size(alu->src[1].src) == 32);
+      assert(nir_op_infos[alu->op].num_inputs == 2);
+      result = spirv_builder_emit_composite_construct(&ctx->builder, get_uvec_type(ctx, 32, num_components * 2), src, 2);
+      result = emit_bitcast(ctx, dest_type, result);
+      break;
+
+   case nir_op_b2f64: {
+      result = emit_select(ctx, get_fvec_type(ctx, 32, num_components), src[0],
+                           get_fvec_constant(ctx, 32, num_components, 1),
+                           get_fvec_constant(ctx, 32, num_components, 0));
+      result = emit_unop(ctx, SpvOpFConvert, dest_type, result);
+      break;
+   }
    case nir_op_inot:
       if (bit_size == 1)
          result = emit_unop(ctx, SpvOpLogicalNot, dest_type, src[0]);
@@ -1269,6 +1669,9 @@ emit_alu(struct ntv_context *ctx, nir_alu_instr *alu)
    BUILTIN_UNOP(nir_op_isign, GLSLstd450SSign)
    BUILTIN_UNOP(nir_op_fsin, GLSLstd450Sin)
    BUILTIN_UNOP(nir_op_fcos, GLSLstd450Cos)
+   BUILTIN_UNOP(nir_op_ufind_msb, GLSLstd450FindUMsb)
+   BUILTIN_UNOP(nir_op_find_lsb, GLSLstd450FindILsb)
+   BUILTIN_UNOP(nir_op_ifind_msb, GLSLstd450FindSMsb)
 #undef BUILTIN_UNOP
 
    case nir_op_frcp:
@@ -1496,6 +1899,21 @@ emit_alu(struct ntv_context *ctx, nir_alu_instr *alu)
    }
    break;
 
+   case nir_op_ubitfield_extract:
+      assert(nir_op_infos[alu->op].num_inputs == 3);
+      result = emit_triop(ctx, SpvOpBitFieldUExtract, dest_type, src[0], src[1], src[2]);
+      break;
+
+   case nir_op_ibitfield_extract:
+      assert(nir_op_infos[alu->op].num_inputs == 3);
+      result = emit_triop(ctx, SpvOpBitFieldSExtract, dest_type, src[0], src[1], src[2]);
+      break;
+
+   case nir_op_bitfield_insert:
+      assert(nir_op_infos[alu->op].num_inputs == 4);
+      result = spirv_builder_emit_quadop(&ctx->builder, SpvOpBitFieldInsert, dest_type, src[0], src[1], src[2], src[3]);
+      break;
+
    default:
       fprintf(stderr, "emit_alu: not implemented (%s)\n",
               nir_op_infos[alu->op].name);
@@ -1503,6 +1921,8 @@ emit_alu(struct ntv_context *ctx, nir_alu_instr *alu)
       unreachable("unsupported opcode");
       return;
    }
+   if (alu->exact)
+      spirv_builder_emit_decoration(&ctx->builder, result, SpvDecorationNoContraction);
 
    store_alu_result(ctx, alu, result);
 }
@@ -1535,65 +1955,129 @@ emit_load_const(struct ntv_context *ctx, nir_load_const_instr *load_const)
       if (bit_size == 1)
          constant = spirv_builder_const_bool(&ctx->builder,
                                              load_const->value[0].b);
-      else
+      else if (bit_size == 32)
          constant = emit_uint_const(ctx, bit_size, load_const->value[0].u32);
+      else if (bit_size == 64)
+         constant = emit_uint_const(ctx, bit_size, load_const->value[0].u64);
+      else
+         unreachable("unhandled constant bit size!");
    }
 
    store_ssa_def(ctx, &load_const->def, constant);
 }
 
 static void
-emit_load_ubo(struct ntv_context *ctx, nir_intrinsic_instr *intr)
+emit_load_bo(struct ntv_context *ctx, nir_intrinsic_instr *intr)
 {
+   bool ssbo = intr->intrinsic == nir_intrinsic_load_ssbo;
    nir_const_value *const_block_index = nir_src_as_const_value(intr->src[0]);
    assert(const_block_index); // no dynamic indexing for now
-   assert(const_block_index->u32 == 0); // we only support the default UBO for now
 
-   nir_const_value *const_offset = nir_src_as_const_value(intr->src[1]);
-   if (const_offset) {
-      SpvId uvec4_type = get_uvec_type(ctx, 32, 4);
-      SpvId pointer_type = spirv_builder_type_pointer(&ctx->builder,
-                                                      SpvStorageClassUniform,
-                                                      uvec4_type);
+   SpvId bo = ssbo ? ctx->ssbos[const_block_index->u32] : ctx->ubos[const_block_index->u32];
 
-      unsigned idx = const_offset->u32;
-      SpvId member = emit_uint_const(ctx, 32, 0);
-      SpvId offset = emit_uint_const(ctx, 32, idx);
-      SpvId offsets[] = { member, offset };
+   unsigned bit_size = nir_dest_bit_size(intr->dest);
+   SpvId uint_type = get_uvec_type(ctx, 32, 1);
+   SpvId one = emit_uint_const(ctx, 32, 1);
+
+   /* number of components being loaded */
+   unsigned num_components = nir_dest_num_components(intr->dest);
+   /* we need to grab 2x32 to fill the 64bit value */
+   if (bit_size == 64)
+      num_components *= 2;
+   SpvId constituents[num_components];
+   SpvId result;
+
+   /* destination type for the load */
+   SpvId type = get_dest_uvec_type(ctx, &intr->dest);
+   /* an id of the array stride in bytes */
+   SpvId vec4_size = emit_uint_const(ctx, 32, sizeof(uint32_t) * 4);
+   /* an id of an array member in bytes */
+   SpvId uint_size = emit_uint_const(ctx, 32, sizeof(uint32_t));
+
+   /* we grab a single array member at a time, so it's a pointer to a uint */
+   SpvId pointer_type = spirv_builder_type_pointer(&ctx->builder,
+                                                   ssbo ? SpvStorageClassStorageBuffer : SpvStorageClassUniform,
+                                                   uint_type);
+
+   /* our generated uniform has a memory layout like
+    *
+    * struct {
+    *    vec4 base[array_size];
+    * };
+    *
+    * where 'array_size' is set as though every member of the ubo takes up a vec4,
+    * even if it's only a vec2 or a float.
+    *
+    * first, access 'base'
+    */
+   SpvId member = emit_uint_const(ctx, 32, 0);
+   /* this is the offset (in bytes) that we're accessing:
+    * it may be a const value or it may be dynamic in the shader
+    */
+   SpvId offset = get_src(ctx, &intr->src[1]);
+   /* convert offset to an array index for 'base' to determine which vec4 to access */
+   SpvId vec_offset = emit_binop(ctx, SpvOpUDiv, uint_type, offset, vec4_size);
+   /* use the remainder to calculate the byte offset in the vec, which tells us the member
+    * that we're going to access
+    */
+   SpvId vec_member_offset = emit_binop(ctx, SpvOpUDiv, uint_type,
+                                        emit_binop(ctx, SpvOpUMod, uint_type, offset, vec4_size),
+                                        uint_size);
+   /* OpAccessChain takes an array of indices that drill into a hierarchy based on the type:
+    * index 0 is accessing 'base'
+    * index 1 is accessing 'base[index 1]'
+    * index 2 is accessing 'base[index 1][index 2]'
+    *
+    * we must perform the access this way in case src[1] is dynamic because there's
+    * no other spirv method for using an id to access a member of a composite, as
+    * (composite|vector)_extract both take literals
+    */
+   for (unsigned i = 0; i < num_components; i++) {
+      SpvId indices[3] = { member, vec_offset, vec_member_offset };
       SpvId ptr = spirv_builder_emit_access_chain(&ctx->builder, pointer_type,
-                                                  ctx->ubos[0], offsets,
-                                                  ARRAY_SIZE(offsets));
-      SpvId result = spirv_builder_emit_load(&ctx->builder, uvec4_type, ptr);
-
-      SpvId type = get_dest_uvec_type(ctx, &intr->dest);
-      unsigned num_components = nir_dest_num_components(intr->dest);
-      if (num_components == 1) {
-         uint32_t components[] = { 0 };
-         result = spirv_builder_emit_composite_extract(&ctx->builder,
-                                                       type,
-                                                       result, components,
-                                                       1);
-      } else if (num_components < 4) {
-         SpvId constituents[num_components];
-         SpvId uint_type = spirv_builder_type_uint(&ctx->builder, 32);
-         for (uint32_t i = 0; i < num_components; ++i)
-            constituents[i] = spirv_builder_emit_composite_extract(&ctx->builder,
-                                                                   uint_type,
-                                                                   result, &i,
-                                                                   1);
-
-         result = spirv_builder_emit_composite_construct(&ctx->builder,
-                                                         type,
-                                                         constituents,
-                                                         num_components);
+                                                  bo, indices,
+                                                  ARRAY_SIZE(indices));
+      /* load a single value into the constituents array */
+      if (ssbo)
+         constituents[i] = emit_atomic(ctx, SpvOpAtomicLoad, uint_type, ptr, 0, 0);
+      else
+         constituents[i] = spirv_builder_emit_load(&ctx->builder, uint_type, ptr);
+      /* increment to the next vec4 member index for the next load */
+      vec_member_offset = emit_binop(ctx, SpvOpIAdd, uint_type, vec_member_offset, one);
+      if (i == 3 && num_components >= 4) {
+         vec_offset = emit_binop(ctx, SpvOpIAdd, uint_type, vec_offset, one);
+         vec_member_offset = emit_uint_const(ctx, 32, 0);
       }
+   }
 
-      if (nir_dest_bit_size(intr->dest) == 1)
-         result = uvec_to_bvec(ctx, result, num_components);
-
-      store_dest(ctx, &intr->dest, result, nir_type_uint);
+   /* if we're loading a 64bit value, we have to reassemble all the u32 values we've loaded into u64 values
+    * by creating uvec2 composites and bitcasting them to u64 values
+    */
+   if (bit_size == 64) {
+      num_components /= 2;
+      type = get_uvec_type(ctx, 64, num_components);
+      SpvId u64_type = get_uvec_type(ctx, 64, 1);
+      for (unsigned i = 0; i < num_components; i++) {
+         constituents[i] = spirv_builder_emit_composite_construct(&ctx->builder, get_uvec_type(ctx, 32, 2), constituents + i * 2, 2);
+         constituents[i] = emit_bitcast(ctx, u64_type, constituents[i]);
+      }
+   }
+   /* if loading more than 1 value, reassemble the results into the desired type,
+    * otherwise just use the loaded result
+    */
+   if (num_components > 1) {
+      result = spirv_builder_emit_composite_construct(&ctx->builder,
+                                                      type,
+                                                      constituents,
+                                                      num_components);
    } else
-      unreachable("uniform-addressing not yet supported");
+      result = constituents[0];
+
+   /* explicitly convert to a bool vector if the destination type is a bool */
+   if (nir_dest_bit_size(intr->dest) == 1)
+      result = uvec_to_bvec(ctx, result, num_components);
+
+   store_dest(ctx, &intr->dest, result, nir_type_uint);
 }
 
 static void
@@ -1626,11 +2110,182 @@ emit_store_deref(struct ntv_context *ctx, nir_intrinsic_instr *intr)
    SpvId ptr = get_src(ctx, &intr->src[0]);
    SpvId src = get_src(ctx, &intr->src[1]);
 
-   SpvId type = get_glsl_type(ctx, nir_src_as_deref(intr->src[0])->type);
-   SpvId result = emit_bitcast(ctx, type, src);
+   const struct glsl_type *gtype = nir_src_as_deref(intr->src[0])->type;
+   SpvId type = get_glsl_type(ctx, gtype);
+   nir_variable *var = nir_deref_instr_get_variable(nir_src_as_deref(intr->src[0]));
+   unsigned num_writes = util_bitcount(nir_intrinsic_write_mask(intr));
+   unsigned wrmask = nir_intrinsic_write_mask(intr);
+   if (num_writes != intr->num_components) {
+      /* no idea what we do if this fails */
+      assert(glsl_type_is_array(gtype) || glsl_type_is_vector(gtype));
+
+      /* this is a partial write, so we have to loop and do a per-component write */
+      SpvId result_type;
+      SpvId member_type;
+      if (glsl_type_is_vector(gtype)) {
+         result_type = get_glsl_basetype(ctx, glsl_get_base_type(gtype));
+         member_type = get_uvec_type(ctx, 32, 1);
+      } else
+         member_type = result_type = get_glsl_type(ctx, glsl_get_array_element(gtype));
+      SpvId ptr_type = spirv_builder_type_pointer(&ctx->builder,
+                                                  SpvStorageClassOutput,
+                                                  result_type);
+      for (unsigned i = 0; i < 4; i++)
+         if ((wrmask >> i) & 1) {
+            SpvId idx = emit_uint_const(ctx, 32, i);
+            SpvId val = spirv_builder_emit_composite_extract(&ctx->builder, member_type, src, &i, 1);
+            val = emit_bitcast(ctx, result_type, val);
+            SpvId member = spirv_builder_emit_access_chain(&ctx->builder, ptr_type,
+                                                           ptr, &idx, 1);
+            spirv_builder_emit_store(&ctx->builder, member, val);
+         }
+      return;
+
+   }
+   SpvId result;
+   if (ctx->stage == MESA_SHADER_FRAGMENT && var->data.location == FRAG_RESULT_SAMPLE_MASK) {
+      src = emit_bitcast(ctx, type, src);
+      /* SampleMask is always an array in spirv, so we need to construct it into one */
+      result = spirv_builder_emit_composite_construct(&ctx->builder, ctx->sample_mask_type, &src, 1);
+   } else
+      result = emit_bitcast(ctx, type, src);
    spirv_builder_emit_store(&ctx->builder, ptr, result);
 }
 
+static void
+emit_load_shared(struct ntv_context *ctx, nir_intrinsic_instr *intr)
+{
+   SpvId dest_type = get_dest_type(ctx, &intr->dest, nir_type_uint);
+   unsigned num_components = nir_dest_num_components(intr->dest);
+   unsigned bit_size = nir_dest_bit_size(intr->dest);
+   SpvId type = get_uvec_type(ctx, 32, num_components);
+   SpvId ptr_type = spirv_builder_type_pointer(&ctx->builder,
+                                               SpvStorageClassWorkgroup,
+                                               type);
+   SpvId offset = emit_binop(ctx, SpvOpUDiv, get_uvec_type(ctx, 32, 1), get_src(ctx, &intr->src[0]), emit_uint_const(ctx, 32, 4));
+
+   SpvId member = spirv_builder_emit_access_chain(&ctx->builder, ptr_type,
+                                                  ctx->shared_block_var, &offset, 1);
+   SpvId result = spirv_builder_emit_load(&ctx->builder, dest_type, member);
+   if (num_components > 1) {
+      SpvId constituents[num_components];
+      /* need to convert array -> vec */
+      for (unsigned i = 0; i < num_components; i++)
+         constituents[i] = spirv_builder_emit_composite_extract(&ctx->builder, get_uvec_type(ctx, bit_size, 1), result, &i, 1);
+      result = spirv_builder_emit_composite_construct(&ctx->builder, dest_type, constituents, num_components);
+   } else
+      result = bitcast_to_uvec(ctx, result, bit_size, num_components);
+   store_dest(ctx, &intr->dest, result, nir_type_uint);
+}
+
+static void
+emit_store_shared(struct ntv_context *ctx, nir_intrinsic_instr *intr)
+{
+   SpvId ptr = get_src(ctx, &intr->src[0]);
+   bool qword = nir_src_bit_size(intr->src[0]) == 64;
+
+   unsigned num_writes = util_bitcount(nir_intrinsic_write_mask(intr));
+   unsigned wrmask = nir_intrinsic_write_mask(intr);
+   /* this is a partial write, so we have to loop and do a per-component write */
+   SpvId type = get_uvec_type(ctx, 32, num_writes);
+   SpvId ptr_type = spirv_builder_type_pointer(&ctx->builder,
+                                               SpvStorageClassWorkgroup,
+                                               type);
+   SpvId uint_type = get_uvec_type(ctx, 32, 1);
+   SpvId offset = emit_binop(ctx, SpvOpUDiv, uint_type, get_src(ctx, &intr->src[1]), emit_uint_const(ctx, 32, 4));
+
+   for (unsigned i = 0; num_writes; i++) {
+      if ((wrmask >> i) & 1) {
+         for (unsigned j = 0; j < 1 + !!qword; j++) {
+            unsigned comp = ((1 + !!qword) * i) + j;
+            SpvId shared_offset = emit_binop(ctx, SpvOpIAdd, uint_type, offset, emit_uint_const(ctx, 32, comp));
+            SpvId val = ptr;
+            if (nir_src_num_components(intr->src[0]) != 1)
+               val = spirv_builder_emit_composite_extract(&ctx->builder, type, ptr, &comp, 1);
+            SpvId member = spirv_builder_emit_access_chain(&ctx->builder, ptr_type,
+                                                           ctx->shared_block_var, &shared_offset, 1);
+             spirv_builder_emit_store(&ctx->builder, member, val);
+         }
+         num_writes--;
+      }
+   }
+}
+
+/* FIXME: this is currently VERY specific to injected TCS usage */
+static void
+emit_load_push_const(struct ntv_context *ctx, nir_intrinsic_instr *intr)
+{
+   unsigned bit_size = nir_dest_bit_size(intr->dest);
+   SpvId uint_type = get_uvec_type(ctx, 32, 1);
+   SpvId load_type = get_uvec_type(ctx, 32, 1);
+
+   /* number of components being loaded */
+   unsigned num_components = nir_dest_num_components(intr->dest);
+   /* we need to grab 2x32 to fill the 64bit value */
+   if (bit_size == 64)
+      num_components *= 2;
+   SpvId constituents[num_components];
+   SpvId result;
+
+   /* destination type for the load */
+   SpvId type = get_dest_uvec_type(ctx, &intr->dest);
+   /* an id of an array member in bytes */
+   SpvId uint_size = emit_uint_const(ctx, 32, sizeof(uint32_t));
+   SpvId one = emit_uint_const(ctx, 32, 1);
+
+   /* we grab a single array member at a time, so it's a pointer to a uint */
+   SpvId pointer_type = spirv_builder_type_pointer(&ctx->builder,
+                                                   SpvStorageClassPushConstant,
+                                                   load_type);
+
+   SpvId member = emit_uint_const(ctx, 32, 0);
+   /* this is the offset (in bytes) that we're accessing:
+    * it may be a const value or it may be dynamic in the shader
+    */
+   SpvId offset = get_src(ctx, &intr->src[0]);
+   offset = emit_binop(ctx, SpvOpUDiv, uint_type, offset, uint_size);
+   /* OpAccessChain takes an array of indices that drill into a hierarchy based on the type:
+    * index 0 is accessing 'base'
+    * index 1 is accessing 'base[index 1]'
+    *
+    */
+   for (unsigned i = 0; i < num_components; i++) {
+      SpvId indices[2] = { member, offset };
+      SpvId ptr = spirv_builder_emit_access_chain(&ctx->builder, pointer_type,
+                                                  ctx->push_const_var, indices,
+                                                  ARRAY_SIZE(indices));
+      /* load a single value into the constituents array */
+      constituents[i] = spirv_builder_emit_load(&ctx->builder, load_type, ptr);
+      /* increment to the next vec4 member index for the next load */
+      offset = emit_binop(ctx, SpvOpIAdd, uint_type, offset, one);
+   }
+
+   /* if we're loading a 64bit value, we have to reassemble all the u32 values we've loaded into u64 values
+    * by creating uvec2 composites and bitcasting them to u64 values
+    */
+   if (bit_size == 64) {
+      num_components /= 2;
+      type = get_uvec_type(ctx, 64, num_components);
+      SpvId u64_type = get_uvec_type(ctx, 64, 1);
+      for (unsigned i = 0; i < num_components; i++) {
+         constituents[i] = spirv_builder_emit_composite_construct(&ctx->builder, get_uvec_type(ctx, 32, 2), constituents + i * 2, 2);
+         constituents[i] = emit_bitcast(ctx, u64_type, constituents[i]);
+      }
+   }
+   /* if loading more than 1 value, reassemble the results into the desired type,
+    * otherwise just use the loaded result
+    */
+   if (num_components > 1) {
+      result = spirv_builder_emit_composite_construct(&ctx->builder,
+                                                      type,
+                                                      constituents,
+                                                      num_components);
+   } else
+      result = constituents[0];
+
+   store_dest(ctx, &intr->dest, result, nir_type_uint);
+}
+
 static SpvId
 create_builtin_var(struct ntv_context *ctx, SpvId var_type,
                    SpvStorageClass storage_class,
@@ -1666,45 +2321,327 @@ emit_load_front_face(struct ntv_context *ctx, nir_intrinsic_instr *intr)
 }
 
 static void
-emit_load_instance_id(struct ntv_context *ctx, nir_intrinsic_instr *intr)
+emit_load_uint_input(struct ntv_context *ctx, nir_intrinsic_instr *intr, SpvId *var_id, const char *var_name, SpvBuiltIn builtin)
 {
    SpvId var_type = spirv_builder_type_uint(&ctx->builder, 32);
-   if (!ctx->instance_id_var)
-      ctx->instance_id_var = create_builtin_var(ctx, var_type,
-                                               SpvStorageClassInput,
-                                               "gl_InstanceId",
-                                               SpvBuiltInInstanceIndex);
+   if (builtin == SpvBuiltInSampleMask) {
+      /* gl_SampleMaskIn is an array[1] in spirv... */
+      var_type = spirv_builder_type_array(&ctx->builder, var_type, emit_uint_const(ctx, 32, 1));
+      spirv_builder_emit_array_stride(&ctx->builder, var_type, sizeof(uint32_t));
+   }
+   if (!*var_id) {
+      *var_id = create_builtin_var(ctx, var_type,
+                                   SpvStorageClassInput,
+                                   var_name,
+                                   builtin);
+      if (builtin == SpvBuiltInSampleMask) {
+         SpvId zero = emit_uint_const(ctx, 32, 0);
+         var_type = spirv_builder_type_uint(&ctx->builder, 32);
+         SpvId pointer_type = spirv_builder_type_pointer(&ctx->builder,
+                                                         SpvStorageClassInput,
+                                                         var_type);
+         *var_id = spirv_builder_emit_access_chain(&ctx->builder, pointer_type, *var_id, &zero, 1);
+      }
+   }
 
-   SpvId result = spirv_builder_emit_load(&ctx->builder, var_type,
-                                          ctx->instance_id_var);
+   SpvId result = spirv_builder_emit_load(&ctx->builder, var_type, *var_id);
    assert(1 == nir_dest_num_components(intr->dest));
+   if (builtin == SpvBuiltInInstanceIndex) {
+      /* GL's gl_InstanceID always begins at 0, so we have to normalize with gl_BaseInstance */
+      SpvId base = spirv_builder_emit_load(&ctx->builder, var_type, ctx->base_instance_var);
+      result = emit_binop(ctx, SpvOpISub, var_type, result, base);
+   }
    store_dest(ctx, &intr->dest, result, nir_type_uint);
 }
 
 static void
-emit_load_vertex_id(struct ntv_context *ctx, nir_intrinsic_instr *intr)
+emit_load_vec_input(struct ntv_context *ctx, nir_intrinsic_instr *intr, SpvId *var_id, const char *var_name, SpvBuiltIn builtin, nir_alu_type type)
+{
+   SpvId var_type;
+
+   if (type == nir_type_bool)
+      var_type = get_bvec_type(ctx, nir_dest_num_components(intr->dest));
+   else if (type == nir_type_int)
+      var_type = get_ivec_type(ctx, nir_dest_bit_size(intr->dest), nir_dest_num_components(intr->dest));
+   else if (type == nir_type_uint)
+      var_type = get_uvec_type(ctx, nir_dest_bit_size(intr->dest), nir_dest_num_components(intr->dest));
+   else if (type == nir_type_float)
+      var_type = get_fvec_type(ctx, nir_dest_bit_size(intr->dest), nir_dest_num_components(intr->dest));
+   else
+      unreachable("unknown type passed");
+   if (!*var_id)
+      *var_id = create_builtin_var(ctx, var_type,
+                                   SpvStorageClassInput,
+                                   var_name,
+                                   builtin);
+
+   SpvId result = spirv_builder_emit_load(&ctx->builder, var_type, *var_id);
+   store_dest(ctx, &intr->dest, result, type);
+}
+
+static void
+emit_interpolate(struct ntv_context *ctx, nir_intrinsic_instr *intr)
 {
-   SpvId var_type = spirv_builder_type_uint(&ctx->builder, 32);
-   if (!ctx->vertex_id_var)
-      ctx->vertex_id_var = create_builtin_var(ctx, var_type,
-                                               SpvStorageClassInput,
-                                               "gl_VertexID",
-                                               SpvBuiltInVertexIndex);
+   SpvId op;
+   switch (intr->intrinsic) {
+   case nir_intrinsic_interp_deref_at_centroid:
+      op = GLSLstd450InterpolateAtCentroid;
+      break;
+   case nir_intrinsic_interp_deref_at_sample:
+      op = GLSLstd450InterpolateAtSample;
+      break;
+   case nir_intrinsic_interp_deref_at_offset:
+      op = GLSLstd450InterpolateAtOffset;
+      break;
+   default:
+      unreachable("unknown interp op");
+   }
+   SpvId ptr = get_src(ctx, &intr->src[0]);
+   SpvId result;
+   if (intr->intrinsic == nir_intrinsic_interp_deref_at_centroid)
+      result = emit_builtin_unop(ctx, op, get_glsl_type(ctx, nir_src_as_deref(intr->src[0])->type), ptr);
+   else
+      result = emit_builtin_binop(ctx, op, get_glsl_type(ctx, nir_src_as_deref(intr->src[0])->type),
+                                  ptr, get_src(ctx, &intr->src[1]));
+   unsigned num_components = nir_dest_num_components(intr->dest);
+   unsigned bit_size = nir_dest_bit_size(intr->dest);
+   result = bitcast_to_uvec(ctx, result, bit_size, num_components);
+   store_dest(ctx, &intr->dest, result, nir_type_uint);
+}
 
-   SpvId result = spirv_builder_emit_load(&ctx->builder, var_type,
-                                          ctx->vertex_id_var);
-   assert(1 == nir_dest_num_components(intr->dest));
+static void
+handle_atomic_op(struct ntv_context *ctx, nir_intrinsic_instr *intr, SpvId ptr, SpvId param, SpvId param2)
+{
+   SpvId dest_type = get_dest_type(ctx, &intr->dest, nir_type_uint32);
+   SpvId result = emit_atomic(ctx, get_atomic_op(intr->intrinsic), dest_type, ptr, param, param2);
+   assert(result);
    store_dest(ctx, &intr->dest, result, nir_type_uint);
 }
 
+static void
+emit_ssbo_atomic_intrinsic(struct ntv_context *ctx, nir_intrinsic_instr *intr)
+{
+   SpvId ssbo;
+   SpvId param;
+   SpvId dest_type = get_dest_type(ctx, &intr->dest, nir_type_uint32);
+
+   nir_const_value *const_block_index = nir_src_as_const_value(intr->src[0]);
+   assert(const_block_index); // no dynamic indexing for now
+   ssbo = ctx->ssbos[const_block_index->u32];
+   param = get_src(ctx, &intr->src[2]);
+
+   SpvId pointer_type = spirv_builder_type_pointer(&ctx->builder,
+                                                   SpvStorageClassStorageBuffer,
+                                                   dest_type);
+   SpvId uint_type = get_uvec_type(ctx, 32, 1);
+   /* an id of the array stride in bytes */
+   SpvId vec4_size = emit_uint_const(ctx, 32, sizeof(uint32_t) * 4);
+   /* an id of an array member in bytes */
+   SpvId uint_size = emit_uint_const(ctx, 32, sizeof(uint32_t));
+   SpvId member = emit_uint_const(ctx, 32, 0);
+   SpvId offset = get_src(ctx, &intr->src[1]);
+   SpvId vec_offset = emit_binop(ctx, SpvOpUDiv, uint_type, offset, vec4_size);
+   SpvId vec_member_offset = emit_binop(ctx, SpvOpUDiv, uint_type,
+                                        emit_binop(ctx, SpvOpUMod, uint_type, offset, vec4_size),
+                                        uint_size);
+   SpvId indices[3] = { member, vec_offset, vec_member_offset };
+   SpvId ptr = spirv_builder_emit_access_chain(&ctx->builder, pointer_type,
+                                               ssbo, indices,
+                                               ARRAY_SIZE(indices));
+
+   SpvId param2 = 0;
+
+   if (intr->intrinsic == nir_intrinsic_ssbo_atomic_comp_swap)
+      param2 = get_src(ctx, &intr->src[3]);
+
+   handle_atomic_op(ctx, intr, ptr, param, param2);
+}
+
+static void
+emit_shared_atomic_intrinsic(struct ntv_context *ctx, nir_intrinsic_instr *intr)
+{
+   SpvId dest_type = get_dest_type(ctx, &intr->dest, nir_type_uint32);
+   SpvId param = get_src(ctx, &intr->src[1]);
+
+   SpvId pointer_type = spirv_builder_type_pointer(&ctx->builder,
+                                                   SpvStorageClassWorkgroup,
+                                                   dest_type);
+   SpvId offset = emit_binop(ctx, SpvOpUDiv, get_uvec_type(ctx, 32, 1), get_src(ctx, &intr->src[0]), emit_uint_const(ctx, 32, 4));
+   SpvId ptr = spirv_builder_emit_access_chain(&ctx->builder, pointer_type,
+                                               ctx->shared_block_var, &offset, 1);
+
+   SpvId param2 = 0;
+
+   if (intr->intrinsic == nir_intrinsic_shared_atomic_comp_swap)
+      param2 = get_src(ctx, &intr->src[2]);
+
+   handle_atomic_op(ctx, intr, ptr, param, param2);
+}
+
+static inline nir_variable *
+get_var_from_image(struct ntv_context *ctx, SpvId var_id)
+{
+   struct hash_entry *he = _mesa_hash_table_search(ctx->image_vars, &var_id);
+   assert(he);
+   return he->data;
+}
+
+static SpvId
+get_coords(struct ntv_context *ctx, const struct glsl_type *type, nir_src *src)
+{
+   uint32_t num_coords = glsl_get_sampler_coordinate_components(type);
+   uint32_t src_components = nir_src_num_components(*src);
+
+   SpvId spv = get_src(ctx, src);
+   if (num_coords == src_components)
+      return spv;
+
+   /* need to extract the coord dimensions that the image can use */
+   SpvId vec_type = get_uvec_type(ctx, 32, num_coords);
+   if (num_coords == 1)
+      return spirv_builder_emit_vector_extract(&ctx->builder, vec_type, spv, 0);
+   uint32_t constituents[4];
+   for (unsigned i = 0; i < num_coords; i++)
+      constituents[i] = i;
+   return spirv_builder_emit_vector_shuffle(&ctx->builder, vec_type, spv, spv, constituents, num_coords);
+}
+
+static void
+emit_image_intrinsic(struct ntv_context *ctx, nir_intrinsic_instr *intr)
+{
+   SpvId img_var = get_src(ctx, &intr->src[0]);
+   SpvId sample = get_src(ctx, &intr->src[2]);
+   SpvId param = get_src(ctx, &intr->src[3]);
+   nir_variable *var = get_var_from_image(ctx, img_var);
+   const struct glsl_type *type = glsl_without_array(var->type);
+   SpvId coord = get_coords(ctx, type, &intr->src[1]);
+   SpvId base_type = get_glsl_basetype(ctx, glsl_get_sampler_result_type(type));
+   SpvId texel = spirv_builder_emit_image_texel_pointer(&ctx->builder, base_type, img_var, coord, sample);
+   SpvId param2 = 0;
+
+   if (intr->intrinsic == nir_intrinsic_image_deref_atomic_comp_swap)
+      param2 = get_src(ctx, &intr->src[4]);
+   handle_atomic_op(ctx, intr, texel, param, param2);
+}
+
+static void
+emit_vote(struct ntv_context *ctx, nir_intrinsic_instr *intr)
+{
+   SpvOp op;
+
+   switch (intr->intrinsic) {
+   case nir_intrinsic_vote_all:
+      op = SpvOpGroupNonUniformAll;
+      break;
+   case nir_intrinsic_vote_any:
+      op = SpvOpGroupNonUniformAny;
+      break;
+   case nir_intrinsic_vote_ieq:
+   case nir_intrinsic_vote_feq:
+      op = SpvOpGroupNonUniformAllEqual;
+      break;
+   default:
+      unreachable("unknown vote intrinsic");
+   }
+   SpvId result = spirv_builder_emit_vote(&ctx->builder, op, get_src(ctx, &intr->src[0]));
+   store_dest_raw(ctx, &intr->dest, result);
+}
+
 static void
 emit_intrinsic(struct ntv_context *ctx, nir_intrinsic_instr *intr)
 {
    switch (intr->intrinsic) {
    case nir_intrinsic_load_ubo:
-      emit_load_ubo(ctx, intr);
+   case nir_intrinsic_load_ssbo:
+      emit_load_bo(ctx, intr);
       break;
 
+   /* TODO: would be great to refactor this in with emit_load_bo() */
+   case nir_intrinsic_store_ssbo: {
+      nir_const_value *const_block_index = nir_src_as_const_value(intr->src[1]);
+      assert(const_block_index);
+
+      SpvId bo = ctx->ssbos[const_block_index->u32];
+
+      unsigned bit_size = nir_src_bit_size(intr->src[0]);
+      SpvId uint_type = get_uvec_type(ctx, 32, 1);
+      SpvId one = emit_uint_const(ctx, 32, 1);
+
+      /* number of components being stored */
+      unsigned wrmask = nir_intrinsic_write_mask(intr);
+      unsigned num_writes = util_bitcount(wrmask);
+      unsigned num_components = num_writes;
+
+      /* we need to grab 2x32 to fill the 64bit value */
+      if (bit_size == 64)
+         num_components *= 2;
+
+      /* an id of the array stride in bytes */
+      SpvId vec4_size = emit_uint_const(ctx, 32, sizeof(uint32_t) * 4);
+      /* an id of an array member in bytes */
+      SpvId uint_size = emit_uint_const(ctx, 32, sizeof(uint32_t));
+      /* we grab a single array member at a time, so it's a pointer to a uint */
+      SpvId pointer_type = spirv_builder_type_pointer(&ctx->builder,
+                                                      SpvStorageClassStorageBuffer,
+                                                      uint_type);
+
+      /* our generated uniform has a memory layout like
+       *
+       * struct {
+       *    vec4 base[array_size];
+       * };
+       *
+       * where 'array_size' is set as though every member of the ubo takes up a vec4,
+       * even if it's only a vec2 or a float.
+       *
+       * first, access 'base'
+       */
+      SpvId member = emit_uint_const(ctx, 32, 0);
+      /* this is the offset (in bytes) that we're accessing:
+       * it may be a const value or it may be dynamic in the shader
+       */
+      SpvId offset = get_src(ctx, &intr->src[2]);
+      /* convert offset to an array index for 'base' to determine which vec4 to access */
+      SpvId vec_offset = emit_binop(ctx, SpvOpUDiv, uint_type, offset, vec4_size);
+      /* use the remainder to calculate the byte offset in the vec, which tells us the member
+       * that we're going to access
+       */
+      SpvId vec_member_offset = emit_binop(ctx, SpvOpUDiv, uint_type,
+                                           emit_binop(ctx, SpvOpUMod, uint_type, offset, vec4_size),
+                                           uint_size);
+      SpvId value = get_src(ctx, &intr->src[0]);
+      /* OpAccessChain takes an array of indices that drill into a hierarchy based on the type:
+       * index 0 is accessing 'base'
+       * index 1 is accessing 'base[index 1]'
+       * index 2 is accessing 'base[index 1][index 2]'
+       *
+       * we must perform the access this way in case src[1] is dynamic because there's
+       * no other spirv method for using an id to access a member of a composite, as
+       * (composite|vector)_extract both take literals
+       */
+      unsigned write_count = 0;
+      for (unsigned i = 0; write_count < num_components; i++) {
+         if (wrmask & (1 << (bit_size == 64 ? i / 2 : i))) {
+            SpvId indices[3] = { member, vec_offset, vec_member_offset };
+            SpvId ptr = spirv_builder_emit_access_chain(&ctx->builder, pointer_type,
+                                                        bo, indices,
+                                                        ARRAY_SIZE(indices));
+            SpvId component = nir_src_num_components(intr->src[0]) > 1 ?
+                             spirv_builder_emit_composite_extract(&ctx->builder, uint_type, value, &i, 1) :
+                             value;
+            spirv_builder_emit_atomic_store(&ctx->builder, ptr, SpvScopeWorkgroup, 0, component);
+            write_count++;
+         }
+         /* increment to the next vec4 member index for the next load */
+         vec_member_offset = emit_binop(ctx, SpvOpIAdd, uint_type, vec_member_offset, one);
+         if (i == 3 && num_components >= 4) {
+            vec_offset = emit_binop(ctx, SpvOpIAdd, uint_type, vec_offset, one);
+            vec_member_offset = emit_uint_const(ctx, 32, 0);
+         }
+      }
+      break;
+   }
+
    case nir_intrinsic_discard:
       emit_discard(ctx, intr);
       break;
@@ -1717,16 +2654,247 @@ emit_intrinsic(struct ntv_context *ctx, nir_intrinsic_instr *intr)
       emit_store_deref(ctx, intr);
       break;
 
+   case nir_intrinsic_load_push_constant:
+      emit_load_push_const(ctx, intr);
+      break;
+
    case nir_intrinsic_load_front_face:
       emit_load_front_face(ctx, intr);
       break;
 
+   case nir_intrinsic_load_base_instance:
+      emit_load_uint_input(ctx, intr, &ctx->base_instance_var, "gl_BaseInstance", SpvBuiltInBaseInstance);
+      break;
+
    case nir_intrinsic_load_instance_id:
-      emit_load_instance_id(ctx, intr);
+      if (!ctx->base_instance_var)
+         emit_load_uint_input(ctx, intr, &ctx->base_instance_var, "gl_BaseInstance", SpvBuiltInBaseInstance);
+      emit_load_uint_input(ctx, intr, &ctx->instance_id_var, "gl_InstanceId", SpvBuiltInInstanceIndex);
+      break;
+
+   case nir_intrinsic_load_base_vertex:
+      emit_load_uint_input(ctx, intr, &ctx->base_vertex_var, "gl_BaseVertex", SpvBuiltInBaseVertex);
+      break;
+
+   case nir_intrinsic_load_draw_id:
+      emit_load_uint_input(ctx, intr, &ctx->draw_id_var, "gl_DrawID", SpvBuiltInDrawIndex);
       break;
 
    case nir_intrinsic_load_vertex_id:
-      emit_load_vertex_id(ctx, intr);
+      emit_load_uint_input(ctx, intr, &ctx->vertex_id_var, "gl_VertexId", SpvBuiltInVertexIndex);
+      break;
+
+   case nir_intrinsic_load_primitive_id:
+      emit_load_uint_input(ctx, intr, &ctx->primitive_id_var, "gl_PrimitiveIdIn", SpvBuiltInPrimitiveId);
+      break;
+
+   case nir_intrinsic_load_invocation_id:
+      emit_load_uint_input(ctx, intr, &ctx->invocation_id_var, "gl_InvocationId", SpvBuiltInInvocationId);
+      break;
+
+   case nir_intrinsic_load_sample_id:
+      emit_load_uint_input(ctx, intr, &ctx->sample_id_var, "gl_SampleId", SpvBuiltInSampleId);
+      break;
+
+   case nir_intrinsic_load_sample_pos:
+      emit_load_vec_input(ctx, intr, &ctx->sample_pos_var, "gl_SamplePosition", SpvBuiltInSamplePosition, nir_type_float);
+      break;
+
+   case nir_intrinsic_load_sample_mask_in:
+      emit_load_uint_input(ctx, intr, &ctx->sample_mask_in_var, "gl_SampleMaskIn", SpvBuiltInSampleMask);
+      break;
+
+   case nir_intrinsic_emit_vertex_with_counter:
+      /* geometry shader emits copied xfb outputs just prior to EmitVertex(),
+       * since that's the end of the shader
+       */
+      if (ctx->so_info)
+         emit_so_outputs(ctx, ctx->so_info);
+      spirv_builder_emit_vertex(&ctx->builder, nir_intrinsic_stream_id(intr));
+      break;
+
+   case nir_intrinsic_end_primitive_with_counter:
+      spirv_builder_end_primitive(&ctx->builder, nir_intrinsic_stream_id(intr));
+      break;
+
+   case nir_intrinsic_load_helper_invocation:
+      emit_load_vec_input(ctx, intr, &ctx->helper_invocation_var, "gl_HelperInvocation", SpvBuiltInHelperInvocation, nir_type_bool);
+      break;
+
+   case nir_intrinsic_load_patch_vertices_in:
+      emit_load_vec_input(ctx, intr, &ctx->tess_patch_vertices_in, "gl_PatchVerticesIn", SpvBuiltInPatchVertices, nir_type_int);
+      break;
+
+   case nir_intrinsic_load_tess_coord:
+      emit_load_vec_input(ctx, intr, &ctx->tess_coord_var, "gl_TessCoord", SpvBuiltInTessCoord, nir_type_float);
+      break;
+
+   case nir_intrinsic_memory_barrier_tcs_patch:
+      spirv_builder_emit_memory_barrier(&ctx->builder, SpvScopeWorkgroup, SpvMemorySemanticsOutputMemoryMask | SpvMemorySemanticsReleaseMask);
+      break;
+
+   case nir_intrinsic_memory_barrier:
+      spirv_builder_emit_memory_barrier(&ctx->builder, SpvScopeWorkgroup,
+                                        SpvMemorySemanticsImageMemoryMask | SpvMemorySemanticsUniformMemoryMask |
+                                        SpvMemorySemanticsMakeVisibleMask  | SpvMemorySemanticsAcquireReleaseMask);
+      break;
+
+   case nir_intrinsic_memory_barrier_shared:
+      spirv_builder_emit_memory_barrier(&ctx->builder, SpvScopeWorkgroup,
+                                        SpvMemorySemanticsWorkgroupMemoryMask |
+                                        SpvMemorySemanticsAcquireReleaseMask);
+      break;
+
+   case nir_intrinsic_control_barrier:
+      spirv_builder_emit_control_barrier(&ctx->builder, SpvScopeWorkgroup, SpvScopeWorkgroup, SpvMemorySemanticsWorkgroupMemoryMask | SpvMemorySemanticsAcquireMask);
+      break;
+
+   case nir_intrinsic_interp_deref_at_centroid:
+   case nir_intrinsic_interp_deref_at_sample:
+   case nir_intrinsic_interp_deref_at_offset:
+      emit_interpolate(ctx, intr);
+      break;
+
+   case nir_intrinsic_ssbo_atomic_add:
+   case nir_intrinsic_ssbo_atomic_umin:
+   case nir_intrinsic_ssbo_atomic_imin:
+   case nir_intrinsic_ssbo_atomic_umax:
+   case nir_intrinsic_ssbo_atomic_imax:
+   case nir_intrinsic_ssbo_atomic_and:
+   case nir_intrinsic_ssbo_atomic_or:
+   case nir_intrinsic_ssbo_atomic_xor:
+   case nir_intrinsic_ssbo_atomic_exchange:
+   case nir_intrinsic_ssbo_atomic_comp_swap:
+      emit_ssbo_atomic_intrinsic(ctx, intr);
+      break;
+
+   case nir_intrinsic_shared_atomic_add:
+   case nir_intrinsic_shared_atomic_umin:
+   case nir_intrinsic_shared_atomic_imin:
+   case nir_intrinsic_shared_atomic_umax:
+   case nir_intrinsic_shared_atomic_imax:
+   case nir_intrinsic_shared_atomic_and:
+   case nir_intrinsic_shared_atomic_or:
+   case nir_intrinsic_shared_atomic_xor:
+   case nir_intrinsic_shared_atomic_exchange:
+   case nir_intrinsic_shared_atomic_comp_swap:
+      emit_shared_atomic_intrinsic(ctx, intr);
+      break;
+
+   case nir_intrinsic_get_buffer_size: {
+      SpvId uint_type = get_uvec_type(ctx, 32, 1);
+      nir_variable *var = ctx->ssbo_vars[nir_src_as_const_value(intr->src[0])->u32];
+      SpvId result = spirv_builder_emit_binop(&ctx->builder, SpvOpArrayLength, uint_type, ctx->ssbos[nir_src_as_const_value(intr->src[0])->u32], 1);
+      /* this is already converted by nir to:
+
+         length = (real_size - offset) / stride
+
+        * so we need to un-convert it
+        */
+      uint32_t array_size = glsl_count_attribute_slots(var->interface_type, false);
+      result = emit_binop(ctx, SpvOpIMul, uint_type, result, emit_uint_const(ctx, 32, 4));
+      unsigned member_size = glsl_get_ifc_packing(var->interface_type) == GLSL_INTERFACE_PACKING_STD430 ? 4 : 16;
+      result = emit_binop(ctx, SpvOpIAdd, uint_type, result, emit_uint_const(ctx, 32, array_size * member_size));
+      if (glsl_get_ifc_packing(var->interface_type) == GLSL_INTERFACE_PACKING_STD430) {
+         /* we always create the ssbos as arrays of vec4s even though std430 packing means they shouldn't be,
+          * so we have to add on the adjustment here for the buffer to provide the "correct" offset value of the
+          * unsized array member, as calculated by:
+
+            real_offset = offset - ((num_vec4_sots * sizeof(vec4)) - (num_dword_slots * sizeof(dword)) + sizeof(dword))
+          */
+         unsigned adjust = member_size * ((array_size * 4) - glsl_count_dword_slots(var->interface_type, false) + 1);
+         result = emit_binop(ctx, SpvOpIAdd, uint_type, result, emit_uint_const(ctx, 32, adjust));
+      }
+      store_dest(ctx, &intr->dest, result, nir_type_uint);
+      break;
+   }
+
+   case nir_intrinsic_image_deref_store: {
+      SpvId img_var = get_src(ctx, &intr->src[0]);
+      nir_variable *var = get_var_from_image(ctx, img_var);
+      SpvId img_type = ctx->image_types[var->data.binding];
+      const struct glsl_type *type = glsl_without_array(var->type);
+      SpvId base_type = get_glsl_basetype(ctx, glsl_get_sampler_result_type(type));
+      SpvId img = spirv_builder_emit_load(&ctx->builder, img_type, img_var);
+      SpvId coord = get_coords(ctx, type, &intr->src[1]);
+      SpvId texel = get_src(ctx, &intr->src[3]);
+      /* texel type must match image type */
+      texel = emit_bitcast(ctx,
+                           spirv_builder_type_vector(&ctx->builder, base_type, 4),
+                           texel);
+      spirv_builder_emit_image_write(&ctx->builder, img, coord, texel, 0, 0, 0);
+      break;
+   }
+   case nir_intrinsic_image_deref_load: {
+      SpvId img_var = get_src(ctx, &intr->src[0]);
+      nir_variable *var = get_var_from_image(ctx, img_var);
+      SpvId img_type = ctx->image_types[var->data.binding];
+      const struct glsl_type *type = glsl_without_array(var->type);
+      SpvId base_type = get_glsl_basetype(ctx, glsl_get_sampler_result_type(type));
+      SpvId img = spirv_builder_emit_load(&ctx->builder, img_type, img_var);
+      SpvId coord = get_coords(ctx, type, &intr->src[1]);
+      SpvId result = spirv_builder_emit_image_read(&ctx->builder,
+                                    spirv_builder_type_vector(&ctx->builder, base_type, nir_dest_num_components(intr->dest)),
+                                    img, coord, 0, 0, 0);
+      store_dest(ctx, &intr->dest, result, nir_type_float);
+      break;
+   }
+   case nir_intrinsic_image_deref_size: {
+      SpvId img_var = get_src(ctx, &intr->src[0]);
+      nir_variable *var = get_var_from_image(ctx, img_var);
+      SpvId img_type = ctx->image_types[var->data.binding];
+      const struct glsl_type *type = glsl_without_array(var->type);
+      SpvId img = spirv_builder_emit_load(&ctx->builder, img_type, img_var);
+      SpvId result = spirv_builder_emit_image_query_size(&ctx->builder, get_uvec_type(ctx, 32, glsl_get_sampler_coordinate_components(type)), img, 0);
+      store_dest(ctx, &intr->dest, result, nir_type_uint);
+      break;
+   }
+   case nir_intrinsic_image_deref_atomic_add:
+   case nir_intrinsic_image_deref_atomic_umin:
+   case nir_intrinsic_image_deref_atomic_imin:
+   case nir_intrinsic_image_deref_atomic_umax:
+   case nir_intrinsic_image_deref_atomic_imax:
+   case nir_intrinsic_image_deref_atomic_and:
+   case nir_intrinsic_image_deref_atomic_or:
+   case nir_intrinsic_image_deref_atomic_xor:
+   case nir_intrinsic_image_deref_atomic_exchange:
+   case nir_intrinsic_image_deref_atomic_comp_swap:
+      emit_image_intrinsic(ctx, intr);
+      break;
+
+   case nir_intrinsic_load_work_group_id:
+      emit_load_vec_input(ctx, intr, &ctx->workgroup_id_var, "gl_WorkGroupID", SpvBuiltInWorkgroupId, nir_type_uint);
+      break;
+
+   case nir_intrinsic_load_num_work_groups:
+      emit_load_vec_input(ctx, intr, &ctx->num_workgroups_var, "gl_NumWorkGroups", SpvBuiltInNumWorkgroups, nir_type_uint);
+      break;
+
+   case nir_intrinsic_load_local_invocation_id:
+      emit_load_vec_input(ctx, intr, &ctx->local_invocation_id_var, "gl_LocalInvocationID", SpvBuiltInLocalInvocationId, nir_type_uint);
+      break;
+
+   case nir_intrinsic_load_global_invocation_id:
+      emit_load_vec_input(ctx, intr, &ctx->global_invocation_id_var, "gl_GlobalInvocationID", SpvBuiltInGlobalInvocationId, nir_type_uint);
+      break;
+
+   case nir_intrinsic_load_local_invocation_index:
+      emit_load_uint_input(ctx, intr, &ctx->local_invocation_index_var, "gl_LocalInvocationIndex", SpvBuiltInLocalInvocationIndex);
+      break;
+
+   case nir_intrinsic_load_shared:
+      emit_load_shared(ctx, intr);
+      break;
+
+   case nir_intrinsic_store_shared:
+      emit_store_shared(ctx, intr);
+      break;
+
+   case nir_intrinsic_vote_all:
+   case nir_intrinsic_vote_any:
+   case nir_intrinsic_vote_ieq:
+   case nir_intrinsic_vote_feq:
+      emit_vote(ctx, intr);
       break;
 
    default:
@@ -1778,10 +2946,14 @@ tex_instr_is_lod_allowed(nir_tex_instr *tex)
 }
 
 static SpvId
-pad_coord_vector(struct ntv_context *ctx, SpvId orig, unsigned old_size, unsigned new_size)
+pad_coord_vector(struct ntv_context *ctx, SpvId orig, unsigned old_size, unsigned new_size, bool f)
 {
     SpvId int_type = spirv_builder_type_int(&ctx->builder, 32);
-    SpvId type = get_ivec_type(ctx, 32, new_size);
+    SpvId type;
+    if (f)
+       type = get_fvec_type(ctx, 32, new_size);
+    else
+       type = get_ivec_type(ctx, 32, new_size);
     SpvId constituents[NIR_MAX_VEC_COMPONENTS] = {0};
     SpvId zero = emit_int_const(ctx, 32, 0);
     assert(new_size < NIR_MAX_VEC_COMPONENTS);
@@ -1809,11 +2981,14 @@ emit_tex(struct ntv_context *ctx, nir_tex_instr *tex)
           tex->op == nir_texop_txd ||
           tex->op == nir_texop_txf ||
           tex->op == nir_texop_txf_ms ||
-          tex->op == nir_texop_txs);
+          tex->op == nir_texop_txs ||
+          tex->op == nir_texop_lod ||
+          tex->op == nir_texop_tg4 ||
+          tex->op == nir_texop_texture_samples);
    assert(tex->texture_index == tex->sampler_index);
 
    SpvId coord = 0, proj = 0, bias = 0, lod = 0, dref = 0, dx = 0, dy = 0,
-         offset = 0, sample = 0;
+         offset = 0, sample = 0, tex_offset = 0;
    unsigned coord_components = 0, coord_bitsize = 0, offset_components = 0;
    for (unsigned i = 0; i < tex->num_srcs; i++) {
       switch (tex->src[i].src_type) {
@@ -1876,6 +3051,14 @@ emit_tex(struct ntv_context *ctx, nir_tex_instr *tex)
          assert(dy != 0);
          break;
 
+      case nir_tex_src_texture_offset:
+         tex_offset = get_src_int(ctx, &tex->src[i].src);
+         break;
+
+      case nir_tex_src_sampler_offset:
+         /* don't care */
+         break;
+
       default:
          fprintf(stderr, "texture source: %d\n", tex->src[i].src_type);
          unreachable("unknown texture source");
@@ -1887,13 +3070,38 @@ emit_tex(struct ntv_context *ctx, nir_tex_instr *tex)
       assert(lod != 0);
    }
 
-   SpvId image_type = ctx->image_types[tex->texture_index];
+   unsigned texture_index = tex->texture_index;
+   if (!tex_offset) {
+      /* convert constant index back to base + offset */
+      unsigned last_sampler = util_last_bit(ctx->samplers_used);
+      for (unsigned i = 0; i < last_sampler; i++) {
+         if (!ctx->sampler_array_sizes[i]) {
+            if (i == texture_index)
+               /* this is a non-array sampler, so we don't need an access chain */
+               break;
+            /* this isn't the sampler we're looking for */
+            continue;
+         }
+         if (texture_index <= i + ctx->sampler_array_sizes[i] - 1) {
+            /* this is the first member of a sampler array */
+            tex_offset = emit_uint_const(ctx, 32, texture_index - i);
+            texture_index = i;
+            break;
+         }
+      }
+   }
+   SpvId image_type = ctx->sampler_types[texture_index];
+   assert(image_type);
    SpvId sampled_type = spirv_builder_type_sampled_image(&ctx->builder,
                                                          image_type);
-
-   assert(ctx->samplers_used & (1u << tex->texture_index));
-   SpvId load = spirv_builder_emit_load(&ctx->builder, sampled_type,
-                                        ctx->samplers[tex->texture_index]);
+   assert(sampled_type);
+   assert(ctx->samplers_used & (1u << texture_index));
+   SpvId sampler_id = ctx->samplers[texture_index];
+   if (tex_offset) {
+       SpvId ptr = spirv_builder_type_pointer(&ctx->builder, SpvStorageClassUniformConstant, sampled_type);
+       sampler_id = spirv_builder_emit_access_chain(&ctx->builder, ptr, sampler_id, &tex_offset, 1);
+   }
+   SpvId load = spirv_builder_emit_load(&ctx->builder, sampled_type, sampler_id);
 
    SpvId dest_type = get_dest_type(ctx, &tex->dest, tex->dest_type);
 
@@ -1907,6 +3115,13 @@ emit_tex(struct ntv_context *ctx, nir_tex_instr *tex)
       store_dest(ctx, &tex->dest, result, tex->dest_type);
       return;
    }
+   if (tex->op == nir_texop_texture_samples) {
+      SpvId image = spirv_builder_emit_image(&ctx->builder, image_type, load);
+      SpvId result = spirv_builder_emit_unop(&ctx->builder, SpvOpImageQuerySamples,
+                                             dest_type, image);
+      store_dest(ctx, &tex->dest, result, tex->dest_type);
+      return;
+   }
 
    if (proj && coord_components > 0) {
       SpvId constituents[coord_components + 1];
@@ -1930,30 +3145,49 @@ emit_tex(struct ntv_context *ctx, nir_tex_instr *tex)
                                                             constituents,
                                                             coord_components);
    }
-
+   if (tex->op == nir_texop_lod) {
+      SpvId result = spirv_builder_emit_image_query_lod(&ctx->builder,
+                                                         dest_type, load,
+                                                         coord);
+      store_dest(ctx, &tex->dest, result, tex->dest_type);
+      return;
+   }
    SpvId actual_dest_type = dest_type;
    if (dref)
       actual_dest_type = spirv_builder_type_float(&ctx->builder, 32);
 
    SpvId result;
    if (tex->op == nir_texop_txf ||
-       tex->op == nir_texop_txf_ms) {
+       tex->op == nir_texop_txf_ms ||
+       tex->op == nir_texop_tg4) {
       SpvId image = spirv_builder_emit_image(&ctx->builder, image_type, load);
-      if (offset) {
+      /* if the driver doesn't support extended features, we have to manually apply the offset */
+      if (offset && !ctx->feats->shaderImageGatherExtended) {
          /* SPIRV requires matched length vectors for OpIAdd, so if a shader
           * uses vecs of differing sizes we need to make a new vec padded with zeroes
           * to mimic how GLSL does this implicitly
           */
          if (offset_components > coord_components)
-            coord = pad_coord_vector(ctx, coord, coord_components, offset_components);
+            coord = pad_coord_vector(ctx, coord, coord_components, offset_components, tex->op == nir_texop_tg4);
          else if (coord_components > offset_components)
-            offset = pad_coord_vector(ctx, offset, offset_components, coord_components);
-         coord = emit_binop(ctx, SpvOpIAdd,
-                            get_ivec_type(ctx, coord_bitsize, coord_components),
-                            coord, offset);
+            offset = pad_coord_vector(ctx, offset, offset_components, coord_components, tex->op == nir_texop_tg4);
+         if (tex->op == nir_texop_tg4)
+            coord = emit_binop(ctx, SpvOpFAdd,
+                               get_fvec_type(ctx, coord_bitsize, coord_components),
+                               coord, offset);
+         else
+            coord = emit_binop(ctx, SpvOpIAdd,
+                               get_ivec_type(ctx, coord_bitsize, coord_components),
+                               coord, offset);
+         offset = 0;
       }
-      result = spirv_builder_emit_image_fetch(&ctx->builder, dest_type,
-                                              image, coord, lod, sample);
+      if (tex->op == nir_texop_tg4)
+         result = spirv_builder_emit_image_gather(&ctx->builder, dest_type,
+                                                 load, coord, emit_uint_const(ctx, 32, tex->component),
+                                                 lod, sample, offset, dref);
+      else
+         result = spirv_builder_emit_image_fetch(&ctx->builder, dest_type,
+                                                 image, coord, lod, sample, offset);
    } else {
       result = spirv_builder_emit_image_sample(&ctx->builder,
                                                actual_dest_type, load,
@@ -1966,7 +3200,7 @@ emit_tex(struct ntv_context *ctx, nir_tex_instr *tex)
    spirv_builder_emit_decoration(&ctx->builder, result,
                                  SpvDecorationRelaxedPrecision);
 
-   if (dref && nir_dest_num_components(tex->dest) > 1) {
+   if (dref && nir_dest_num_components(tex->dest) > 1 && tex->op != nir_texop_tg4) {
       SpvId components[4] = { result, result, result, result };
       result = spirv_builder_emit_composite_construct(&ctx->builder,
                                                       dest_type,
@@ -2044,13 +3278,27 @@ emit_deref_array(struct ntv_context *ctx, nir_deref_instr *deref)
    nir_variable *var = nir_deref_instr_get_variable(deref);
 
    SpvStorageClass storage_class;
+   SpvId base;
+   SpvId type;
    switch (var->data.mode) {
    case nir_var_shader_in:
       storage_class = SpvStorageClassInput;
+      base = get_src(ctx, &deref->parent);
+      type = get_glsl_type(ctx, deref->type);
       break;
 
    case nir_var_shader_out:
       storage_class = SpvStorageClassOutput;
+      base = get_src(ctx, &deref->parent);
+      type = get_glsl_type(ctx, deref->type);
+      break;
+
+   case nir_var_uniform:
+      storage_class = SpvStorageClassUniformConstant;
+      struct hash_entry *he = _mesa_hash_table_search(ctx->vars, var);
+      assert(he);
+      base = (SpvId)(intptr_t)he->data;
+      type = ctx->image_types[var->data.binding];
       break;
 
    default:
@@ -2059,6 +3307,47 @@ emit_deref_array(struct ntv_context *ctx, nir_deref_instr *deref)
 
    SpvId index = get_src(ctx, &deref->arr.index);
 
+   SpvId ptr_type = spirv_builder_type_pointer(&ctx->builder,
+                                               storage_class,
+                                               type);
+
+   SpvId result = spirv_builder_emit_access_chain(&ctx->builder,
+                                                  ptr_type,
+                                                  base,
+                                                  &index, 1);
+   /* uint is a bit of a lie here, it's really just an opaque type */
+   store_dest(ctx, &deref->dest, result, nir_type_uint);
+
+   /* image ops always need to be able to get the variable to check out sampler types and such */
+   if (glsl_type_is_image(glsl_without_array(var->type))) {
+      uint32_t *key = ralloc_size(ctx->mem_ctx, sizeof(uint32_t));
+      *key = result;
+      _mesa_hash_table_insert(ctx->image_vars, key, var);
+   }
+}
+
+static void
+emit_deref_struct(struct ntv_context *ctx, nir_deref_instr *deref)
+{
+   assert(deref->deref_type == nir_deref_type_struct);
+   nir_variable *var = nir_deref_instr_get_variable(deref);
+
+   SpvStorageClass storage_class;
+   switch (var->data.mode) {
+   case nir_var_shader_in:
+      storage_class = SpvStorageClassInput;
+      break;
+
+   case nir_var_shader_out:
+      storage_class = SpvStorageClassOutput;
+      break;
+
+   default:
+      unreachable("Unsupported nir_variable_mode\n");
+   }
+
+   SpvId index = emit_uint_const(ctx, 32, deref->strct.index);
+
    SpvId ptr_type = spirv_builder_type_pointer(&ctx->builder,
                                                storage_class,
                                                get_glsl_type(ctx, deref->type));
@@ -2083,6 +3372,10 @@ emit_deref(struct ntv_context *ctx, nir_deref_instr *deref)
       emit_deref_array(ctx, deref);
       break;
 
+   case nir_deref_type_struct:
+      emit_deref_struct(ctx, deref);
+      break;
+
    default:
       unreachable("unexpected deref_type");
    }
@@ -2228,8 +3521,63 @@ emit_cf_list(struct ntv_context *ctx, struct exec_list *list)
    }
 }
 
+static SpvExecutionMode
+get_prim_type_mode(uint16_t type, bool output)
+{
+   SpvExecutionMode mode = 0;
+   switch (type) {
+   case GL_POINTS:
+      mode = output ? SpvExecutionModeOutputPoints : SpvExecutionModeInputPoints;
+      break;
+   case GL_LINES:
+   case GL_LINE_LOOP:
+      if (output)
+         unreachable("GL_LINES/LINE_LOOP passed as gs output");
+      /* fallthrough */
+   case GL_LINE_STRIP:
+      mode = output ? SpvExecutionModeOutputLineStrip : SpvExecutionModeInputLines;
+      break;
+   case GL_TRIANGLE_STRIP:
+      if (output)
+         return SpvExecutionModeOutputTriangleStrip;
+   /* fallthrough */
+   case GL_TRIANGLES:
+   case GL_TRIANGLE_FAN: //FIXME: not sure if right for output
+      mode = SpvExecutionModeTriangles;
+      break;
+   case GL_QUADS:
+   case GL_QUAD_STRIP:
+      mode = SpvExecutionModeQuads;
+      break;
+   case GL_POLYGON:
+      unreachable("handle polygons in gs");
+      break;
+   case GL_LINES_ADJACENCY:
+   case GL_LINE_STRIP_ADJACENCY:
+      if (output)
+         unreachable("handle line adjacency in gs");
+      mode = SpvExecutionModeInputLinesAdjacency;
+      break;
+   case GL_TRIANGLES_ADJACENCY:
+   case GL_TRIANGLE_STRIP_ADJACENCY:
+      if (output)
+         unreachable("handle triangle adjacency in gs");
+      mode = SpvExecutionModeInputTrianglesAdjacency;
+      break;
+   case GL_ISOLINES:
+      mode = SpvExecutionModeIsolines;
+      break;
+   default:
+      debug_printf("unknown geometry shader %s mode %u\n", output ? "output" : "input", type);
+      break;
+   }
+   
+   assert(mode);
+   return mode;
+}
+
 struct spirv_shader *
-nir_to_spirv(struct nir_shader *s, const struct zink_so_info *so_info)
+nir_to_spirv(struct nir_shader *s, const struct zink_so_info *so_info, unsigned char *shader_slot_map, unsigned char *shader_slots_reserved, const VkPhysicalDeviceFeatures *feats)
 {
    struct spirv_shader *ret = NULL;
 
@@ -2242,34 +3590,95 @@ nir_to_spirv(struct nir_shader *s, const struct zink_so_info *so_info)
    case MESA_SHADER_FRAGMENT:
    case MESA_SHADER_COMPUTE:
       spirv_builder_emit_cap(&ctx.builder, SpvCapabilityShader);
+      spirv_builder_emit_cap(&ctx.builder, SpvCapabilityImageBuffer);
+      spirv_builder_emit_cap(&ctx.builder, SpvCapabilitySampledBuffer);
       break;
 
    case MESA_SHADER_TESS_CTRL:
    case MESA_SHADER_TESS_EVAL:
       spirv_builder_emit_cap(&ctx.builder, SpvCapabilityTessellation);
+      /* TODO: check features for this */
+      if (s->info.outputs_written & BITFIELD64_BIT(VARYING_SLOT_PSIZ))
+         spirv_builder_emit_cap(&ctx.builder, SpvCapabilityTessellationPointSize);
       break;
 
    case MESA_SHADER_GEOMETRY:
       spirv_builder_emit_cap(&ctx.builder, SpvCapabilityGeometry);
+      if (s->info.gs.active_stream_mask)
+         spirv_builder_emit_cap(&ctx.builder, SpvCapabilityGeometryStreams);
+      if (s->info.outputs_written & BITFIELD64_BIT(VARYING_SLOT_PSIZ))
+         spirv_builder_emit_cap(&ctx.builder, SpvCapabilityGeometryPointSize);
       break;
 
    default:
       unreachable("invalid stage");
    }
 
+   if (s->info.num_ssbos)
+      spirv_builder_emit_extension(&ctx.builder, "SPV_KHR_storage_buffer_storage_class");
+
+   if (s->info.outputs_written & BITFIELD64_BIT(VARYING_SLOT_VIEWPORT)) {
+      if (s->info.stage < MESA_SHADER_GEOMETRY)
+         spirv_builder_emit_cap(&ctx.builder, SpvCapabilityShaderViewportIndex);
+      else
+         spirv_builder_emit_cap(&ctx.builder, SpvCapabilityMultiViewport);
+   }
+
    // TODO: only enable when needed
-   if (s->info.stage == MESA_SHADER_FRAGMENT) {
+   if (s->info.stage == MESA_SHADER_FRAGMENT || s->info.num_images) {
       spirv_builder_emit_cap(&ctx.builder, SpvCapabilitySampled1D);
+      spirv_builder_emit_cap(&ctx.builder, SpvCapabilityImage1D);
       spirv_builder_emit_cap(&ctx.builder, SpvCapabilityImageQuery);
       spirv_builder_emit_cap(&ctx.builder, SpvCapabilityDerivativeControl);
+      spirv_builder_emit_cap(&ctx.builder, SpvCapabilitySampleRateShading);
+      if (feats->shaderStorageImageExtendedFormats)
+         spirv_builder_emit_cap(&ctx.builder, SpvCapabilityStorageImageExtendedFormats);
+   }
+   if (s->info.num_images) {
+      /* what if these aren't available? */
+      if (feats->shaderStorageImageWriteWithoutFormat)
+         spirv_builder_emit_cap(&ctx.builder, SpvCapabilityStorageImageWriteWithoutFormat);
+      if (feats->shaderStorageImageReadWithoutFormat)
+         spirv_builder_emit_cap(&ctx.builder, SpvCapabilityStorageImageReadWithoutFormat);
    }
+   if (feats->shaderInt64) {
+      spirv_builder_emit_cap(&ctx.builder, SpvCapabilityInt64);
+      spirv_builder_emit_cap(&ctx.builder, SpvCapabilityFloat64);
+   }
+   if (feats->shaderImageGatherExtended)
+      spirv_builder_emit_cap(&ctx.builder, SpvCapabilityImageGatherExtended);
 
    ctx.stage = s->info.stage;
+   ctx.so_info = so_info;
+   ctx.feats = feats;
+   ctx.num_ssbos = s->info.num_ssbos;
+   if (shader_slot_map) {
+      /* COMPUTE doesn't have this */
+      ctx.shader_slot_map = shader_slot_map;
+      ctx.shader_slots_reserved = *shader_slots_reserved;
+   }
    ctx.GLSL_std_450 = spirv_builder_import(&ctx.builder, "GLSL.std.450");
    spirv_builder_emit_source(&ctx.builder, SpvSourceLanguageGLSL, 450);
 
-   spirv_builder_emit_mem_model(&ctx.builder, SpvAddressingModelLogical,
-                                SpvMemoryModelGLSL450);
+   if (s->info.stage == MESA_SHADER_TESS_CTRL || s->info.num_images) {
+      spirv_builder_emit_extension(&ctx.builder, "SPV_KHR_vulkan_memory_model");
+      spirv_builder_emit_cap(&ctx.builder, SpvCapabilityVulkanMemoryModel);
+      spirv_builder_emit_cap(&ctx.builder, SpvCapabilityVulkanMemoryModelDeviceScope);
+      spirv_builder_emit_mem_model(&ctx.builder, SpvAddressingModelLogical,
+                                   SpvMemoryModelVulkan);
+   } else if (s->info.stage == MESA_SHADER_COMPUTE) {
+      SpvAddressingModel model;
+      if (s->info.cs.ptr_size == 32)
+         model = SpvAddressingModelPhysical32;
+      else if (s->info.cs.ptr_size == 64)
+         model = SpvAddressingModelPhysical64;
+      else
+         model = SpvAddressingModelLogical;
+      spirv_builder_emit_mem_model(&ctx.builder, model,
+                                   SpvMemoryModelGLSL450);
+   } else
+      spirv_builder_emit_mem_model(&ctx.builder, SpvAddressingModelLogical,
+                                   SpvMemoryModelGLSL450);
 
    SpvExecutionModel exec_model;
    switch (s->info.stage) {
@@ -2304,36 +3713,101 @@ nir_to_spirv(struct nir_shader *s, const struct zink_so_info *so_info)
    ctx.vars = _mesa_hash_table_create(ctx.mem_ctx, _mesa_hash_pointer,
                                       _mesa_key_pointer_equal);
 
+   ctx.image_vars = _mesa_hash_table_create(ctx.mem_ctx, _mesa_hash_u32,
+                                      _mesa_key_u32_equal);
+
    ctx.so_outputs = _mesa_hash_table_create(ctx.mem_ctx, _mesa_hash_u32,
                                             _mesa_key_u32_equal);
 
-   nir_foreach_shader_in_variable(var, s)
+   nir_foreach_variable_with_modes(var, s, nir_var_shader_in | nir_var_mem_push_const)
       emit_input(&ctx, var);
 
    nir_foreach_shader_out_variable(var, s)
       emit_output(&ctx, var);
 
+
    if (so_info)
-      emit_so_info(&ctx, util_last_bit64(s->info.outputs_written), so_info);
-   nir_foreach_variable_with_modes(var, s, nir_var_uniform |
-                                           nir_var_mem_ubo |
-                                           nir_var_mem_ssbo)
-      emit_uniform(&ctx, var);
+      emit_so_info(&ctx, so_info);
+   /* we have to reverse iterate to match what's done in zink_compiler.c */
+   foreach_list_typed_reverse(nir_variable, var, node, &s->variables)
+      if (_nir_shader_variable_has_mode(var, nir_var_uniform |
+                                        nir_var_mem_ubo |
+                                        nir_var_mem_ssbo))
+         emit_uniform(&ctx, var);
 
-   if (s->info.stage == MESA_SHADER_FRAGMENT) {
+   switch (s->info.stage) {
+   case MESA_SHADER_VERTEX:
+      spirv_builder_emit_extension(&ctx.builder, "SPV_KHR_shader_draw_parameters");
+      spirv_builder_emit_cap(&ctx.builder, SpvCapabilityDrawParameters);
+      break;
+   case MESA_SHADER_FRAGMENT:
+      spirv_builder_emit_cap(&ctx.builder, SpvCapabilityInterpolationFunction);
       spirv_builder_emit_exec_mode(&ctx.builder, entry_point,
                                    SpvExecutionModeOriginUpperLeft);
       if (s->info.outputs_written & BITFIELD64_BIT(FRAG_RESULT_DEPTH))
          spirv_builder_emit_exec_mode(&ctx.builder, entry_point,
                                       SpvExecutionModeDepthReplacing);
+      if (s->info.fs.early_fragment_tests)
+         spirv_builder_emit_exec_mode(&ctx.builder, entry_point, SpvExecutionModeEarlyFragmentTests);
+      break;
+   case MESA_SHADER_TESS_CTRL:
+      spirv_builder_emit_exec_mode_literal(&ctx.builder, entry_point, SpvExecutionModeOutputVertices, s->info.tess.tcs_vertices_out);
+      break;
+   case MESA_SHADER_TESS_EVAL:
+      switch (s->info.tess.primitive_mode) {
+      case GL_TRIANGLES:
+         spirv_builder_emit_exec_mode(&ctx.builder, entry_point, SpvExecutionModeTriangles);
+         break;
+      case GL_QUADS:
+         spirv_builder_emit_exec_mode(&ctx.builder, entry_point, SpvExecutionModeQuads);
+         break;
+      case GL_ISOLINES:
+         spirv_builder_emit_exec_mode(&ctx.builder, entry_point, SpvExecutionModeIsolines);
+         break;
+      default:
+         unreachable("unknown tess prim type!");
+      }
+      if (s->info.tess.ccw)
+         spirv_builder_emit_exec_mode(&ctx.builder, entry_point, SpvExecutionModeVertexOrderCcw);
+      else
+         spirv_builder_emit_exec_mode(&ctx.builder, entry_point, SpvExecutionModeVertexOrderCw);
+      switch (s->info.tess.spacing) {
+      case TESS_SPACING_EQUAL:
+         spirv_builder_emit_exec_mode(&ctx.builder, entry_point, SpvExecutionModeSpacingEqual);
+         break;
+      case TESS_SPACING_FRACTIONAL_ODD:
+         spirv_builder_emit_exec_mode(&ctx.builder, entry_point, SpvExecutionModeSpacingFractionalOdd);
+         break;
+      case TESS_SPACING_FRACTIONAL_EVEN:
+         spirv_builder_emit_exec_mode(&ctx.builder, entry_point, SpvExecutionModeSpacingFractionalEven);
+         break;
+      default:
+         unreachable("unknown tess spacing!");
+      }
+      if (s->info.tess.point_mode)
+         spirv_builder_emit_exec_mode(&ctx.builder, entry_point, SpvExecutionModePointMode);
+      break;
+   case MESA_SHADER_GEOMETRY:
+      spirv_builder_emit_exec_mode(&ctx.builder, entry_point, get_prim_type_mode(s->info.gs.input_primitive, false));
+      spirv_builder_emit_exec_mode(&ctx.builder, entry_point, get_prim_type_mode(s->info.gs.output_primitive, true));
+      spirv_builder_emit_exec_mode_literal(&ctx.builder, entry_point, SpvExecutionModeInvocations, s->info.gs.invocations);
+      spirv_builder_emit_exec_mode_literal(&ctx.builder, entry_point, SpvExecutionModeOutputVertices, s->info.gs.vertices_out);
+      break;
+   case MESA_SHADER_COMPUTE:
+      spirv_builder_emit_exec_mode_literal3(&ctx.builder, entry_point, SpvExecutionModeLocalSize,
+                                            (uint32_t[3]){(uint32_t)s->info.cs.local_size[0], (uint32_t)s->info.cs.local_size[1],
+                                            (uint32_t)s->info.cs.local_size[2]});
+      if (s->info.cs.shared_size)
+         create_shared_block(&ctx, s->info.cs.shared_size);
+      break;
+   default:
+      break;
    }
-
    if (so_info && so_info->so_info.num_outputs) {
       spirv_builder_emit_cap(&ctx.builder, SpvCapabilityTransformFeedback);
       spirv_builder_emit_exec_mode(&ctx.builder, entry_point,
                                    SpvExecutionModeXfb);
    }
-
    spirv_builder_function(&ctx.builder, entry_point, type_void,
                                             SpvFunctionControlMaskNone,
                                             type_main);
@@ -2380,7 +3854,8 @@ nir_to_spirv(struct nir_shader *s, const struct zink_so_info *so_info)
 
    emit_cf_list(&ctx, &entry->body);
 
-   if (so_info)
+   /* vertex shader emits copied xfb outputs at the end of the shader */
+   if (so_info && ctx.stage == MESA_SHADER_VERTEX)
       emit_so_outputs(&ctx, so_info);
 
    spirv_builder_return(&ctx.builder); // doesn't belong here, but whatevz
@@ -2404,6 +3879,8 @@ nir_to_spirv(struct nir_shader *s, const struct zink_so_info *so_info)
    assert(ret->num_words == num_words);
 
    ralloc_free(ctx.mem_ctx);
+   if (shader_slots_reserved)
+      *shader_slots_reserved = ctx.shader_slots_reserved;
 
    return ret;
 
diff --git a/src/gallium/drivers/zink/nir_to_spirv/nir_to_spirv.h b/src/gallium/drivers/zink/nir_to_spirv/nir_to_spirv.h
index 77d77add4e4..e13230a2762 100644
--- a/src/gallium/drivers/zink/nir_to_spirv/nir_to_spirv.h
+++ b/src/gallium/drivers/zink/nir_to_spirv/nir_to_spirv.h
@@ -28,6 +28,7 @@
 #include <stdint.h>
 #include <vulkan/vulkan.h>
 
+#include "compiler/nir/nir.h"
 #include "compiler/shader_enums.h"
 #include "pipe/p_state.h"
 
@@ -42,7 +43,7 @@ struct nir_shader;
 struct pipe_stream_output_info;
 
 struct spirv_shader *
-nir_to_spirv(struct nir_shader *s, const struct zink_so_info *so_info);
+nir_to_spirv(struct nir_shader *s, const struct zink_so_info *so_info, unsigned char *shader_slot_map, unsigned char *shader_slots_reserved, const VkPhysicalDeviceFeatures *feats);
 
 void
 spirv_shader_delete(struct spirv_shader *s);
@@ -50,6 +51,30 @@ spirv_shader_delete(struct spirv_shader *s);
 uint32_t
 zink_binding(gl_shader_stage stage, VkDescriptorType type, int index);
 
+static inline VkDescriptorType
+zink_sampler_type(const struct glsl_type *type)
+{
+   assert(glsl_type_is_sampler(type));
+   if (glsl_get_sampler_dim(type) < GLSL_SAMPLER_DIM_BUF || glsl_get_sampler_dim(type) == GLSL_SAMPLER_DIM_MS)
+      return VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER;
+   if (glsl_get_sampler_dim(type) == GLSL_SAMPLER_DIM_BUF)
+      return VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER;
+   unreachable("unimplemented");
+   return 0;
+}
+
+static inline VkDescriptorType
+zink_image_type(const struct glsl_type *type)
+{
+   assert(glsl_type_is_image(type));
+   if (glsl_get_sampler_dim(type) < GLSL_SAMPLER_DIM_BUF || glsl_get_sampler_dim(type) == GLSL_SAMPLER_DIM_MS)
+      return VK_DESCRIPTOR_TYPE_STORAGE_IMAGE;
+   if (glsl_get_sampler_dim(type) == GLSL_SAMPLER_DIM_BUF)
+      return VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER;
+   unreachable("unimplemented");
+   return 0;
+}
+
 struct nir_shader;
 
 bool
diff --git a/src/gallium/drivers/zink/nir_to_spirv/spirv_builder.c b/src/gallium/drivers/zink/nir_to_spirv/spirv_builder.c
index 1b7c676486a..8553698091a 100644
--- a/src/gallium/drivers/zink/nir_to_spirv/spirv_builder.c
+++ b/src/gallium/drivers/zink/nir_to_spirv/spirv_builder.c
@@ -145,6 +145,29 @@ spirv_builder_emit_entry_point(struct spirv_builder *b,
         spirv_buffer_emit_word(&b->entry_points, interfaces[i]);
 }
 
+void
+spirv_builder_emit_exec_mode_literal(struct spirv_builder *b, SpvId entry_point,
+                                     SpvExecutionMode exec_mode, uint32_t param)
+{
+   spirv_buffer_prepare(&b->exec_modes, b->mem_ctx, 4);
+   spirv_buffer_emit_word(&b->exec_modes, SpvOpExecutionMode | (4 << 16));
+   spirv_buffer_emit_word(&b->exec_modes, entry_point);
+   spirv_buffer_emit_word(&b->exec_modes, exec_mode);
+   spirv_buffer_emit_word(&b->exec_modes, param);
+}
+
+void
+spirv_builder_emit_exec_mode_literal3(struct spirv_builder *b, SpvId entry_point,
+                                     SpvExecutionMode exec_mode, uint32_t param[3])
+{
+   spirv_buffer_prepare(&b->exec_modes, b->mem_ctx, 6);
+   spirv_buffer_emit_word(&b->exec_modes, SpvOpExecutionMode | (6 << 16));
+   spirv_buffer_emit_word(&b->exec_modes, entry_point);
+   spirv_buffer_emit_word(&b->exec_modes, exec_mode);
+   for (unsigned i = 0; i < 3; i++)
+      spirv_buffer_emit_word(&b->exec_modes, param[i]);
+}
+
 void
 spirv_builder_emit_exec_mode(struct spirv_builder *b, SpvId entry_point,
                              SpvExecutionMode exec_mode)
@@ -212,6 +235,28 @@ spirv_builder_emit_builtin(struct spirv_builder *b, SpvId target,
    emit_decoration(b, target, SpvDecorationBuiltIn, args, ARRAY_SIZE(args));
 }
 
+void
+spirv_builder_emit_vertex(struct spirv_builder *b, uint32_t stream)
+{
+   unsigned words = 1 + !!stream;
+   SpvOp op = !!stream ? SpvOpEmitStreamVertex : SpvOpEmitVertex;
+   spirv_buffer_prepare(&b->instructions, b->mem_ctx, words);
+   spirv_buffer_emit_word(&b->instructions, op | (words << 16));
+   if (stream)
+      spirv_buffer_emit_word(&b->instructions, spirv_builder_const_uint(b, 32, stream));
+}
+
+void
+spirv_builder_end_primitive(struct spirv_builder *b, uint32_t stream)
+{
+   unsigned words = 1 + !!stream;
+   SpvOp op = !!stream ? SpvOpEndStreamPrimitive : SpvOpEndPrimitive;
+   spirv_buffer_prepare(&b->instructions, b->mem_ctx, words);
+   spirv_buffer_emit_word(&b->instructions, op | (words << 16));
+   if (stream)
+      spirv_buffer_emit_word(&b->instructions, spirv_builder_const_uint(b, 32, stream));
+}
+
 void
 spirv_builder_emit_descriptor_set(struct spirv_builder *b, SpvId target,
                                   uint32_t descriptor_set)
@@ -268,6 +313,13 @@ spirv_builder_emit_index(struct spirv_builder *b, SpvId target, int index)
    emit_decoration(b, target, SpvDecorationIndex, args, ARRAY_SIZE(args));
 }
 
+void
+spirv_builder_emit_stream(struct spirv_builder *b, SpvId target, int stream)
+{
+   uint32_t args[] = { stream };
+   emit_decoration(b, target, SpvDecorationStream, args, ARRAY_SIZE(args));
+}
+
 static void
 emit_member_decoration(struct spirv_builder *b, SpvId target, uint32_t member,
                        SpvDecoration decoration, const uint32_t extra_operands[],
@@ -356,6 +408,17 @@ spirv_builder_emit_store(struct spirv_builder *b, SpvId pointer, SpvId object)
    spirv_buffer_emit_word(&b->instructions, object);
 }
 
+void
+spirv_builder_emit_atomic_store(struct spirv_builder *b, SpvId pointer, SpvScope scope, SpvMemorySemanticsMask semantics, SpvId object)
+{
+   spirv_buffer_prepare(&b->instructions, b->mem_ctx, 5);
+   spirv_buffer_emit_word(&b->instructions, SpvOpAtomicStore | (5 << 16));
+   spirv_buffer_emit_word(&b->instructions, pointer);
+   spirv_buffer_emit_word(&b->instructions, spirv_builder_const_uint(b, 32, scope));
+   spirv_buffer_emit_word(&b->instructions, spirv_builder_const_uint(b, 32, semantics));
+   spirv_buffer_emit_word(&b->instructions, object);
+}
+
 SpvId
 spirv_builder_emit_access_chain(struct spirv_builder *b, SpvId result_type,
                                 SpvId base, const SpvId indexes[],
@@ -419,6 +482,41 @@ spirv_builder_emit_triop(struct spirv_builder *b, SpvOp op, SpvId result_type,
    return result;
 }
 
+SpvId
+spirv_builder_emit_quadop(struct spirv_builder *b, SpvOp op, SpvId result_type,
+                         SpvId operand0, SpvId operand1, SpvId operand2, SpvId operand3)
+{
+   SpvId result = spirv_builder_new_id(b);
+   spirv_buffer_prepare(&b->instructions, b->mem_ctx, 7);
+   spirv_buffer_emit_word(&b->instructions, op | (7 << 16));
+   spirv_buffer_emit_word(&b->instructions, result_type);
+   spirv_buffer_emit_word(&b->instructions, result);
+   spirv_buffer_emit_word(&b->instructions, operand0);
+   spirv_buffer_emit_word(&b->instructions, operand1);
+   spirv_buffer_emit_word(&b->instructions, operand2);
+   spirv_buffer_emit_word(&b->instructions, operand3);
+   return result;
+}
+
+SpvId
+spirv_builder_emit_hexop(struct spirv_builder *b, SpvOp op, SpvId result_type,
+                         SpvId operand0, SpvId operand1, SpvId operand2, SpvId operand3,
+                         SpvId operand4, SpvId operand5)
+{
+   SpvId result = spirv_builder_new_id(b);
+   spirv_buffer_prepare(&b->instructions, b->mem_ctx, 9);
+   spirv_buffer_emit_word(&b->instructions, op | (9 << 16));
+   spirv_buffer_emit_word(&b->instructions, result_type);
+   spirv_buffer_emit_word(&b->instructions, result);
+   spirv_buffer_emit_word(&b->instructions, operand0);
+   spirv_buffer_emit_word(&b->instructions, operand1);
+   spirv_buffer_emit_word(&b->instructions, operand2);
+   spirv_buffer_emit_word(&b->instructions, operand3);
+   spirv_buffer_emit_word(&b->instructions, operand4);
+   spirv_buffer_emit_word(&b->instructions, operand5);
+   return result;
+}
+
 SpvId
 spirv_builder_emit_composite_extract(struct spirv_builder *b, SpvId result_type,
                                      SpvId composite, const uint32_t indexes[],
@@ -589,6 +687,13 @@ spirv_builder_emit_kill(struct spirv_builder *b)
    spirv_buffer_emit_word(&b->instructions, SpvOpKill | (1 << 16));
 }
 
+SpvId
+spirv_builder_emit_vote(struct spirv_builder *b, SpvOp op, SpvId src)
+{
+   return spirv_builder_emit_binop(b, op, spirv_builder_type_bool(b),
+                                   spirv_builder_const_uint(b, 32, SpvScopeWorkgroup), src);
+}
+
 SpvId
 spirv_builder_emit_image_sample(struct spirv_builder *b,
                                 SpvId result_type,
@@ -667,18 +772,169 @@ spirv_builder_emit_image(struct spirv_builder *b, SpvId result_type,
    return result;
 }
 
+SpvId
+spirv_builder_emit_image_texel_pointer(struct spirv_builder *b,
+                                       SpvId result_type,
+                                       SpvId image,
+                                       SpvId coordinate,
+                                       SpvId sample)
+{
+   SpvId pointer_type = spirv_builder_type_pointer(b,
+                                                   SpvStorageClassImage,
+                                                   result_type);
+   return spirv_builder_emit_triop(b, SpvOpImageTexelPointer, pointer_type, image, coordinate, sample);
+}
+
+SpvId
+spirv_builder_emit_image_read(struct spirv_builder *b,
+                              SpvId result_type,
+                              SpvId image,
+                              SpvId coordinate,
+                              SpvId lod,
+                              SpvId sample,
+                              SpvId offset)
+{
+   SpvId result = spirv_builder_new_id(b);
+
+   SpvImageOperandsMask operand_mask = SpvImageOperandsMaskNone;
+   SpvId extra_operands[4];
+   int num_extra_operands = 0;
+   if (lod) {
+      extra_operands[++num_extra_operands] = lod;
+      operand_mask |= SpvImageOperandsLodMask;
+   }
+   if (sample) {
+      extra_operands[++num_extra_operands] = sample;
+      operand_mask |= SpvImageOperandsSampleMask;
+   }
+   if (offset) {
+      extra_operands[++num_extra_operands] = offset;
+      operand_mask |= SpvImageOperandsOffsetMask;
+   }
+   /* finalize num_extra_operands / extra_operands */
+   if (num_extra_operands > 0) {
+      extra_operands[0] = operand_mask;
+      num_extra_operands++;
+   }
+
+   spirv_buffer_prepare(&b->instructions, b->mem_ctx, 5 + num_extra_operands);
+   spirv_buffer_emit_word(&b->instructions, SpvOpImageRead |
+                          ((5 + num_extra_operands) << 16));
+   spirv_buffer_emit_word(&b->instructions, result_type);
+   spirv_buffer_emit_word(&b->instructions, result);
+   spirv_buffer_emit_word(&b->instructions, image);
+   spirv_buffer_emit_word(&b->instructions, coordinate);
+   for (int i = 0; i < num_extra_operands; ++i)
+      spirv_buffer_emit_word(&b->instructions, extra_operands[i]);
+   return result;
+}
+
+void
+spirv_builder_emit_image_write(struct spirv_builder *b,
+                               SpvId image,
+                               SpvId coordinate,
+                               SpvId texel,
+                               SpvId lod,
+                               SpvId sample,
+                               SpvId offset)
+{
+   SpvImageOperandsMask operand_mask = SpvImageOperandsMaskNone;
+   SpvId extra_operands[4];
+   int num_extra_operands = 0;
+   if (lod) {
+      extra_operands[++num_extra_operands] = lod;
+      operand_mask |= SpvImageOperandsLodMask;
+   }
+   if (sample) {
+      extra_operands[++num_extra_operands] = sample;
+      operand_mask |= SpvImageOperandsSampleMask;
+   }
+   if (offset) {
+      extra_operands[++num_extra_operands] = offset;
+      operand_mask |= SpvImageOperandsOffsetMask;
+   }
+   /* finalize num_extra_operands / extra_operands */
+   if (num_extra_operands > 0) {
+      extra_operands[0] = operand_mask;
+      num_extra_operands++;
+   }
+
+   spirv_buffer_prepare(&b->instructions, b->mem_ctx, 4 + num_extra_operands);
+   spirv_buffer_emit_word(&b->instructions, SpvOpImageWrite |
+                          ((4 + num_extra_operands) << 16));
+   spirv_buffer_emit_word(&b->instructions, image);
+   spirv_buffer_emit_word(&b->instructions, coordinate);
+   spirv_buffer_emit_word(&b->instructions, texel);
+   for (int i = 0; i < num_extra_operands; ++i)
+      spirv_buffer_emit_word(&b->instructions, extra_operands[i]);
+}
+
+SpvId
+spirv_builder_emit_image_gather(struct spirv_builder *b,
+                               SpvId result_type,
+                               SpvId image,
+                               SpvId coordinate,
+                               SpvId component,
+                               SpvId lod,
+                               SpvId sample,
+                               SpvId offset,
+                               SpvId dref)
+{
+   SpvId result = spirv_builder_new_id(b);
+   SpvId op = SpvOpImageGather;
+
+   SpvImageOperandsMask operand_mask = SpvImageOperandsMaskNone;
+   SpvId extra_operands[4];
+   int num_extra_operands = 0;
+   if (lod) {
+      extra_operands[++num_extra_operands] = lod;
+      operand_mask |= SpvImageOperandsLodMask;
+   }
+   if (sample) {
+      extra_operands[++num_extra_operands] = sample;
+      operand_mask |= SpvImageOperandsSampleMask;
+   }
+   if (offset) {
+      extra_operands[++num_extra_operands] = offset;
+      operand_mask |= SpvImageOperandsOffsetMask;
+   }
+   if (dref)
+      op = SpvOpImageDrefGather;
+   /* finalize num_extra_operands / extra_operands */
+   if (num_extra_operands > 0) {
+      extra_operands[0] = operand_mask;
+      num_extra_operands++;
+   }
+
+   spirv_buffer_prepare(&b->instructions, b->mem_ctx, 6 + num_extra_operands);
+   spirv_buffer_emit_word(&b->instructions, op |
+                          ((6 + num_extra_operands) << 16));
+   spirv_buffer_emit_word(&b->instructions, result_type);
+   spirv_buffer_emit_word(&b->instructions, result);
+   spirv_buffer_emit_word(&b->instructions, image);
+   spirv_buffer_emit_word(&b->instructions, coordinate);
+   if (dref)
+      spirv_buffer_emit_word(&b->instructions, dref);
+   else
+      spirv_buffer_emit_word(&b->instructions, component);
+   for (int i = 0; i < num_extra_operands; ++i)
+      spirv_buffer_emit_word(&b->instructions, extra_operands[i]);
+   return result;
+}
+
 SpvId
 spirv_builder_emit_image_fetch(struct spirv_builder *b,
                                SpvId result_type,
                                SpvId image,
                                SpvId coordinate,
                                SpvId lod,
-                               SpvId sample)
+                               SpvId sample,
+                               SpvId offset)
 {
    SpvId result = spirv_builder_new_id(b);
 
    SpvImageOperandsMask operand_mask = SpvImageOperandsMaskNone;
-   SpvId extra_operands[3];
+   SpvId extra_operands[4];
    int num_extra_operands = 0;
    if (lod) {
       extra_operands[++num_extra_operands] = lod;
@@ -688,6 +944,10 @@ spirv_builder_emit_image_fetch(struct spirv_builder *b,
       extra_operands[++num_extra_operands] = sample;
       operand_mask |= SpvImageOperandsSampleMask;
    }
+   if (offset) {
+      extra_operands[++num_extra_operands] = offset;
+      operand_mask |= SpvImageOperandsOffsetMask;
+   }
 
    /* finalize num_extra_operands / extra_operands */
    if (num_extra_operands > 0) {
@@ -733,6 +993,26 @@ spirv_builder_emit_image_query_size(struct spirv_builder *b,
    return result;
 }
 
+SpvId
+spirv_builder_emit_image_query_lod(struct spirv_builder *b,
+                                    SpvId result_type,
+                                    SpvId image,
+                                    SpvId coords)
+{
+   int opcode = SpvOpImageQueryLod;
+   int words = 5;
+
+   SpvId result = spirv_builder_new_id(b);
+   spirv_buffer_prepare(&b->instructions, b->mem_ctx, words);
+   spirv_buffer_emit_word(&b->instructions, opcode | (words << 16));
+   spirv_buffer_emit_word(&b->instructions, result_type);
+   spirv_buffer_emit_word(&b->instructions, result);
+   spirv_buffer_emit_word(&b->instructions, image);
+   spirv_buffer_emit_word(&b->instructions, coords);
+
+   return result;
+}
+
 SpvId
 spirv_builder_emit_ext_inst(struct spirv_builder *b, SpvId result_type,
                             SpvId set, uint32_t instruction,
@@ -915,6 +1195,26 @@ spirv_builder_type_vector(struct spirv_builder *b, SpvId component_type,
    return get_type_def(b, SpvOpTypeVector, args, ARRAY_SIZE(args));
 }
 
+SpvId
+spirv_builder_type_matrix(struct spirv_builder *b, SpvId component_type,
+                          unsigned component_count)
+{
+   assert(component_count > 1);
+   uint32_t args[] = { component_type, component_count };
+   return get_type_def(b, SpvOpTypeMatrix, args, ARRAY_SIZE(args));
+}
+
+SpvId
+spirv_builder_type_runtime_array(struct spirv_builder *b, SpvId component_type)
+{
+   SpvId type = spirv_builder_new_id(b);
+   spirv_buffer_prepare(&b->types_const_defs, b->mem_ctx, 3);
+   spirv_buffer_emit_word(&b->types_const_defs, SpvOpTypeRuntimeArray | (3 << 16));
+   spirv_buffer_emit_word(&b->types_const_defs, type);
+   spirv_buffer_emit_word(&b->types_const_defs, component_type);
+   return type;
+}
+
 SpvId
 spirv_builder_type_array(struct spirv_builder *b, SpvId component_type,
                          SpvId length)
@@ -1044,30 +1344,27 @@ spirv_builder_const_bool(struct spirv_builder *b, bool val)
 }
 
 SpvId
-spirv_builder_const_int(struct spirv_builder *b, int width, int32_t val)
+spirv_builder_const_int(struct spirv_builder *b, int width, int64_t val)
 {
-   assert(width <= 32);
-   uint32_t args[] = { val };
+   uint32_t args[] = { u64_low(val), u64_high(val) };
    return get_const_def(b, SpvOpConstant, spirv_builder_type_int(b, width),
-                        args, ARRAY_SIZE(args));
+                        args, width == 64 ? 2 : 1);
 }
 
 SpvId
-spirv_builder_const_uint(struct spirv_builder *b, int width, uint32_t val)
+spirv_builder_const_uint(struct spirv_builder *b, int width, uint64_t val)
 {
-   assert(width <= 32);
-   uint32_t args[] = { val };
+   uint32_t args[] = { u64_low(val), u64_high(val) };
    return get_const_def(b, SpvOpConstant, spirv_builder_type_uint(b, width),
-                        args, ARRAY_SIZE(args));
+                        args, width == 64 ? 2 : 1);
 }
 
 SpvId
-spirv_builder_const_float(struct spirv_builder *b, int width, float val)
+spirv_builder_const_float(struct spirv_builder *b, int width, double val)
 {
-   assert(width <= 32);
-   uint32_t args[] = { u_bitcast_f2u(val) };
+   uint32_t args[] = { u_bitcast_f2u(u64_low(val)), u_bitcast_f2u(u64_high(val)) };
    return get_const_def(b, SpvOpConstant, spirv_builder_type_float(b, width),
-                        args, ARRAY_SIZE(args));
+                        args, width == 64 ? 2 : 1);
 }
 
 SpvId
@@ -1097,6 +1394,25 @@ spirv_builder_emit_var(struct spirv_builder *b, SpvId type,
    return ret;
 }
 
+void
+spirv_builder_emit_memory_barrier(struct spirv_builder *b, SpvScope scope, SpvMemorySemanticsMask semantics)
+{
+   spirv_buffer_prepare(&b->instructions, b->mem_ctx, 3);
+   spirv_buffer_emit_word(&b->instructions, SpvOpMemoryBarrier | (3 << 16));
+   spirv_buffer_emit_word(&b->instructions, spirv_builder_const_uint(b, 32, scope));
+   spirv_buffer_emit_word(&b->instructions, spirv_builder_const_uint(b, 32, semantics));
+}
+
+void
+spirv_builder_emit_control_barrier(struct spirv_builder *b, SpvScope scope, SpvScope mem_scope, SpvMemorySemanticsMask semantics)
+{
+   spirv_buffer_prepare(&b->instructions, b->mem_ctx, 4);
+   spirv_buffer_emit_word(&b->instructions, SpvOpControlBarrier | (4 << 16));
+   spirv_buffer_emit_word(&b->instructions, spirv_builder_const_uint(b, 32, scope));
+   spirv_buffer_emit_word(&b->instructions, spirv_builder_const_uint(b, 32, mem_scope));
+   spirv_buffer_emit_word(&b->instructions, spirv_builder_const_uint(b, 32, semantics));
+}
+
 SpvId
 spirv_builder_import(struct spirv_builder *b, const char *name)
 {
diff --git a/src/gallium/drivers/zink/nir_to_spirv/spirv_builder.h b/src/gallium/drivers/zink/nir_to_spirv/spirv_builder.h
index a1e9b6c655e..7dd0ff29fe1 100644
--- a/src/gallium/drivers/zink/nir_to_spirv/spirv_builder.h
+++ b/src/gallium/drivers/zink/nir_to_spirv/spirv_builder.h
@@ -31,6 +31,28 @@
 #include <stdint.h>
 #include <stdlib.h>
 
+union u64_32_t {
+   uint64_t i64;
+   struct {
+      uint32_t low;
+      uint32_t high;
+   };
+};
+
+static inline uint32_t
+u64_low(uint64_t i64)
+{
+   union u64_32_t u = { .i64 = i64 };
+   return u.low;
+}
+
+static inline uint32_t
+u64_high(uint64_t i64)
+{
+   union u64_32_t u = { .i64 = i64 };
+   return u.high;
+}
+
 struct hash_table;
 
 struct spirv_buffer {
@@ -102,6 +124,9 @@ spirv_builder_emit_builtin(struct spirv_builder *b, SpvId target,
 void
 spirv_builder_emit_index(struct spirv_builder *b, SpvId target, int index);
 
+void
+spirv_builder_emit_stream(struct spirv_builder *b, SpvId target, int stream);
+
 void
 spirv_builder_emit_descriptor_set(struct spirv_builder *b, SpvId target,
                                   uint32_t descriptor_set);
@@ -135,7 +160,12 @@ spirv_builder_emit_entry_point(struct spirv_builder *b,
                                SpvExecutionModel exec_model, SpvId entry_point,
                                const char *name, const SpvId interfaces[],
                                size_t num_interfaces);
-
+void
+spirv_builder_emit_exec_mode_literal(struct spirv_builder *b, SpvId entry_point,
+                                     SpvExecutionMode exec_mode, uint32_t param);
+void
+spirv_builder_emit_exec_mode_literal3(struct spirv_builder *b, SpvId entry_point,
+                                     SpvExecutionMode exec_mode, uint32_t param[3]);
 void
 spirv_builder_emit_exec_mode(struct spirv_builder *b, SpvId entry_point,
                              SpvExecutionMode exec_mode);
@@ -162,6 +192,9 @@ SpvId
 spirv_builder_emit_load(struct spirv_builder *b, SpvId result_type,
                         SpvId pointer);
 
+void
+spirv_builder_emit_atomic_store(struct spirv_builder *b, SpvId pointer, SpvScope scope, SpvMemorySemanticsMask semantics, SpvId object);
+
 void
 spirv_builder_emit_store(struct spirv_builder *b, SpvId pointer, SpvId object);
 
@@ -183,6 +216,14 @@ spirv_builder_emit_triop(struct spirv_builder *b, SpvOp op, SpvId result_type,
                          SpvId operand0, SpvId operand1, SpvId operand2);
 
 SpvId
+spirv_builder_emit_quadop(struct spirv_builder *b, SpvOp op, SpvId result_type,
+                         SpvId operand0, SpvId operand1, SpvId operand2, SpvId operand3);
+
+SpvId
+spirv_builder_emit_hexop(struct spirv_builder *b, SpvOp op, SpvId result_type,
+                         SpvId operand0, SpvId operand1, SpvId operand2, SpvId operand3,
+                         SpvId operand4, SpvId operand5);
+SpvId
 spirv_builder_emit_composite_extract(struct spirv_builder *b, SpvId result_type,
                                      SpvId composite, const uint32_t indexes[],
                                      size_t num_indexes);
@@ -233,6 +274,8 @@ spirv_builder_set_phi_operand(struct spirv_builder *b, size_t position,
 void
 spirv_builder_emit_kill(struct spirv_builder *b);
 
+SpvId
+spirv_builder_emit_vote(struct spirv_builder *b, SpvOp op, SpvId src);
 
 SpvId
 spirv_builder_emit_image_sample(struct spirv_builder *b,
@@ -251,20 +294,61 @@ SpvId
 spirv_builder_emit_image(struct spirv_builder *b, SpvId result_type,
                          SpvId sampled_image);
 
+SpvId
+spirv_builder_emit_image_texel_pointer(struct spirv_builder *b,
+                                       SpvId result_type,
+                                       SpvId image,
+                                       SpvId coordinate,
+                                       SpvId sample);
+
+SpvId
+spirv_builder_emit_image_read(struct spirv_builder *b,
+                              SpvId result_type,
+                              SpvId image,
+                              SpvId coordinate,
+                              SpvId lod,
+                              SpvId sample,
+                              SpvId offset);
+
+void
+spirv_builder_emit_image_write(struct spirv_builder *b,
+                               SpvId image,
+                               SpvId coordinate,
+                               SpvId texel,
+                               SpvId lod,
+                               SpvId sample,
+                               SpvId offset);
+
 SpvId
 spirv_builder_emit_image_fetch(struct spirv_builder *b,
                                SpvId result_type,
                                SpvId image,
                                SpvId coordinate,
                                SpvId lod,
-                               SpvId sample);
-
+                               SpvId sample,
+                               SpvId offset);
+SpvId
+spirv_builder_emit_image_gather(struct spirv_builder *b,
+                               SpvId result_type,
+                               SpvId image,
+                               SpvId coordinate,
+                               SpvId component,
+                               SpvId lod,
+                               SpvId sample,
+                               SpvId offset,
+                               SpvId dref);
 SpvId
 spirv_builder_emit_image_query_size(struct spirv_builder *b,
                                     SpvId result_type,
                                     SpvId image,
                                     SpvId lod);
 
+SpvId
+spirv_builder_emit_image_query_lod(struct spirv_builder *b,
+                                    SpvId result_type,
+                                    SpvId image,
+                                    SpvId coords);
+
 SpvId
 spirv_builder_emit_ext_inst(struct spirv_builder *b, SpvId result_type,
                             SpvId set, uint32_t instruction,
@@ -301,6 +385,13 @@ SpvId
 spirv_builder_type_vector(struct spirv_builder *b, SpvId component_type,
                           unsigned component_count);
 
+SpvId
+spirv_builder_type_matrix(struct spirv_builder *b, SpvId component_type,
+                          unsigned component_count);
+
+SpvId
+spirv_builder_type_runtime_array(struct spirv_builder *b, SpvId component_type);
+
 SpvId
 spirv_builder_type_array(struct spirv_builder *b, SpvId component_type,
                          SpvId length);
@@ -318,13 +409,13 @@ SpvId
 spirv_builder_const_bool(struct spirv_builder *b, bool val);
 
 SpvId
-spirv_builder_const_int(struct spirv_builder *b, int width, int32_t val);
+spirv_builder_const_int(struct spirv_builder *b, int width, int64_t val);
 
 SpvId
-spirv_builder_const_uint(struct spirv_builder *b, int width, uint32_t val);
+spirv_builder_const_uint(struct spirv_builder *b, int width, uint64_t val);
 
 SpvId
-spirv_builder_const_float(struct spirv_builder *b, int width, float val);
+spirv_builder_const_float(struct spirv_builder *b, int width, double val);
 
 SpvId
 spirv_builder_const_composite(struct spirv_builder *b, SpvId result_type,
@@ -335,6 +426,12 @@ SpvId
 spirv_builder_emit_var(struct spirv_builder *b, SpvId type,
                        SpvStorageClass storage_class);
 
+void
+spirv_builder_emit_memory_barrier(struct spirv_builder *b, SpvScope scope, SpvMemorySemanticsMask semantics);
+
+void
+spirv_builder_emit_control_barrier(struct spirv_builder *b, SpvScope scope, SpvScope mem_scope, SpvMemorySemanticsMask semantics);
+
 SpvId
 spirv_builder_import(struct spirv_builder *b, const char *name);
 
@@ -345,4 +442,8 @@ size_t
 spirv_builder_get_words(struct spirv_builder *b, uint32_t *words,
                         size_t num_words);
 
+void
+spirv_builder_emit_vertex(struct spirv_builder *b, uint32_t stream);
+void
+spirv_builder_end_primitive(struct spirv_builder *b, uint32_t stream);
 #endif
diff --git a/src/gallium/drivers/zink/zink_batch.c b/src/gallium/drivers/zink/zink_batch.c
index 2b92b1a540a..4407222af8c 100644
--- a/src/gallium/drivers/zink/zink_batch.c
+++ b/src/gallium/drivers/zink/zink_batch.c
@@ -9,6 +9,7 @@
 #include "zink_resource.h"
 #include "zink_screen.h"
 
+#include "util/hash_table.h"
 #include "util/u_debug.h"
 #include "util/set.h"
 
@@ -28,14 +29,20 @@ reset_batch(struct zink_context *ctx, struct zink_batch *batch)
    zink_render_pass_reference(screen, &batch->rp, NULL);
    zink_framebuffer_reference(screen, &batch->fb, NULL);
    set_foreach(batch->programs, entry) {
-      struct zink_gfx_program *prog = (struct zink_gfx_program*)entry->key;
-      zink_gfx_program_reference(screen, &prog, NULL);
+      if (batch->batch_id == ZINK_COMPUTE_BATCH_ID) {
+         struct zink_compute_program *comp = (struct zink_compute_program*)entry->key;
+         zink_compute_program_reference(screen, &comp, NULL);
+      } else {
+         struct zink_gfx_program *prog = (struct zink_gfx_program*)entry->key;
+         zink_gfx_program_reference(screen, &prog, NULL);
+      }
    }
    _mesa_set_clear(batch->programs, NULL);
 
    /* unref all used resources */
    set_foreach(batch->resources, entry) {
       struct pipe_resource *pres = (struct pipe_resource *)entry->key;
+
       pipe_resource_reference(&pres, NULL);
    }
    _mesa_set_clear(batch->resources, NULL);
@@ -47,6 +54,12 @@ reset_batch(struct zink_context *ctx, struct zink_batch *batch)
    }
    _mesa_set_clear(batch->sampler_views, NULL);
 
+   set_foreach(batch->surfaces, entry) {
+      struct pipe_surface *surf = (struct pipe_surface *)entry->key;
+      pipe_surface_reference(&surf, NULL);
+   }
+   _mesa_set_clear(batch->surfaces, NULL);
+
    util_dynarray_foreach(&batch->zombie_samplers, VkSampler, samp) {
       vkDestroySampler(screen->dev, *samp, NULL);
    }
@@ -54,6 +67,7 @@ reset_batch(struct zink_context *ctx, struct zink_batch *batch)
 
    if (vkResetDescriptorPool(screen->dev, batch->descpool, 0) != VK_SUCCESS)
       fprintf(stderr, "vkResetDescriptorPool failed\n");
+   batch->has_draw = false;
 }
 
 void
@@ -67,14 +81,14 @@ zink_start_batch(struct zink_context *ctx, struct zink_batch *batch)
    if (vkBeginCommandBuffer(batch->cmdbuf, &cbbi) != VK_SUCCESS)
       debug_printf("vkBeginCommandBuffer failed\n");
 
-   if (!ctx->queries_disabled)
+   if (!ctx->queries_disabled && batch->batch_id != ZINK_COMPUTE_BATCH_ID)
       zink_resume_queries(ctx, batch);
 }
 
 void
 zink_end_batch(struct zink_context *ctx, struct zink_batch *batch)
 {
-   if (!ctx->queries_disabled)
+   if (!ctx->queries_disabled && batch->batch_id != ZINK_COMPUTE_BATCH_ID)
       zink_suspend_queries(ctx, batch);
 
    if (vkEndCommandBuffer(batch->cmdbuf) != VK_SUCCESS) {
@@ -97,21 +111,52 @@ zink_end_batch(struct zink_context *ctx, struct zink_batch *batch)
    si.commandBufferCount = 1;
    si.pCommandBuffers = &batch->cmdbuf;
 
-   if (vkQueueSubmit(ctx->queue, 1, &si, batch->fence->fence) != VK_SUCCESS) {
+   if (vkQueueSubmit(batch->batch_id != ZINK_COMPUTE_BATCH_ID ? ctx->queue : ctx->compute_queue, 1, &si, batch->fence->fence) != VK_SUCCESS) {
       debug_printf("vkQueueSubmit failed\n");
       abort();
    }
 }
 
-void
-zink_batch_reference_resoure(struct zink_batch *batch,
-                             struct zink_resource *res)
+int
+zink_batch_reference_resource_rw(struct zink_batch *batch, struct zink_resource *res, bool write)
 {
+   unsigned mask = write ? ZINK_RESOURCE_ACCESS_WRITE : ZINK_RESOURCE_ACCESS_READ;
+   int batch_to_flush = -1;
+
+   /* u_transfer_helper unrefs the stencil buffer when the depth buffer is unrefed,
+    * so we add an extra ref here to the stencil buffer to compensate
+    */
+   struct zink_resource *stencil;
+
+   zink_get_depth_stencil_resources((struct pipe_resource*)res, NULL, &stencil);
+
+   uint32_t cur_uses = zink_get_resource_usage(res);
+   cur_uses &= ~(ZINK_RESOURCE_ACCESS_READ << batch->batch_id);
+   cur_uses &= ~(ZINK_RESOURCE_ACCESS_WRITE << batch->batch_id);
+   if (batch->batch_id == ZINK_COMPUTE_BATCH_ID) {
+      if (cur_uses >= ZINK_RESOURCE_ACCESS_WRITE || (write && cur_uses))
+         batch_to_flush = 0;
+   } else {
+      if (cur_uses & (ZINK_RESOURCE_ACCESS_WRITE << ZINK_COMPUTE_BATCH_ID) ||
+          (write && cur_uses & (ZINK_RESOURCE_ACCESS_READ << ZINK_COMPUTE_BATCH_ID)))
+         batch_to_flush = ZINK_COMPUTE_BATCH_ID;
+   }
+
    struct set_entry *entry = _mesa_set_search(batch->resources, res);
    if (!entry) {
       entry = _mesa_set_add(batch->resources, res);
       pipe_reference(NULL, &res->base.reference);
+      if (stencil)
+         pipe_reference(NULL, &stencil->base.reference);
    }
+   /* the batch_uses value for this batch is guaranteed to not be in use now because
+    * reset_batch() waits on the fence and removes access before resetting
+    */
+   res->batch_uses[batch->batch_id] |= mask;
+
+   if (stencil)
+      stencil->batch_uses[batch->batch_id] |= mask;
+   return batch_to_flush;
 }
 
 void
@@ -127,11 +172,23 @@ zink_batch_reference_sampler_view(struct zink_batch *batch,
 
 void
 zink_batch_reference_program(struct zink_batch *batch,
-                             struct zink_gfx_program *prog)
+                             struct pipe_reference *prog)
 {
    struct set_entry *entry = _mesa_set_search(batch->programs, prog);
    if (!entry) {
       entry = _mesa_set_add(batch->programs, prog);
-      pipe_reference(NULL, &prog->reference);
+      pipe_reference(NULL, prog);
+   }
+}
+
+void
+zink_batch_reference_surface(struct zink_batch *batch,
+                             struct zink_surface *surface)
+{
+   struct pipe_surface *surf = (void*)surface;
+   struct set_entry *entry = _mesa_set_search(batch->surfaces, surf);
+   if (!entry) {
+      entry = _mesa_set_add(batch->surfaces, surf);
+      pipe_reference(NULL, &surf->reference);
    }
 }
diff --git a/src/gallium/drivers/zink/zink_batch.h b/src/gallium/drivers/zink/zink_batch.h
index 7b5ffe9fc95..c3b2c4fb204 100644
--- a/src/gallium/drivers/zink/zink_batch.h
+++ b/src/gallium/drivers/zink/zink_batch.h
@@ -29,17 +29,20 @@
 #include "util/list.h"
 #include "util/u_dynarray.h"
 
+struct pipe_reference;
+
 struct zink_context;
 struct zink_fence;
 struct zink_framebuffer;
-struct zink_gfx_program;
 struct zink_render_pass;
 struct zink_resource;
 struct zink_sampler_view;
+struct zink_surface;
 
 #define ZINK_BATCH_DESC_SIZE 1000
 
 struct zink_batch {
+   unsigned batch_id : 3;
    VkCommandBuffer cmdbuf;
    VkDescriptorPool descpool;
    int descs_left;
@@ -51,10 +54,12 @@ struct zink_batch {
 
    struct set *resources;
    struct set *sampler_views;
+   struct set *surfaces;
 
    struct util_dynarray zombie_samplers;
 
    struct set *active_queries; /* zink_query objects which were active at some point in this batch */
+   bool has_draw;
 };
 
 void
@@ -63,9 +68,10 @@ zink_start_batch(struct zink_context *ctx, struct zink_batch *batch);
 void
 zink_end_batch(struct zink_context *ctx, struct zink_batch *batch);
 
-void
-zink_batch_reference_resoure(struct zink_batch *batch,
-                             struct zink_resource *res);
+int
+zink_batch_reference_resource_rw(struct zink_batch *batch,
+                                 struct zink_resource *res,
+                                 bool write);
 
 void
 zink_batch_reference_sampler_view(struct zink_batch *batch,
@@ -73,5 +79,9 @@ zink_batch_reference_sampler_view(struct zink_batch *batch,
 
 void
 zink_batch_reference_program(struct zink_batch *batch,
-                             struct zink_gfx_program *prog);
+                             struct pipe_reference *prog);
+
+void
+zink_batch_reference_surface(struct zink_batch *batch,
+                             struct zink_surface *surface);
 #endif
diff --git a/src/gallium/drivers/zink/zink_blit.c b/src/gallium/drivers/zink/zink_blit.c
index 96824c8a049..d1f38df885f 100644
--- a/src/gallium/drivers/zink/zink_blit.c
+++ b/src/gallium/drivers/zink/zink_blit.c
@@ -28,20 +28,14 @@ blit_resolve(struct zink_context *ctx, const struct pipe_blit_info *info)
 
    struct zink_batch *batch = zink_batch_no_rp(ctx);
 
-   zink_batch_reference_resoure(batch, src);
-   zink_batch_reference_resoure(batch, dst);
+   zink_batch_reference_resource_rw(batch, src, false);
+   zink_batch_reference_resource_rw(batch, dst, true);
 
-   if (src->layout != VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL)
-      zink_resource_barrier(batch->cmdbuf, src, src->aspect,
-                            VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL);
-
-   if (dst->layout != VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL)
-      zink_resource_barrier(batch->cmdbuf, dst, dst->aspect,
-                            VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL);
+   zink_resource_setup_layouts(batch, src, dst);
 
    VkImageResolve region = {};
 
-   region.srcSubresource.aspectMask = src->aspect;
+   region.srcSubresource.aspectMask = zink_resource_aspect_from_format(&src->base);
    region.srcSubresource.mipLevel = info->src.level;
    region.srcSubresource.baseArrayLayer = 0; // no clue
    region.srcSubresource.layerCount = 1; // no clue
@@ -49,7 +43,7 @@ blit_resolve(struct zink_context *ctx, const struct pipe_blit_info *info)
    region.srcOffset.y = info->src.box.y;
    region.srcOffset.z = info->src.box.z;
 
-   region.dstSubresource.aspectMask = dst->aspect;
+   region.dstSubresource.aspectMask = zink_resource_aspect_from_format(&dst->base);
    region.dstSubresource.mipLevel = info->dst.level;
    region.dstSubresource.baseArrayLayer = 0; // no clue
    region.dstSubresource.layerCount = 1; // no clue
@@ -94,41 +88,13 @@ blit_native(struct zink_context *ctx, const struct pipe_blit_info *info)
       return false;
 
    struct zink_batch *batch = zink_batch_no_rp(ctx);
-   zink_batch_reference_resoure(batch, src);
-   zink_batch_reference_resoure(batch, dst);
-
-   if (src == dst) {
-      /* The Vulkan 1.1 specification says the following about valid usage
-       * of vkCmdBlitImage:
-       *
-       * "srcImageLayout must be VK_IMAGE_LAYOUT_SHARED_PRESENT_KHR,
-       *  VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL or VK_IMAGE_LAYOUT_GENERAL"
-       *
-       * and:
-       *
-       * "dstImageLayout must be VK_IMAGE_LAYOUT_SHARED_PRESENT_KHR,
-       *  VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL or VK_IMAGE_LAYOUT_GENERAL"
-       *
-       * Since we cant have the same image in two states at the same time,
-       * we're effectively left with VK_IMAGE_LAYOUT_SHARED_PRESENT_KHR or
-       * VK_IMAGE_LAYOUT_GENERAL. And since this isn't a present-related
-       * operation, VK_IMAGE_LAYOUT_GENERAL seems most appropriate.
-       */
-      if (src->layout != VK_IMAGE_LAYOUT_GENERAL)
-         zink_resource_barrier(batch->cmdbuf, src, src->aspect,
-                               VK_IMAGE_LAYOUT_GENERAL);
-   } else {
-      if (src->layout != VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL)
-         zink_resource_barrier(batch->cmdbuf, src, src->aspect,
-                               VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL);
+   zink_batch_reference_resource_rw(batch, src, false);
+   zink_batch_reference_resource_rw(batch, dst, true);
 
-      if (dst->layout != VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL)
-         zink_resource_barrier(batch->cmdbuf, dst, dst->aspect,
-                               VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL);
-   }
+   zink_resource_setup_layouts(batch, src, dst);
 
    VkImageBlit region = {};
-   region.srcSubresource.aspectMask = src->aspect;
+   region.srcSubresource.aspectMask = zink_resource_aspect_from_format(&src->base);
    region.srcSubresource.mipLevel = info->src.level;
    region.srcOffsets[0].x = info->src.box.x;
    region.srcOffsets[0].y = info->src.box.y;
@@ -147,7 +113,7 @@ blit_native(struct zink_context *ctx, const struct pipe_blit_info *info)
       region.srcSubresource.layerCount = 1;
    }
 
-   region.dstSubresource.aspectMask = dst->aspect;
+   region.dstSubresource.aspectMask = zink_resource_aspect_from_format(&dst->base);
    region.dstSubresource.mipLevel = info->dst.level;
    region.dstOffsets[0].x = info->dst.box.x;
    region.dstOffsets[0].y = info->dst.box.y;
@@ -191,7 +157,7 @@ zink_blit(struct pipe_context *pctx,
    struct zink_resource *src = zink_resource(info->src.resource);
    struct zink_resource *dst = zink_resource(info->dst.resource);
    /* if we're copying between resources with matching aspects then we can probably just copy_region */
-   if (src->aspect == dst->aspect && util_try_blit_via_copy_region(pctx, info))
+   if (zink_resource_aspect_from_format(&src->base) == zink_resource_aspect_from_format(&dst->base) && util_try_blit_via_copy_region(pctx, info))
       return;
 
    if (!util_blitter_is_blit_supported(ctx->blitter, info)) {
@@ -201,26 +167,45 @@ zink_blit(struct pipe_context *pctx,
       return;
    }
 
-   util_blitter_save_blend(ctx->blitter, ctx->gfx_pipeline_state.blend_state);
-   util_blitter_save_depth_stencil_alpha(ctx->blitter, ctx->gfx_pipeline_state.depth_stencil_alpha_state);
+
    util_blitter_save_vertex_elements(ctx->blitter, ctx->element_state);
-   util_blitter_save_stencil_ref(ctx->blitter, &ctx->stencil_ref);
-   util_blitter_save_rasterizer(ctx->blitter, ctx->rast_state);
-   util_blitter_save_fragment_shader(ctx->blitter, ctx->gfx_stages[PIPE_SHADER_FRAGMENT]);
-   util_blitter_save_vertex_shader(ctx->blitter, ctx->gfx_stages[PIPE_SHADER_VERTEX]);
-   util_blitter_save_framebuffer(ctx->blitter, &ctx->fb_state);
-   util_blitter_save_viewport(ctx->blitter, ctx->viewport_states);
-   util_blitter_save_scissor(ctx->blitter, ctx->scissor_states);
+   util_blitter_save_viewport(ctx->blitter, ctx->gfx_pipeline_state.viewport_states);
+
    util_blitter_save_fragment_sampler_states(ctx->blitter,
                                              ctx->num_samplers[PIPE_SHADER_FRAGMENT],
                                              ctx->sampler_states[PIPE_SHADER_FRAGMENT]);
    util_blitter_save_fragment_sampler_views(ctx->blitter,
-                                            ctx->num_image_views[PIPE_SHADER_FRAGMENT],
-                                            ctx->image_views[PIPE_SHADER_FRAGMENT]);
+                                            ctx->num_sampler_views[PIPE_SHADER_FRAGMENT],
+                                            ctx->sampler_views[PIPE_SHADER_FRAGMENT]);
    util_blitter_save_fragment_constant_buffer_slot(ctx->blitter, ctx->ubos[PIPE_SHADER_FRAGMENT]);
    util_blitter_save_vertex_buffer_slot(ctx->blitter, ctx->buffers);
-   util_blitter_save_sample_mask(ctx->blitter, ctx->gfx_pipeline_state.sample_mask);
-   util_blitter_save_so_targets(ctx->blitter, ctx->num_so_targets, ctx->so_targets);
+   zink_blit_begin(ctx, ZINK_BLIT_SAVE_FB | ZINK_BLIT_SAVE_FS);
 
    util_blitter_blit(ctx->blitter, info);
 }
+
+/* similar to radeonsi */
+void
+zink_blit_begin(struct zink_context *ctx, enum zink_blit_flags flags)
+{
+   util_blitter_save_vertex_shader(ctx->blitter, ctx->gfx_stages[PIPE_SHADER_VERTEX]);
+   util_blitter_save_tessctrl_shader(ctx->blitter, ctx->gfx_stages[PIPE_SHADER_TESS_CTRL]);
+   util_blitter_save_tesseval_shader(ctx->blitter, ctx->gfx_stages[PIPE_SHADER_TESS_EVAL]);
+   util_blitter_save_geometry_shader(ctx->blitter, ctx->gfx_stages[PIPE_SHADER_GEOMETRY]);
+   util_blitter_save_rasterizer(ctx->blitter, ctx->rast_state);
+   util_blitter_save_so_targets(ctx->blitter, ctx->num_so_targets, ctx->so_targets);
+
+   if (flags & ZINK_BLIT_SAVE_FS) {
+      util_blitter_save_blend(ctx->blitter, ctx->gfx_pipeline_state.blend_state);
+      util_blitter_save_depth_stencil_alpha(ctx->blitter, ctx->gfx_pipeline_state.depth_stencil_alpha_state);
+      util_blitter_save_stencil_ref(ctx->blitter, &ctx->stencil_ref);
+      util_blitter_save_sample_mask(ctx->blitter, ctx->gfx_pipeline_state.sample_mask);
+      util_blitter_save_scissor(ctx->blitter, ctx->gfx_pipeline_state.scissor_states);
+      /* also util_blitter_save_window_rectangles when we have that? */
+
+      util_blitter_save_fragment_shader(ctx->blitter, ctx->gfx_stages[PIPE_SHADER_FRAGMENT]);
+   }
+
+   if (flags & ZINK_BLIT_SAVE_FB)
+      util_blitter_save_framebuffer(ctx->blitter, &ctx->fb_state);
+}
diff --git a/src/gallium/drivers/zink/zink_clear.c b/src/gallium/drivers/zink/zink_clear.c
new file mode 100644
index 00000000000..5bee760e870
--- /dev/null
+++ b/src/gallium/drivers/zink/zink_clear.c
@@ -0,0 +1,305 @@
+/*
+ * Copyright 2018 Collabora Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * on the rights to use, copy, modify, merge, publish, distribute, sub
+ * license, and/or sell copies of the Software, and to permit persons to whom
+ * the Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
+ * THE AUTHOR(S) AND/OR THEIR SUPPLIERS BE LIABLE FOR ANY CLAIM,
+ * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
+ * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
+ * USE OR OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+#include "zink_context.h"
+#include "zink_resource.h"
+#include "zink_screen.h"
+
+#include "util/u_blitter.h"
+#include "util/format/u_format.h"
+#include "util/format_srgb.h"
+#include "util/u_framebuffer.h"
+#include "util/u_inlines.h"
+#include "util/u_rect.h"
+#include "util/u_surface.h"
+
+static inline bool
+check_3d_layers(struct pipe_surface *psurf)
+{
+   /* SPEC PROBLEM:
+    * though the vk spec doesn't seem to explicitly address this, currently drivers
+    * are claiming that all 3D images have a single "3D" layer regardless of layercount,
+    * so we can never clear them if we aren't trying to clear only layer 0
+    */
+   if (psurf->u.tex.first_layer)
+      return false;
+      
+   if (psurf->u.tex.last_layer - psurf->u.tex.first_layer > 0)
+      return false;
+   return true;
+}
+
+static void
+clear_in_rp(struct pipe_context *pctx,
+           unsigned buffers,
+           const struct pipe_scissor_state *scissor_state,
+           const union pipe_color_union *pcolor,
+           double depth, unsigned stencil)
+{
+   struct zink_context *ctx = zink_context(pctx);
+   struct pipe_framebuffer_state *fb = &ctx->fb_state;
+
+   struct zink_batch *batch = zink_batch_rp(ctx);
+
+   VkClearAttachment attachments[1 + PIPE_MAX_COLOR_BUFS];
+   int num_attachments = 0;
+
+   if (buffers & PIPE_CLEAR_COLOR) {
+      VkClearColorValue color;
+      color.float32[0] = pcolor->f[0];
+      color.float32[1] = pcolor->f[1];
+      color.float32[2] = pcolor->f[2];
+      color.float32[3] = pcolor->f[3];
+
+      for (unsigned i = 0; i < fb->nr_cbufs; i++) {
+         if (!(buffers & (PIPE_CLEAR_COLOR0 << i)) || !fb->cbufs[i])
+            continue;
+
+         attachments[num_attachments].aspectMask = VK_IMAGE_ASPECT_COLOR_BIT;
+         attachments[num_attachments].colorAttachment = i;
+         attachments[num_attachments].clearValue.color = color;
+         ++num_attachments;
+      }
+   }
+
+   if (buffers & PIPE_CLEAR_DEPTHSTENCIL && fb->zsbuf) {
+      VkImageAspectFlags aspect = 0;
+      if (buffers & PIPE_CLEAR_DEPTH)
+         aspect |= VK_IMAGE_ASPECT_DEPTH_BIT;
+      if (buffers & PIPE_CLEAR_STENCIL)
+         aspect |= VK_IMAGE_ASPECT_STENCIL_BIT;
+
+      attachments[num_attachments].aspectMask = aspect;
+      attachments[num_attachments].clearValue.depthStencil.depth = depth;
+      attachments[num_attachments].clearValue.depthStencil.stencil = stencil;
+      ++num_attachments;
+   }
+
+   VkClearRect cr = {};
+   if (scissor_state) {
+      cr.rect.offset.x = scissor_state->minx;
+      /* invert y value to convert from gl coords */
+      cr.rect.offset.y = scissor_state->miny > fb->height ? 0 : fb->height - scissor_state->miny;
+      cr.rect.extent.width = MIN2(fb->width, scissor_state->maxx - scissor_state->minx);
+      cr.rect.extent.height = MIN2(fb->height, scissor_state->maxy - scissor_state->miny);
+   } else {
+      cr.rect.extent.width = fb->width;
+      cr.rect.extent.height = fb->height;
+   }
+   cr.baseArrayLayer = 0;
+   cr.layerCount = util_framebuffer_get_num_layers(fb);
+   vkCmdClearAttachments(batch->cmdbuf, num_attachments, attachments, 1, &cr);
+}
+
+static void
+clear_color_no_rp(struct zink_batch *batch, struct zink_resource *res, const union pipe_color_union *pcolor, unsigned level, unsigned layer, unsigned layerCount)
+{
+   VkImageSubresourceRange range = {};
+   range.baseMipLevel = level;
+   range.levelCount = 1;
+   range.baseArrayLayer = layer;
+   range.layerCount = layerCount;
+   range.aspectMask = VK_IMAGE_ASPECT_COLOR_BIT;
+
+   VkClearColorValue color;
+   color.float32[0] = pcolor->f[0];
+   color.float32[1] = pcolor->f[1];
+   color.float32[2] = pcolor->f[2];
+   color.float32[3] = pcolor->f[3];
+
+   if (res->layout != VK_IMAGE_LAYOUT_GENERAL && res->layout != VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL)
+      zink_resource_barrier(batch, res, VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL, 0);
+   vkCmdClearColorImage(batch->cmdbuf, res->image, res->layout, &color, 1, &range);
+}
+
+static void
+clear_zs_no_rp(struct zink_batch *batch, struct zink_resource *res, VkImageAspectFlags aspects, double depth, unsigned stencil, unsigned level, unsigned layer, unsigned layerCount)
+{
+   VkImageSubresourceRange range = {};
+   range.baseMipLevel = level;
+   range.levelCount = 1;
+   range.baseArrayLayer = layer;
+   range.layerCount = layerCount;
+   range.aspectMask = aspects;
+
+   VkClearDepthStencilValue zs_value = {depth, stencil};
+
+   if (res->layout != VK_IMAGE_LAYOUT_GENERAL && res->layout != VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL)
+      zink_resource_barrier(batch, res, VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL, 0);
+   vkCmdClearDepthStencilImage(batch->cmdbuf, res->image, res->layout, &zs_value, 1, &range);
+}
+
+static bool
+clear_needs_rp(struct zink_context *ctx, unsigned width, unsigned height, struct u_rect *region)
+{
+   struct u_rect intersect = {0, width, 0, height};
+
+   /* FIXME: this is very inefficient; if no renderpass has been started yet,
+    * we should record the clear if it's full-screen, and apply it as we
+    * start the render-pass. Otherwise we can do a partial out-of-renderpass
+    * clear.
+    */
+   if (!u_rect_test_intersection(region, &intersect))
+      /* is this even a thing? */
+      return true;
+
+    u_rect_find_intersection(region, &intersect);
+    if (intersect.x0 != 0 || intersect.y0 != 0 ||
+        intersect.x1 != width || intersect.y1 != height)
+       return true;
+
+   return false;
+}
+
+void
+zink_clear(struct pipe_context *pctx,
+           unsigned buffers,
+           const struct pipe_scissor_state *scissor_state,
+           const union pipe_color_union *pcolor,
+           double depth, unsigned stencil)
+{
+   struct zink_context *ctx = zink_context(pctx);
+   struct pipe_framebuffer_state *fb = &ctx->fb_state;
+   struct zink_batch *batch = zink_curr_batch(ctx);
+   bool needs_rp = false;
+
+   if (scissor_state) {
+      struct u_rect scissor = {scissor_state->minx, scissor_state->maxx, scissor_state->miny, scissor_state->maxy};
+      needs_rp = clear_needs_rp(ctx, fb->width, fb->height, &scissor);
+   }
+
+
+   if (needs_rp || batch->rp || ctx->conditional_render_enabled) {
+      clear_in_rp(pctx, buffers, scissor_state, pcolor, depth, stencil);
+      return;
+   }
+
+   if (buffers & PIPE_CLEAR_COLOR) {
+      for (unsigned i = 0; i < fb->nr_cbufs; i++) {
+         if ((buffers & (PIPE_CLEAR_COLOR0 << i)) && fb->cbufs[i]) {
+            struct pipe_surface *psurf = fb->cbufs[i];
+
+            if (psurf->texture->target == PIPE_TEXTURE_3D && !check_3d_layers(psurf)) {
+               clear_in_rp(pctx, buffers, scissor_state, pcolor, depth, stencil);
+               return;
+            }
+            struct zink_resource *res = zink_resource(psurf->texture);
+            union pipe_color_union color = *pcolor;
+            if (psurf->format != res->base.format &&
+                !util_format_is_srgb(psurf->format) && util_format_is_srgb(res->base.format)) {
+               /* if SRGB mode is disabled for the fb with a backing srgb image then we have to
+                * convert this to srgb color
+                */
+               color.f[0] = util_format_srgb_to_linear_float(pcolor->f[0]);
+               color.f[1] = util_format_srgb_to_linear_float(pcolor->f[1]);
+               color.f[2] = util_format_srgb_to_linear_float(pcolor->f[2]);
+            }
+            clear_color_no_rp(batch, zink_resource(fb->cbufs[i]->texture), &color,
+                              psurf->u.tex.level, psurf->u.tex.first_layer,
+                              psurf->u.tex.last_layer - psurf->u.tex.first_layer + 1);
+         }
+      }
+   }
+
+   if (buffers & PIPE_CLEAR_DEPTHSTENCIL && fb->zsbuf) {
+      if (fb->zsbuf->texture->target == PIPE_TEXTURE_3D && !check_3d_layers(fb->zsbuf)) {
+         clear_in_rp(pctx, buffers, scissor_state, pcolor, depth, stencil);
+         return;
+      }
+      VkImageAspectFlags aspects = 0;
+      if (buffers & PIPE_CLEAR_DEPTH)
+         aspects |= VK_IMAGE_ASPECT_DEPTH_BIT;
+      if (buffers & PIPE_CLEAR_STENCIL)
+         aspects |= VK_IMAGE_ASPECT_STENCIL_BIT;
+      clear_zs_no_rp(batch, zink_resource(fb->zsbuf->texture), aspects,
+                     depth, stencil, fb->zsbuf->u.tex.level, fb->zsbuf->u.tex.first_layer,
+                     fb->zsbuf->u.tex.last_layer - fb->zsbuf->u.tex.first_layer + 1);
+   }
+}
+
+static struct pipe_surface *
+create_clear_surface(struct pipe_context *pctx, struct pipe_resource *pres, unsigned level, const struct pipe_box *box)
+{
+   struct pipe_surface tmpl = {{0}};
+
+   tmpl.format = pres->format;
+   tmpl.u.tex.first_layer = box->z;
+   tmpl.u.tex.last_layer = box->z + box->depth - 1;
+   tmpl.u.tex.level = level;
+   return pctx->create_surface(pctx, pres, &tmpl);
+}
+
+void
+zink_clear_texture(struct pipe_context *pctx,
+                   struct pipe_resource *pres,
+                   unsigned level,
+                   const struct pipe_box *box,
+                   const void *data)
+{
+   struct zink_context *ctx = zink_context(pctx);
+   struct zink_resource *res = zink_resource(pres);
+   struct pipe_screen *pscreen = pctx->screen;
+   struct u_rect region = {box->x, box->x + box->width, box->y, box->y + box->height};
+   bool needs_rp = clear_needs_rp(ctx, pres->width0, pres->height0, &region) || ctx->conditional_render_enabled;
+   struct zink_batch *batch = zink_curr_batch(ctx);
+   struct pipe_surface *surf = NULL;
+
+   VkImageAspectFlags aspects = zink_resource_aspect_from_format(&res->base);
+   if (aspects & VK_IMAGE_ASPECT_COLOR_BIT) {
+      union pipe_color_union color;
+
+      util_format_unpack_rgba(pres->format, color.ui, data, 1);
+
+      if (pscreen->is_format_supported(pscreen, pres->format, pres->target, 0, 0,
+                                      PIPE_BIND_RENDER_TARGET) && !needs_rp && !batch->rp) {
+         clear_color_no_rp(batch, res, &color, level, box->z, box->depth);
+      } else {
+         surf = create_clear_surface(pctx, pres, level, box);
+         zink_blit_begin(ctx, ZINK_BLIT_SAVE_FB | ZINK_BLIT_SAVE_FS);
+         util_clear_render_target(pctx, surf, &color, box->x, box->y, box->width, box->height);
+      }
+   } else {
+      float depth = 0.0;
+      uint8_t stencil = 0;
+
+      if (aspects & VK_IMAGE_ASPECT_DEPTH_BIT)
+         util_format_unpack_z_float(pres->format, &depth, data, 1);
+
+      if (aspects & VK_IMAGE_ASPECT_STENCIL_BIT)
+         util_format_unpack_s_8uint(pres->format, &stencil, data, 1);
+
+      if (!needs_rp && !batch->rp)
+         clear_zs_no_rp(batch, res, aspects, depth, stencil, level, box->z, box->depth);
+      else {
+         unsigned flags = 0;
+         if (aspects & VK_IMAGE_ASPECT_DEPTH_BIT)
+            flags |= PIPE_CLEAR_DEPTH;
+         if (aspects & VK_IMAGE_ASPECT_STENCIL_BIT)
+            flags |= PIPE_CLEAR_STENCIL;
+         surf = create_clear_surface(pctx, pres, level, box);
+         zink_blit_begin(ctx, ZINK_BLIT_SAVE_FB | ZINK_BLIT_SAVE_FS);
+         util_blitter_clear_depth_stencil(ctx->blitter, surf, flags, depth, stencil, box->x, box->y, box->width, box->height);
+      }
+   }
+   pipe_surface_reference(&surf, NULL);
+}
diff --git a/src/gallium/drivers/zink/zink_compiler.c b/src/gallium/drivers/zink/zink_compiler.c
index b9d1b666b14..690d6e2173c 100644
--- a/src/gallium/drivers/zink/zink_compiler.c
+++ b/src/gallium/drivers/zink/zink_compiler.c
@@ -38,6 +38,30 @@
 
 #include "util/u_memory.h"
 
+static void
+create_vs_pushconst(nir_shader *nir)
+{
+   nir_variable *vs_pushconst;
+   /* create compatible layout for the ntv push constant loader */
+   struct glsl_struct_field *fields = ralloc_size(nir, 2 * sizeof(struct glsl_struct_field));
+   fields[0].type = glsl_array_type(glsl_uint_type(), 1, 0);
+   fields[0].name = ralloc_asprintf(nir, "draw_mode_is_indexed");
+   fields[0].offset = 0;
+   fields[1].type = glsl_array_type(glsl_uint_type(), 1, 0);
+   fields[1].name = ralloc_asprintf(nir, "draw_id");
+   fields[1].offset = 4;
+   vs_pushconst = nir_variable_create(nir, nir_var_shader_in,
+                                                 glsl_struct_type(fields, 2, "struct", false), "vs_pushconst");
+   vs_pushconst->data.location = INT_MAX; //doesn't really matter
+}
+
+static bool
+reads_draw_params(nir_shader *shader)
+{
+   return (shader->info.system_values_read & (1ull << SYSTEM_VALUE_BASE_VERTEX)) ||
+          (shader->info.system_values_read & (1ull << SYSTEM_VALUE_DRAW_ID));
+}
+
 static bool
 lower_discard_if_instr(nir_intrinsic_instr *instr, nir_builder *b)
 {
@@ -124,8 +148,76 @@ lower_discard_if(nir_shader *shader)
    return progress;
 }
 
+static bool
+lower_draw_params_instr(nir_intrinsic_instr *instr, nir_builder *b)
+{
+   if (instr->intrinsic != nir_intrinsic_load_base_vertex &&
+       instr->intrinsic != nir_intrinsic_load_draw_id)
+      return false;
+
+   if (instr->intrinsic == nir_intrinsic_load_base_vertex) {
+      b->cursor = nir_after_instr(&instr->instr);
+      nir_intrinsic_instr *load = nir_intrinsic_instr_create(b->shader, nir_intrinsic_load_push_constant);
+      load->src[0] = nir_src_for_ssa(nir_imm_int(b, offsetof(struct zink_push_constant, draw_mode_is_indexed)));
+      nir_intrinsic_set_base(load, offsetof(struct zink_push_constant, draw_mode_is_indexed));
+      nir_intrinsic_set_range(load, 4);
+      load->num_components = 1;
+      nir_ssa_dest_init(&load->instr, &load->dest, 1, 32, "draw_mode_is_indexed");
+      nir_builder_instr_insert(b, &load->instr);
+
+      nir_ssa_def *composite = nir_build_alu(b, nir_op_bcsel,
+                                             nir_build_alu(b, nir_op_ieq, &load->dest.ssa, nir_imm_int(b, 1), NULL, NULL),
+                                             &instr->dest.ssa,
+                                             nir_imm_int(b, 0),
+                                             NULL);
+
+      nir_ssa_def_rewrite_uses_after(&instr->dest.ssa, nir_src_for_ssa(composite), composite->parent_instr);
+   } else {
+      b->cursor = nir_before_instr(&instr->instr);
+      nir_intrinsic_instr *load = nir_intrinsic_instr_create(b->shader, nir_intrinsic_load_push_constant);
+      load->src[0] = nir_src_for_ssa(nir_imm_int(b, offsetof(struct zink_push_constant, draw_id)));
+      nir_intrinsic_set_base(load, offsetof(struct zink_push_constant, draw_id));
+      nir_intrinsic_set_range(load, 4);
+      load->num_components = 1;
+      nir_ssa_dest_init(&load->instr, &load->dest, 1, 32, "draw_id");
+      nir_builder_instr_insert(b, &load->instr);
+
+      nir_ssa_def_rewrite_uses(&instr->dest.ssa, nir_src_for_ssa(&load->dest.ssa));
+   }
+   return true;
+}
+
+static bool
+lower_draw_params(nir_shader *shader)
+{
+   bool progress = false;
+
+   if (shader->info.stage != MESA_SHADER_VERTEX)
+      return false;
+
+   if (!reads_draw_params(shader))
+      return false;
+
+   nir_foreach_function(function, shader) {
+      if (function->impl) {
+         nir_builder builder;
+         nir_builder_init(&builder, function->impl);
+         nir_foreach_block(block, function->impl) {
+            nir_foreach_instr_safe(instr, block) {
+               if (instr->type == nir_instr_type_intrinsic)
+                  progress |= lower_draw_params_instr(nir_instr_as_intrinsic(instr),
+                                                     &builder);
+            }
+         }
+
+         nir_metadata_preserve(function->impl, nir_metadata_dominance);
+      }
+   }
+
+   return progress;
+}
+
 static const struct nir_shader_compiler_options nir_options = {
-   .lower_all_io_to_temps = true,
    .lower_ffma = true,
    .lower_fdph = true,
    .lower_flrp32 = true,
@@ -136,6 +228,26 @@ static const struct nir_shader_compiler_options nir_options = {
    .lower_mul_high = true,
    .lower_rotate = true,
    .lower_uadd_carry = true,
+   .lower_to_scalar = true,
+   .lower_int64_options = ~0,
+   .lower_doubles_options = ~0 & ~nir_lower_fp64_full_software,
+};
+
+static const struct nir_shader_compiler_options softfp_nir_options = {
+   .lower_ffma = true,
+   .lower_fdph = true,
+   .lower_flrp32 = true,
+   .lower_fpow = true,
+   .lower_fsat = true,
+   .lower_extract_byte = true,
+   .lower_extract_word = true,
+   .lower_mul_high = true,
+   .lower_rotate = true,
+   .lower_uadd_carry = true,
+   .lower_to_scalar = true,
+   .use_scoped_barrier = true,
+   .lower_int64_options = ~0,
+   .lower_doubles_options = ~0,
 };
 
 const void *
@@ -144,6 +256,10 @@ zink_get_compiler_options(struct pipe_screen *screen,
                           enum pipe_shader_type shader)
 {
    assert(ir == PIPE_SHADER_IR_NIR);
+   struct zink_screen *zscreen = zink_screen(screen);
+   /* do we actually want this? fails a lot and not just from bugs I've added */
+   if (!zscreen->feats.shaderFloat64)
+      return &softfp_nir_options;
    return &nir_options;
 }
 
@@ -215,13 +331,67 @@ update_so_info(struct zink_shader *sh,
 }
 
 VkShaderModule
-zink_shader_compile(struct zink_screen *screen, struct zink_shader *zs)
+zink_shader_compile(struct zink_screen *screen, struct zink_shader *zs, struct zink_shader_key *key, unsigned char *shader_slot_map, unsigned char *shader_slots_reserved)
 {
    VkShaderModule mod = VK_NULL_HANDLE;
-   void *streamout = zs->streamout.so_info_slots ? &zs->streamout : NULL;
-   struct spirv_shader *spirv = nir_to_spirv(zs->nir, streamout);
+   void *streamout = NULL;
+   nir_shader *needs_free = NULL, *nir = zs->nir;
+   bool want_halfz = false;
+   nir_variable *vs_pushconst = NULL;
+   /* TODO: use a separate mem ctx here for ralloc */
+   if (zs->has_geometry_shader) {
+      if (zs->nir->info.stage == MESA_SHADER_GEOMETRY) {
+         streamout = &zs->streamout;
+         want_halfz = true;
+      }
+   } else if (zs->has_tess_shader) {
+      if (zs->nir->info.stage == MESA_SHADER_TESS_EVAL) {
+         streamout = &zs->streamout;
+         want_halfz = true;
+      }
+   } else {
+      streamout = &zs->streamout;
+      want_halfz = true;
+   }
+   /* only run this if we aren't already using halfz */
+   if (want_halfz && key && !zink_vs_key(key)->clip_halfz) {
+      needs_free = nir = nir_shader_clone(NULL, nir);
+      NIR_PASS_V(nir, nir_lower_clip_halfz);
+   }
+   if (!zs->streamout.so_info_slots)
+       streamout = NULL;
+   if (zs->nir->info.stage == MESA_SHADER_VERTEX && reads_draw_params(nir)) {
+      nir_foreach_shader_in_variable(var, nir) {
+         if (var->data.location == INT_MAX) {
+            vs_pushconst = var;
+            break;
+         }
+      }
+      /* this breaks validation, so we have to swizzle it when we know there's no more nir nannying us */
+      vs_pushconst->data.mode = nir_var_mem_push_const;
+   }
+   if (zs->nir->info.stage == MESA_SHADER_FRAGMENT) {
+      needs_free = nir = nir_shader_clone(NULL, nir);
+      NIR_PASS_V(nir, nir_lower_fragcolor, zink_fs_key(key)->nr_cbufs);
+      if (!zink_fs_key(key)->samples && nir->info.outputs_written & BITFIELD64_BIT(FRAG_RESULT_SAMPLE_MASK)) {
+         /* VK will always use gl_SampleMask[] values even if sample count is 0,
+          * so we need to skip this write here to mimic GL's behavior of ignoring it
+          */
+         nir_foreach_shader_out_variable(var, nir) {
+            if (var->data.location == FRAG_RESULT_SAMPLE_MASK)
+               var->data.mode = nir_var_shader_temp;
+         }
+         nir_fixup_deref_modes(nir);
+         NIR_PASS_V(nir, nir_remove_dead_variables, nir_var_shader_temp, NULL);
+         optimize_nir(nir);
+      }
+   }
+   struct spirv_shader *spirv = nir_to_spirv(nir, streamout, shader_slot_map, shader_slots_reserved, &screen->feats);
    assert(spirv);
 
+   if (vs_pushconst)
+      vs_pushconst->data.mode = nir_var_shader_in;
+
    if (zink_debug & ZINK_DEBUG_SPIRV) {
       char buf[256];
       static int i;
@@ -242,6 +412,9 @@ zink_shader_compile(struct zink_screen *screen, struct zink_shader *zs)
    if (vkCreateShaderModule(screen->dev, &smci, NULL, &mod) != VK_SUCCESS)
       mod = VK_NULL_HANDLE;
 
+   if (needs_free)
+      ralloc_free(needs_free);
+
    /* TODO: determine if there's any reason to cache spirv output? */
    free(spirv->words);
    free(spirv);
@@ -255,17 +428,28 @@ zink_shader_create(struct zink_screen *screen, struct nir_shader *nir,
    struct zink_shader *ret = CALLOC_STRUCT(zink_shader);
    bool have_psiz = false;
 
+   ret->shader_id = ++screen->shader_id;
    ret->programs = _mesa_pointer_set_create(NULL);
 
-   NIR_PASS_V(nir, nir_lower_uniforms_to_ubo, 1);
-   NIR_PASS_V(nir, nir_lower_clip_halfz);
    if (nir->info.stage == MESA_SHADER_VERTEX)
+      create_vs_pushconst(nir);
+
+   /* only do uniforms -> ubo if we have uniforms, otherwise we're just
+    * screwing with the bindings for no reason
+    */
+   if (nir->num_uniforms)
+      NIR_PASS_V(nir, nir_lower_uniforms_to_ubo, 16);
+   if (nir->info.stage < MESA_SHADER_FRAGMENT)
       have_psiz = check_psiz(nir);
+   if (nir->info.stage == MESA_SHADER_GEOMETRY)
+      NIR_PASS_V(nir, nir_lower_gs_intrinsics, true);
+   NIR_PASS_V(nir, lower_draw_params);
    NIR_PASS_V(nir, nir_lower_regs_to_ssa);
    optimize_nir(nir);
    NIR_PASS_V(nir, nir_remove_dead_variables, nir_var_function_temp, NULL);
    NIR_PASS_V(nir, lower_discard_if);
-   NIR_PASS_V(nir, nir_lower_fragcolor);
+   if (nir->info.num_ubos || nir->info.num_ssbos)
+      NIR_PASS_V(nir, nir_lower_dynamic_bo_access);
    NIR_PASS_V(nir, nir_convert_from_ssa, true);
 
    if (zink_debug & ZINK_DEBUG_NIR) {
@@ -275,37 +459,82 @@ zink_shader_create(struct zink_screen *screen, struct nir_shader *nir,
    }
 
    ret->num_bindings = 0;
-   nir_foreach_variable_with_modes(var, nir, nir_var_uniform |
-                                             nir_var_mem_ubo) {
-      if (var->data.mode == nir_var_mem_ubo) {
-         int binding = zink_binding(nir->info.stage,
-                                    VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER,
-                                    var->data.binding);
-         ret->bindings[ret->num_bindings].index = var->data.binding;
-         ret->bindings[ret->num_bindings].binding = binding;
-         ret->bindings[ret->num_bindings].type = VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER;
-         ret->num_bindings++;
-      } else {
-         assert(var->data.mode == nir_var_uniform);
-         if (glsl_type_is_array(var->type) &&
-             glsl_type_is_sampler(glsl_get_array_element(var->type))) {
-            for (int i = 0; i < glsl_get_length(var->type); ++i) {
+   uint32_t cur_ubo = 0;
+   /* UBO buffers are zero-indexed, but buffer 0 is always the one created by nir_lower_uniforms_to_ubo,
+    * which means there is no buffer 0 if there are no uniforms
+    */
+   int ubo_index = !nir->num_uniforms;
+   /* need to set up var->data.binding for UBOs, which means we need to start at
+    * the "first" UBO, which is at the end of the list
+    */
+   int ssbo_array_index = 0;
+   foreach_list_typed_reverse(nir_variable, var, node, &nir->variables) {
+      if (_nir_shader_variable_has_mode(var, nir_var_uniform |
+                                        nir_var_mem_ubo |
+                                        nir_var_mem_ssbo)) {
+         if (var->data.mode == nir_var_mem_ubo) {
+            /* ignore variables being accessed if they aren't the base of the UBO */
+            bool ubo_array = glsl_type_is_array(var->type) && glsl_type_is_interface(glsl_without_array(var->type));
+            if (var->data.location && !ubo_array && var->type != var->interface_type)
+               continue;
+            var->data.binding = cur_ubo;
+            /* if this is a ubo array, create a binding point for each array member:
+             * 
+               "For uniform blocks declared as arrays, each individual array element
+                corresponds to a separate buffer object backing one instance of the block."
+                - ARB_gpu_shader5
+
+               (also it's just easier)
+             */
+            for (unsigned i = 0; i < (ubo_array ? glsl_get_aoa_size(var->type) : 1); i++) {
+
                int binding = zink_binding(nir->info.stage,
-                                          VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER,
+                                          VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER,
+                                          cur_ubo++);
+               ret->bindings[ret->num_bindings].index = ubo_index++;
+               ret->bindings[ret->num_bindings].binding = binding;
+               ret->bindings[ret->num_bindings].type = VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER;
+               ret->bindings[ret->num_bindings].size = 1;
+               ret->num_bindings++;
+            }
+         } else if (var->data.mode == nir_var_mem_ssbo) {
+            /* same-ish mechanics as ubos */
+            bool bo_array = glsl_type_is_array(var->type) && glsl_type_is_interface(glsl_without_array(var->type));
+            if (var->data.location && !bo_array)
+               continue;
+            if (!var->data.explicit_binding) {
+               var->data.binding = ssbo_array_index;
+            }
+            for (unsigned i = 0; i < (bo_array ? glsl_get_aoa_size(var->type) : 1); i++) {
+               int binding = zink_binding(nir->info.stage,
+                                          VK_DESCRIPTOR_TYPE_STORAGE_BUFFER,
                                           var->data.binding + i);
-               ret->bindings[ret->num_bindings].index = var->data.binding + i;
+               if (strcmp(glsl_get_type_name(var->interface_type), "counters"))
+                  ret->bindings[ret->num_bindings].index = ssbo_array_index++;
+               else
+                  ret->bindings[ret->num_bindings].index = var->data.binding;
                ret->bindings[ret->num_bindings].binding = binding;
-               ret->bindings[ret->num_bindings].type = VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER;
+               ret->bindings[ret->num_bindings].type = VK_DESCRIPTOR_TYPE_STORAGE_BUFFER;
+               ret->bindings[ret->num_bindings].size = 1;
+               ret->num_bindings++;
+            }
+         } else {
+            assert(var->data.mode == nir_var_uniform);
+            const struct glsl_type *type = glsl_without_array(var->type);
+            if (glsl_type_is_sampler(type) || glsl_type_is_image(type)) {
+               VkDescriptorType vktype = glsl_type_is_image(type) ? zink_image_type(type) : zink_sampler_type(type);
+               int binding = zink_binding(nir->info.stage,
+                                          vktype,
+                                          var->data.binding);
+               ret->bindings[ret->num_bindings].index = var->data.binding;
+               ret->bindings[ret->num_bindings].binding = binding;
+               ret->bindings[ret->num_bindings].type = vktype;
+               if (glsl_type_is_array(var->type))
+                  ret->bindings[ret->num_bindings].size = glsl_get_aoa_size(var->type);
+               else
+                  ret->bindings[ret->num_bindings].size = 1;
                ret->num_bindings++;
             }
-         } else if (glsl_type_is_sampler(var->type)) {
-            int binding = zink_binding(nir->info.stage,
-                                       VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER,
-                                       var->data.binding);
-            ret->bindings[ret->num_bindings].index = var->data.binding;
-            ret->bindings[ret->num_bindings].binding = binding;
-            ret->bindings[ret->num_bindings].type = VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER;
-            ret->num_bindings++;
          }
       }
    }
@@ -326,13 +555,157 @@ zink_shader_free(struct zink_context *ctx, struct zink_shader *shader)
 {
    struct zink_screen *screen = zink_screen(ctx->base.screen);
    set_foreach(shader->programs, entry) {
-      struct zink_gfx_program *prog = (void*)entry->key;
-      _mesa_hash_table_remove_key(ctx->program_cache, prog->shaders);
-      prog->shaders[pipe_shader_type_from_mesa(shader->nir->info.stage)] = NULL;
-      zink_gfx_program_reference(screen, &prog, NULL);
+      if (shader->nir->info.stage == MESA_SHADER_COMPUTE) {
+         struct zink_compute_program *comp = (void*)entry->key;
+         _mesa_hash_table_remove_key(ctx->compute_program_cache, &comp->shader->shader_id);
+         comp->shader = NULL;
+         zink_compute_program_reference(screen, &comp, NULL);
+      } else {
+         struct zink_gfx_program *prog = (void*)entry->key;
+         _mesa_hash_table_remove_key(ctx->program_cache, prog->shaders);
+         prog->shaders[pipe_shader_type_from_mesa(shader->nir->info.stage)] = NULL;
+         if (shader->nir->info.stage == MESA_SHADER_TESS_EVAL && shader->generated)
+            /* automatically destroy generated tcs shaders when tes is destroyed */
+            zink_shader_free(ctx, shader->generated);
+         zink_gfx_program_reference(screen, &prog, NULL);
+      }
    }
    _mesa_set_destroy(shader->programs, NULL);
    free(shader->streamout.so_info_slots);
    ralloc_free(shader->nir);
    FREE(shader);
 }
+
+
+/* creating a passthrough tcs shader that's roughly:
+
+#version 150
+#extension GL_ARB_tessellation_shader : require
+
+in vec4 some_var[gl_MaxPatchVertices];
+out vec4 some_var_out;
+
+layout(push_constant) uniform tcsPushConstants {
+    layout(offset = 0) float TessLevelInner[2];
+    layout(offset = 8) float TessLevelOuter[4];
+} u_tcsPushConstants;
+layout(vertices = $vertices_per_patch) out;
+void main()
+{
+  gl_TessLevelInner = u_tcsPushConstants.TessLevelInner;
+  gl_TessLevelOuter = u_tcsPushConstants.TessLevelOuter;
+  some_var_out = some_var[gl_InvocationID];
+}
+
+*/
+struct zink_shader *
+zink_shader_tcs_create(struct zink_context *ctx, struct zink_shader *vs)
+{
+   unsigned vertices_per_patch = ctx->gfx_pipeline_state.vertices_per_patch;
+   struct zink_shader *ret = CALLOC_STRUCT(zink_shader);
+   ret->shader_id = 0; //special value for internal shaders
+   ret->programs = _mesa_pointer_set_create(NULL);
+
+   nir_shader *nir = nir_shader_create(NULL, MESA_SHADER_TESS_CTRL, &nir_options, NULL);
+   nir_function *fn = nir_function_create(nir, "main");
+   fn->is_entrypoint = true;
+   nir_function_impl *impl = nir_function_impl_create(fn);
+
+   nir_builder b;
+   nir_builder_init(&b, impl);
+   b.cursor = nir_before_block(nir_start_block(impl));
+
+   nir_intrinsic_instr *invocation_id = nir_intrinsic_instr_create(b.shader, nir_intrinsic_load_invocation_id);
+   nir_ssa_dest_init(&invocation_id->instr, &invocation_id->dest, 1, 32, "gl_InvocationID");
+   nir_builder_instr_insert(&b, &invocation_id->instr);
+
+   nir_foreach_shader_out_variable(var, vs->nir) {
+      const struct glsl_type *type = var->type;
+      const struct glsl_type *in_type = var->type;
+      const struct glsl_type *out_type = var->type;
+      char buf[1024];
+      snprintf(buf, sizeof(buf), "%s_out", var->name);
+      in_type = glsl_array_type(type, 32 /* MAX_PATCH_VERTICES */, 0);
+      out_type = glsl_array_type(type, vertices_per_patch, 0);
+
+      nir_variable *in = nir_variable_create(nir, nir_var_shader_in, in_type, var->name);
+      nir_variable *out = nir_variable_create(nir, nir_var_shader_out, out_type, buf);
+      out->data.location = in->data.location = var->data.location;
+      out->data.location_frac = in->data.location_frac = var->data.location_frac;
+
+      /* gl_in[] receives values from equivalent built-in output
+         variables written by the vertex shader (section 2.14.7).  Each array
+         element of gl_in[] is a structure holding values for a specific vertex of
+         the input patch.  The length of gl_in[] is equal to the
+         implementation-dependent maximum patch size (gl_MaxPatchVertices).
+         - ARB_tessellation_shader
+       */
+      for (unsigned i = 0; i < vertices_per_patch; i++) {
+         /* we need to load the invocation-specific value of the vertex output and then store it to the per-patch output */
+         nir_if *start_block = nir_push_if(&b, nir_ieq(&b, &invocation_id->dest.ssa, nir_imm_int(&b, i)));
+         nir_deref_instr *in_array_var = nir_build_deref_array(&b, nir_build_deref_var(&b, in), &invocation_id->dest.ssa);
+         nir_ssa_def *load = nir_load_deref(&b, in_array_var);
+         nir_deref_instr *out_array_var = nir_build_deref_array_imm(&b, nir_build_deref_var(&b, out), i);
+         nir_store_deref(&b, out_array_var, load, 0xff);
+         nir_pop_if(&b, start_block);
+      }
+   }
+   nir_variable *gl_TessLevelInner = nir_variable_create(nir, nir_var_shader_out, glsl_array_type(glsl_float_type(), 2, 0), "gl_TessLevelInner");
+   gl_TessLevelInner->data.location = VARYING_SLOT_TESS_LEVEL_INNER;
+   gl_TessLevelInner->data.patch = 1;
+   nir_variable *gl_TessLevelOuter = nir_variable_create(nir, nir_var_shader_out, glsl_array_type(glsl_float_type(), 4, 0), "gl_TessLevelOuter");
+   gl_TessLevelOuter->data.location = VARYING_SLOT_TESS_LEVEL_OUTER;
+   gl_TessLevelOuter->data.patch = 1;
+
+   /* hacks so we can size these right for now */
+   struct glsl_struct_field *fields = ralloc_size(nir, 2 * sizeof(struct glsl_struct_field));
+   fields[0].type = glsl_array_type(glsl_uint_type(), 2, 0);
+   fields[0].name = ralloc_asprintf(nir, "gl_TessLevelInner");
+   fields[0].offset = 0;
+   fields[1].type = glsl_array_type(glsl_uint_type(), 4, 0);
+   fields[1].name = ralloc_asprintf(nir, "gl_TessLevelOuter");
+   fields[1].offset = 8;
+   nir_variable *pushconst = nir_variable_create(nir, nir_var_shader_in,
+                                                 glsl_struct_type(fields, 2, "struct", false), "pushconst");
+   pushconst->data.location = VARYING_SLOT_VAR0;
+
+   nir_intrinsic_instr *load_inner = nir_intrinsic_instr_create(b.shader, nir_intrinsic_load_push_constant);
+   load_inner->src[0] = nir_src_for_ssa(nir_imm_int(&b, offsetof(struct zink_push_constant, default_inner_level)));
+   nir_intrinsic_set_base(load_inner, offsetof(struct zink_push_constant, default_inner_level));
+   nir_intrinsic_set_range(load_inner, 8);
+   load_inner->num_components = 2;
+   nir_ssa_dest_init(&load_inner->instr, &load_inner->dest, 2, 32, "TessLevelInner");
+   nir_builder_instr_insert(&b, &load_inner->instr);
+
+   nir_intrinsic_instr *load_outer = nir_intrinsic_instr_create(b.shader, nir_intrinsic_load_push_constant);
+   load_outer->src[0] = nir_src_for_ssa(nir_imm_int(&b, offsetof(struct zink_push_constant, default_outer_level)));
+   nir_intrinsic_set_base(load_outer, offsetof(struct zink_push_constant, default_outer_level));
+   nir_intrinsic_set_range(load_outer, 16);
+   load_outer->num_components = 4;
+   nir_ssa_dest_init(&load_outer->instr, &load_outer->dest, 4, 32, "TessLevelOuter");
+   nir_builder_instr_insert(&b, &load_outer->instr);
+
+   for (unsigned i = 0; i < 2; i++) {
+      nir_deref_instr *store_idx = nir_build_deref_array_imm(&b, nir_build_deref_var(&b, gl_TessLevelInner), i);
+      nir_store_deref(&b, store_idx, nir_channel(&b, &load_inner->dest.ssa, i), 0xff);
+   }
+   for (unsigned i = 0; i < 4; i++) {
+      nir_deref_instr *store_idx = nir_build_deref_array_imm(&b, nir_build_deref_var(&b, gl_TessLevelOuter), i);
+      nir_store_deref(&b, store_idx, nir_channel(&b, &load_outer->dest.ssa, i), 0xff);
+   }
+
+
+   nir->info.tess.tcs_vertices_out = vertices_per_patch;
+   nir_validate_shader(nir, "created");
+
+   NIR_PASS_V(nir, nir_lower_regs_to_ssa);
+   optimize_nir(nir);
+   NIR_PASS_V(nir, nir_remove_dead_variables, nir_var_function_temp, NULL);
+   NIR_PASS_V(nir, lower_discard_if);
+   NIR_PASS_V(nir, nir_convert_from_ssa, true);
+
+   pushconst->data.mode = nir_var_mem_push_const;
+   ret->nir = nir;
+   ret->is_generated = true;
+   return ret;
+}
diff --git a/src/gallium/drivers/zink/zink_compiler.h b/src/gallium/drivers/zink/zink_compiler.h
index 810be0163bd..160dbe58cf1 100644
--- a/src/gallium/drivers/zink/zink_compiler.h
+++ b/src/gallium/drivers/zink/zink_compiler.h
@@ -35,6 +35,7 @@
 struct pipe_screen;
 struct zink_context;
 struct zink_screen;
+struct zink_shader_key;
 struct zink_gfx_program;
 
 struct nir_shader_compiler_options;
@@ -58,6 +59,7 @@ struct nir_shader *
 zink_tgsi_to_nir(struct pipe_screen *screen, const struct tgsi_token *tokens);
 
 struct zink_shader {
+   unsigned shader_id;
    struct nir_shader *nir;
 
    struct zink_so_info streamout;
@@ -66,13 +68,21 @@ struct zink_shader {
       int index;
       int binding;
       VkDescriptorType type;
-   } bindings[PIPE_MAX_CONSTANT_BUFFERS + PIPE_MAX_SHADER_SAMPLER_VIEWS];
+      unsigned char size;
+   } bindings[PIPE_MAX_CONSTANT_BUFFERS + PIPE_MAX_SHADER_SAMPLER_VIEWS + PIPE_MAX_SHADER_BUFFERS + PIPE_MAX_SHADER_IMAGES];
    size_t num_bindings;
    struct set *programs;
+
+   bool has_tess_shader; // vertex shaders need to know if a tesseval shader exists
+   bool has_geometry_shader; // vertex shaders need to know if a geometry shader exists
+   union {
+      struct zink_shader *generated; // a generated shader that this shader "owns"
+      bool is_generated; // if this is a driver-created shader (e.g., tcs)
+   };
 };
 
 VkShaderModule
-zink_shader_compile(struct zink_screen *screen, struct zink_shader *zs);
+zink_shader_compile(struct zink_screen *screen, struct zink_shader *zs, struct zink_shader_key *key, unsigned char *shader_slot_map, unsigned char *shader_slots_reserved);
 
 struct zink_shader *
 zink_shader_create(struct zink_screen *screen, struct nir_shader *nir,
@@ -81,4 +91,7 @@ zink_shader_create(struct zink_screen *screen, struct nir_shader *nir,
 void
 zink_shader_free(struct zink_context *ctx, struct zink_shader *shader);
 
+struct zink_shader *
+zink_shader_tcs_create(struct zink_context *ctx, struct zink_shader *vs);
+
 #endif
diff --git a/src/gallium/drivers/zink/zink_context.c b/src/gallium/drivers/zink/zink_context.c
index 7d397a1dc60..49a0e4c2f7c 100644
--- a/src/gallium/drivers/zink/zink_context.c
+++ b/src/gallium/drivers/zink/zink_context.c
@@ -50,6 +50,9 @@
 #include "util/u_memory.h"
 #include "util/u_upload_mgr.h"
 
+#define XXH_INLINE_ALL
+#include "util/xxhash.h"
+
 static void
 zink_context_destroy(struct pipe_context *pctx)
 {
@@ -119,13 +122,27 @@ compare_op(enum pipe_compare_func op)
    unreachable("unexpected compare");
 }
 
+static inline bool
+wrap_needs_border_color(unsigned wrap)
+{
+   return wrap == PIPE_TEX_WRAP_CLAMP || wrap == PIPE_TEX_WRAP_CLAMP_TO_BORDER;
+}
+
+struct zink_sampler_state
+{
+   VkSampler sampler;
+   bool custom_border_color;
+};
+
 static void *
 zink_create_sampler_state(struct pipe_context *pctx,
                           const struct pipe_sampler_state *state)
 {
    struct zink_screen *screen = zink_screen(pctx->screen);
+   bool need_custom = false;
 
    VkSamplerCreateInfo sci = {};
+   VkSamplerCustomBorderColorCreateInfoEXT cbci = {};
    sci.sType = VK_STRUCTURE_TYPE_SAMPLER_CREATE_INFO;
    sci.magFilter = zink_filter(state->mag_img_filter);
    sci.minFilter = zink_filter(state->min_img_filter);
@@ -145,6 +162,10 @@ zink_create_sampler_state(struct pipe_context *pctx,
    sci.addressModeW = sampler_address_mode(state->wrap_r);
    sci.mipLodBias = state->lod_bias;
 
+   need_custom |= wrap_needs_border_color(state->wrap_s);
+   need_custom |= wrap_needs_border_color(state->wrap_t);
+   need_custom |= wrap_needs_border_color(state->wrap_r);
+
    if (state->compare_mode == PIPE_TEX_COMPARE_NONE)
       sci.compareOp = VK_COMPARE_OP_NEVER;
    else {
@@ -152,7 +173,19 @@ zink_create_sampler_state(struct pipe_context *pctx,
       sci.compareEnable = VK_TRUE;
    }
 
-   sci.borderColor = VK_BORDER_COLOR_FLOAT_TRANSPARENT_BLACK; // TODO
+   if (screen->have_EXT_custom_border_color && need_custom) {
+      cbci.sType = VK_STRUCTURE_TYPE_SAMPLER_CUSTOM_BORDER_COLOR_CREATE_INFO_EXT;
+      /* TODO: handle this? */
+      assert(screen->border_color_feats.customBorderColorWithoutFormat);
+      cbci.format = VK_FORMAT_UNDEFINED;
+      /* these are identical unions */
+      memcpy(&cbci.customBorderColor, &state->border_color, sizeof(union pipe_color_union));
+      sci.pNext = &cbci;
+      sci.borderColor = VK_BORDER_COLOR_INT_CUSTOM_EXT;
+      uint32_t check = p_atomic_inc_return(&screen->cur_custom_border_color_samplers);
+      assert(check <= screen->max_custom_border_color_samplers);
+   } else
+      sci.borderColor = VK_BORDER_COLOR_FLOAT_TRANSPARENT_BLACK; // TODO with custom shader if we're super interested?
    sci.unnormalizedCoordinates = !state->normalized_coords;
 
    if (state->max_anisotropy > 1) {
@@ -160,11 +193,11 @@ zink_create_sampler_state(struct pipe_context *pctx,
       sci.anisotropyEnable = VK_TRUE;
    }
 
-   VkSampler *sampler = CALLOC(1, sizeof(VkSampler));
+   struct zink_sampler_state *sampler = CALLOC(1, sizeof(struct zink_sampler_state));
    if (!sampler)
       return NULL;
 
-   if (vkCreateSampler(screen->dev, &sci, NULL, sampler) != VK_SUCCESS) {
+   if (vkCreateSampler(screen->dev, &sci, NULL, &sampler->sampler) != VK_SUCCESS) {
       FREE(sampler);
       return NULL;
    }
@@ -192,10 +225,13 @@ static void
 zink_delete_sampler_state(struct pipe_context *pctx,
                           void *sampler_state)
 {
+   struct zink_sampler_state *sampler = sampler_state;
    struct zink_batch *batch = zink_curr_batch(zink_context(pctx));
    util_dynarray_append(&batch->zombie_samplers, VkSampler,
-                        *(VkSampler *)sampler_state);
-   FREE(sampler_state);
+                        sampler->sampler);
+   if (sampler->custom_border_color)
+      p_atomic_dec(&zink_screen(pctx->screen)->cur_custom_border_color_samplers);
+   FREE(sampler);
 }
 
 
@@ -245,6 +281,23 @@ sampler_aspect_from_format(enum pipe_format fmt)
      return VK_IMAGE_ASPECT_COLOR_BIT;
 }
 
+static VkBufferView
+create_buffer_view(struct zink_screen *screen, struct zink_resource *res, enum pipe_format format, uint32_t offset, uint32_t range)
+{
+   VkBufferView view = VK_NULL_HANDLE;
+   VkBufferViewCreateInfo bvci = {};
+   bvci.sType = VK_STRUCTURE_TYPE_BUFFER_VIEW_CREATE_INFO;
+   bvci.buffer = res->buffer;
+   bvci.format = zink_get_format(screen, format);
+   assert(bvci.format);
+   bvci.offset = offset;
+   bvci.range = range;
+
+   if (vkCreateBufferView(screen->dev, &bvci, NULL, &view) == VK_SUCCESS)
+      return view;
+   return VK_NULL_HANDLE;
+}
+
 static struct pipe_sampler_view *
 zink_create_sampler_view(struct pipe_context *pctx, struct pipe_resource *pres,
                          const struct pipe_sampler_view *state)
@@ -252,6 +305,7 @@ zink_create_sampler_view(struct pipe_context *pctx, struct pipe_resource *pres,
    struct zink_screen *screen = zink_screen(pctx->screen);
    struct zink_resource *res = zink_resource(pres);
    struct zink_sampler_view *sampler_view = CALLOC_STRUCT(zink_sampler_view);
+   VkResult err;
 
    sampler_view->base = *state;
    sampler_view->base.texture = NULL;
@@ -259,28 +313,38 @@ zink_create_sampler_view(struct pipe_context *pctx, struct pipe_resource *pres,
    sampler_view->base.reference.count = 1;
    sampler_view->base.context = pctx;
 
-   VkImageViewCreateInfo ivci = {};
-   ivci.sType = VK_STRUCTURE_TYPE_IMAGE_VIEW_CREATE_INFO;
-   ivci.image = res->image;
-   ivci.viewType = image_view_type(state->target);
-   ivci.format = zink_get_format(screen, state->format);
-   ivci.components.r = component_mapping(state->swizzle_r);
-   ivci.components.g = component_mapping(state->swizzle_g);
-   ivci.components.b = component_mapping(state->swizzle_b);
-   ivci.components.a = component_mapping(state->swizzle_a);
-
-   ivci.subresourceRange.aspectMask = sampler_aspect_from_format(state->format);
-   ivci.subresourceRange.baseMipLevel = state->u.tex.first_level;
-   ivci.subresourceRange.baseArrayLayer = state->u.tex.first_layer;
-   ivci.subresourceRange.levelCount = state->u.tex.last_level - state->u.tex.first_level + 1;
-   ivci.subresourceRange.layerCount = state->u.tex.last_layer - state->u.tex.first_layer + 1;
-
-   VkResult err = vkCreateImageView(screen->dev, &ivci, NULL, &sampler_view->image_view);
+   if (state->target != PIPE_BUFFER) {
+      VkImageViewCreateInfo ivci = {};
+      ivci.sType = VK_STRUCTURE_TYPE_IMAGE_VIEW_CREATE_INFO;
+      ivci.image = res->image;
+      ivci.viewType = image_view_type(state->target);
+
+      ivci.subresourceRange.aspectMask = sampler_aspect_from_format(state->format);
+      /* samplers for stencil aspects of packed formats need to always use stencil type */
+      if (ivci.subresourceRange.aspectMask == VK_IMAGE_ASPECT_STENCIL_BIT)
+         ivci.format = VK_FORMAT_S8_UINT;
+      else
+         ivci.format = zink_get_format(screen, state->format);
+      assert(ivci.format);
+
+      ivci.components.r = component_mapping(state->swizzle_r);
+      ivci.components.g = component_mapping(state->swizzle_g);
+      ivci.components.b = component_mapping(state->swizzle_b);
+      ivci.components.a = component_mapping(state->swizzle_a);
+      ivci.subresourceRange.baseMipLevel = state->u.tex.first_level;
+      ivci.subresourceRange.baseArrayLayer = state->u.tex.first_layer;
+      ivci.subresourceRange.levelCount = state->u.tex.last_level - state->u.tex.first_level + 1;
+      ivci.subresourceRange.layerCount = state->u.tex.last_layer - state->u.tex.first_layer + 1;
+
+      err = vkCreateImageView(screen->dev, &ivci, NULL, &sampler_view->image_view);
+   } else {
+      sampler_view->buffer_view = create_buffer_view(screen, res, state->format, state->u.buf.offset, state->u.buf.size);
+      err = !sampler_view->buffer_view;
+   }
    if (err != VK_SUCCESS) {
       FREE(sampler_view);
       return NULL;
    }
-
    return &sampler_view->base;
 }
 
@@ -289,10 +353,91 @@ zink_sampler_view_destroy(struct pipe_context *pctx,
                           struct pipe_sampler_view *pview)
 {
    struct zink_sampler_view *view = zink_sampler_view(pview);
-   vkDestroyImageView(zink_screen(pctx->screen)->dev, view->image_view, NULL);
+   if (pview->texture->target == PIPE_BUFFER)
+      vkDestroyBufferView(zink_screen(pctx->screen)->dev, view->buffer_view, NULL);
+   else
+      vkDestroyImageView(zink_screen(pctx->screen)->dev, view->image_view, NULL);
+   /* FIXME: somehow we're missing adequate tracking on this and a regular unref
+    * destroys it too early
+    * see also piglit's texsubimage test
+    */
+   zink_batch_reference_resource_rw(zink_curr_batch(zink_context(pctx)), (void*)pview->texture, false);
+   pipe_resource_reference(&pview->texture, NULL);
    FREE(view);
 }
 
+static void
+zink_get_sample_position(struct pipe_context *ctx,
+                         unsigned sample_count,
+                         unsigned sample_index,
+                         float *out_value)
+{
+   /* TODO: handle this I guess */
+   assert(zink_screen(ctx->screen)->props.limits.standardSampleLocations);
+   /* from 26.4. Multisampling */
+   switch (sample_count) {
+   case 0:
+   case 1: {
+      float pos[][2] = { {0.5,0.5}, };
+      out_value[0] = pos[sample_index][0];
+      out_value[1] = pos[sample_index][1];
+      break;
+   }
+   case 2: {
+      float pos[][2] = { {0.75,0.75},
+                        {0.25,0.25}, };
+      out_value[0] = pos[sample_index][0];
+      out_value[1] = pos[sample_index][1];
+      break;
+   }
+   case 4: {
+      float pos[][2] = { {0.375, 0.125},
+                        {0.875, 0.375},
+                        {0.125, 0.625},
+                        {0.625, 0.875}, };
+      out_value[0] = pos[sample_index][0];
+      out_value[1] = pos[sample_index][1];
+      break;
+   }
+   case 8: {
+      float pos[][2] = { {0.5625, 0.3125},
+                        {0.4375, 0.6875},
+                        {0.8125, 0.5625},
+                        {0.3125, 0.1875},
+                        {0.1875, 0.8125},
+                        {0.0625, 0.4375},
+                        {0.6875, 0.9375},
+                        {0.9375, 0.0625}, };
+      out_value[0] = pos[sample_index][0];
+      out_value[1] = pos[sample_index][1];
+      break;
+   }
+   case 16: {
+      float pos[][2] = { {0.5625, 0.5625},
+                        {0.4375, 0.3125},
+                        {0.3125, 0.625},
+                        {0.75, 0.4375},
+                        {0.1875, 0.375},
+                        {0.625, 0.8125},
+                        {0.8125, 0.6875},
+                        {0.6875, 0.1875},
+                        {0.375, 0.875},
+                        {0.5, 0.0625},
+                        {0.25, 0.125},
+                        {0.125, 0.75},
+                        {0.0, 0.5},
+                        {0.9375, 0.25},
+                        {0.875, 0.9375},
+                        {0.0625, 0.0}, };
+      out_value[0] = pos[sample_index][0];
+      out_value[1] = pos[sample_index][1];
+      break;
+   }
+   default:
+      unreachable("unhandled sample count!");
+   }
+}
+
 static void
 zink_set_polygon_stipple(struct pipe_context *pctx,
                          const struct pipe_poly_stipple *ps)
@@ -310,16 +455,7 @@ zink_set_vertex_buffers(struct pipe_context *pctx,
    if (buffers) {
       for (int i = 0; i < num_buffers; ++i) {
          const struct pipe_vertex_buffer *vb = buffers + i;
-         struct zink_resource *res = zink_resource(vb->buffer.resource);
-
          ctx->gfx_pipeline_state.bindings[start_slot + i].stride = vb->stride;
-         if (res && res->needs_xfb_barrier) {
-            /* if we're binding a previously-used xfb buffer, we need cmd buffer synchronization to ensure
-             * that we use the right buffer data
-             */
-            pctx->flush(pctx, NULL, 0);
-            res->needs_xfb_barrier = false;
-         }
       }
       ctx->gfx_pipeline_state.hash = 0;
    }
@@ -336,19 +472,10 @@ zink_set_viewport_states(struct pipe_context *pctx,
 {
    struct zink_context *ctx = zink_context(pctx);
 
-   for (unsigned i = 0; i < num_viewports; ++i) {
-      VkViewport viewport = {
-         state[i].translate[0] - state[i].scale[0],
-         state[i].translate[1] - state[i].scale[1],
-         state[i].scale[0] * 2,
-         state[i].scale[1] * 2,
-         state[i].translate[2] - state[i].scale[2],
-         state[i].translate[2] + state[i].scale[2]
-      };
-      ctx->viewport_states[start_slot + i] = state[i];
-      ctx->viewports[start_slot + i] = viewport;
-   }
-   ctx->num_viewports = start_slot + num_viewports;
+   for (unsigned i = 0; i < num_viewports; ++i)
+      ctx->gfx_pipeline_state.viewport_states[start_slot + i] = state[i];
+   ctx->gfx_pipeline_state.num_viewports = start_slot + num_viewports;
+   ctx->gfx_pipeline_state.hash = 0;
 }
 
 static void
@@ -365,9 +492,10 @@ zink_set_scissor_states(struct pipe_context *pctx,
       scissor.offset.y = states[i].miny;
       scissor.extent.width = states[i].maxx - states[i].minx;
       scissor.extent.height = states[i].maxy - states[i].miny;
-      ctx->scissor_states[start_slot + i] = states[i];
-      ctx->scissors[start_slot + i] = scissor;
+      ctx->gfx_pipeline_state.scissor_states[start_slot + i] = states[i];
+      ctx->gfx_pipeline_state.scissors[start_slot + i] = scissor;
    }
+   ctx->gfx_pipeline_state.hash = 0;
 }
 
 static void
@@ -402,6 +530,73 @@ zink_set_constant_buffer(struct pipe_context *pctx,
    }
 }
 
+static void
+zink_set_shader_buffers(struct pipe_context *pctx,
+                        enum pipe_shader_type p_stage,
+                        unsigned start_slot, unsigned count,
+                        const struct pipe_shader_buffer *buffers,
+                        unsigned writable_bitmask)
+{
+   struct zink_context *ctx = zink_context(pctx);
+
+   unsigned modified_bits = u_bit_consecutive(start_slot, count);
+   ctx->writable_ssbos &= ~modified_bits;
+   ctx->writable_ssbos |= writable_bitmask << start_slot;
+
+   for (unsigned i = 0; i < count; i++) {
+      struct pipe_shader_buffer *ssbo = &ctx->ssbos[p_stage][start_slot + i];
+      if (buffers && buffers[i].buffer) {
+         struct zink_resource *res = (void *) buffers[i].buffer;
+         pipe_resource_reference(&ssbo->buffer, &res->base);
+         ssbo->buffer_offset = buffers[i].buffer_offset;
+         ssbo->buffer_size = MIN2(buffers[i].buffer_size, res->size - ssbo->buffer_offset);
+      } else {
+         pipe_resource_reference(&ssbo->buffer, NULL);
+         ssbo->buffer_offset = 0;
+         ssbo->buffer_size = 0;
+      }
+   }
+}
+
+static void
+zink_set_shader_images(struct pipe_context *pctx,
+                       enum pipe_shader_type p_stage,
+                       unsigned start_slot, unsigned count,
+                       const struct pipe_image_view *images)
+{
+   struct zink_context *ctx = zink_context(pctx);
+
+   for (unsigned i = 0; i < count; i++) {
+      struct zink_image_view *image_view = &ctx->image_views[p_stage][start_slot + i];
+      if (images && images[i].resource) {
+         struct zink_resource *res = (void *) images[i].resource;
+         util_copy_image_view(&image_view->base, images + i);
+         if (images[i].resource->target == PIPE_BUFFER) {
+            image_view->buffer_view = create_buffer_view(zink_screen(pctx->screen), res, images[i].format, images[i].u.buf.offset, images[i].u.buf.size);
+            assert(image_view->buffer_view);
+         } else {
+            struct pipe_surface tmpl = {};
+            tmpl.format = images[i].format;
+            tmpl.nr_samples = 1;
+            tmpl.u.tex.level = images[i].u.tex.level;
+            tmpl.u.tex.first_layer = images[i].u.tex.first_layer;
+            tmpl.u.tex.last_layer = images[i].u.tex.last_layer;
+            image_view->surface = (void*)pctx->create_surface(pctx, &res->base, &tmpl);
+            assert(image_view->surface);
+         }
+         pipe_resource_reference(&image_view->base.resource, &res->base);
+      } else if (image_view->base.resource) {
+         if (image_view->base.resource->target == PIPE_BUFFER)
+            vkDestroyBufferView(zink_screen(pctx->screen)->dev, image_view->buffer_view, NULL);
+         else
+            pipe_surface_reference((struct pipe_surface**)&image_view->surface, NULL);
+         pipe_resource_reference(&image_view->base.resource, NULL);
+         image_view->base.resource = NULL;
+         image_view->surface = NULL;
+      }
+   }
+}
+
 static void
 zink_set_sampler_views(struct pipe_context *pctx,
                        enum pipe_shader_type shader_type,
@@ -413,10 +608,10 @@ zink_set_sampler_views(struct pipe_context *pctx,
    assert(views);
    for (unsigned i = 0; i < num_views; ++i) {
       pipe_sampler_view_reference(
-         &ctx->image_views[shader_type][start_slot + i],
+         &ctx->sampler_views[shader_type][start_slot + i],
          views[i]);
    }
-   ctx->num_image_views[shader_type] = start_slot + num_views;
+   ctx->num_sampler_views[shader_type] = start_slot + num_views;
 }
 
 static void
@@ -433,6 +628,17 @@ zink_set_clip_state(struct pipe_context *pctx,
 {
 }
 
+static void
+zink_set_tess_state(struct pipe_context *pctx,
+                    const float default_outer_level[4],
+                    const float default_inner_level[2])
+{
+   struct zink_context *ctx = zink_context(pctx);
+   memcpy(&ctx->gfx_pipeline_state.default_inner_level, default_inner_level, sizeof(ctx->gfx_pipeline_state.default_inner_level));
+   memcpy(&ctx->gfx_pipeline_state.default_outer_level, default_outer_level, sizeof(ctx->gfx_pipeline_state.default_outer_level));
+   ctx->gfx_pipeline_state.hash = 0;
+}
+
 static struct zink_render_pass *
 get_render_pass(struct zink_context *ctx)
 {
@@ -444,7 +650,7 @@ get_render_pass(struct zink_context *ctx)
       struct pipe_surface *surf = fb->cbufs[i];
       if (surf) {
          state.rts[i].format = zink_get_format(screen, surf->format);
-         state.rts[i].samples = surf->nr_samples > 0 ? surf->nr_samples :
+         state.rts[i].samples = surf->texture->nr_samples > 0 ? surf->texture->nr_samples :
                                                        VK_SAMPLE_COUNT_1_BIT;
       } else {
          state.rts[i].format = VK_FORMAT_R8_UINT;
@@ -492,9 +698,9 @@ create_framebuffer(struct zink_context *ctx)
       state.attachments[state.num_attachments++] = zink_surface(psurf);
    }
 
-   state.width = ctx->fb_state.width;
-   state.height = ctx->fb_state.height;
-   state.layers = MAX2(ctx->fb_state.layers, 1);
+   state.width = MAX2(ctx->fb_state.width, 1);
+   state.height = MAX2(ctx->fb_state.height, 1);
+   state.layers = MAX2(util_framebuffer_get_num_layers(&ctx->fb_state), 1);
    state.samples = ctx->fb_state.samples;
 
    return zink_create_framebuffer(ctx, screen, &state);
@@ -510,15 +716,15 @@ framebuffer_state_buffer_barriers_setup(struct zink_context *ctx,
          surf = ctx->framebuffer->null_surface;
       struct zink_resource *res = zink_resource(surf->texture);
       if (res->layout != VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL)
-         zink_resource_barrier(batch->cmdbuf, res, res->aspect,
-                               VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL);
+         zink_resource_barrier(batch, res,
+                               VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL, 0);
    }
 
    if (state->zsbuf) {
       struct zink_resource *res = zink_resource(state->zsbuf->texture);
       if (res->layout != VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL)
-         zink_resource_barrier(batch->cmdbuf, res, res->aspect,
-                               VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL);
+         zink_resource_barrier(batch, res,
+                               VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL, 0);
    }
 }
 
@@ -550,6 +756,8 @@ zink_begin_render_pass(struct zink_context *ctx, struct zink_batch *batch)
 
    zink_render_pass_reference(screen, &batch->rp, ctx->gfx_pipeline_state.render_pass);
    zink_framebuffer_reference(screen, &batch->fb, ctx->framebuffer);
+   for (struct zink_surface **surf = (struct zink_surface **)batch->fb->surfaces; *surf; surf++)
+      zink_batch_reference_resource_rw(batch, zink_resource((*surf)->base.texture), true);
 
    vkCmdBeginRenderPass(batch->cmdbuf, &rpbi, VK_SUBPASS_CONTENTS_INLINE);
 }
@@ -601,6 +809,10 @@ zink_set_framebuffer_state(struct pipe_context *pctx,
    struct zink_context *ctx = zink_context(pctx);
    struct zink_screen *screen = zink_screen(pctx->screen);
 
+   /* fragcolor output needs to be redone */
+   if (ctx->fb_state.nr_cbufs != state->nr_cbufs)
+      ctx->dirty_shader_stages |= 1 << PIPE_SHADER_FRAGMENT;
+
    util_copy_framebuffer_state(&ctx->fb_state, state);
 
    struct zink_framebuffer *fb = ctx->framebuffer;
@@ -611,13 +823,19 @@ zink_set_framebuffer_state(struct pipe_context *pctx,
    zink_framebuffer_reference(screen, &ctx->framebuffer, fb);
    zink_render_pass_reference(screen, &ctx->gfx_pipeline_state.render_pass, fb->rp);
 
-   ctx->gfx_pipeline_state.rast_samples = MAX2(state->samples, 1);
+   uint8_t rast_samples = util_framebuffer_get_num_samples(state);
+   /* in vulkan, gl_SampleMask needs to be explicitly ignored for sampleCount == 1 */
+   if ((ctx->gfx_pipeline_state.rast_samples > 1) != (rast_samples > 1))
+      ctx->dirty_shader_stages |= 1 << PIPE_SHADER_FRAGMENT;
+   ctx->gfx_pipeline_state.rast_samples = rast_samples;
    ctx->gfx_pipeline_state.num_attachments = state->nr_cbufs;
    ctx->gfx_pipeline_state.hash = 0;
 
-   struct zink_batch *batch = zink_batch_no_rp(ctx);
+   /* need to start a new renderpass */
+   if (zink_curr_batch(ctx)->rp)
+      flush_batch(ctx);
 
-   framebuffer_state_buffer_barriers_setup(ctx, state, batch);
+   framebuffer_state_buffer_barriers_setup(ctx, &ctx->fb_state, zink_curr_batch(ctx));
 }
 
 static void
@@ -649,6 +867,7 @@ access_src_flags(VkImageLayout layout)
    case VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL:
       return VK_ACCESS_DEPTH_STENCIL_ATTACHMENT_READ_BIT;
 
+   case VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL:
    case VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL:
       return VK_ACCESS_SHADER_READ_BIT;
 
@@ -682,6 +901,9 @@ access_dst_flags(VkImageLayout layout)
    case VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL:
       return VK_ACCESS_TRANSFER_READ_BIT;
 
+   case VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL:
+      return VK_ACCESS_SHADER_READ_BIT;
+
    case VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL:
       return VK_ACCESS_TRANSFER_WRITE_BIT;
 
@@ -704,35 +926,32 @@ pipeline_dst_stage(VkImageLayout layout)
    case VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL:
       return VK_PIPELINE_STAGE_TRANSFER_BIT;
 
+   case VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL:
+      return VK_PIPELINE_STAGE_FRAGMENT_SHADER_BIT;
+
    default:
       return VK_PIPELINE_STAGE_BOTTOM_OF_PIPE_BIT;
    }
 }
 
-static VkPipelineStageFlags
-pipeline_src_stage(VkImageLayout layout)
+bool
+zink_resource_image_needs_barrier(struct zink_resource *res, VkImageLayout new_layout, VkPipelineStageFlags pipeline)
 {
-   switch (layout) {
-   case VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL:
-      return VK_PIPELINE_STAGE_COLOR_ATTACHMENT_OUTPUT_BIT;
-   case VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL:
-      return VK_PIPELINE_STAGE_EARLY_FRAGMENT_TESTS_BIT;
-
-   case VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL:
-      return VK_PIPELINE_STAGE_TRANSFER_BIT;
-   case VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL:
-      return VK_PIPELINE_STAGE_TRANSFER_BIT;
-
-   default:
-      return VK_PIPELINE_STAGE_TOP_OF_PIPE_BIT;
-   }
+   if (!pipeline)
+      pipeline = pipeline_dst_stage(new_layout);
+   return res->layout != new_layout || (res->access_stage & pipeline) != pipeline;
 }
 
-
 void
-zink_resource_barrier(VkCommandBuffer cmdbuf, struct zink_resource *res,
-                      VkImageAspectFlags aspect, VkImageLayout new_layout)
+zink_resource_barrier(struct zink_batch *batch, struct zink_resource *res,
+                      VkImageLayout new_layout, VkPipelineStageFlags pipeline)
 {
+   assert(!batch->rp);
+   VkImageAspectFlags aspect = zink_resource_aspect_from_format(&res->base);
+   if (!pipeline)
+      pipeline = pipeline_dst_stage(new_layout);
+   if (!zink_resource_image_needs_barrier(res, new_layout, pipeline))
+      return;
    VkImageSubresourceRange isr = {
       aspect,
       0, VK_REMAINING_MIP_LEVELS,
@@ -752,9 +971,9 @@ zink_resource_barrier(VkCommandBuffer cmdbuf, struct zink_resource *res,
       isr
    };
    vkCmdPipelineBarrier(
-      cmdbuf,
-      pipeline_src_stage(res->layout),
-      pipeline_dst_stage(new_layout),
+      batch->cmdbuf,
+      res->access_stage ?: VK_PIPELINE_STAGE_TOP_OF_PIPE_BIT,
+      pipeline,
       0,
       0, NULL,
       0, NULL,
@@ -762,67 +981,99 @@ zink_resource_barrier(VkCommandBuffer cmdbuf, struct zink_resource *res,
    );
 
    res->layout = new_layout;
+   res->access_stage = pipeline;
 }
 
-static void
-zink_clear(struct pipe_context *pctx,
-           unsigned buffers,
-           const struct pipe_scissor_state *scissor_state,
-           const union pipe_color_union *pcolor,
-           double depth, unsigned stencil)
-{
-   struct zink_context *ctx = zink_context(pctx);
-   struct pipe_framebuffer_state *fb = &ctx->fb_state;
-
-   /* FIXME: this is very inefficient; if no renderpass has been started yet,
-    * we should record the clear if it's full-screen, and apply it as we
-    * start the render-pass. Otherwise we can do a partial out-of-renderpass
-    * clear.
-    */
-   struct zink_batch *batch = zink_batch_rp(ctx);
 
-   VkClearAttachment attachments[1 + PIPE_MAX_COLOR_BUFS];
-   int num_attachments = 0;
+VkPipelineStageFlags
+zink_pipeline_flags_from_stage(VkShaderStageFlagBits stage)
+{
+   switch (stage) {
+   case VK_SHADER_STAGE_VERTEX_BIT:
+      return VK_PIPELINE_STAGE_VERTEX_SHADER_BIT;
+   case VK_SHADER_STAGE_FRAGMENT_BIT:
+      return VK_PIPELINE_STAGE_FRAGMENT_SHADER_BIT;
+   case VK_SHADER_STAGE_GEOMETRY_BIT:
+      return VK_PIPELINE_STAGE_GEOMETRY_SHADER_BIT;
+   case VK_SHADER_STAGE_TESSELLATION_CONTROL_BIT:
+      return VK_PIPELINE_STAGE_TESSELLATION_CONTROL_SHADER_BIT;
+   case VK_SHADER_STAGE_TESSELLATION_EVALUATION_BIT:
+      return VK_PIPELINE_STAGE_TESSELLATION_EVALUATION_SHADER_BIT;
+   case VK_SHADER_STAGE_COMPUTE_BIT:
+      return VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT;
+   default:
+      unreachable("unknown shader stage bit");
+   }
+}
 
-   if (buffers & PIPE_CLEAR_COLOR) {
-      VkClearColorValue color;
-      color.float32[0] = pcolor->f[0];
-      color.float32[1] = pcolor->f[1];
-      color.float32[2] = pcolor->f[2];
-      color.float32[3] = pcolor->f[3];
+static VkPipelineStageFlags
+pipeline_access_stage(VkAccessFlags flags)
+{
+   switch (flags) {
+   case VK_ACCESS_UNIFORM_READ_BIT:
+   case VK_ACCESS_SHADER_READ_BIT:
+   case VK_ACCESS_SHADER_WRITE_BIT:
+      return VK_PIPELINE_STAGE_TASK_SHADER_BIT_NV |
+             VK_PIPELINE_STAGE_MESH_SHADER_BIT_NV |
+             VK_PIPELINE_STAGE_RAY_TRACING_SHADER_BIT_KHR |
+             VK_PIPELINE_STAGE_VERTEX_SHADER_BIT |
+             VK_PIPELINE_STAGE_TESSELLATION_CONTROL_SHADER_BIT |
+             VK_PIPELINE_STAGE_TESSELLATION_EVALUATION_SHADER_BIT |
+             VK_PIPELINE_STAGE_GEOMETRY_SHADER_BIT |
+             VK_PIPELINE_STAGE_FRAGMENT_SHADER_BIT |
+             VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT;
+   default:
+      return VK_PIPELINE_STAGE_TRANSFER_BIT;
+   }
+}
 
-      for (unsigned i = 0; i < fb->nr_cbufs; i++) {
-         if (!(buffers & (PIPE_CLEAR_COLOR0 << i)) || !fb->cbufs[i])
-            continue;
+bool
+zink_resource_buffer_needs_barrier(struct zink_resource *res, VkAccessFlags flags, VkPipelineStageFlags pipeline)
+{
+   if (!pipeline)
+      pipeline = pipeline_access_stage(flags);
+   return res->access != flags || (res->access_stage & pipeline) != pipeline;
+}
 
-         attachments[num_attachments].aspectMask = VK_IMAGE_ASPECT_COLOR_BIT;
-         attachments[num_attachments].colorAttachment = i;
-         attachments[num_attachments].clearValue.color = color;
-         ++num_attachments;
-      }
-   }
+void
+zink_resource_buffer_barrier(struct zink_batch *batch, struct zink_resource *res, VkAccessFlags flags, VkPipelineStageFlags pipeline)
+{
+   assert(!batch->rp);
+   if (!pipeline)
+      pipeline = pipeline_access_stage(flags);
+   if (!zink_resource_buffer_needs_barrier(res, flags, pipeline))
+      return;
+   VkBufferMemoryBarrier bmb = {
+      VK_STRUCTURE_TYPE_BUFFER_MEMORY_BARRIER,
+      NULL,
+      res->access,
+      flags,
+      VK_QUEUE_FAMILY_IGNORED,
+      VK_QUEUE_FAMILY_IGNORED,
+      res->buffer,
+      res->offset,
+      res->size
+   };
 
-   if (buffers & PIPE_CLEAR_DEPTHSTENCIL && fb->zsbuf) {
-      VkImageAspectFlags aspect = 0;
-      if (buffers & PIPE_CLEAR_DEPTH)
-         aspect |= VK_IMAGE_ASPECT_DEPTH_BIT;
-      if (buffers & PIPE_CLEAR_STENCIL)
-         aspect |= VK_IMAGE_ASPECT_STENCIL_BIT;
-
-      attachments[num_attachments].aspectMask = aspect;
-      attachments[num_attachments].clearValue.depthStencil.depth = depth;
-      attachments[num_attachments].clearValue.depthStencil.stencil = stencil;
-      ++num_attachments;
-   }
+   vkCmdPipelineBarrier(
+      batch->cmdbuf,
+      res->access_stage ?: pipeline_access_stage(res->access),
+      pipeline,
+      0,
+      0, NULL,
+      1, &bmb,
+      0, NULL
+   );
+   res->access = flags;
+   res->access_stage = pipeline;
+}
 
-   VkClearRect cr;
-   cr.rect.offset.x = 0;
-   cr.rect.offset.y = 0;
-   cr.rect.extent.width = fb->width;
-   cr.rect.extent.height = fb->height;
-   cr.baseArrayLayer = 0;
-   cr.layerCount = util_framebuffer_get_num_layers(fb);
-   vkCmdClearAttachments(batch->cmdbuf, num_attachments, attachments, 1, &cr);
+bool
+zink_resource_needs_barrier(struct zink_resource *res, unsigned flags, VkPipelineStageFlags pipeline)
+{
+   if (res->base.target == PIPE_BUFFER)
+      return zink_resource_buffer_needs_barrier(res, flags, pipeline);
+   return zink_resource_image_needs_barrier(res, flags, pipeline);
 }
 
 VkShaderStageFlagBits
@@ -842,13 +1093,27 @@ zink_shader_stage(enum pipe_shader_type type)
 static uint32_t
 hash_gfx_program(const void *key)
 {
-   return _mesa_hash_data(key, sizeof(struct zink_shader *) * (ZINK_SHADER_COUNT));
+   const struct zink_shader **shaders = (void*)key;
+   uint32_t hash = 0;
+   unsigned zero = 0;
+   /* pointers can be recycled, so we need to check the shader ids */
+   for (unsigned i = 0; i < ZINK_SHADER_COUNT; i++)
+      hash = XXH32(shaders[i] ? &shaders[i]->shader_id : &zero, sizeof(unsigned), hash);
+   return hash;
 }
 
 static bool
 equals_gfx_program(const void *a, const void *b)
 {
-   return memcmp(a, b, sizeof(struct zink_shader *) * (ZINK_SHADER_COUNT)) == 0;
+   const struct zink_shader **left = (void*)a, **right = (void*)b;
+   /* if any shaders are set/unset or shader ids don't match then these aren't equal */
+   for (unsigned i = 0; i < ZINK_SHADER_COUNT; i++) {
+      if (!!left[i] != !!right[i])
+         return false;
+      if (left[i] && right[i] && left[i]->shader_id != right[i]->shader_id)
+         return false;
+   }
+   return true;
 }
 
 static uint32_t
@@ -895,6 +1160,196 @@ zink_flush(struct pipe_context *pctx,
                                  PIPE_TIMEOUT_INFINITE);
 }
 
+void
+zink_fence_wait(struct pipe_context *pctx)
+{
+   struct pipe_fence_handle *fence = NULL;
+   pctx->flush(pctx, &fence, PIPE_FLUSH_HINT_FINISH);
+   if (fence) {
+      pctx->screen->fence_finish(pctx->screen, NULL, fence,
+                                 PIPE_TIMEOUT_INFINITE);
+      pctx->screen->fence_reference(pctx->screen, &fence, NULL);
+   }
+}
+
+void
+zink_wait_on_batch(struct zink_context *ctx, int batch_id)
+{
+   if (batch_id >= 0) {
+      struct zink_batch *batch = batch_id == ZINK_COMPUTE_BATCH_ID ? &ctx->compute_batch : &ctx->batches[batch_id];
+      if (batch != zink_curr_batch(ctx)) {
+         if (!batch->fence) { // this is the compute batch
+            zink_end_batch(ctx, batch);
+            zink_start_batch(ctx, batch);
+         } else
+            ctx->base.screen->fence_finish(ctx->base.screen, NULL, (struct pipe_fence_handle*)batch->fence,
+                                       PIPE_TIMEOUT_INFINITE);
+         return;
+      }
+   }
+   zink_fence_wait(&ctx->base);
+}
+
+static void
+zink_texture_barrier(struct pipe_context *pctx, unsigned flags)
+{
+   struct zink_context *ctx = zink_context(pctx);
+   /* TODO: if we ever start using fully parallelized batches, this probably needs a stall */
+   if (zink_curr_batch(ctx)->has_draw)
+      pctx->flush(pctx, NULL, 0);
+   if (ctx->compute_batch.has_draw) {
+      zink_end_batch(ctx, &ctx->compute_batch);
+      zink_start_batch(ctx, &ctx->compute_batch);
+   }
+}
+
+static void
+zink_memory_barrier(struct pipe_context *pctx, unsigned flags)
+{
+   struct zink_context *ctx = zink_context(pctx);
+   VkAccessFlags sflags = 0;
+   VkAccessFlags dflags = 0;
+   VkPipelineStageFlags src = 0;
+   VkPipelineStageFlags dst = 0;
+
+   VkPipelineStageFlags all_flags = VK_PIPELINE_STAGE_VERTEX_SHADER_BIT |
+                                    VK_PIPELINE_STAGE_TESSELLATION_CONTROL_SHADER_BIT |
+                                    VK_PIPELINE_STAGE_TESSELLATION_EVALUATION_SHADER_BIT |
+                                    VK_PIPELINE_STAGE_GEOMETRY_SHADER_BIT |
+                                    VK_PIPELINE_STAGE_FRAGMENT_SHADER_BIT |
+                                    VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT;
+
+   if (flags == PIPE_BARRIER_ALL) {
+      sflags = dflags = VK_ACCESS_MEMORY_READ_BIT | VK_ACCESS_MEMORY_WRITE_BIT;
+      src = dst = VK_PIPELINE_STAGE_ALL_COMMANDS_BIT;
+   } else {
+      while (flags) {
+         unsigned flag = u_bit_scan(&flags);
+         
+         switch (1 << flag) {
+         case PIPE_BARRIER_MAPPED_BUFFER:
+            sflags |= VK_ACCESS_SHADER_WRITE_BIT;
+            dflags |= VK_ACCESS_TRANSFER_READ_BIT;
+            break;
+         case PIPE_BARRIER_SHADER_BUFFER:
+            sflags |= VK_ACCESS_SHADER_WRITE_BIT;
+            dflags |= VK_ACCESS_SHADER_READ_BIT;
+            src |= all_flags;
+            dst |= all_flags;
+            break;
+         case PIPE_BARRIER_QUERY_BUFFER:
+            sflags |= VK_ACCESS_SHADER_WRITE_BIT | VK_ACCESS_TRANSFER_WRITE_BIT;
+            dflags |= VK_ACCESS_TRANSFER_WRITE_BIT | VK_ACCESS_SHADER_WRITE_BIT |
+                      VK_ACCESS_TRANSFER_READ_BIT | VK_ACCESS_SHADER_READ_BIT;
+            break;
+         case PIPE_BARRIER_VERTEX_BUFFER:
+            sflags |= VK_ACCESS_VERTEX_ATTRIBUTE_READ_BIT;
+            dflags |= VK_ACCESS_VERTEX_ATTRIBUTE_READ_BIT;
+            src |= VK_PIPELINE_STAGE_VERTEX_INPUT_BIT;
+            dst |= VK_PIPELINE_STAGE_VERTEX_INPUT_BIT;
+            break;
+         case PIPE_BARRIER_INDEX_BUFFER:
+            sflags |= VK_ACCESS_INDEX_READ_BIT;
+            dflags |= VK_ACCESS_INDEX_READ_BIT;
+            src |= VK_PIPELINE_STAGE_VERTEX_INPUT_BIT;
+            dst |= VK_PIPELINE_STAGE_VERTEX_INPUT_BIT;
+            break;
+         case PIPE_BARRIER_CONSTANT_BUFFER:
+            sflags |= VK_ACCESS_UNIFORM_READ_BIT;
+            dflags |= VK_ACCESS_UNIFORM_READ_BIT;
+            src |= all_flags;
+            dst |= all_flags;
+            break;
+         case PIPE_BARRIER_INDIRECT_BUFFER:
+            sflags |= VK_ACCESS_INDIRECT_COMMAND_READ_BIT;
+            dflags |= VK_ACCESS_INDIRECT_COMMAND_READ_BIT;
+            src |= VK_PIPELINE_STAGE_DRAW_INDIRECT_BIT;
+            dst |= VK_PIPELINE_STAGE_DRAW_INDIRECT_BIT;
+            break;
+         case PIPE_BARRIER_TEXTURE:
+            sflags |= VK_ACCESS_TRANSFER_WRITE_BIT | VK_ACCESS_SHADER_WRITE_BIT;
+            dflags |= VK_ACCESS_TRANSFER_READ_BIT | VK_ACCESS_SHADER_READ_BIT;
+            src |= all_flags;
+            dst |= all_flags;
+            break;
+         case PIPE_BARRIER_IMAGE:
+            sflags |= VK_ACCESS_TRANSFER_WRITE_BIT | VK_ACCESS_SHADER_WRITE_BIT;
+            dflags |= VK_ACCESS_TRANSFER_READ_BIT | VK_ACCESS_SHADER_READ_BIT | VK_ACCESS_UNIFORM_READ_BIT;
+            src |= all_flags;
+            dst |= all_flags;
+            break;
+         case PIPE_BARRIER_FRAMEBUFFER:
+            sflags |= VK_ACCESS_COLOR_ATTACHMENT_WRITE_BIT |
+                      VK_ACCESS_DEPTH_STENCIL_ATTACHMENT_WRITE_BIT;
+            dflags |= VK_ACCESS_INPUT_ATTACHMENT_READ_BIT |
+                      VK_ACCESS_DEPTH_STENCIL_ATTACHMENT_READ_BIT;
+            src |= VK_PIPELINE_STAGE_EARLY_FRAGMENT_TESTS_BIT;
+            dst |= VK_PIPELINE_STAGE_COLOR_ATTACHMENT_OUTPUT_BIT;
+            break;
+         case PIPE_BARRIER_STREAMOUT_BUFFER:
+            sflags |= VK_ACCESS_TRANSFORM_FEEDBACK_WRITE_BIT_EXT;
+            dflags |= VK_ACCESS_VERTEX_ATTRIBUTE_READ_BIT;
+            src |= VK_PIPELINE_STAGE_TRANSFORM_FEEDBACK_BIT_EXT;
+            dst |= VK_PIPELINE_STAGE_VERTEX_INPUT_BIT;
+            break;
+         case PIPE_BARRIER_GLOBAL_BUFFER:
+            debug_printf("zink: unhandled barrier flag %u\n", flag);
+            break;
+         case PIPE_BARRIER_UPDATE_BUFFER:
+            sflags |= VK_ACCESS_TRANSFER_WRITE_BIT;
+            dflags |= VK_ACCESS_TRANSFER_READ_BIT;
+            src |= VK_PIPELINE_STAGE_TRANSFER_BIT;
+            dst |= VK_PIPELINE_STAGE_TRANSFER_BIT;
+            break;
+         case PIPE_BARRIER_UPDATE_TEXTURE:
+            sflags |= VK_ACCESS_TRANSFER_WRITE_BIT;
+            dflags |= VK_ACCESS_TRANSFER_READ_BIT;
+            src |= VK_PIPELINE_STAGE_TRANSFER_BIT;
+            dst |= VK_PIPELINE_STAGE_TRANSFER_BIT;
+            break;
+         }
+      }
+   }
+   VkMemoryBarrier b = {};
+   b.sType = VK_STRUCTURE_TYPE_MEMORY_BARRIER;
+   /* TODO: these are all probably wrong */
+   b.srcAccessMask = sflags;
+   b.dstAccessMask = dflags;
+
+   struct zink_batch *batch = zink_curr_batch(ctx);
+   if (batch->has_draw) {
+      /* TODO: figure out self-referencing renderpass dependency and remove this
+       * 
+       * can't barrier during renderpass without inlining flush_batch() here
+       */
+
+      if (batch->rp)
+         vkCmdEndRenderPass(batch->cmdbuf);
+
+      /* this should be the only call needed */
+      vkCmdPipelineBarrier(batch->cmdbuf, src, dst, 0, 0, &b, 0, NULL, 0, NULL);
+      zink_end_batch(ctx, batch);
+      if (batch->rp)
+         ctx->base.screen->fence_finish(ctx->base.screen, NULL, (void*)batch->fence,
+                                    PIPE_TIMEOUT_INFINITE);
+
+      ctx->curr_batch++;
+      if (ctx->curr_batch == ARRAY_SIZE(ctx->batches))
+         ctx->curr_batch = 0;
+
+      zink_start_batch(ctx, zink_curr_batch(ctx));
+   }
+   batch = &ctx->compute_batch;
+   if (batch->has_draw) {
+      /* this should be the only call needed */
+      vkCmdPipelineBarrier(batch->cmdbuf, src, dst, 0, 0, &b, 0, NULL, 0, NULL);
+      zink_end_batch(ctx, batch);
+      ctx->base.screen->fence_finish(ctx->base.screen, NULL, (void*)batch->fence,
+                                 PIPE_TIMEOUT_INFINITE);
+      zink_start_batch(ctx, batch);
+   }
+}
+
 static void
 zink_flush_resource(struct pipe_context *pipe,
                     struct pipe_resource *resource)
@@ -913,8 +1368,19 @@ zink_resource_copy_region(struct pipe_context *pctx,
    struct zink_context *ctx = zink_context(pctx);
    if (dst->base.target != PIPE_BUFFER && src->base.target != PIPE_BUFFER) {
       VkImageCopy region = {};
-
-      region.srcSubresource.aspectMask = src->aspect;
+      if (util_format_get_num_planes(src->base.format) == 1 &&
+          util_format_get_num_planes(dst->base.format) == 1) {
+      /* If neither the calling command’s srcImage nor the calling command’s dstImage
+       * has a multi-planar image format then the aspectMask member of srcSubresource
+       * and dstSubresource must match
+       *
+       * -VkImageCopy spec
+       */
+         assert(zink_resource_aspect_from_format(&src->base) == zink_resource_aspect_from_format(&dst->base));
+      } else
+         unreachable("planar formats not yet handled");
+
+      region.srcSubresource.aspectMask = zink_resource_aspect_from_format(&src->base);
       region.srcSubresource.mipLevel = src_level;
       region.srcSubresource.layerCount = 1;
       if (src->base.array_size > 1) {
@@ -930,7 +1396,7 @@ zink_resource_copy_region(struct pipe_context *pctx,
       region.srcOffset.x = src_box->x;
       region.srcOffset.y = src_box->y;
 
-      region.dstSubresource.aspectMask = dst->aspect;
+      region.dstSubresource.aspectMask = zink_resource_aspect_from_format(&dst->base);
       region.dstSubresource.mipLevel = dst_level;
       if (dst->base.array_size > 1) {
          region.dstSubresource.baseArrayLayer = dstz;
@@ -946,19 +1412,10 @@ zink_resource_copy_region(struct pipe_context *pctx,
       region.extent.height = src_box->height;
 
       struct zink_batch *batch = zink_batch_no_rp(ctx);
-      zink_batch_reference_resoure(batch, src);
-      zink_batch_reference_resoure(batch, dst);
-
-      if (src->layout != VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL) {
-         zink_resource_barrier(batch->cmdbuf, src, src->aspect,
-                               VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL);
-      }
-
-      if (dst->layout != VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL) {
-         zink_resource_barrier(batch->cmdbuf, dst, dst->aspect,
-                               VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL);
-      }
+      zink_batch_reference_resource_rw(batch, src, false);
+      zink_batch_reference_resource_rw(batch, dst, true);
 
+      zink_resource_setup_layouts(batch, src, dst);
       vkCmdCopyImage(batch->cmdbuf, src->image, src->layout,
                      dst->image, dst->layout,
                      1, &region);
@@ -970,8 +1427,8 @@ zink_resource_copy_region(struct pipe_context *pctx,
       region.size = src_box->width;
 
       struct zink_batch *batch = zink_batch_no_rp(ctx);
-      zink_batch_reference_resoure(batch, src);
-      zink_batch_reference_resoure(batch, dst);
+      zink_batch_reference_resource_rw(batch, src, false);
+      zink_batch_reference_resource_rw(batch, dst, true);
 
       vkCmdCopyBuffer(batch->cmdbuf, src->buffer, dst->buffer, 1, &region);
    } else
@@ -1031,20 +1488,83 @@ zink_set_stream_output_targets(struct pipe_context *pctx,
          pipe_so_target_reference(&ctx->so_targets[i], NULL);
       ctx->num_so_targets = 0;
    } else {
-      for (unsigned i = 0; i < num_targets; i++)
+      for (unsigned i = 0; i < num_targets; i++) {
+         struct zink_so_target *t = zink_so_target(targets[i]);
          pipe_so_target_reference(&ctx->so_targets[i], targets[i]);
+         if (!t)
+            continue;
+         struct zink_resource *res = zink_resource(t->counter_buffer);
+         if (offsets[0] == (unsigned)-1)
+            ctx->xfb_barrier |= zink_resource_buffer_needs_barrier(res,
+                                                                   VK_ACCESS_TRANSFORM_FEEDBACK_COUNTER_READ_BIT_EXT,
+                                                                   VK_PIPELINE_STAGE_DRAW_INDIRECT_BIT);
+         else
+            ctx->xfb_barrier |= zink_resource_buffer_needs_barrier(res,
+                                                                   VK_ACCESS_TRANSFORM_FEEDBACK_COUNTER_WRITE_BIT_EXT,
+                                                                   VK_PIPELINE_STAGE_TRANSFORM_FEEDBACK_BIT_EXT);
+      }
       for (unsigned i = num_targets; i < ctx->num_so_targets; i++)
          pipe_so_target_reference(&ctx->so_targets[i], NULL);
       ctx->num_so_targets = num_targets;
 
-      /* emit memory barrier on next draw for synchronization */
-      if (offsets[0] == (unsigned)-1)
-         ctx->xfb_barrier = true;
       /* TODO: possibly avoid rebinding on resume if resuming from same buffers? */
       ctx->dirty_so_targets = true;
    }
 }
 
+static bool
+init_batch(struct zink_context *ctx, struct zink_batch *batch, unsigned idx)
+{
+   struct zink_screen *screen = zink_screen(ctx->base.screen);
+   VkCommandBufferAllocateInfo cbai = {};
+   cbai.sType = VK_STRUCTURE_TYPE_COMMAND_BUFFER_ALLOCATE_INFO;
+   cbai.commandPool = idx == ZINK_COMPUTE_BATCH_ID ? ctx->compute_cmdpool : ctx->cmdpool;
+   cbai.level = VK_COMMAND_BUFFER_LEVEL_PRIMARY;
+   cbai.commandBufferCount = 1;
+
+   VkDescriptorPoolSize sizes[] = {
+      {VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER,         ZINK_BATCH_DESC_SIZE},
+      {VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER,   ZINK_BATCH_DESC_SIZE},
+      {VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER, ZINK_BATCH_DESC_SIZE},
+      {VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER,   ZINK_BATCH_DESC_SIZE},
+      {VK_DESCRIPTOR_TYPE_STORAGE_IMAGE,          ZINK_BATCH_DESC_SIZE},
+      {VK_DESCRIPTOR_TYPE_STORAGE_BUFFER,         ZINK_BATCH_DESC_SIZE},
+   };
+   VkDescriptorPoolCreateInfo dpci = {};
+   dpci.sType = VK_STRUCTURE_TYPE_DESCRIPTOR_POOL_CREATE_INFO;
+   dpci.pPoolSizes = sizes;
+   dpci.poolSizeCount = ARRAY_SIZE(sizes);
+   dpci.flags = VK_DESCRIPTOR_POOL_CREATE_FREE_DESCRIPTOR_SET_BIT;
+   dpci.maxSets = ZINK_BATCH_DESC_SIZE;
+
+   if (vkAllocateCommandBuffers(screen->dev, &cbai, &batch->cmdbuf) != VK_SUCCESS)
+      return false;
+
+   batch->resources = _mesa_set_create(NULL, _mesa_hash_pointer,
+                                                _mesa_key_pointer_equal);
+   batch->sampler_views = _mesa_set_create(NULL,
+                                                    _mesa_hash_pointer,
+                                                    _mesa_key_pointer_equal);
+   batch->surfaces = _mesa_set_create(NULL,
+                                                    _mesa_hash_pointer,
+                                                    _mesa_key_pointer_equal);
+   batch->programs = _mesa_set_create(NULL,
+                                               _mesa_hash_pointer,
+                                               _mesa_key_pointer_equal);
+
+   if (!batch->resources || !batch->sampler_views)
+      return false;
+
+   util_dynarray_init(&batch->zombie_samplers, NULL);
+
+   if (vkCreateDescriptorPool(screen->dev, &dpci, 0,
+                              &batch->descpool) != VK_SUCCESS)
+      return false;
+
+   batch->batch_id = idx;
+   return true;
+}
+
 struct pipe_context *
 zink_context_create(struct pipe_screen *pscreen, void *priv, unsigned flags)
 {
@@ -1069,6 +1589,7 @@ zink_context_create(struct pipe_screen *pscreen, void *priv, unsigned flags)
    ctx->base.create_sampler_view = zink_create_sampler_view;
    ctx->base.set_sampler_views = zink_set_sampler_views;
    ctx->base.sampler_view_destroy = zink_sampler_view_destroy;
+   ctx->base.get_sample_position = zink_get_sample_position;
 
    zink_program_init(ctx);
 
@@ -1077,16 +1598,24 @@ zink_context_create(struct pipe_screen *pscreen, void *priv, unsigned flags)
    ctx->base.set_viewport_states = zink_set_viewport_states;
    ctx->base.set_scissor_states = zink_set_scissor_states;
    ctx->base.set_constant_buffer = zink_set_constant_buffer;
+   ctx->base.set_shader_buffers = zink_set_shader_buffers;
+   ctx->base.set_shader_images = zink_set_shader_images;
    ctx->base.set_framebuffer_state = zink_set_framebuffer_state;
    ctx->base.set_stencil_ref = zink_set_stencil_ref;
    ctx->base.set_clip_state = zink_set_clip_state;
    ctx->base.set_blend_color = zink_set_blend_color;
+   ctx->base.set_tess_state = zink_set_tess_state;
 
    ctx->base.set_sample_mask = zink_set_sample_mask;
 
    ctx->base.clear = zink_clear;
+   ctx->base.clear_texture = zink_clear_texture;
+
    ctx->base.draw_vbo = zink_draw_vbo;
+   ctx->base.launch_grid = zink_launch_grid;
    ctx->base.flush = zink_flush;
+   ctx->base.memory_barrier = zink_memory_barrier;
+   ctx->base.texture_barrier = zink_texture_barrier;
 
    ctx->base.resource_copy_region = zink_resource_copy_region;
    ctx->base.blit = zink_blit;
@@ -1125,45 +1654,24 @@ zink_context_create(struct pipe_screen *pscreen, void *priv, unsigned flags)
    cpci.flags = VK_COMMAND_POOL_CREATE_RESET_COMMAND_BUFFER_BIT;
    if (vkCreateCommandPool(screen->dev, &cpci, NULL, &ctx->cmdpool) != VK_SUCCESS)
       goto fail;
-
-   VkCommandBufferAllocateInfo cbai = {};
-   cbai.sType = VK_STRUCTURE_TYPE_COMMAND_BUFFER_ALLOCATE_INFO;
-   cbai.commandPool = ctx->cmdpool;
-   cbai.level = VK_COMMAND_BUFFER_LEVEL_PRIMARY;
-   cbai.commandBufferCount = 1;
-
-   VkDescriptorPoolSize sizes[] = {
-      {VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER,         ZINK_BATCH_DESC_SIZE},
-      {VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER, ZINK_BATCH_DESC_SIZE}
-   };
-   VkDescriptorPoolCreateInfo dpci = {};
-   dpci.sType = VK_STRUCTURE_TYPE_DESCRIPTOR_POOL_CREATE_INFO;
-   dpci.pPoolSizes = sizes;
-   dpci.poolSizeCount = ARRAY_SIZE(sizes);
-   dpci.flags = VK_DESCRIPTOR_POOL_CREATE_FREE_DESCRIPTOR_SET_BIT;
-   dpci.maxSets = ZINK_BATCH_DESC_SIZE;
+   if (screen->compute_queue != UINT_MAX) {
+      cpci.queueFamilyIndex = screen->compute_queue;
+      if (vkCreateCommandPool(screen->dev, &cpci, NULL, &ctx->compute_cmdpool) != VK_SUCCESS)
+         goto fail;
+   }
 
    for (int i = 0; i < ARRAY_SIZE(ctx->batches); ++i) {
-      if (vkAllocateCommandBuffers(screen->dev, &cbai, &ctx->batches[i].cmdbuf) != VK_SUCCESS)
+      if (!init_batch(ctx, &ctx->batches[i], i))
          goto fail;
+   }
 
-      ctx->batches[i].resources = _mesa_set_create(NULL, _mesa_hash_pointer,
-                                                   _mesa_key_pointer_equal);
-      ctx->batches[i].sampler_views = _mesa_set_create(NULL,
-                                                       _mesa_hash_pointer,
-                                                       _mesa_key_pointer_equal);
-      ctx->batches[i].programs = _mesa_set_create(NULL,
-                                                  _mesa_hash_pointer,
-                                                  _mesa_key_pointer_equal);
-
-      if (!ctx->batches[i].resources || !ctx->batches[i].sampler_views)
+   if (screen->compute_queue != UINT_MAX) {
+      cpci.queueFamilyIndex = screen->compute_queue;
+      if (vkCreateCommandPool(screen->dev, &cpci, NULL, &ctx->compute_cmdpool) != VK_SUCCESS)
          goto fail;
-
-      util_dynarray_init(&ctx->batches[i].zombie_samplers, NULL);
-
-      if (vkCreateDescriptorPool(screen->dev, &dpci, 0,
-                                 &ctx->batches[i].descpool) != VK_SUCCESS)
+      if (!init_batch(ctx, &ctx->compute_batch, ZINK_COMPUTE_BATCH_ID))
          goto fail;
+      vkGetDeviceQueue(screen->dev, screen->compute_queue, 0, &ctx->compute_queue);
    }
 
    vkGetDeviceQueue(screen->dev, screen->gfx_queue, 0, &ctx->queue);
@@ -1171,10 +1679,13 @@ zink_context_create(struct pipe_screen *pscreen, void *priv, unsigned flags)
    ctx->program_cache = _mesa_hash_table_create(NULL,
                                                 hash_gfx_program,
                                                 equals_gfx_program);
+   ctx->compute_program_cache = _mesa_hash_table_create(NULL,
+                                                _mesa_hash_uint,
+                                                _mesa_key_uint_equal);
    ctx->render_pass_cache = _mesa_hash_table_create(NULL,
                                                     hash_render_pass_state,
                                                     equals_render_pass_state);
-   if (!ctx->program_cache || !ctx->render_pass_cache)
+   if (!ctx->program_cache || !ctx->compute_program_cache || !ctx->render_pass_cache)
       goto fail;
 
    const uint8_t data[] = { 0 };
@@ -1191,6 +1702,8 @@ zink_context_create(struct pipe_screen *pscreen, void *priv, unsigned flags)
 fail:
    if (ctx) {
       vkDestroyCommandPool(screen->dev, ctx->cmdpool, NULL);
+      if (ctx->compute_cmdpool)
+         vkDestroyCommandPool(screen->dev, ctx->compute_cmdpool, NULL);
       FREE(ctx);
    }
    return NULL;
diff --git a/src/gallium/drivers/zink/zink_context.h b/src/gallium/drivers/zink/zink_context.h
index a9c3b718cee..830d166164b 100644
--- a/src/gallium/drivers/zink/zink_context.h
+++ b/src/gallium/drivers/zink/zink_context.h
@@ -44,11 +44,29 @@ struct zink_depth_stencil_alpha_state;
 struct zink_gfx_program;
 struct zink_rasterizer_state;
 struct zink_resource;
+struct zink_surface;
 struct zink_vertex_elements_state;
 
+enum zink_blit_flags {
+   ZINK_BLIT_NORMAL = 1 << 0,
+   ZINK_BLIT_SAVE_FS = 1 << 1,
+   ZINK_BLIT_SAVE_FB = 1 << 2,
+};
+
 struct zink_sampler_view {
    struct pipe_sampler_view base;
-   VkImageView image_view;
+   union {
+      VkImageView image_view;
+      VkBufferView buffer_view;
+   };
+};
+
+struct zink_image_view {
+   struct pipe_image_view base;
+   union {
+      struct zink_surface *surface;
+      VkBufferView buffer_view;
+   };
 };
 
 static inline struct zink_sampler_view *
@@ -72,6 +90,7 @@ zink_so_target(struct pipe_stream_output_target *so_target)
 }
 
 #define ZINK_SHADER_COUNT (PIPE_SHADER_TYPES - 1)
+#define ZINK_COMPUTE_BATCH_ID 4
 
 struct zink_context {
    struct pipe_context base;
@@ -84,7 +103,14 @@ struct zink_context {
 
    VkQueue queue;
 
+   VkCommandPool compute_cmdpool;
+   struct zink_batch compute_batch;
+   VkQueue compute_queue;
+
    struct pipe_constant_buffer ubos[PIPE_SHADER_TYPES][PIPE_MAX_CONSTANT_BUFFERS];
+   struct pipe_shader_buffer ssbos[PIPE_SHADER_TYPES][PIPE_MAX_SHADER_BUFFERS];
+   uint32_t writable_ssbos;
+   struct zink_image_view image_views[PIPE_SHADER_TYPES][PIPE_MAX_SHADER_IMAGES];
    struct pipe_framebuffer_state fb_state;
 
    struct zink_vertex_elements_state *element_state;
@@ -95,7 +121,12 @@ struct zink_context {
    struct hash_table *program_cache;
    struct zink_gfx_program *curr_program;
 
-   unsigned dirty_shader_stages : 6; /* mask of changed shader stages */
+   struct zink_shader *compute_stage;
+   struct zink_compute_pipeline_state compute_pipeline_state;
+   struct hash_table *compute_program_cache;
+   struct zink_compute_program *curr_compute;
+
+   unsigned dirty_shader_stages : 7; /* mask of changed shader stages */
 
    struct hash_table *render_pass_cache;
 
@@ -103,20 +134,14 @@ struct zink_context {
 
    struct zink_framebuffer *framebuffer;
 
-   struct pipe_viewport_state viewport_states[PIPE_MAX_VIEWPORTS];
-   struct pipe_scissor_state scissor_states[PIPE_MAX_VIEWPORTS];
-   VkViewport viewports[PIPE_MAX_VIEWPORTS];
-   VkRect2D scissors[PIPE_MAX_VIEWPORTS];
-   unsigned num_viewports;
-
    struct pipe_vertex_buffer buffers[PIPE_MAX_ATTRIBS];
    uint32_t buffers_enabled_mask;
 
    void *sampler_states[PIPE_SHADER_TYPES][PIPE_MAX_SAMPLERS];
    VkSampler samplers[PIPE_SHADER_TYPES][PIPE_MAX_SAMPLERS];
    unsigned num_samplers[PIPE_SHADER_TYPES];
-   struct pipe_sampler_view *image_views[PIPE_SHADER_TYPES][PIPE_MAX_SHADER_SAMPLER_VIEWS];
-   unsigned num_image_views[PIPE_SHADER_TYPES];
+   struct pipe_sampler_view *sampler_views[PIPE_SHADER_TYPES][PIPE_MAX_SHADER_SAMPLER_VIEWS];
+   unsigned num_sampler_views[PIPE_SHADER_TYPES];
 
    float line_width;
    float blend_constants[4];
@@ -124,6 +149,7 @@ struct zink_context {
    struct pipe_stencil_ref stencil_ref;
 
    struct list_head suspended_queries;
+   struct list_head primitives_generated_queries;
    bool queries_disabled;
 
    struct pipe_resource *dummy_buffer;
@@ -133,6 +159,7 @@ struct zink_context {
    struct pipe_stream_output_target *so_targets[PIPE_MAX_SO_OUTPUTS];
    bool dirty_so_targets;
    bool xfb_barrier;
+   bool conditional_render_enabled;
 };
 
 static inline struct zink_context *
@@ -155,13 +182,32 @@ struct zink_batch *
 zink_batch_no_rp(struct zink_context *ctx);
 
 void
-zink_resource_barrier(VkCommandBuffer cmdbuf, struct zink_resource *res,
-                      VkImageAspectFlags aspect, VkImageLayout new_layout);
+zink_fence_wait(struct pipe_context *ctx);
+
+void
+zink_wait_on_batch(struct zink_context *ctx, int batch_id);
+
+bool
+zink_resource_buffer_needs_barrier(struct zink_resource *res, VkAccessFlags flags, VkPipelineStageFlags pipeline);
+
+void
+zink_resource_buffer_barrier(struct zink_batch *batch, struct zink_resource *res, VkAccessFlags flags, VkPipelineStageFlags pipeline);
+
+bool
+zink_resource_image_needs_barrier(struct zink_resource *res, VkImageLayout new_layout, VkPipelineStageFlags pipeline);
+void
+zink_resource_barrier(struct zink_batch *batch, struct zink_resource *res,
+                      VkImageLayout new_layout, VkPipelineStageFlags pipeline);
+
+bool
+zink_resource_needs_barrier(struct zink_resource *res, unsigned flags, VkPipelineStageFlags pipeline);
 
  void
  zink_begin_render_pass(struct zink_context *ctx,
                         struct zink_batch *batch);
 
+VkPipelineStageFlags
+zink_pipeline_flags_from_stage(VkShaderStageFlagBits stage);
 
 VkShaderStageFlagBits
 zink_shader_stage(enum pipe_shader_type type);
@@ -172,12 +218,30 @@ zink_context_create(struct pipe_screen *pscreen, void *priv, unsigned flags);
 void
 zink_context_query_init(struct pipe_context *ctx);
 
+void
+zink_blit_begin(struct zink_context *ctx, enum zink_blit_flags flags);
+
 void
 zink_blit(struct pipe_context *pctx,
           const struct pipe_blit_info *info);
 
+void
+zink_clear(struct pipe_context *pctx,
+           unsigned buffers,
+           const struct pipe_scissor_state *scissor_state,
+           const union pipe_color_union *pcolor,
+           double depth, unsigned stencil);
+void
+zink_clear_texture(struct pipe_context *ctx,
+                   struct pipe_resource *p_res,
+                   unsigned level,
+                   const struct pipe_box *box,
+                   const void *data);
+
 void
 zink_draw_vbo(struct pipe_context *pctx,
               const struct pipe_draw_info *dinfo);
 
+void
+zink_launch_grid(struct pipe_context *pctx, const struct pipe_grid_info *info);
 #endif
diff --git a/src/gallium/drivers/zink/zink_draw.c b/src/gallium/drivers/zink/zink_draw.c
index b103d089df3..683fc7f0feb 100644
--- a/src/gallium/drivers/zink/zink_draw.c
+++ b/src/gallium/drivers/zink/zink_draw.c
@@ -1,11 +1,14 @@
 #include "zink_compiler.h"
 #include "zink_context.h"
 #include "zink_program.h"
+#include "zink_query.h"
 #include "zink_resource.h"
 #include "zink_screen.h"
 #include "zink_state.h"
+#include "zink_surface.h"
 
 #include "indices/u_primconvert.h"
+#include "tgsi/tgsi_from_mesa.h"
 #include "util/hash_table.h"
 #include "util/u_debug.h"
 #include "util/u_helpers.h"
@@ -16,16 +19,17 @@
 static VkDescriptorSet
 allocate_descriptor_set(struct zink_screen *screen,
                         struct zink_batch *batch,
-                        struct zink_gfx_program *prog)
+                        VkDescriptorSetLayout dsl,
+                        unsigned num_descriptors)
 {
-   assert(batch->descs_left >= prog->num_descriptors);
+   assert(batch->descs_left >= num_descriptors);
    VkDescriptorSetAllocateInfo dsai;
    memset((void *)&dsai, 0, sizeof(dsai));
    dsai.sType = VK_STRUCTURE_TYPE_DESCRIPTOR_SET_ALLOCATE_INFO;
    dsai.pNext = NULL;
    dsai.descriptorPool = batch->descpool;
    dsai.descriptorSetCount = 1;
-   dsai.pSetLayouts = &prog->dsl;
+   dsai.pSetLayouts = &dsl;
 
    VkDescriptorSet desc_set;
    if (vkAllocateDescriptorSets(screen->dev, &dsai, &desc_set) != VK_SUCCESS) {
@@ -33,7 +37,7 @@ allocate_descriptor_set(struct zink_screen *screen,
       return VK_NULL_HANDLE;
    }
 
-   batch->descs_left -= prog->num_descriptors;
+   batch->descs_left -= num_descriptors;
    return desc_set;
 }
 
@@ -48,29 +52,17 @@ zink_emit_xfb_counter_barrier(struct zink_context *ctx)
     *
     * - from VK_EXT_transform_feedback spec
     */
-   VkBufferMemoryBarrier barriers[PIPE_MAX_SO_OUTPUTS] = {};
-   unsigned barrier_count = 0;
-
+   struct zink_batch *batch = zink_batch_no_rp(ctx);
    for (unsigned i = 0; i < ctx->num_so_targets; i++) {
       struct zink_so_target *t = zink_so_target(ctx->so_targets[i]);
-      if (t->counter_buffer_valid) {
-          barriers[i].sType = VK_STRUCTURE_TYPE_BUFFER_MEMORY_BARRIER;
-          barriers[i].srcAccessMask = VK_ACCESS_TRANSFORM_FEEDBACK_COUNTER_WRITE_BIT_EXT;
-          barriers[i].dstAccessMask = VK_ACCESS_TRANSFORM_FEEDBACK_COUNTER_READ_BIT_EXT;
-          barriers[i].buffer = zink_resource(t->counter_buffer)->buffer;
-          barriers[i].size = VK_WHOLE_SIZE;
-          barrier_count++;
-      }
+      struct zink_resource *res = zink_resource(t->counter_buffer);
+      if (t->counter_buffer_valid)
+          zink_resource_buffer_barrier(batch, res, VK_ACCESS_TRANSFORM_FEEDBACK_COUNTER_READ_BIT_EXT,
+                                       VK_PIPELINE_STAGE_DRAW_INDIRECT_BIT);
+      else
+          zink_resource_buffer_barrier(batch, res, VK_ACCESS_TRANSFORM_FEEDBACK_COUNTER_WRITE_BIT_EXT,
+                                       VK_PIPELINE_STAGE_TRANSFORM_FEEDBACK_BIT_EXT);
    }
-   struct zink_batch *batch = zink_batch_no_rp(ctx);
-   vkCmdPipelineBarrier(batch->cmdbuf,
-      VK_PIPELINE_STAGE_TRANSFORM_FEEDBACK_BIT_EXT,
-      VK_PIPELINE_STAGE_DRAW_INDIRECT_BIT,
-      0,
-      0, NULL,
-      barrier_count, barriers,
-      0, NULL
-   );
    ctx->xfb_barrier = false;
 }
 
@@ -88,23 +80,9 @@ zink_emit_xfb_vertex_input_barrier(struct zink_context *ctx, struct zink_resourc
     *
     * - 20.3.1. Drawing Transform Feedback
     */
-   VkBufferMemoryBarrier barriers[1] = {};
-   barriers[0].sType = VK_STRUCTURE_TYPE_BUFFER_MEMORY_BARRIER;
-   barriers[0].srcAccessMask = VK_ACCESS_TRANSFORM_FEEDBACK_COUNTER_WRITE_BIT_EXT;
-   barriers[0].dstAccessMask = VK_ACCESS_VERTEX_ATTRIBUTE_READ_BIT;
-   barriers[0].buffer = res->buffer;
-   barriers[0].size = VK_WHOLE_SIZE;
    struct zink_batch *batch = zink_batch_no_rp(ctx);
-   zink_batch_reference_resoure(batch, res);
-   vkCmdPipelineBarrier(batch->cmdbuf,
-      VK_PIPELINE_STAGE_TRANSFORM_FEEDBACK_BIT_EXT,
-      VK_PIPELINE_STAGE_VERTEX_INPUT_BIT,
-      0,
-      0, NULL,
-      ARRAY_SIZE(barriers), barriers,
-      0, NULL
-   );
-   res->needs_xfb_barrier = false;
+   zink_resource_buffer_barrier(batch, res, VK_ACCESS_VERTEX_ATTRIBUTE_READ_BIT,
+                                VK_PIPELINE_STAGE_VERTEX_INPUT_BIT);
 }
 
 static void
@@ -113,14 +91,18 @@ zink_emit_stream_output_targets(struct pipe_context *pctx)
    struct zink_context *ctx = zink_context(pctx);
    struct zink_screen *screen = zink_screen(pctx->screen);
    struct zink_batch *batch = zink_curr_batch(ctx);
-   VkBuffer buffers[PIPE_MAX_SO_OUTPUTS];
-   VkDeviceSize buffer_offsets[PIPE_MAX_SO_OUTPUTS];
-   VkDeviceSize buffer_sizes[PIPE_MAX_SO_OUTPUTS];
+   VkBuffer buffers[PIPE_MAX_SO_OUTPUTS] = {};
+   VkDeviceSize buffer_offsets[PIPE_MAX_SO_OUTPUTS] = {};
+   VkDeviceSize buffer_sizes[PIPE_MAX_SO_OUTPUTS] = {};
 
    for (unsigned i = 0; i < ctx->num_so_targets; i++) {
       struct zink_so_target *t = (struct zink_so_target *)ctx->so_targets[i];
+      if (!t) {
+         buffers[i] = VK_NULL_HANDLE;
+         continue;
+      }
       buffers[i] = zink_resource(t->base.buffer)->buffer;
-      zink_batch_reference_resoure(batch, zink_resource(t->base.buffer));
+      zink_batch_reference_resource_rw(batch, zink_resource(t->base.buffer), true);
       buffer_offsets[i] = t->base.buffer_offset;
       buffer_sizes[i] = t->base.buffer_size;
    }
@@ -131,6 +113,46 @@ zink_emit_stream_output_targets(struct pipe_context *pctx)
    ctx->dirty_so_targets = false;
 }
 
+static void
+barrier_vertex_buffers(struct zink_context *ctx)
+{
+   const struct zink_vertex_elements_state *elems = ctx->element_state;
+   for (unsigned i = 0; i < elems->hw_state.num_bindings; i++) {
+      struct pipe_vertex_buffer *vb = ctx->buffers + ctx->element_state->binding_map[i];
+      assert(vb);
+      if (vb->buffer.resource) {
+         struct zink_resource *res = zink_resource(vb->buffer.resource);
+         struct zink_batch *batch = zink_batch_no_rp(ctx);
+         zink_resource_buffer_barrier(batch, res, VK_ACCESS_VERTEX_ATTRIBUTE_READ_BIT,
+                                      VK_PIPELINE_STAGE_VERTEX_INPUT_BIT);
+      }
+   }
+}
+
+static void
+check_buffer_barrier(struct zink_context *ctx, struct pipe_resource *pres, VkAccessFlags flags, VkPipelineStageFlags pipeline)
+{
+   struct zink_resource *res = zink_resource(pres);
+   if (zink_resource_buffer_needs_barrier(res, VK_ACCESS_INDEX_READ_BIT, VK_PIPELINE_STAGE_VERTEX_INPUT_BIT)) {
+       struct zink_batch *batch = zink_batch_no_rp(ctx);
+       zink_resource_buffer_barrier(batch, res, VK_ACCESS_INDEX_READ_BIT, VK_PIPELINE_STAGE_VERTEX_INPUT_BIT);
+    }
+}
+
+static void
+barrier_draw_buffers(struct zink_context *ctx, const struct pipe_draw_info *dinfo, struct pipe_resource *index_buffer)
+{
+   if (index_buffer)
+      check_buffer_barrier(ctx, index_buffer, VK_ACCESS_INDEX_READ_BIT, VK_PIPELINE_STAGE_VERTEX_INPUT_BIT);
+   if (dinfo->indirect) {
+      check_buffer_barrier(ctx, dinfo->indirect->buffer,
+                           VK_ACCESS_INDIRECT_COMMAND_READ_BIT, VK_PIPELINE_STAGE_DRAW_INDIRECT_BIT);
+      if (dinfo->indirect->indirect_draw_count)
+         check_buffer_barrier(ctx, dinfo->indirect->indirect_draw_count,
+                              VK_ACCESS_INDIRECT_COMMAND_READ_BIT, VK_PIPELINE_STAGE_DRAW_INDIRECT_BIT);
+   }
+}
+
 static void
 zink_bind_vertex_buffers(struct zink_batch *batch, struct zink_context *ctx)
 {
@@ -144,7 +166,7 @@ zink_bind_vertex_buffers(struct zink_batch *batch, struct zink_context *ctx)
          struct zink_resource *res = zink_resource(vb->buffer.resource);
          buffers[i] = res->buffer;
          buffer_offsets[i] = vb->buffer_offset;
-         zink_batch_reference_resoure(batch, res);
+         zink_batch_reference_resource_rw(batch, res, false);
       } else {
          buffers[i] = zink_resource(ctx->dummy_buffer)->buffer;
          buffer_offsets[i] = 0;
@@ -157,13 +179,358 @@ zink_bind_vertex_buffers(struct zink_batch *batch, struct zink_context *ctx)
                              buffers, buffer_offsets);
 }
 
+static struct zink_compute_program *
+get_compute_program(struct zink_context *ctx)
+{
+   if (ctx->dirty_shader_stages) {
+      struct hash_entry *entry = _mesa_hash_table_search(ctx->compute_program_cache,
+                                                         &ctx->compute_stage->shader_id);
+      if (!entry) {
+         struct zink_compute_program *comp;
+         comp = zink_create_compute_program(ctx, ctx->compute_stage);
+         entry = _mesa_hash_table_insert(ctx->compute_program_cache, &comp->shader->shader_id, comp);
+         if (!entry)
+            return NULL;
+      }
+      ctx->curr_compute = entry->data;
+      ctx->dirty_shader_stages &= (1 << PIPE_SHADER_COMPUTE);
+   }
+
+   assert(ctx->curr_compute);
+   return ctx->curr_compute;
+}
+
+struct zink_transition {
+   struct zink_resource *res;
+   unsigned layout;
+   VkPipelineStageFlagBits stage;
+};
+
+#define MAX_DESCRIPTORS (PIPE_SHADER_TYPES * (PIPE_MAX_CONSTANT_BUFFERS + PIPE_MAX_SHADER_SAMPLER_VIEWS + PIPE_MAX_SHADER_BUFFERS + PIPE_MAX_SHADER_IMAGES))
+
+static bool
+transition_equals(const void *a, const void *b)
+{
+   const struct zink_transition *t1 = a, *t2 = b;
+   if (t1->res != t2->res)
+      return false;
+   if (t1->layout != t2->layout)
+      return false;
+   return true;
+}
+
+static uint32_t
+transition_hash(const void *key)
+{
+   return _mesa_hash_data(key, offsetof(struct zink_transition, stage));
+}
+
+static inline void
+add_transition(struct zink_resource *res, unsigned layout, enum pipe_shader_type stage, struct zink_transition *t, int *num_transitions, struct hash_table *ht)
+{
+   VkPipelineStageFlags pipeline = zink_pipeline_flags_from_stage(zink_shader_stage(stage));
+   struct zink_transition key = {res, layout, 0};
+
+   uint32_t hash = transition_hash(&key);
+   struct hash_entry *entry = _mesa_hash_table_search_pre_hashed(ht, hash, &key);
+   if (entry)
+      t = entry->data;
+   else {
+      (*num_transitions)++;
+      _mesa_hash_table_insert_pre_hashed(ht, hash, &key, t);
+   }
+   t->layout = layout;
+   t->stage |= pipeline;
+   t->res = res;
+}
+
+static void
+update_descriptors(struct zink_context *ctx, struct zink_screen *screen, bool is_compute)
+{
+   VkWriteDescriptorSet wds[MAX_DESCRIPTORS];
+   struct zink_resource *read_desc_resources[MAX_DESCRIPTORS] = {};
+   struct zink_resource *write_desc_resources[MAX_DESCRIPTORS] = {};
+   struct zink_surface *surface_refs[PIPE_SHADER_TYPES * PIPE_MAX_SHADER_IMAGES] = {};
+   VkDescriptorBufferInfo buffer_infos[PIPE_SHADER_TYPES * (PIPE_MAX_CONSTANT_BUFFERS + PIPE_MAX_SHADER_BUFFERS + PIPE_MAX_SHADER_IMAGES)];
+   VkDescriptorImageInfo image_infos[PIPE_SHADER_TYPES * (PIPE_MAX_SHADER_SAMPLER_VIEWS + PIPE_MAX_SHADER_IMAGES)];
+   VkBufferView buffer_view[] = {VK_NULL_HANDLE};
+   unsigned num_wds = 0, num_buffer_info = 0, num_image_info = 0;
+   struct zink_shader **stages;
+
+   unsigned num_stages = is_compute ? 1 : ZINK_SHADER_COUNT;
+   if (is_compute)
+      stages = &ctx->curr_compute->shader;
+   else
+      stages = &ctx->gfx_stages[0];
+
+   struct zink_transition transitions[MAX_DESCRIPTORS] = {};
+   int num_transitions = 0;
+   struct hash_table *ht = _mesa_hash_table_create(NULL, transition_hash, transition_equals);
+
+   for (int i = 0; i < num_stages; i++) {
+      struct zink_shader *shader = stages[i];
+      if (!shader)
+         continue;
+      enum pipe_shader_type stage = pipe_shader_type_from_mesa(shader->nir->info.stage);
+      if (ctx->num_so_targets &&
+          (stage == PIPE_SHADER_GEOMETRY ||
+          (stage == PIPE_SHADER_TESS_EVAL && !ctx->gfx_stages[PIPE_SHADER_GEOMETRY]) ||
+          (stage == PIPE_SHADER_VERTEX && !ctx->gfx_stages[PIPE_SHADER_GEOMETRY] && !ctx->gfx_stages[PIPE_SHADER_TESS_EVAL]))) {
+         for (unsigned j = 0; j < ctx->num_so_targets; j++) {
+            struct zink_so_target *t = zink_so_target(ctx->so_targets[j]);
+            if (t)
+               t->stride = shader->streamout.so_info.stride[j] * sizeof(uint32_t);
+         }
+      }
+
+      for (int j = 0; j < shader->num_bindings; j++) {
+         int index = shader->bindings[j].index;
+         if (shader->bindings[j].type == VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER) {
+            assert(ctx->ubos[stage][index].buffer_size > 0);
+            assert(ctx->ubos[stage][index].buffer_size <= screen->props.limits.maxUniformBufferRange);
+            assert(ctx->ubos[stage][index].buffer);
+            struct zink_resource *res = zink_resource(ctx->ubos[stage][index].buffer);
+            read_desc_resources[num_wds] = res;
+            buffer_infos[num_buffer_info].buffer = res->buffer;
+            buffer_infos[num_buffer_info].offset = ctx->ubos[stage][index].buffer_offset;
+            buffer_infos[num_buffer_info].range  = ctx->ubos[stage][index].buffer_size;
+            add_transition(res, VK_ACCESS_UNIFORM_READ_BIT, stage, &transitions[num_transitions], &num_transitions, ht);
+            wds[num_wds].pBufferInfo = buffer_infos + num_buffer_info;
+            ++num_buffer_info;
+         } else if (shader->bindings[j].type == VK_DESCRIPTOR_TYPE_STORAGE_BUFFER) {
+            struct zink_resource *res = zink_resource(ctx->ssbos[stage][index].buffer);
+            if (res) {
+               assert(ctx->ssbos[stage][index].buffer_size > 0);
+               assert(ctx->ssbos[stage][index].buffer_size <= screen->props.limits.maxStorageBufferRange);
+               unsigned flag = 0;
+               if (ctx->writable_ssbos & (1 << index)) {
+                  write_desc_resources[num_wds] = res;
+                  flag = VK_ACCESS_SHADER_WRITE_BIT;
+               } else {
+                  read_desc_resources[num_wds] = res;
+                  flag = VK_ACCESS_SHADER_READ_BIT;
+               }
+               add_transition(res, flag, stage, &transitions[num_transitions], &num_transitions, ht);
+               buffer_infos[num_buffer_info].buffer = res->buffer;
+               buffer_infos[num_buffer_info].offset = ctx->ssbos[stage][index].buffer_offset;
+               buffer_infos[num_buffer_info].range  = ctx->ssbos[stage][index].buffer_size;
+            } else {
+               assert(screen->rb2_feats.nullDescriptor);
+               buffer_infos[num_buffer_info].buffer = VK_NULL_HANDLE;
+               buffer_infos[num_buffer_info].offset = 0;
+               buffer_infos[num_buffer_info].range  = VK_WHOLE_SIZE;
+            }
+            wds[num_wds].pBufferInfo = buffer_infos + num_buffer_info;
+            ++num_buffer_info;
+         } else {
+            for (unsigned k = 0; k < shader->bindings[j].size; k++) {
+               VkImageView imageview = VK_NULL_HANDLE;
+               struct zink_resource *res = NULL;
+               VkImageLayout layout = VK_IMAGE_LAYOUT_UNDEFINED;
+               VkSampler sampler = VK_NULL_HANDLE;
+
+               switch (shader->bindings[j].type) {
+               case VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER:
+               /* fallthrough */
+               case VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER: {
+                  struct pipe_sampler_view *psampler_view = ctx->sampler_views[stage][index + k];
+                  struct zink_sampler_view *sampler_view = zink_sampler_view(psampler_view);
+                  res = psampler_view ? zink_resource(psampler_view->texture) : NULL;
+                  if (res) {
+                     if (res->base.target == PIPE_BUFFER)
+                        wds[num_wds].pTexelBufferView = &sampler_view->buffer_view;
+                     else {
+                        imageview = sampler_view->image_view;
+                        layout = 0;
+                        if (util_format_is_depth_or_stencil(psampler_view->format))
+                           layout = VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL;
+                        else
+                           layout = VK_IMAGE_LAYOUT_GENERAL;
+                        add_transition(res, layout, stage, &transitions[num_transitions], &num_transitions, ht);
+                        sampler = ctx->samplers[stage][index + k];
+                     }
+                  }
+                  read_desc_resources[num_wds] = res;
+               }
+               break;
+               case VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER:
+               /* fallthrough */
+               case VK_DESCRIPTOR_TYPE_STORAGE_IMAGE: {
+                  struct zink_image_view *image_view = &ctx->image_views[stage][index + k];
+                  assert(image_view);
+                  res = zink_resource(image_view->base.resource);
+                  if (!res)
+                     break;
+                  if (image_view->base.resource->target == PIPE_BUFFER) {
+                     wds[num_wds].pTexelBufferView = &image_view->buffer_view;
+                  } else {
+                     imageview = image_view->surface->image_view;
+                     layout = VK_IMAGE_LAYOUT_GENERAL;
+                     add_transition(res, layout, stage, &transitions[num_transitions], &num_transitions, ht);
+                     surface_refs[num_wds] = image_view->surface;
+                  }
+                  write_desc_resources[num_wds] = res;
+               }
+               break;
+               default:
+                  unreachable("unknown descriptor type");
+               }
+
+               if (!res) {
+                  /* if we're hitting this assert often, we can probably just throw a junk buffer in since
+                   * the results of this codepath are undefined in ARB_texture_buffer_object spec
+                   */
+                  assert(screen->rb2_feats.nullDescriptor);
+                  switch (shader->bindings[j].type) {
+                  case VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER:
+                  case VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER:
+                     wds[num_wds].pTexelBufferView = &buffer_view[0];
+                     break;
+                  case VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER:
+                  case VK_DESCRIPTOR_TYPE_STORAGE_IMAGE:
+                     image_infos[num_image_info].imageLayout = VK_IMAGE_LAYOUT_UNDEFINED;
+                     image_infos[num_image_info].imageView = VK_NULL_HANDLE;
+                     image_infos[num_image_info].sampler = sampler;
+                     if (!k)
+                        wds[num_wds].pImageInfo = image_infos + num_image_info;
+                     ++num_image_info;
+                     break;
+                  default:
+                     unreachable("unknown descriptor type");
+                  }
+               } else if (res->base.target != PIPE_BUFFER) {
+                  assert(layout != VK_IMAGE_LAYOUT_UNDEFINED);
+                  image_infos[num_image_info].imageLayout = layout;
+                  image_infos[num_image_info].imageView = imageview;
+                  image_infos[num_image_info].sampler = ctx->samplers[stage][index + k];
+                  if (!k)
+                     wds[num_wds].pImageInfo = image_infos + num_image_info;
+                  ++num_image_info;
+               }
+            }
+         }
+
+         wds[num_wds].sType = VK_STRUCTURE_TYPE_WRITE_DESCRIPTOR_SET;
+         wds[num_wds].pNext = NULL;
+         wds[num_wds].dstBinding = shader->bindings[j].binding;
+         wds[num_wds].dstArrayElement = 0;
+         wds[num_wds].descriptorCount = shader->bindings[j].size;
+         wds[num_wds].descriptorType = shader->bindings[j].type;
+         ++num_wds;
+      }
+   }
+   _mesa_hash_table_destroy(ht, NULL);
+
+   struct zink_batch *batch = NULL;
+   if (num_transitions > 0) {
+      for (int i = 0; i < num_transitions; ++i) {
+         if (!zink_resource_needs_barrier(transitions[i].res,
+                                                   transitions[i].layout,
+                                                   transitions[i].stage))
+            continue;
+         if (is_compute)
+            batch = &ctx->compute_batch;
+         else
+            batch = zink_batch_no_rp(ctx);
+
+         if (transitions[i].res->base.target == PIPE_BUFFER)
+            zink_resource_buffer_barrier(batch, transitions[i].res,
+                                         transitions[i].layout, transitions[i].stage);
+         else
+            zink_resource_barrier(batch, transitions[i].res,
+                                  transitions[i].layout, transitions[i].stage);
+      }
+   }
+
+   unsigned num_descriptors;
+   VkDescriptorSetLayout dsl;
+   if (is_compute) {
+      num_descriptors = ctx->curr_compute->num_descriptors;
+      dsl = ctx->curr_compute->dsl;
+      batch = &ctx->compute_batch;
+   } else {
+      batch = zink_batch_rp(ctx);
+      num_descriptors = ctx->curr_program->num_descriptors;
+      dsl = ctx->curr_program->dsl;
+   }
+
+   if (batch->descs_left < num_descriptors) {
+      if (is_compute)
+         zink_wait_on_batch(ctx, ZINK_COMPUTE_BATCH_ID);
+      else {
+         ctx->base.flush(&ctx->base, NULL, 0);
+         batch = zink_batch_rp(ctx);
+      }
+      assert(batch->descs_left >= num_descriptors);
+   }
+   if (is_compute)
+      zink_batch_reference_program(batch, &ctx->curr_compute->reference);
+   else
+      zink_batch_reference_program(batch, &ctx->curr_program->reference);
+
+   VkDescriptorSet desc_set = allocate_descriptor_set(screen, batch,
+                                                      dsl, num_descriptors);
+   assert(desc_set != VK_NULL_HANDLE);
+
+   for (int i = 0; i < num_stages; i++) {
+      struct zink_shader *shader = stages[i];
+      if (!shader)
+         continue;
+      enum pipe_shader_type stage = pipe_shader_type_from_mesa(shader->nir->info.stage);
+
+      for (int j = 0; j < shader->num_bindings; j++) {
+         int index = shader->bindings[j].index;
+         if (shader->bindings[j].type != VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER) {
+            struct zink_sampler_view *sampler_view = zink_sampler_view(ctx->sampler_views[stage][index]);
+            if (sampler_view)
+               zink_batch_reference_sampler_view(batch, sampler_view);
+         }
+      }
+   }
+
+   unsigned check_flush_id = is_compute ? 0 : ZINK_COMPUTE_BATCH_ID;
+   bool need_flush = false;
+   if (num_wds > 0) {
+      for (int i = 0; i < num_wds; ++i) {
+         wds[i].dstSet = desc_set;
+         if (read_desc_resources[i])
+            need_flush |= zink_batch_reference_resource_rw(batch, read_desc_resources[i], false) == check_flush_id;
+         else if (write_desc_resources[i])
+            need_flush |= zink_batch_reference_resource_rw(batch, write_desc_resources[i], true) == check_flush_id;
+         if (surface_refs[i])
+            zink_batch_reference_surface(batch, surface_refs[i]);
+      }
+      vkUpdateDescriptorSets(screen->dev, num_wds, wds, 0, NULL);
+   }
+
+   if (is_compute)
+      vkCmdBindDescriptorSets(batch->cmdbuf, VK_PIPELINE_BIND_POINT_COMPUTE,
+                              ctx->curr_compute->layout, 0, 1, &desc_set, 0, NULL);
+   else
+      vkCmdBindDescriptorSets(batch->cmdbuf, VK_PIPELINE_BIND_POINT_GRAPHICS,
+                              ctx->curr_program->layout, 0, 1, &desc_set, 0, NULL);
+   if (!need_flush)
+      return;
+
+   if (is_compute)
+      /* flush gfx batch */
+      ctx->base.flush(&ctx->base, NULL, PIPE_FLUSH_HINT_FINISH);
+   else {
+      /* flush compute batch */
+      zink_end_batch(ctx, &ctx->compute_batch);
+      zink_start_batch(ctx, &ctx->compute_batch);
+   }
+}
+
 static struct zink_gfx_program *
 get_gfx_program(struct zink_context *ctx)
 {
    if (ctx->dirty_shader_stages) {
       struct hash_entry *entry = _mesa_hash_table_search(ctx->program_cache,
                                                          ctx->gfx_stages);
-      if (!entry) {
+      if (entry)
+         zink_update_gfx_program(ctx, entry->data);
+      else {
          struct zink_gfx_program *prog;
          prog = zink_create_gfx_program(ctx, ctx->gfx_stages);
          entry = _mesa_hash_table_insert(ctx->program_cache, prog->shaders, prog);
@@ -171,13 +538,33 @@ get_gfx_program(struct zink_context *ctx)
             return NULL;
       }
       ctx->curr_program = entry->data;
-      ctx->dirty_shader_stages = 0;
+      unsigned bits = u_bit_consecutive(PIPE_SHADER_VERTEX, 5);
+      ctx->dirty_shader_stages &= ~bits;
    }
 
    assert(ctx->curr_program);
    return ctx->curr_program;
 }
 
+static void
+flush_persistent_maps(struct zink_screen *screen, struct zink_batch *batch)
+{
+   set_foreach(batch->resources, entry) {
+      struct zink_resource *res = (void*)entry->key;
+      if (res->persistent_maps) {
+         /* TODO: only flush the actual mapped region */
+         VkMappedMemoryRange range = {
+            VK_STRUCTURE_TYPE_MAPPED_MEMORY_RANGE,
+            NULL,
+            res->mem,
+            res->offset,
+            res->size,
+         };
+         vkFlushMappedMemoryRanges(screen->dev, 1, &range);
+      }
+   }
+}
+
 static bool
 line_width_needed(enum pipe_prim_type reduced_prim,
                   VkPolygonMode polygon_mode)
@@ -220,7 +607,9 @@ zink_draw_vbo(struct pipe_context *pctx,
        util_draw_vbo_without_prim_restart(pctx, dinfo);
        return;
    }
-   if (dinfo->mode >= PIPE_PRIM_QUADS ||
+   if (dinfo->mode == PIPE_PRIM_QUADS ||
+       dinfo->mode == PIPE_PRIM_QUAD_STRIP ||
+       dinfo->mode == PIPE_PRIM_POLYGON ||
        dinfo->mode == PIPE_PRIM_LINE_LOOP) {
       if (!u_trim_pipe_prim(dinfo->mode, (unsigned *)&dinfo->count))
          return;
@@ -229,7 +618,9 @@ zink_draw_vbo(struct pipe_context *pctx,
       util_primconvert_draw_vbo(ctx->primconvert, dinfo);
       return;
    }
-
+   if (ctx->gfx_pipeline_state.vertices_per_patch != dinfo->vertices_per_patch)
+      ctx->gfx_pipeline_state.hash = 0;
+   ctx->gfx_pipeline_state.vertices_per_patch = dinfo->vertices_per_patch;
    struct zink_gfx_program *gfx_program = get_gfx_program(ctx);
    if (!gfx_program)
       return;
@@ -280,127 +671,48 @@ zink_draw_vbo(struct pipe_context *pctx,
              index_buffer = dinfo->index.resource;
        }
    }
-
-   VkWriteDescriptorSet wds[PIPE_SHADER_TYPES * PIPE_MAX_CONSTANT_BUFFERS + PIPE_SHADER_TYPES * PIPE_MAX_SHADER_SAMPLER_VIEWS];
-   VkDescriptorBufferInfo buffer_infos[PIPE_SHADER_TYPES * PIPE_MAX_CONSTANT_BUFFERS];
-   VkDescriptorImageInfo image_infos[PIPE_SHADER_TYPES * PIPE_MAX_SHADER_SAMPLER_VIEWS];
-   int num_wds = 0, num_buffer_info = 0, num_image_info = 0;
-
-   struct zink_resource *transitions[PIPE_SHADER_TYPES * PIPE_MAX_SHADER_SAMPLER_VIEWS];
-   int num_transitions = 0;
-
-   for (int i = 0; i < ARRAY_SIZE(ctx->gfx_stages); i++) {
-      struct zink_shader *shader = ctx->gfx_stages[i];
-      if (!shader)
-         continue;
-
-      if (i == MESA_SHADER_VERTEX && ctx->num_so_targets) {
-         for (unsigned i = 0; i < ctx->num_so_targets; i++) {
-            struct zink_so_target *t = zink_so_target(ctx->so_targets[i]);
-            t->stride = shader->streamout.so_info.stride[i] * sizeof(uint32_t);
-         }
-      }
-
-      for (int j = 0; j < shader->num_bindings; j++) {
-         int index = shader->bindings[j].index;
-         if (shader->bindings[j].type == VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER) {
-            assert(ctx->ubos[i][index].buffer_size > 0);
-            assert(ctx->ubos[i][index].buffer_size <= screen->props.limits.maxUniformBufferRange);
-            assert(ctx->ubos[i][index].buffer);
-            struct zink_resource *res = zink_resource(ctx->ubos[i][index].buffer);
-            buffer_infos[num_buffer_info].buffer = res->buffer;
-            buffer_infos[num_buffer_info].offset = ctx->ubos[i][index].buffer_offset;
-            buffer_infos[num_buffer_info].range  = ctx->ubos[i][index].buffer_size;
-            wds[num_wds].pBufferInfo = buffer_infos + num_buffer_info;
-            ++num_buffer_info;
-         } else {
-            struct pipe_sampler_view *psampler_view = ctx->image_views[i][index];
-            assert(psampler_view);
-            struct zink_sampler_view *sampler_view = zink_sampler_view(psampler_view);
-
-            struct zink_resource *res = zink_resource(psampler_view->texture);
-            VkImageLayout layout = res->layout;
-            if (layout != VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL &&
-                layout != VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL &&
-                layout != VK_IMAGE_LAYOUT_GENERAL) {
-               transitions[num_transitions++] = res;
-               layout = VK_IMAGE_LAYOUT_GENERAL;
-            }
-            image_infos[num_image_info].imageLayout = layout;
-            image_infos[num_image_info].imageView = sampler_view->image_view;
-            image_infos[num_image_info].sampler = ctx->samplers[i][index];
-            wds[num_wds].pImageInfo = image_infos + num_image_info;
-            ++num_image_info;
-         }
-
-         wds[num_wds].sType = VK_STRUCTURE_TYPE_WRITE_DESCRIPTOR_SET;
-         wds[num_wds].pNext = NULL;
-         wds[num_wds].dstBinding = shader->bindings[j].binding;
-         wds[num_wds].dstArrayElement = 0;
-         wds[num_wds].descriptorCount = 1;
-         wds[num_wds].descriptorType = shader->bindings[j].type;
-         ++num_wds;
-      }
-   }
-
-   struct zink_batch *batch;
-   if (num_transitions > 0) {
-      batch = zink_batch_no_rp(ctx);
-
-      for (int i = 0; i < num_transitions; ++i)
-         zink_resource_barrier(batch->cmdbuf, transitions[i],
-                               transitions[i]->aspect,
-                               VK_IMAGE_LAYOUT_GENERAL);
-   }
-
    if (ctx->xfb_barrier)
       zink_emit_xfb_counter_barrier(ctx);
 
-   if (ctx->dirty_so_targets)
+   if (ctx->dirty_so_targets && ctx->num_so_targets)
       zink_emit_stream_output_targets(pctx);
 
-   if (so_target && zink_resource(so_target->base.buffer)->needs_xfb_barrier)
+   if (so_target && zink_resource_buffer_needs_barrier(zink_resource(so_target->base.buffer),
+                                                       VK_ACCESS_VERTEX_ATTRIBUTE_READ_BIT,
+                                                       VK_PIPELINE_STAGE_VERTEX_INPUT_BIT))
       zink_emit_xfb_vertex_input_barrier(ctx, zink_resource(so_target->base.buffer));
 
-
-   batch = zink_batch_rp(ctx);
-
-   if (batch->descs_left < gfx_program->num_descriptors) {
-      ctx->base.flush(&ctx->base, NULL, 0);
-      batch = zink_batch_rp(ctx);
-      assert(batch->descs_left >= gfx_program->num_descriptors);
+   barrier_vertex_buffers(ctx);
+   barrier_draw_buffers(ctx, dinfo, index_buffer);
+
+   update_descriptors(ctx, screen, false);
+
+   struct zink_batch *batch = zink_batch_rp(ctx);
+   VkViewport viewports[PIPE_MAX_VIEWPORTS] = {};
+   for (unsigned i = 0; i < ctx->gfx_pipeline_state.num_viewports; i++) {
+      VkViewport viewport = {
+         ctx->gfx_pipeline_state.viewport_states[i].translate[0] - ctx->gfx_pipeline_state.viewport_states[i].scale[0],
+         ctx->gfx_pipeline_state.viewport_states[i].translate[1] - ctx->gfx_pipeline_state.viewport_states[i].scale[1],
+         ctx->gfx_pipeline_state.viewport_states[i].scale[0] * 2,
+         ctx->gfx_pipeline_state.viewport_states[i].scale[1] * 2,
+         ctx->rast_state->base.clip_halfz ?
+            ctx->gfx_pipeline_state.viewport_states[i].translate[2] :
+            ctx->gfx_pipeline_state.viewport_states[i].translate[2] - ctx->gfx_pipeline_state.viewport_states[i].scale[2],
+         ctx->gfx_pipeline_state.viewport_states[i].translate[2] + ctx->gfx_pipeline_state.viewport_states[i].scale[2]
+      };
+      viewports[i] = viewport;
    }
-   zink_batch_reference_program(batch, ctx->curr_program);
-
-   VkDescriptorSet desc_set = allocate_descriptor_set(screen, batch,
-                                                      gfx_program);
-   assert(desc_set != VK_NULL_HANDLE);
-
-   for (int i = 0; i < ARRAY_SIZE(ctx->gfx_stages); i++) {
-      struct zink_shader *shader = ctx->gfx_stages[i];
-      if (!shader)
-         continue;
-
-      for (int j = 0; j < shader->num_bindings; j++) {
-         int index = shader->bindings[j].index;
-         if (shader->bindings[j].type == VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER) {
-            struct zink_resource *res = zink_resource(ctx->ubos[i][index].buffer);
-            zink_batch_reference_resoure(batch, res);
-         } else {
-            struct zink_sampler_view *sampler_view = zink_sampler_view(ctx->image_views[i][index]);
-            zink_batch_reference_sampler_view(batch, sampler_view);
-         }
-      }
-   }
-
-   vkCmdSetViewport(batch->cmdbuf, 0, ctx->num_viewports, ctx->viewports);
+   vkCmdSetViewport(batch->cmdbuf, 0, ctx->gfx_pipeline_state.num_viewports, viewports);
    if (ctx->rast_state->base.scissor)
-      vkCmdSetScissor(batch->cmdbuf, 0, ctx->num_viewports, ctx->scissors);
+      vkCmdSetScissor(batch->cmdbuf, 0, ctx->gfx_pipeline_state.num_viewports, ctx->gfx_pipeline_state.scissors);
    else if (ctx->fb_state.width && ctx->fb_state.height) {
-      VkRect2D fb_scissor = {};
-      fb_scissor.extent.width = ctx->fb_state.width;
-      fb_scissor.extent.height = ctx->fb_state.height;
-      vkCmdSetScissor(batch->cmdbuf, 0, 1, &fb_scissor);
+      VkRect2D fb_scissor[ctx->gfx_pipeline_state.num_viewports];
+      for (unsigned i = 0; i < ctx->gfx_pipeline_state.num_viewports; i++) {
+         fb_scissor[i].offset.x = fb_scissor[i].offset.y = 0;
+         fb_scissor[i].extent.width = ctx->fb_state.width;
+         fb_scissor[i].extent.height = ctx->fb_state.height;
+      }
+      vkCmdSetScissor(batch->cmdbuf, 0, ctx->gfx_pipeline_state.num_viewports, fb_scissor);
    }
 
    if (line_width_needed(reduced_prim, rast_state->hw_state.polygon_mode)) {
@@ -421,23 +733,36 @@ zink_draw_vbo(struct pipe_context *pctx,
    if (ctx->gfx_pipeline_state.blend_state->need_blend_constants)
       vkCmdSetBlendConstants(batch->cmdbuf, ctx->blend_constants);
 
-   if (num_wds > 0) {
-      for (int i = 0; i < num_wds; ++i)
-         wds[i].dstSet = desc_set;
-      vkUpdateDescriptorSets(screen->dev, num_wds, wds, 0, NULL);
-   }
 
    vkCmdBindPipeline(batch->cmdbuf, VK_PIPELINE_BIND_POINT_GRAPHICS, pipeline);
-   vkCmdBindDescriptorSets(batch->cmdbuf, VK_PIPELINE_BIND_POINT_GRAPHICS,
-                           gfx_program->layout, 0, 1, &desc_set, 0, NULL);
+
    zink_bind_vertex_buffers(batch, ctx);
 
+   if (ctx->gfx_stages[PIPE_SHADER_VERTEX]->nir->info.system_values_read & (1ull << SYSTEM_VALUE_BASE_VERTEX)) {
+      unsigned draw_mode_is_indexed = dinfo->index_size > 0;
+      vkCmdPushConstants(batch->cmdbuf, gfx_program->layout, VK_SHADER_STAGE_VERTEX_BIT,
+                         offsetof(struct zink_push_constant, draw_mode_is_indexed), sizeof(unsigned),
+                         &draw_mode_is_indexed);
+   }
+   if (ctx->gfx_stages[PIPE_SHADER_VERTEX]->nir->info.system_values_read & (1ull << SYSTEM_VALUE_DRAW_ID)) {
+      unsigned draw_id = dinfo->drawid;
+      vkCmdPushConstants(batch->cmdbuf, gfx_program->layout, VK_SHADER_STAGE_VERTEX_BIT,
+                         offsetof(struct zink_push_constant, draw_id), sizeof(unsigned),
+                         &draw_id);
+   }
+   if (gfx_program->shaders[PIPE_SHADER_TESS_CTRL])
+      vkCmdPushConstants(batch->cmdbuf, gfx_program->layout, VK_SHADER_STAGE_TESSELLATION_CONTROL_BIT,
+                         offsetof(struct zink_push_constant, default_inner_level), sizeof(float) * 6,
+                         &ctx->gfx_pipeline_state.default_inner_level[0]);
+
+   zink_query_update_gs_states(ctx);
+
    if (ctx->num_so_targets) {
       for (unsigned i = 0; i < ctx->num_so_targets; i++) {
          struct zink_so_target *t = zink_so_target(ctx->so_targets[i]);
-         struct zink_resource *res = zink_resource(t->counter_buffer);
-         if (t->counter_buffer_valid) {
-            zink_batch_reference_resoure(batch, zink_resource(t->counter_buffer));
+         if (t && t->counter_buffer_valid) {
+            struct zink_resource *res = zink_resource(t->counter_buffer);
+            zink_batch_reference_resource_rw(batch, res, true);
             counter_buffers[i] = res->buffer;
             counter_buffer_offsets[i] = t->counter_buffer_offset;
          } else
@@ -446,6 +771,8 @@ zink_draw_vbo(struct pipe_context *pctx,
       screen->vk_CmdBeginTransformFeedbackEXT(batch->cmdbuf, 0, ctx->num_so_targets, counter_buffers, counter_buffer_offsets);
    }
 
+   flush_persistent_maps(screen, batch);
+
    if (dinfo->index_size > 0) {
       VkIndexType index_type;
       unsigned index_size = dinfo->index_size;
@@ -468,18 +795,41 @@ zink_draw_vbo(struct pipe_context *pctx,
       }
       struct zink_resource *res = zink_resource(index_buffer);
       vkCmdBindIndexBuffer(batch->cmdbuf, res->buffer, index_offset, index_type);
-      zink_batch_reference_resoure(batch, res);
-      vkCmdDrawIndexed(batch->cmdbuf,
-         dinfo->count, dinfo->instance_count,
-         need_index_buffer_unref ? 0 : dinfo->start, dinfo->index_bias, dinfo->start_instance);
+      zink_batch_reference_resource_rw(batch, res, false);
+      if (dinfo->indirect) {
+         struct zink_resource *indirect = zink_resource(dinfo->indirect->buffer);
+         zink_batch_reference_resource_rw(batch, indirect, false);
+         if (dinfo->indirect->indirect_draw_count) {
+             struct zink_resource *indirect_draw_count = zink_resource(dinfo->indirect->indirect_draw_count);
+             zink_batch_reference_resource_rw(batch, indirect_draw_count, false);
+             vkCmdDrawIndexedIndirectCount(batch->cmdbuf, indirect->buffer, dinfo->indirect->offset,
+                                           indirect_draw_count->buffer, dinfo->indirect->indirect_draw_count_offset,
+                                           dinfo->indirect->draw_count, dinfo->indirect->stride);
+         } else
+            vkCmdDrawIndexedIndirect(batch->cmdbuf, indirect->buffer, dinfo->indirect->offset, dinfo->indirect->draw_count, dinfo->indirect->stride);
+      } else
+         vkCmdDrawIndexed(batch->cmdbuf,
+            dinfo->count, dinfo->instance_count,
+            need_index_buffer_unref ? 0 : dinfo->start, dinfo->index_bias, dinfo->start_instance);
    } else {
       if (so_target && screen->tf_props.transformFeedbackDraw) {
-         zink_batch_reference_resoure(batch, zink_resource(so_target->counter_buffer));
+         zink_batch_reference_resource_rw(batch, zink_resource(so_target->base.buffer), false);
+         zink_batch_reference_resource_rw(batch, zink_resource(so_target->counter_buffer), true);
          screen->vk_CmdDrawIndirectByteCountEXT(batch->cmdbuf, dinfo->instance_count, dinfo->start_instance,
                                        zink_resource(so_target->counter_buffer)->buffer, so_target->counter_buffer_offset, 0,
                                        MIN2(so_target->stride, screen->tf_props.maxTransformFeedbackBufferDataStride));
-      }
-      else
+      } else if (dinfo->indirect) {
+         struct zink_resource *indirect = zink_resource(dinfo->indirect->buffer);
+         zink_batch_reference_resource_rw(batch, indirect, false);
+         if (dinfo->indirect->indirect_draw_count) {
+             struct zink_resource *indirect_draw_count = zink_resource(dinfo->indirect->indirect_draw_count);
+             zink_batch_reference_resource_rw(batch, indirect_draw_count, false);
+             vkCmdDrawIndirectCount(batch->cmdbuf, indirect->buffer, dinfo->indirect->offset,
+                                           indirect_draw_count->buffer, dinfo->indirect->indirect_draw_count_offset,
+                                           dinfo->indirect->draw_count, dinfo->indirect->stride);
+         } else
+            vkCmdDrawIndirect(batch->cmdbuf, indirect->buffer, dinfo->indirect->offset, dinfo->indirect->draw_count, dinfo->indirect->stride);
+      } else
          vkCmdDraw(batch->cmdbuf, dinfo->count, dinfo->instance_count, dinfo->start, dinfo->start_instance);
    }
 
@@ -489,11 +839,43 @@ zink_draw_vbo(struct pipe_context *pctx,
    if (ctx->num_so_targets) {
       for (unsigned i = 0; i < ctx->num_so_targets; i++) {
          struct zink_so_target *t = zink_so_target(ctx->so_targets[i]);
-         counter_buffers[i] = zink_resource(t->counter_buffer)->buffer;
-         counter_buffer_offsets[i] = t->counter_buffer_offset;
-         t->counter_buffer_valid = true;
-         zink_resource(ctx->so_targets[i]->buffer)->needs_xfb_barrier = true;
+         if (t) {
+            counter_buffers[i] = zink_resource(t->counter_buffer)->buffer;
+            counter_buffer_offsets[i] = t->counter_buffer_offset;
+            t->counter_buffer_valid = true;
+         }
       }
       screen->vk_CmdEndTransformFeedbackEXT(batch->cmdbuf, 0, ctx->num_so_targets, counter_buffers, counter_buffer_offsets);
    }
+   batch->has_draw = true;
+}
+
+void
+zink_launch_grid(struct pipe_context *pctx, const struct pipe_grid_info *info)
+{
+   struct zink_context *ctx = zink_context(pctx);
+   struct zink_screen *screen = zink_screen(pctx->screen);
+   struct zink_batch *batch = &ctx->compute_batch;
+   if (!batch->descs_left)
+      zink_start_batch(ctx, batch);
+   struct zink_compute_program *comp_program = get_compute_program(ctx);
+   if (!comp_program)
+      return;
+
+   VkPipeline pipeline = zink_get_compute_pipeline(screen, comp_program,
+                                               &ctx->compute_pipeline_state);
+
+   update_descriptors(ctx, screen, true);
+
+
+   vkCmdBindPipeline(batch->cmdbuf, VK_PIPELINE_BIND_POINT_COMPUTE, pipeline);
+
+   flush_persistent_maps(screen, batch);
+
+   if (info->indirect) {
+      vkCmdDispatchIndirect(batch->cmdbuf, zink_resource(info->indirect)->buffer, info->indirect_offset);
+      zink_batch_reference_resource_rw(batch, zink_resource(info->indirect), false);
+   } else
+      vkCmdDispatch(batch->cmdbuf, info->grid[0], info->grid[1], info->grid[2]);
+   batch->has_draw = true;
 }
diff --git a/src/gallium/drivers/zink/zink_fence.c b/src/gallium/drivers/zink/zink_fence.c
index 86679f2caf7..8341debccaf 100644
--- a/src/gallium/drivers/zink/zink_fence.c
+++ b/src/gallium/drivers/zink/zink_fence.c
@@ -25,8 +25,10 @@
 #include "zink_fence.h"
 
 #include "zink_query.h"
+#include "zink_resource.h"
 #include "zink_screen.h"
 
+#include "util/set.h"
 #include "util/u_memory.h"
 
 static void
@@ -58,6 +60,17 @@ zink_create_fence(struct pipe_screen *pscreen, struct zink_batch *batch)
    ret->active_queries = batch->active_queries;
    batch->active_queries = NULL;
 
+   ret->batch_id = batch->batch_id;
+   util_dynarray_init(&ret->resources, NULL);
+   set_foreach(batch->resources, entry) {
+      /* the fence needs its own reference to ensure it can safely access lifetime-dependent
+       * resource members
+       */
+      struct pipe_resource *r = NULL, *pres = (struct pipe_resource *)entry->key;
+      pipe_resource_reference(&r, pres);
+      util_dynarray_append(&ret->resources, struct pipe_resource*, pres);
+   }
+
    pipe_reference_init(&ret->reference, 1);
    return ret;
 
@@ -86,14 +99,35 @@ fence_reference(struct pipe_screen *pscreen,
                         zink_fence(pfence));
 }
 
+static inline void
+fence_remove_resource_access(struct zink_fence *fence, struct zink_resource *res)
+{
+   p_atomic_set(&res->batch_uses[fence->batch_id], 0);
+}
+
 bool
 zink_fence_finish(struct zink_screen *screen, struct zink_fence *fence,
                   uint64_t timeout_ns)
 {
    bool success = vkWaitForFences(screen->dev, 1, &fence->fence, VK_TRUE,
                                   timeout_ns) == VK_SUCCESS;
-   if (success && fence->active_queries)
-      zink_prune_queries(screen, fence);
+   if (success) {
+      if (fence->active_queries)
+         zink_prune_queries(screen, fence);
+
+      /* unref all used resources */
+      util_dynarray_foreach(&fence->resources, struct pipe_resource*, pres) {
+         struct zink_resource *stencil, *res = zink_resource(*pres);
+         fence_remove_resource_access(fence, res);
+
+         /* we still hold a ref, so this doesn't need to be atomic */
+         zink_get_depth_stencil_resources((struct pipe_resource*)res, NULL, &stencil);
+         if (stencil)
+            fence_remove_resource_access(fence, stencil);
+         pipe_resource_reference(pres, NULL);
+      }
+      util_dynarray_clear(&fence->resources);
+   }
    return success;
 }
 
diff --git a/src/gallium/drivers/zink/zink_fence.h b/src/gallium/drivers/zink/zink_fence.h
index dab6f5ece8d..06ef49947e6 100644
--- a/src/gallium/drivers/zink/zink_fence.h
+++ b/src/gallium/drivers/zink/zink_fence.h
@@ -25,6 +25,7 @@
 #define ZINK_FENCE_H
 
 #include "util/u_inlines.h"
+#include "util/u_dynarray.h"
 
 #include <vulkan/vulkan.h>
 
@@ -33,8 +34,10 @@ struct zink_screen;
 
 struct zink_fence {
    struct pipe_reference reference;
+   unsigned batch_id : 2;
    VkFence fence;
    struct set *active_queries; /* zink_query objects which were active at some point in this batch */
+   struct util_dynarray resources;
 };
 
 static inline struct zink_fence *
diff --git a/src/gallium/drivers/zink/zink_format.c b/src/gallium/drivers/zink/zink_format.c
index 50f89995a90..d19ead4f02d 100644
--- a/src/gallium/drivers/zink/zink_format.c
+++ b/src/gallium/drivers/zink/zink_format.c
@@ -91,8 +91,15 @@ static const VkFormat formats[PIPE_FORMAT_COUNT] = {
    [PIPE_FORMAT_B5G5R5A1_UNORM] = VK_FORMAT_B5G5R5A1_UNORM_PACK16,
    [PIPE_FORMAT_R11G11B10_FLOAT] = VK_FORMAT_B10G11R11_UFLOAT_PACK32,
    [PIPE_FORMAT_R9G9B9E5_FLOAT] = VK_FORMAT_E5B9G9R9_UFLOAT_PACK32,
+   /* ARB_vertex_type_2_10_10_10 */
    [PIPE_FORMAT_R10G10B10A2_UNORM] = VK_FORMAT_A2B10G10R10_UNORM_PACK32,
+   [PIPE_FORMAT_R10G10B10A2_SNORM] = VK_FORMAT_A2B10G10R10_SNORM_PACK32,
    [PIPE_FORMAT_B10G10R10A2_UNORM] = VK_FORMAT_A2R10G10B10_UNORM_PACK32,
+   [PIPE_FORMAT_B10G10R10A2_SNORM] = VK_FORMAT_A2B10G10R10_SNORM_PACK32,
+   [PIPE_FORMAT_R10G10B10A2_USCALED] = VK_FORMAT_A2B10G10R10_USCALED_PACK32,
+   [PIPE_FORMAT_R10G10B10A2_SSCALED] = VK_FORMAT_A2B10G10R10_SSCALED_PACK32,
+   [PIPE_FORMAT_B10G10R10A2_USCALED] = VK_FORMAT_A2R10G10B10_USCALED_PACK32,
+   [PIPE_FORMAT_B10G10R10A2_SSCALED] = VK_FORMAT_A2B10G10R10_SSCALED_PACK32,
    [PIPE_FORMAT_R10G10B10A2_UINT] = VK_FORMAT_A2B10G10R10_UINT_PACK32,
    [PIPE_FORMAT_B10G10R10A2_UINT] = VK_FORMAT_A2R10G10B10_UINT_PACK32,
 
@@ -101,7 +108,9 @@ static const VkFormat formats[PIPE_FORMAT_COUNT] = {
    [PIPE_FORMAT_Z32_FLOAT_S8X24_UINT] = VK_FORMAT_D32_SFLOAT_S8_UINT,
    [PIPE_FORMAT_Z16_UNORM] = VK_FORMAT_D16_UNORM,
    [PIPE_FORMAT_Z24X8_UNORM] = VK_FORMAT_X8_D24_UNORM_PACK32,
+   [PIPE_FORMAT_X24S8_UINT] = VK_FORMAT_X8_D24_UNORM_PACK32,
    [PIPE_FORMAT_Z24_UNORM_S8_UINT] = VK_FORMAT_D24_UNORM_S8_UINT,
+   [PIPE_FORMAT_S8_UINT] = VK_FORMAT_S8_UINT,
 
    // compressed formats
    [PIPE_FORMAT_DXT1_RGB] = VK_FORMAT_BC1_RGB_UNORM_BLOCK,
diff --git a/src/gallium/drivers/zink/zink_pipeline.c b/src/gallium/drivers/zink/zink_pipeline.c
index 976c3afc857..fbee216b429 100644
--- a/src/gallium/drivers/zink/zink_pipeline.c
+++ b/src/gallium/drivers/zink/zink_pipeline.c
@@ -46,6 +46,14 @@ zink_create_gfx_pipeline(struct zink_screen *screen,
    vertex_input_state.pVertexAttributeDescriptions = state->element_state->attribs;
    vertex_input_state.vertexAttributeDescriptionCount = state->element_state->num_attribs;
 
+   VkPipelineVertexInputDivisorStateCreateInfoEXT vdiv_state = {};
+   if (state->divisors_present) {
+       vertex_input_state.pNext = &vdiv_state;
+       vdiv_state.sType = VK_STRUCTURE_TYPE_PIPELINE_VERTEX_INPUT_DIVISOR_STATE_CREATE_INFO_EXT;
+       vdiv_state.vertexBindingDivisorCount = state->divisors_present;
+       vdiv_state.pVertexBindingDivisors = state->divisors;
+   }
+
    VkPipelineInputAssemblyStateCreateInfo primitive_state = {};
    primitive_state.sType = VK_STRUCTURE_TYPE_PIPELINE_INPUT_ASSEMBLY_STATE_CREATE_INFO;
    primitive_state.topology = primitive_topology;
@@ -65,11 +73,19 @@ zink_create_gfx_pipeline(struct zink_screen *screen,
    }
 
    VkPipelineColorBlendStateCreateInfo blend_state = {};
+   VkPipelineColorBlendAdvancedStateCreateInfoEXT advanced_blend_state = {};
    blend_state.sType = VK_STRUCTURE_TYPE_PIPELINE_COLOR_BLEND_STATE_CREATE_INFO;
    blend_state.pAttachments = state->blend_state->attachments;
    blend_state.attachmentCount = state->num_attachments;
    blend_state.logicOpEnable = state->blend_state->logicop_enable;
    blend_state.logicOp = state->blend_state->logicop_func;
+   if (state->blend_state->advanced_blend) {
+      blend_state.pNext = &advanced_blend_state;
+      advanced_blend_state.sType = VK_STRUCTURE_TYPE_PIPELINE_COLOR_BLEND_ADVANCED_STATE_CREATE_INFO_EXT;
+      advanced_blend_state.blendOverlap = VK_BLEND_OVERLAP_UNCORRELATED_EXT;
+      advanced_blend_state.srcPremultiplied = VK_TRUE;
+      advanced_blend_state.dstPremultiplied = VK_TRUE;
+   }
 
    VkPipelineMultisampleStateCreateInfo ms_state = {};
    ms_state.sType = VK_STRUCTURE_TYPE_PIPELINE_MULTISAMPLE_STATE_CREATE_INFO;
@@ -77,12 +93,16 @@ zink_create_gfx_pipeline(struct zink_screen *screen,
    ms_state.alphaToCoverageEnable = state->blend_state->alpha_to_coverage;
    ms_state.alphaToOneEnable = state->blend_state->alpha_to_one;
    ms_state.pSampleMask = state->sample_mask ? &state->sample_mask : NULL;
+   if (state->rast_state->force_persample_interp) {
+      ms_state.sampleShadingEnable = VK_TRUE;
+      ms_state.minSampleShading = 1.0;
+   }
 
    VkPipelineViewportStateCreateInfo viewport_state = {};
    viewport_state.sType = VK_STRUCTURE_TYPE_PIPELINE_VIEWPORT_STATE_CREATE_INFO;
-   viewport_state.viewportCount = 1;
+   viewport_state.viewportCount = state->num_viewports;
    viewport_state.pViewports = NULL;
-   viewport_state.scissorCount = 1;
+   viewport_state.scissorCount = state->num_viewports;
    viewport_state.pScissors = NULL;
 
    VkPipelineRasterizationStateCreateInfo rast_state = {};
@@ -140,6 +160,13 @@ zink_create_gfx_pipeline(struct zink_screen *screen,
    pci.pDepthStencilState = &depth_stencil_state;
    pci.pDynamicState = &pipelineDynamicStateCreateInfo;
 
+   VkPipelineTessellationStateCreateInfo tci = {};
+   if (prog->shaders[PIPE_SHADER_TESS_CTRL] && prog->shaders[PIPE_SHADER_TESS_EVAL]) {
+      tci.sType = VK_STRUCTURE_TYPE_PIPELINE_TESSELLATION_STATE_CREATE_INFO;
+      tci.patchControlPoints = state->vertices_per_patch;
+      pci.pTessellationState = &tci;
+   }
+
    VkPipelineShaderStageCreateInfo shader_stages[ZINK_SHADER_COUNT];
    uint32_t num_stages = 0;
    for (int i = 0; i < ZINK_SHADER_COUNT; ++i) {
@@ -167,3 +194,29 @@ zink_create_gfx_pipeline(struct zink_screen *screen,
 
    return pipeline;
 }
+
+VkPipeline
+zink_create_compute_pipeline(struct zink_screen *screen, struct zink_compute_program *comp, struct zink_compute_pipeline_state *state)
+{
+   VkComputePipelineCreateInfo pci = {};
+   pci.sType = VK_STRUCTURE_TYPE_COMPUTE_PIPELINE_CREATE_INFO;
+   pci.flags = VK_PIPELINE_CREATE_DISABLE_OPTIMIZATION_BIT;
+   pci.layout = comp->layout;
+
+   VkPipelineShaderStageCreateInfo stage = {};
+   stage.sType = VK_STRUCTURE_TYPE_PIPELINE_SHADER_STAGE_CREATE_INFO;
+   stage.stage = VK_SHADER_STAGE_COMPUTE_BIT;
+   stage.module = comp->module->shader;
+   stage.pName = "main";
+
+   pci.stage = stage;
+
+   VkPipeline pipeline;
+   if (vkCreateComputePipelines(screen->dev, VK_NULL_HANDLE, 1, &pci,
+                                 NULL, &pipeline) != VK_SUCCESS) {
+      debug_printf("vkCreateComputePipelines failed\n");
+      return VK_NULL_HANDLE;
+   }
+
+   return pipeline;
+}
diff --git a/src/gallium/drivers/zink/zink_pipeline.h b/src/gallium/drivers/zink/zink_pipeline.h
index 116ce656405..19fee5beba2 100644
--- a/src/gallium/drivers/zink/zink_pipeline.h
+++ b/src/gallium/drivers/zink/zink_pipeline.h
@@ -31,6 +31,7 @@
 struct zink_blend_state;
 struct zink_depth_stencil_alpha_state;
 struct zink_gfx_program;
+struct zink_compute_program;
 struct zink_rasterizer_state;
 struct zink_render_pass;
 struct zink_screen;
@@ -41,6 +42,8 @@ struct zink_gfx_pipeline_state {
 
    struct zink_vertex_elements_hw_state *element_state;
    VkVertexInputBindingDescription bindings[PIPE_MAX_ATTRIBS]; // combination of element_state and stride
+   VkVertexInputBindingDivisorDescriptionEXT divisors[PIPE_MAX_ATTRIBS];
+   uint8_t divisors_present;
 
    uint32_t num_attachments;
    struct zink_blend_state *blend_state;
@@ -51,9 +54,28 @@ struct zink_gfx_pipeline_state {
 
    VkSampleMask sample_mask;
    uint8_t rast_samples;
+   uint8_t vertices_per_patch;
+
+   float default_inner_level[2];
+   float default_outer_level[4];
+
+   struct pipe_viewport_state viewport_states[PIPE_MAX_VIEWPORTS];
+   struct pipe_scissor_state scissor_states[PIPE_MAX_VIEWPORTS];
+   VkRect2D scissors[PIPE_MAX_VIEWPORTS];
+   unsigned num_viewports;
 
    bool primitive_restart;
 
+#ifndef NDEBUG
+   /* needed when assert()s can trigger because mesa pre-hash asserts the hash value */
+   uint32_t stages[PIPE_SHADER_TYPES - 1];
+#endif
+   /* Pre-hashed value for table lookup, invalid when zero.
+    * Members after this point are not included in pipeline state hash key */
+   uint32_t hash;
+};
+
+struct zink_compute_pipeline_state {
    /* Pre-hashed value for table lookup, invalid when zero.
     * Members after this point are not included in pipeline state hash key */
    uint32_t hash;
@@ -65,4 +87,6 @@ zink_create_gfx_pipeline(struct zink_screen *screen,
                          struct zink_gfx_pipeline_state *state,
                          VkPrimitiveTopology primitive_topology);
 
+VkPipeline
+zink_create_compute_pipeline(struct zink_screen *screen, struct zink_compute_program *comp, struct zink_compute_pipeline_state *state);
 #endif
diff --git a/src/gallium/drivers/zink/zink_program.c b/src/gallium/drivers/zink/zink_program.c
index 0bcaa4a03e7..e051a1d9f2c 100644
--- a/src/gallium/drivers/zink/zink_program.c
+++ b/src/gallium/drivers/zink/zink_program.c
@@ -27,6 +27,7 @@
 #include "zink_context.h"
 #include "zink_render_pass.h"
 #include "zink_screen.h"
+#include "zink_state.h"
 
 #include "util/hash_table.h"
 #include "util/set.h"
@@ -34,23 +35,86 @@
 #include "util/u_memory.h"
 #include "tgsi/tgsi_from_mesa.h"
 
-struct pipeline_cache_entry {
+#ifdef NDEBUG
+/* for pipeline cache */
+# define XXH_INLINE_ALL
+# include "util/xxhash.h"
+#endif
+
+struct gfx_pipeline_cache_entry {
    struct zink_gfx_pipeline_state state;
    VkPipeline pipeline;
 };
 
+struct compute_pipeline_cache_entry {
+   struct zink_compute_pipeline_state state;
+   VkPipeline pipeline;
+};
+
 void
 debug_describe_zink_gfx_program(char *buf, const struct zink_gfx_program *ptr)
 {
    sprintf(buf, "zink_gfx_program");
 }
 
+void
+debug_describe_zink_compute_program(char *buf, const struct zink_compute_program *ptr)
+{
+   sprintf(buf, "zink_compute_program");
+}
+
 static void
 debug_describe_zink_shader_module(char *buf, const struct zink_shader_module *ptr)
 {
    sprintf(buf, "zink_shader_module");
 }
 
+static void
+debug_describe_zink_shader_cache(char* buf, const struct zink_shader_cache *ptr)
+{
+   sprintf(buf, "zink_shader_cache");
+}
+
+/* copied from iris */
+struct keybox {
+   uint16_t size;
+   gl_shader_stage stage;
+   uint8_t data[0];
+};
+
+static struct keybox *
+make_keybox(void *mem_ctx,
+            gl_shader_stage stage,
+            const void *key,
+            uint32_t key_size)
+{
+   struct keybox *keybox =
+      ralloc_size(mem_ctx, sizeof(struct keybox) + key_size);
+
+   keybox->stage = stage;
+   keybox->size = key_size;
+   memcpy(keybox->data, key, key_size);
+
+   return keybox;
+}
+
+static uint32_t
+keybox_hash(const void *void_key)
+{
+   const struct keybox *key = void_key;
+   return _mesa_hash_data(&key->stage, key->size + sizeof(key->stage));
+}
+
+static bool
+keybox_equals(const void *void_a, const void *void_b)
+{
+   const struct keybox *a = void_a, *b = void_b;
+   if (a->size != b->size)
+      return false;
+
+   return memcmp(a->data, b->data, a->size) == 0;
+}
+
 static VkDescriptorSetLayout
 create_desc_set_layout(VkDevice dev,
                        struct zink_shader *stages[ZINK_SHADER_COUNT],
@@ -64,12 +128,12 @@ create_desc_set_layout(VkDevice dev,
       if (!shader)
          continue;
 
-      VkShaderStageFlagBits stage_flags = zink_shader_stage(i);
+      VkShaderStageFlagBits stage_flags = zink_shader_stage(pipe_shader_type_from_mesa(shader->nir->info.stage));
       for (int j = 0; j < shader->num_bindings; j++) {
          assert(num_bindings < ARRAY_SIZE(bindings));
          bindings[num_bindings].binding = shader->bindings[j].binding;
          bindings[num_bindings].descriptorType = shader->bindings[j].type;
-         bindings[num_bindings].descriptorCount = 1;
+         bindings[num_bindings].descriptorCount = shader->bindings[j].size;
          bindings[num_bindings].stageFlags = stage_flags;
          bindings[num_bindings].pImmutableSamplers = NULL;
          ++num_bindings;
@@ -94,7 +158,38 @@ create_desc_set_layout(VkDevice dev,
 }
 
 static VkPipelineLayout
-create_pipeline_layout(VkDevice dev, VkDescriptorSetLayout dsl)
+create_gfx_pipeline_layout(VkDevice dev, VkDescriptorSetLayout dsl)
+{
+   assert(dsl != VK_NULL_HANDLE);
+
+   VkPipelineLayoutCreateInfo plci = {};
+   plci.sType = VK_STRUCTURE_TYPE_PIPELINE_LAYOUT_CREATE_INFO;
+
+   plci.pSetLayouts = &dsl;
+   plci.setLayoutCount = 1;
+
+
+   VkPushConstantRange pcr[2] = {};
+   pcr[0].stageFlags = VK_SHADER_STAGE_VERTEX_BIT;
+   pcr[0].offset = offsetof(struct zink_push_constant, draw_mode_is_indexed);
+   pcr[0].size = 2 * sizeof(unsigned);
+   pcr[1].stageFlags = VK_SHADER_STAGE_TESSELLATION_CONTROL_BIT;
+   pcr[1].offset = offsetof(struct zink_push_constant, default_inner_level);
+   pcr[1].size = sizeof(float) * 6;
+   plci.pushConstantRangeCount = 2;
+   plci.pPushConstantRanges = &pcr[0];
+
+   VkPipelineLayout layout;
+   if (vkCreatePipelineLayout(dev, &plci, NULL, &layout) != VK_SUCCESS) {
+      debug_printf("vkCreatePipelineLayout failed!\n");
+      return VK_NULL_HANDLE;
+   }
+
+   return layout;
+}
+
+static VkPipelineLayout
+create_compute_pipeline_layout(VkDevice dev, VkDescriptorSetLayout dsl)
 {
    assert(dsl != VK_NULL_HANDLE);
 
@@ -113,13 +208,97 @@ create_pipeline_layout(VkDevice dev, VkDescriptorSetLayout dsl)
    return layout;
 }
 
+ 
+static void
+shader_key_vs_gen(struct zink_context *ctx, struct zink_shader *zs, struct zink_shader *shaders[ZINK_SHADER_COUNT], struct zink_shader_key *key)
+{
+   struct zink_vs_key *vs_key = &key->key.vs;
+   key->size = sizeof(struct zink_vs_key);
+
+   vs_key->shader_id = zs->shader_id;
+   vs_key->clip_halfz = ctx->rast_state->base.clip_halfz;
+}
+
+static void
+shader_key_fs_gen(struct zink_context *ctx, struct zink_shader *zs, struct zink_shader *shaders[ZINK_SHADER_COUNT], struct zink_shader_key *key)
+{
+   struct zink_fs_key *fs_key = &key->key.fs;
+   key->size = sizeof(struct zink_fs_key);
+
+   fs_key->shader_id = zs->shader_id;
+   //fs_key->flat_shade = ctx->rast_state->base.flatshade;
+   fs_key->nr_cbufs = ctx->fb_state.nr_cbufs;
+   /* if gl_SampleMask[] is written to, we have to ensure that we get a shader with the same sample count:
+    * in GL, rast_samples==1 means ignore gl_SampleMask[]
+    * in VK, gl_SampleMask[] is never ignored
+    */
+   if (zs->nir->info.outputs_written & (1 << FRAG_RESULT_SAMPLE_MASK))
+      fs_key->samples = !!ctx->fb_state.samples;
+}
+
+static void
+shader_key_tcs_gen(struct zink_context *ctx, struct zink_shader *zs, struct zink_shader *shaders[ZINK_SHADER_COUNT], struct zink_shader_key *key)
+{
+   struct zink_tcs_key *tcs_key = &key->key.tcs;
+   key->size = sizeof(struct zink_tcs_key);
+
+   tcs_key->shader_id = zs->shader_id;
+   tcs_key->vertices_per_patch = ctx->gfx_pipeline_state.vertices_per_patch;
+   tcs_key->vs_outputs_written = shaders[PIPE_SHADER_VERTEX]->nir->info.outputs_written;
+}
+
+typedef void (*zink_shader_key_gen)(struct zink_context *ctx, struct zink_shader *zs, struct zink_shader *shaders[ZINK_SHADER_COUNT], struct zink_shader_key *key);
+static zink_shader_key_gen shader_key_vtbl[] =
+{
+   [MESA_SHADER_VERTEX] = shader_key_vs_gen,
+   [MESA_SHADER_TESS_CTRL] = shader_key_tcs_gen,
+   /* reusing vs key for now since we're only using clip_halfz */
+   [MESA_SHADER_TESS_EVAL] = shader_key_vs_gen,
+   [MESA_SHADER_GEOMETRY] = shader_key_vs_gen,
+   [MESA_SHADER_FRAGMENT] = shader_key_fs_gen,
+};
+
+static VkShaderModule
+get_shader_module_for_stage(struct zink_context *ctx, struct zink_shader *zs, struct zink_gfx_program *prog, uint32_t *hash)
+{
+   gl_shader_stage stage = zs->nir->info.stage;
+   struct zink_shader_key key = {};
+   VkShaderModule mod;
+   struct keybox *keybox;
+
+   shader_key_vtbl[stage](ctx, zs, ctx->gfx_stages, &key);
+   keybox = make_keybox(NULL, stage, &key, key.size);
+   *hash = keybox_hash(keybox);
+   struct hash_entry *entry = _mesa_hash_table_search_pre_hashed(prog->shader_cache->shader_cache, *hash, keybox);
+
+   if (entry) {
+      ralloc_free(keybox);
+      mod = entry->data;
+   }
+   else {
+      mod = zink_shader_compile(zink_screen(ctx->base.screen), zs, &key, prog->shader_slot_map, &prog->shader_slots_reserved);
+
+      _mesa_hash_table_insert_pre_hashed(prog->shader_cache->shader_cache, *hash, keybox, mod);
+   }
+   return mod;
+}
+
 static void
 zink_destroy_shader_module(struct zink_screen *screen, struct zink_shader_module *zm)
 {
-   vkDestroyShaderModule(screen->dev, zm->shader, NULL);
+   /* the actual shader module is in the shader cache */
    free(zm);
 }
 
+static void
+zink_destroy_shader_cache(struct zink_screen *screen, struct zink_shader_cache *sc)
+{
+   hash_table_foreach(sc->shader_cache, entry)
+      vkDestroyShaderModule(screen->dev, entry->data, NULL);
+   _mesa_hash_table_destroy(sc->shader_cache, NULL);
+   free(sc);
+}
+
 static inline void
 zink_shader_module_reference(struct zink_screen *screen,
                            struct zink_shader_module **dst,
@@ -133,11 +312,52 @@ zink_shader_module_reference(struct zink_screen *screen,
    if (dst) *dst = src;
 }
 
+static inline void
+zink_shader_cache_reference(struct zink_screen *screen,
+                           struct zink_shader_cache **dst,
+                           struct zink_shader_cache *src)
+{
+   struct zink_shader_cache *old_dst = dst ? *dst : NULL;
+
+   if (pipe_reference_described(old_dst ? &old_dst->reference : NULL, &src->reference,
+                                (debug_reference_descriptor)debug_describe_zink_shader_cache))
+      zink_destroy_shader_cache(screen, old_dst);
+   if (dst) *dst = src;
+}
+
 static void
-update_shader_modules(struct zink_context *ctx, struct zink_shader *stages[ZINK_SHADER_COUNT], struct zink_gfx_program *prog)
+update_shader_modules(struct zink_context *ctx, struct zink_shader *stages[ZINK_SHADER_COUNT], struct zink_gfx_program *prog, bool update)
 {
    struct zink_shader *dirty[ZINK_SHADER_COUNT] = {NULL};
 
+   if (!update) {
+      unsigned existing_shaders = 0;
+      /* if there's a case where we'll be reusing any shaders, we need to reuse the slot map too */
+      if (ctx->curr_program) {
+         for (int i = 0; i < ZINK_SHADER_COUNT; ++i) {
+             if (ctx->curr_program->shaders[i])
+                existing_shaders |= 1 << i;
+         }
+      }
+      if (ctx->dirty_shader_stages == existing_shaders || !existing_shaders) {
+         /* all shaders are being recompiled: new slot map */
+         memset(prog->shader_slot_map, -1, sizeof(prog->shader_slot_map));
+         /* we need the slot map to match up, so we can't reuse the previous cache if we can't guarantee
+          * the slots match up
+          * TOOD: if we compact the slot map table, we can store it on the shader keys and reuse the cache
+          */
+         prog->shader_cache = CALLOC_STRUCT(zink_shader_cache);
+         pipe_reference_init(&prog->shader_cache->reference, 1);
+         prog->shader_cache->shader_cache =  _mesa_hash_table_create(NULL, keybox_hash, keybox_equals);
+      } else {
+         /* at least some shaders are being reused: use existing slot map so locations match up */
+         memcpy(prog->shader_slot_map, ctx->curr_program->shader_slot_map, sizeof(prog->shader_slot_map));
+         prog->shader_slots_reserved = ctx->curr_program->shader_slots_reserved;
+         /* and then we can also reuse the shader cache since we know the slots are the same */
+         zink_shader_cache_reference(zink_screen(ctx->base.screen), &prog->shader_cache, ctx->curr_program->shader_cache);
+      }
+   }
+
    /* we need to map pipe_shader_type -> gl_shader_stage so we can ensure that we're compiling
     * the shaders in pipeline order and have builtin input/output locations match up after being compacted
     */
@@ -146,18 +366,31 @@ update_shader_modules(struct zink_context *ctx, struct zink_shader *stages[ZINK_
       unsigned type = u_bit_scan(&dirty_shader_stages);
       dirty[tgsi_processor_to_shader_stage(type)] = stages[type];
    }
+   if (ctx->dirty_shader_stages & (1 << PIPE_SHADER_TESS_EVAL)) {
+      if (dirty[MESA_SHADER_TESS_EVAL] && !dirty[MESA_SHADER_TESS_CTRL]) {
+         dirty[MESA_SHADER_TESS_CTRL] = stages[PIPE_SHADER_TESS_CTRL] = zink_shader_tcs_create(ctx, stages[PIPE_SHADER_VERTEX]);
+         dirty[MESA_SHADER_TESS_EVAL]->generated = stages[PIPE_SHADER_TESS_CTRL];
+      }
+   }
+
    for (int i = 0; i < ZINK_SHADER_COUNT; ++i) {
       enum pipe_shader_type type = pipe_shader_type_from_mesa(i);
       if (dirty[i]) {
+         zink_shader_module_reference(zink_screen(ctx->base.screen), &prog->modules[type], NULL);
          prog->modules[type] = CALLOC_STRUCT(zink_shader_module);
          assert(prog->modules[type]);
          pipe_reference_init(&prog->modules[type]->reference, 1);
-         prog->modules[type]->shader = zink_shader_compile(zink_screen(ctx->base.screen), dirty[i]);
-      } else if (stages[type]) /* reuse existing shader module */
+         dirty[i]->has_geometry_shader = dirty[MESA_SHADER_GEOMETRY] || stages[PIPE_SHADER_GEOMETRY];
+         dirty[i]->has_tess_shader = dirty[MESA_SHADER_TESS_EVAL] || stages[PIPE_SHADER_TESS_EVAL];
+         prog->modules[type]->shader = get_shader_module_for_stage(ctx, dirty[i], prog, &prog->modules[type]->hash);
+         /* we probably need a new pipeline when we switch shader modules */
+         ctx->gfx_pipeline_state.hash = 0;
+      } else if (stages[type] && !update) /* reuse existing shader module */
          zink_shader_module_reference(zink_screen(ctx->base.screen), &prog->modules[type], ctx->curr_program->modules[type]);
       prog->shaders[type] = stages[type];
    }
-   ctx->dirty_shader_stages = 0;
+   unsigned clean = u_bit_consecutive(PIPE_SHADER_VERTEX, 5);;
+   ctx->dirty_shader_stages &= ~clean;
 }
 
 static uint32_t
@@ -172,6 +405,12 @@ equals_gfx_pipeline_state(const void *a, const void *b)
    return memcmp(a, b, offsetof(struct zink_gfx_pipeline_state, hash)) == 0;
 }
 
+void
+zink_update_gfx_program(struct zink_context *ctx, struct zink_gfx_program *prog)
+{
+   update_shader_modules(ctx, ctx->gfx_stages, prog, true);
+}
+
 struct zink_gfx_program *
 zink_create_gfx_program(struct zink_context *ctx,
                         struct zink_shader *stages[ZINK_SHADER_COUNT])
@@ -183,7 +422,7 @@ zink_create_gfx_program(struct zink_context *ctx,
 
    pipe_reference_init(&prog->reference, 1);
 
-   update_shader_modules(ctx, stages, prog);
+   update_shader_modules(ctx, stages, prog, false);
 
    for (int i = 0; i < ARRAY_SIZE(prog->pipelines); ++i) {
       prog->pipelines[i] = _mesa_hash_table_create(NULL,
@@ -205,7 +444,7 @@ zink_create_gfx_program(struct zink_context *ctx,
    if (!prog->dsl)
       goto fail;
 
-   prog->layout = create_pipeline_layout(screen->dev, prog->dsl);
+   prog->layout = create_gfx_pipeline_layout(screen->dev, prog->dsl);
    if (!prog->layout)
       goto fail;
 
@@ -222,6 +461,77 @@ fail:
    return NULL;
 }
 
+static uint32_t
+hash_compute_pipeline_state(const void *key)
+{
+   return _mesa_hash_data(key, offsetof(struct zink_compute_pipeline_state, hash));
+}
+
+static bool
+equals_compute_pipeline_state(const void *a, const void *b)
+{
+   return memcmp(a, b, offsetof(struct zink_compute_pipeline_state, hash)) == 0;
+}
+
+struct zink_compute_program *
+zink_create_compute_program(struct zink_context *ctx, struct zink_shader *shader)
+{
+   struct zink_screen *screen = zink_screen(ctx->base.screen);
+   struct zink_compute_program *comp = CALLOC_STRUCT(zink_compute_program);
+   if (!comp)
+      goto fail;
+
+   pipe_reference_init(&comp->reference, 1);
+
+   if (!ctx->curr_compute || !ctx->curr_compute->shader_cache) {
+      /* TODO: cs shader keys */
+      comp->shader_cache = CALLOC_STRUCT(zink_shader_cache);
+      pipe_reference_init(&comp->shader_cache->reference, 1);
+      comp->shader_cache->shader_cache =  _mesa_hash_table_create(NULL, _mesa_hash_u32, _mesa_key_u32_equal);
+   } else
+      zink_shader_cache_reference(zink_screen(ctx->base.screen), &comp->shader_cache, ctx->curr_compute->shader_cache);
+
+   if (ctx->dirty_shader_stages & (1 << PIPE_SHADER_COMPUTE)) {
+      comp->module = CALLOC_STRUCT(zink_shader_module);
+      assert(comp->module);
+      pipe_reference_init(&comp->module->reference, 1);
+      struct hash_entry *he = _mesa_hash_table_search(comp->shader_cache->shader_cache, &shader->shader_id);
+      if (he)
+         comp->module->shader = he->data;
+      else {
+         comp->module->shader = zink_shader_compile(screen, shader, NULL, NULL, NULL);
+         _mesa_hash_table_insert(comp->shader_cache->shader_cache, &shader->shader_id, comp->module->shader);
+      }
+   } else
+     zink_shader_module_reference(zink_screen(ctx->base.screen), &comp->module, ctx->curr_compute->module);
+   ctx->dirty_shader_stages &= ~(1 << PIPE_SHADER_COMPUTE);
+
+   comp->pipelines = _mesa_hash_table_create(NULL, hash_compute_pipeline_state,
+                                             equals_compute_pipeline_state);
+
+   _mesa_set_add(shader->programs, comp);
+   zink_compute_program_reference(screen, NULL, comp);
+   comp->shader = shader;
+
+   struct zink_shader *stages[ZINK_SHADER_COUNT] = {};
+   stages[0] = shader;
+   comp->dsl = create_desc_set_layout(screen->dev, stages,
+                                      &comp->num_descriptors);
+   if (!comp->dsl)
+      goto fail;
+
+   comp->layout = create_compute_pipeline_layout(screen->dev, comp->dsl);
+   if (!comp->layout)
+      goto fail;
+
+   return comp;
+
+fail:
+   if (comp)
+      zink_destroy_compute_program(screen, comp);
+   return NULL;
+}
+
 static void
 gfx_program_remove_shader(struct zink_gfx_program *prog, struct zink_shader *shader)
 {
@@ -260,17 +570,44 @@ zink_destroy_gfx_program(struct zink_screen *screen,
 
    for (int i = 0; i < ARRAY_SIZE(prog->pipelines); ++i) {
       hash_table_foreach(prog->pipelines[i], entry) {
-         struct pipeline_cache_entry *pc_entry = entry->data;
+         struct gfx_pipeline_cache_entry *pc_entry = entry->data;
 
          vkDestroyPipeline(screen->dev, pc_entry->pipeline, NULL);
          free(pc_entry);
       }
       _mesa_hash_table_destroy(prog->pipelines[i], NULL);
    }
+   zink_shader_cache_reference(screen, &prog->shader_cache, NULL);
 
    FREE(prog);
 }
 
+void
+zink_destroy_compute_program(struct zink_screen *screen,
+                         struct zink_compute_program *comp)
+{
+   if (comp->layout)
+      vkDestroyPipelineLayout(screen->dev, comp->layout, NULL);
+
+   if (comp->dsl)
+      vkDestroyDescriptorSetLayout(screen->dev, comp->dsl, NULL);
+
+   _mesa_set_remove_key(comp->shader->programs, comp);
+   if (comp->module)
+      zink_shader_module_reference(screen, &comp->module, NULL);
+
+   hash_table_foreach(comp->pipelines, entry) {
+      struct compute_pipeline_cache_entry *pc_entry = entry->data;
+
+      vkDestroyPipeline(screen->dev, pc_entry->pipeline, NULL);
+      free(pc_entry);
+   }
+   _mesa_hash_table_destroy(comp->pipelines, NULL);
+   zink_shader_cache_reference(screen, &comp->shader_cache, NULL);
+
+   FREE(comp);
+}
+
 static VkPrimitiveTopology
 primitive_topology(enum pipe_prim_type mode)
 {
@@ -305,6 +642,9 @@ primitive_topology(enum pipe_prim_type mode)
    case PIPE_PRIM_TRIANGLES_ADJACENCY:
       return VK_PRIMITIVE_TOPOLOGY_TRIANGLE_LIST_WITH_ADJACENCY;
 
+   case PIPE_PRIM_PATCHES:
+      return VK_PRIMITIVE_TOPOLOGY_PATCH_LIST;
+
    default:
       unreachable("unexpected enum pipe_prim_type");
    }
@@ -335,7 +675,26 @@ zink_get_gfx_pipeline(struct zink_screen *screen,
    struct hash_entry *entry = NULL;
    
    if (!state->hash) {
+#ifndef NDEBUG
+      /* mesa hash table "pre_hashed" functions will re-hash and assert the pre-hashed value,
+       * so the state needs to include that if asserts are enabled
+       */
+      for (unsigned i = 0; i < ZINK_SHADER_COUNT; i++) {
+         state->stages[i] = prog->modules[i] ? prog->modules[i]->hash : 0;
+      }
+#endif
+
       state->hash = hash_gfx_pipeline_state(state);
+
+#ifdef NDEBUG
+      for (unsigned i = 0; i < ZINK_SHADER_COUNT; i++) {
+         uint32_t zero = 0;
+         if (prog->modules[i])
+            state->hash = XXH32(&prog->modules[i]->hash, sizeof(uint32_t), state->hash);
+         else
+            state->hash = XXH32(&zero, sizeof(uint32_t), state->hash);
+      }
+#endif
       /* make sure the hash is not zero, as we take it as invalid.
        * TODO: rework this using a separate dirty-bit */
       assert(state->hash != 0);
@@ -348,7 +707,7 @@ zink_get_gfx_pipeline(struct zink_screen *screen,
       if (pipeline == VK_NULL_HANDLE)
          return VK_NULL_HANDLE;
 
-      struct pipeline_cache_entry *pc_entry = CALLOC_STRUCT(pipeline_cache_entry);
+      struct gfx_pipeline_cache_entry *pc_entry = CALLOC_STRUCT(gfx_pipeline_cache_entry);
       if (!pc_entry)
          return VK_NULL_HANDLE;
 
@@ -362,7 +721,43 @@ zink_get_gfx_pipeline(struct zink_screen *screen,
       reference_render_pass(screen, prog, state->render_pass);
    }
 
-   return ((struct pipeline_cache_entry *)(entry->data))->pipeline;
+   return ((struct gfx_pipeline_cache_entry *)(entry->data))->pipeline;
+}
+
+VkPipeline
+zink_get_compute_pipeline(struct zink_screen *screen,
+                      struct zink_compute_program *comp,
+                      struct zink_compute_pipeline_state *state)
+{
+   struct hash_entry *entry = NULL;
+
+   if (!state->hash) {
+      state->hash = hash_compute_pipeline_state(state);
+      /* make sure the hash is not zero, as we take it as invalid.
+       * TODO: rework this using a separate dirty-bit */
+      assert(state->hash != 0);
+   }
+   entry = _mesa_hash_table_search_pre_hashed(comp->pipelines, state->hash, state);
+
+   if (!entry) {
+      VkPipeline pipeline = zink_create_compute_pipeline(screen, comp, state);
+
+      if (pipeline == VK_NULL_HANDLE)
+         return VK_NULL_HANDLE;
+
+      struct compute_pipeline_cache_entry *pc_entry = CALLOC_STRUCT(compute_pipeline_cache_entry);
+      if (!pc_entry)
+         return VK_NULL_HANDLE;
+
+      memcpy(&pc_entry->state, state, sizeof(*state));
+      pc_entry->pipeline = pipeline;
+
+      assert(state->hash);
+      entry = _mesa_hash_table_insert_pre_hashed(comp->pipelines, state->hash, state, pc_entry);
+      assert(entry);
+   }
+
+   return ((struct compute_pipeline_cache_entry *)(entry->data))->pipeline;
 }
 
 
@@ -383,8 +778,10 @@ static void
 bind_stage(struct zink_context *ctx, enum pipe_shader_type stage,
            struct zink_shader *shader)
 {
-   assert(stage < PIPE_SHADER_COMPUTE);
-   ctx->gfx_stages[stage] = shader;
+   if (stage == PIPE_SHADER_COMPUTE)
+      ctx->compute_stage = shader;
+   else
+      ctx->gfx_stages[stage] = shader;
    ctx->dirty_shader_stages |= 1 << stage;
 }
 
@@ -395,15 +792,68 @@ zink_bind_vs_state(struct pipe_context *pctx,
    bind_stage(zink_context(pctx), PIPE_SHADER_VERTEX, cso);
 }
 
+static void *
+zink_create_fs_state(struct pipe_context *pctx,
+                     const struct pipe_shader_state *shader)
+{
+   struct nir_shader *nir;
+   if (shader->type != PIPE_SHADER_IR_NIR)
+      nir = zink_tgsi_to_nir(pctx->screen, shader->tokens);
+   else
+      nir = (struct nir_shader *)shader->ir.nir;
+
+   return zink_shader_create(zink_screen(pctx->screen), nir, NULL);
+}
+
 static void
-zink_delete_vs_state(struct pipe_context *pctx,
-                     void *cso)
+zink_bind_fs_state(struct pipe_context *pctx,
+                   void *cso)
 {
-   zink_shader_free(zink_context(pctx), cso);
+   bind_stage(zink_context(pctx), PIPE_SHADER_FRAGMENT, cso);
 }
 
 static void *
-zink_create_fs_state(struct pipe_context *pctx,
+zink_create_gs_state(struct pipe_context *pctx,
+                     const struct pipe_shader_state *shader)
+{
+   struct nir_shader *nir;
+   if (shader->type != PIPE_SHADER_IR_NIR)
+      nir = zink_tgsi_to_nir(pctx->screen, shader->tokens);
+   else
+      nir = (struct nir_shader *)shader->ir.nir;
+
+   return zink_shader_create(zink_screen(pctx->screen), nir, &shader->stream_output);
+}
+
+static void
+zink_bind_gs_state(struct pipe_context *pctx,
+                   void *cso)
+{
+   bind_stage(zink_context(pctx), PIPE_SHADER_GEOMETRY, cso);
+}
+
+static void *
+zink_create_tcs_state(struct pipe_context *pctx,
+                     const struct pipe_shader_state *shader)
+{
+   struct nir_shader *nir;
+   if (shader->type != PIPE_SHADER_IR_NIR)
+      nir = zink_tgsi_to_nir(pctx->screen, shader->tokens);
+   else
+      nir = (struct nir_shader *)shader->ir.nir;
+
+   return zink_shader_create(zink_screen(pctx->screen), nir, &shader->stream_output);
+}
+
+static void
+zink_bind_tcs_state(struct pipe_context *pctx,
+                   void *cso)
+{
+   bind_stage(zink_context(pctx), PIPE_SHADER_TESS_CTRL, cso);
+}
+
+static void *
+zink_create_tes_state(struct pipe_context *pctx,
                      const struct pipe_shader_state *shader)
 {
    struct nir_shader *nir;
@@ -416,28 +866,62 @@ zink_create_fs_state(struct pipe_context *pctx,
 }
 
 static void
-zink_bind_fs_state(struct pipe_context *pctx,
+zink_bind_tes_state(struct pipe_context *pctx,
                    void *cso)
 {
-   bind_stage(zink_context(pctx), PIPE_SHADER_FRAGMENT, cso);
+   bind_stage(zink_context(pctx), PIPE_SHADER_TESS_EVAL, cso);
 }
 
 static void
-zink_delete_fs_state(struct pipe_context *pctx,
-                     void *cso)
+zink_delete_shader_state(struct pipe_context *pctx, void *cso)
 {
    zink_shader_free(zink_context(pctx), cso);
 }
 
+static void *
+zink_create_cs_state(struct pipe_context *pctx,
+                     const struct pipe_compute_state *shader)
+{
+   struct nir_shader *nir;
+   if (shader->ir_type != PIPE_SHADER_IR_NIR)
+      nir = zink_tgsi_to_nir(pctx->screen, shader->prog);
+   else
+      nir = (struct nir_shader *)shader->prog;
+
+   return zink_shader_create(zink_screen(pctx->screen), nir, NULL);
+}
+
+static void
+zink_bind_cs_state(struct pipe_context *pctx,
+                   void *cso)
+{
+   bind_stage(zink_context(pctx), PIPE_SHADER_COMPUTE, cso);
+}
 
 void
 zink_program_init(struct zink_context *ctx)
 {
    ctx->base.create_vs_state = zink_create_vs_state;
    ctx->base.bind_vs_state = zink_bind_vs_state;
-   ctx->base.delete_vs_state = zink_delete_vs_state;
+   ctx->base.delete_vs_state = zink_delete_shader_state;
 
    ctx->base.create_fs_state = zink_create_fs_state;
    ctx->base.bind_fs_state = zink_bind_fs_state;
-   ctx->base.delete_fs_state = zink_delete_fs_state;
+   ctx->base.delete_fs_state = zink_delete_shader_state;
+
+   ctx->base.create_gs_state = zink_create_gs_state;
+   ctx->base.bind_gs_state = zink_bind_gs_state;
+   ctx->base.delete_gs_state = zink_delete_shader_state;
+
+   ctx->base.create_tcs_state = zink_create_tcs_state;
+   ctx->base.bind_tcs_state = zink_bind_tcs_state;
+   ctx->base.delete_tcs_state = zink_delete_shader_state;
+
+   ctx->base.create_tes_state = zink_create_tes_state;
+   ctx->base.bind_tes_state = zink_bind_tes_state;
+   ctx->base.delete_tes_state = zink_delete_shader_state;
+
+   ctx->base.create_compute_state = zink_create_cs_state;
+   ctx->base.bind_compute_state = zink_bind_cs_state;
+   ctx->base.delete_compute_state = zink_delete_shader_state;
 }
diff --git a/src/gallium/drivers/zink/zink_program.h b/src/gallium/drivers/zink/zink_program.h
index 5e68783a2f8..6fc8bb71d43 100644
--- a/src/gallium/drivers/zink/zink_program.h
+++ b/src/gallium/drivers/zink/zink_program.h
@@ -26,10 +26,12 @@
 
 #include <vulkan/vulkan.h>
 
+#include "compiler/shader_enums.h"
 #include "pipe/p_state.h"
 #include "util/u_inlines.h"
 
 #include "zink_context.h"
+#include "zink_shader_keys.h"
 
 struct zink_screen;
 struct zink_shader;
@@ -38,9 +40,27 @@ struct zink_gfx_pipeline_state;
 struct hash_table;
 struct set;
 
+struct zink_push_constant {
+   unsigned draw_mode_is_indexed;
+   unsigned draw_id;
+   float default_inner_level[2];
+   float default_outer_level[4];
+};
+
+/* a shader module is used for directly reusing a shader module between programs,
+ * e.g., in the case where we're swapping out only one shader,
+ * allowing us to skip going through shader keys
+ */
 struct zink_shader_module {
    struct pipe_reference reference;
    VkShaderModule shader;
+   uint32_t hash;
+};
+
+/* the shader cache stores a mapping of zink_shader_key::VkShaderModule */
+struct zink_shader_cache {
+   struct pipe_reference reference;
+   struct hash_table *shader_cache;
 };
 
 struct zink_gfx_program {
@@ -48,13 +68,31 @@ struct zink_gfx_program {
 
    struct zink_shader_module *modules[ZINK_SHADER_COUNT]; // compute stage doesn't belong here
    struct zink_shader *shaders[ZINK_SHADER_COUNT];
+   struct zink_shader_cache *shader_cache;
+   unsigned char shader_slot_map[VARYING_SLOT_MAX];
+   unsigned char shader_slots_reserved;
    VkDescriptorSetLayout dsl;
    VkPipelineLayout layout;
    unsigned num_descriptors;
-   struct hash_table *pipelines[10]; // number of draw modes we support
+   struct hash_table *pipelines[11]; // number of draw modes we support
    struct set *render_passes;
 };
 
+struct zink_compute_program {
+   struct pipe_reference reference;
+
+   struct zink_shader_module *module;
+   struct zink_shader *shader;
+   struct zink_shader_cache *shader_cache;
+   VkDescriptorSetLayout dsl;
+   VkPipelineLayout layout;
+   unsigned num_descriptors;
+   struct hash_table *pipelines;
+};
+
+void
+zink_update_gfx_program(struct zink_context *ctx, struct zink_gfx_program *prog);
+
 struct zink_gfx_program *
 zink_create_gfx_program(struct zink_context *ctx,
                         struct zink_shader *stages[ZINK_SHADER_COUNT]);
@@ -87,4 +125,31 @@ zink_gfx_program_reference(struct zink_screen *screen,
       zink_destroy_gfx_program(screen, old_dst);
    if (dst) *dst = src;
 }
+
+struct zink_compute_program *
+zink_create_compute_program(struct zink_context *ctx, struct zink_shader *shader);
+void
+zink_destroy_compute_program(struct zink_screen *screen,
+                         struct zink_compute_program *comp);
+
+void
+debug_describe_zink_compute_program(char* buf, const struct zink_compute_program *ptr);
+
+static inline void
+zink_compute_program_reference(struct zink_screen *screen,
+                           struct zink_compute_program **dst,
+                           struct zink_compute_program *src)
+{
+   struct zink_compute_program *old_dst = dst ? *dst : NULL;
+
+   if (pipe_reference_described(old_dst ? &old_dst->reference : NULL, &src->reference,
+                                (debug_reference_descriptor)debug_describe_zink_compute_program))
+      zink_destroy_compute_program(screen, old_dst);
+   if (dst) *dst = src;
+}
+
+VkPipeline
+zink_get_compute_pipeline(struct zink_screen *screen,
+                      struct zink_compute_program *comp,
+                      struct zink_compute_pipeline_state *state);
 #endif
diff --git a/src/gallium/drivers/zink/zink_query.c b/src/gallium/drivers/zink/zink_query.c
index 0d0c4fe53d0..0511d5019b9 100644
--- a/src/gallium/drivers/zink/zink_query.c
+++ b/src/gallium/drivers/zink/zink_query.c
@@ -17,13 +17,15 @@ struct zink_query {
    enum pipe_query_type type;
 
    VkQueryPool query_pool;
-   unsigned last_checked_query, curr_query, num_queries;
+   VkQueryPool xfb_query_pool[PIPE_MAX_VERTEX_STREAMS - 1];
+   unsigned curr_query, num_queries, last_start;
 
    VkQueryType vkqtype;
    unsigned index;
    bool use_64bit;
    bool precise;
    bool xfb_running;
+   bool xfb_overflow;
 
    bool active; /* query is considered active by vk */
    bool needs_reset; /* query is considered active by vk and cannot be destroyed */
@@ -32,9 +34,50 @@ struct zink_query {
    unsigned fences;
    struct list_head active_list;
 
+   struct list_head stats_list; /* when active, statistics queries are added to ctx->primitives_generated_queries */
+   bool have_gs[NUM_QUERIES]; /* geometry shaders use GEOMETRY_SHADER_PRIMITIVES_BIT */
+   bool have_xfb[NUM_QUERIES]; /* xfb was active during this query */
+
+   unsigned batch_id : 3; //batch that the query was started in
+
    union pipe_query_result accumulated_result;
 };
 
+static VkQueryPipelineStatisticFlags
+pipeline_statistic_convert(enum pipe_statistics_query_index idx)
+{
+   unsigned map[] = {
+      [PIPE_STAT_QUERY_IA_VERTICES] = VK_QUERY_PIPELINE_STATISTIC_INPUT_ASSEMBLY_VERTICES_BIT,
+      [PIPE_STAT_QUERY_IA_PRIMITIVES] = VK_QUERY_PIPELINE_STATISTIC_INPUT_ASSEMBLY_PRIMITIVES_BIT,
+      [PIPE_STAT_QUERY_VS_INVOCATIONS] = VK_QUERY_PIPELINE_STATISTIC_VERTEX_SHADER_INVOCATIONS_BIT,
+      [PIPE_STAT_QUERY_GS_INVOCATIONS] = VK_QUERY_PIPELINE_STATISTIC_GEOMETRY_SHADER_INVOCATIONS_BIT,
+      [PIPE_STAT_QUERY_GS_PRIMITIVES] = VK_QUERY_PIPELINE_STATISTIC_GEOMETRY_SHADER_PRIMITIVES_BIT,
+      [PIPE_STAT_QUERY_C_INVOCATIONS] = VK_QUERY_PIPELINE_STATISTIC_CLIPPING_INVOCATIONS_BIT,
+      [PIPE_STAT_QUERY_C_PRIMITIVES] = VK_QUERY_PIPELINE_STATISTIC_CLIPPING_PRIMITIVES_BIT,
+      [PIPE_STAT_QUERY_PS_INVOCATIONS] = VK_QUERY_PIPELINE_STATISTIC_FRAGMENT_SHADER_INVOCATIONS_BIT,
+      [PIPE_STAT_QUERY_HS_INVOCATIONS] = VK_QUERY_PIPELINE_STATISTIC_TESSELLATION_CONTROL_SHADER_PATCHES_BIT,
+      [PIPE_STAT_QUERY_DS_INVOCATIONS] = VK_QUERY_PIPELINE_STATISTIC_TESSELLATION_EVALUATION_SHADER_INVOCATIONS_BIT,
+      [PIPE_STAT_QUERY_CS_INVOCATIONS] = VK_QUERY_PIPELINE_STATISTIC_COMPUTE_SHADER_INVOCATIONS_BIT
+   };
+   assert(idx < ARRAY_SIZE(map));
+   return map[idx];
+}
+
+static void
+timestamp_to_nanoseconds(struct zink_screen *screen, uint64_t *timestamp)
+{
+   /* The number of valid bits in a timestamp value is determined by
+    * the VkQueueFamilyProperties::timestampValidBits property of the queue on which the timestamp is written.
+    * - 17.5. Timestamp Queries
+    */
+   *timestamp &= ((1ull << screen->timestamp_valid_bits) - 1);;
+   /* The number of nanoseconds it takes for a timestamp value to be incremented by 1
+    * can be obtained from VkPhysicalDeviceLimits::timestampPeriod
+    * - 17.5. Timestamp Queries
+    */
+   *timestamp *= screen->props.limits.timestampPeriod;
+}
+
 static VkQueryType
 convert_query_type(unsigned query_type, bool *use_64bit, bool *precise)
 {
@@ -48,12 +91,18 @@ convert_query_type(unsigned query_type, bool *use_64bit, bool *precise)
    case PIPE_QUERY_OCCLUSION_PREDICATE:
    case PIPE_QUERY_OCCLUSION_PREDICATE_CONSERVATIVE:
       return VK_QUERY_TYPE_OCCLUSION;
+   case PIPE_QUERY_TIME_ELAPSED:
    case PIPE_QUERY_TIMESTAMP:
       *use_64bit = true;
       return VK_QUERY_TYPE_TIMESTAMP;
+   case PIPE_QUERY_PIPELINE_STATISTICS_SINGLE:
    case PIPE_QUERY_PIPELINE_STATISTICS:
+      *use_64bit = true;
+      /* fallthrough */
    case PIPE_QUERY_PRIMITIVES_GENERATED:
       return VK_QUERY_TYPE_PIPELINE_STATISTICS;
+   case PIPE_QUERY_SO_OVERFLOW_ANY_PREDICATE:
+   case PIPE_QUERY_SO_OVERFLOW_PREDICATE:
    case PIPE_QUERY_PRIMITIVES_EMITTED:
       *use_64bit = true;
       return VK_QUERY_TYPE_TRANSFORM_FEEDBACK_STREAM_EXT;
@@ -64,6 +113,34 @@ convert_query_type(unsigned query_type, bool *use_64bit, bool *precise)
    }
 }
 
+static bool
+is_cs_query(struct zink_query *query)
+{
+   return query->type == PIPE_QUERY_PIPELINE_STATISTICS_SINGLE && query->index == PIPE_STAT_QUERY_CS_INVOCATIONS;
+}
+
+static bool
+is_time_query(struct zink_query *query)
+{
+   return query->type == PIPE_QUERY_TIMESTAMP || query->type == PIPE_QUERY_TIME_ELAPSED;
+}
+
+static bool
+is_so_overflow_query(struct zink_query *query)
+{
+   return query->type == PIPE_QUERY_SO_OVERFLOW_ANY_PREDICATE || query->type == PIPE_QUERY_SO_OVERFLOW_PREDICATE;
+}
+
+static struct zink_batch *
+get_batch_for_query(struct zink_context *ctx, struct zink_query *query, bool no_rp)
+{
+   if (is_cs_query(query))
+      return &ctx->compute_batch;
+   if (no_rp)
+      return zink_batch_no_rp(ctx);
+   return zink_curr_batch(ctx);
+}
+
 static struct pipe_query *
 zink_create_query(struct pipe_context *pctx,
                   unsigned query_type, unsigned index)
@@ -81,43 +158,68 @@ zink_create_query(struct pipe_context *pctx,
    if (query->vkqtype == -1)
       return NULL;
 
-   query->num_queries = query_type == PIPE_QUERY_TIMESTAMP ? 1 : NUM_QUERIES;
+   query->num_queries = NUM_QUERIES;
    query->curr_query = 0;
 
    pool_create.sType = VK_STRUCTURE_TYPE_QUERY_POOL_CREATE_INFO;
    pool_create.queryType = query->vkqtype;
    pool_create.queryCount = query->num_queries;
    if (query_type == PIPE_QUERY_PRIMITIVES_GENERATED)
-     pool_create.pipelineStatistics = VK_QUERY_PIPELINE_STATISTIC_CLIPPING_INVOCATIONS_BIT;
+     pool_create.pipelineStatistics = VK_QUERY_PIPELINE_STATISTIC_GEOMETRY_SHADER_PRIMITIVES_BIT |
+                                      VK_QUERY_PIPELINE_STATISTIC_INPUT_ASSEMBLY_PRIMITIVES_BIT;
+   else if (query_type == PIPE_QUERY_PIPELINE_STATISTICS)
+      /* TODO? this is broken if the user is expecting cs invocations */
+      pool_create.pipelineStatistics = 0xffffffff;
+   else if (query_type == PIPE_QUERY_PIPELINE_STATISTICS_SINGLE)
+      pool_create.pipelineStatistics = pipeline_statistic_convert(index);
 
    VkResult status = vkCreateQueryPool(screen->dev, &pool_create, NULL, &query->query_pool);
    if (status != VK_SUCCESS) {
       FREE(query);
       return NULL;
    }
-   struct zink_batch *batch = zink_batch_no_rp(zink_context(pctx));
+   if (query_type == PIPE_QUERY_PRIMITIVES_GENERATED) {
+      /* if xfb is active, we need to use an xfb query, otherwise we need pipeline statistics */
+      pool_create.sType = VK_STRUCTURE_TYPE_QUERY_POOL_CREATE_INFO;
+      pool_create.queryType = VK_QUERY_TYPE_TRANSFORM_FEEDBACK_STREAM_EXT;
+      pool_create.queryCount = query->num_queries;
+
+      status = vkCreateQueryPool(screen->dev, &pool_create, NULL, &query->xfb_query_pool[0]);
+      if (status != VK_SUCCESS) {
+         vkDestroyQueryPool(screen->dev, query->query_pool, NULL);
+         FREE(query);
+         return NULL;
+      }
+   } else if (query_type == PIPE_QUERY_SO_OVERFLOW_ANY_PREDICATE) {
+      /* need to monitor all xfb streams */
+      for (unsigned i = 0; i < ARRAY_SIZE(query->xfb_query_pool); i++) {
+         status = vkCreateQueryPool(screen->dev, &pool_create, NULL, &query->xfb_query_pool[i]);
+         if (status != VK_SUCCESS) {
+            vkDestroyQueryPool(screen->dev, query->query_pool, NULL);
+            for (unsigned j = 0; j < i; j++)
+               vkDestroyQueryPool(screen->dev, query->xfb_query_pool[j], NULL);
+            FREE(query);
+            return NULL;
+         }
+      }
+   }
+   struct zink_batch *batch = get_batch_for_query(zink_context(pctx), query, true);
    vkCmdResetQueryPool(batch->cmdbuf, query->query_pool, 0, query->num_queries);
+   if (query->type == PIPE_QUERY_PRIMITIVES_GENERATED)
+      vkCmdResetQueryPool(batch->cmdbuf, query->xfb_query_pool[0], 0, query->num_queries);
+   if (query->type == PIPE_QUERY_TIMESTAMP)
+      query->active = true;
    return (struct pipe_query *)query;
 }
 
-static void
-wait_query(struct pipe_context *pctx, struct zink_query *query)
-{
-   struct pipe_fence_handle *fence = NULL;
-
-   pctx->flush(pctx, &fence, PIPE_FLUSH_HINT_FINISH);
-   if (fence) {
-      pctx->screen->fence_finish(pctx->screen, NULL, fence,
-                                 PIPE_TIMEOUT_INFINITE);
-      pctx->screen->fence_reference(pctx->screen, &fence, NULL);
-   }
-}
-
 static void
 destroy_query(struct zink_screen *screen, struct zink_query *query)
 {
    assert(!p_atomic_read(&query->fences));
    vkDestroyQueryPool(screen->dev, query->query_pool, NULL);
+   for (unsigned i = 0; i < ARRAY_SIZE(query->xfb_query_pool); i++)
+      if (query->xfb_query_pool[i])
+         vkDestroyQueryPool(screen->dev, query->xfb_query_pool[i], NULL);
    FREE(query);
 }
 
@@ -131,7 +233,7 @@ zink_destroy_query(struct pipe_context *pctx,
    p_atomic_set(&query->dead, true);
    if (p_atomic_read(&query->fences)) {
       if (query->xfb_running)
-        wait_query(pctx, query);
+        zink_fence_wait(pctx);
       return;
    }
 
@@ -152,6 +254,72 @@ zink_prune_queries(struct zink_screen *screen, struct zink_fence *fence)
    fence->active_queries = NULL;
 }
 
+static void
+check_query_results(struct zink_query *query, union pipe_query_result *result,
+                    int num_results, int result_size, uint64_t *results, uint64_t *xfb_results)
+{
+   uint64_t last_val = 0;
+   for (int i = 0; i < num_results * result_size; i += result_size) {
+      switch (query->type) {
+      case PIPE_QUERY_OCCLUSION_PREDICATE:
+      case PIPE_QUERY_OCCLUSION_PREDICATE_CONSERVATIVE:
+      case PIPE_QUERY_GPU_FINISHED:
+         result->b |= results[i] != 0;
+         break;
+
+      case PIPE_QUERY_TIME_ELAPSED:
+      case PIPE_QUERY_TIMESTAMP:
+         /* the application can sum the differences between all N queries to determine the total execution time.
+          * - 17.5. Timestamp Queries
+          */
+         result->u64 += results[i] - last_val;
+         last_val = results[i];
+         break;
+      case PIPE_QUERY_OCCLUSION_COUNTER:
+         result->u64 += results[i];
+         break;
+      case PIPE_QUERY_PRIMITIVES_GENERATED:
+         if (query->have_xfb[query->last_start + i / 2] || query->index)
+            result->u64 += xfb_results[i + 1];
+         else
+            /* if a given draw had a geometry shader, we need to use the second result */
+            result->u64 += ((uint32_t*)results)[i + query->have_gs[query->last_start + i / 2]];
+         break;
+      case PIPE_QUERY_PRIMITIVES_EMITTED:
+         /* A query pool created with this type will capture 2 integers -
+          * numPrimitivesWritten and numPrimitivesNeeded -
+          * for the specified vertex stream output from the last vertex processing stage.
+          * - from VK_EXT_transform_feedback spec
+          */
+         result->u64 += results[i];
+         break;
+      case PIPE_QUERY_SO_OVERFLOW_ANY_PREDICATE:
+      case PIPE_QUERY_SO_OVERFLOW_PREDICATE:
+         /* A query pool created with this type will capture 2 integers -
+          * numPrimitivesWritten and numPrimitivesNeeded -
+          * for the specified vertex stream output from the last vertex processing stage.
+          * - from VK_EXT_transform_feedback spec
+          */
+         result->b |= results[i] != results[i + 1];
+         break;
+      case PIPE_QUERY_PIPELINE_STATISTICS_SINGLE:
+         result->u64 += results[i];
+         break;
+      case PIPE_QUERY_PIPELINE_STATISTICS: {
+         uint64_t *statistics = (uint64_t*)&result->pipeline_statistics;
+         for (unsigned j = 0; j < 11; j++)
+            statistics[j] += results[i + j];
+         break;
+      }
+
+      default:
+         debug_printf("unhangled query type: %s\n",
+                      util_str_query_type(query->type, true));
+         unreachable("unexpected query type");
+      }
+   }
+}
+
 static bool
 get_query_result(struct pipe_context *pctx,
                       struct pipe_query *q,
@@ -162,87 +330,127 @@ get_query_result(struct pipe_context *pctx,
    struct zink_query *query = (struct zink_query *)q;
    VkQueryResultFlagBits flags = 0;
 
-   if (wait)
+   if (wait && !is_time_query(query))
       flags |= VK_QUERY_RESULT_WAIT_BIT;
 
    if (query->use_64bit)
       flags |= VK_QUERY_RESULT_64_BIT;
 
    if (result != &query->accumulated_result) {
-      memcpy(result, &query->accumulated_result, sizeof(query->accumulated_result));
-      util_query_clear_result(&query->accumulated_result, query->type);
-   } else {
-      assert(query->vkqtype != VK_QUERY_TYPE_TIMESTAMP);
+      if (query->type == PIPE_QUERY_TIMESTAMP ||
+          is_so_overflow_query(query))
+         util_query_clear_result(result, query->type);
+      else {
+         memcpy(result, &query->accumulated_result, sizeof(query->accumulated_result));
+         util_query_clear_result(&query->accumulated_result, query->type);
+      }
+   } else
       flags |= VK_QUERY_RESULT_PARTIAL_BIT;
-   }
 
    // union pipe_query_result results[NUM_QUERIES * 2];
    /* xfb queries return 2 results */
    uint64_t results[NUM_QUERIES * 2];
    memset(results, 0, sizeof(results));
-   int num_results = query->curr_query - query->last_checked_query;
-   if (query->vkqtype == VK_QUERY_TYPE_TRANSFORM_FEEDBACK_STREAM_EXT) {
-      /* this query emits 2 values */
-      assert(query->curr_query <= ARRAY_SIZE(results) / 2);
-      VkResult status = vkGetQueryPoolResults(screen->dev, query->query_pool,
-                                              query->last_checked_query, num_results,
-                                              sizeof(results),
-                                              results,
-                                              sizeof(uint64_t),
-                                              flags);
-      if (status != VK_SUCCESS)
-         return false;
-      /* multiply for correct looping behavior below */
-      num_results *= 2;
-   } else {
-      assert(query->curr_query <= ARRAY_SIZE(results));
+   uint64_t xfb_results[NUM_QUERIES * 2];
+   memset(xfb_results, 0, sizeof(xfb_results));
+   int num_results = query->curr_query - query->last_start;
+   int result_size = 1;
+      /* these query types emit 2 values */
+   if (query->vkqtype == VK_QUERY_TYPE_TRANSFORM_FEEDBACK_STREAM_EXT ||
+       query->type == PIPE_QUERY_PRIMITIVES_EMITTED)
+      result_size = 2;
+   else if (query->type == PIPE_QUERY_PIPELINE_STATISTICS)
+      result_size = 11;
+
+   if (query->type == PIPE_QUERY_PIPELINE_STATISTICS)
+      num_results = 1;
+   for (unsigned last_start = query->last_start; last_start + num_results <= query->curr_query; last_start++) {
+      /* verify that we have the expected number of results pending */
+      assert(num_results <= ARRAY_SIZE(results) / result_size);
       VkResult status = vkGetQueryPoolResults(screen->dev, query->query_pool,
-                                              query->last_checked_query, num_results,
+                                              last_start, num_results,
                                               sizeof(results),
                                               results,
                                               sizeof(uint64_t),
                                               flags);
       if (status != VK_SUCCESS)
          return false;
-   }
 
-   for (int i = 0; i < num_results; ++i) {
-      switch (query->type) {
-      case PIPE_QUERY_OCCLUSION_PREDICATE:
-      case PIPE_QUERY_OCCLUSION_PREDICATE_CONSERVATIVE:
-      case PIPE_QUERY_SO_OVERFLOW_PREDICATE:
-      case PIPE_QUERY_SO_OVERFLOW_ANY_PREDICATE:
-      case PIPE_QUERY_GPU_FINISHED:
-         result->b |= results[i] != 0;
-         break;
+      if (query->type == PIPE_QUERY_PRIMITIVES_GENERATED) {
+         status = vkGetQueryPoolResults(screen->dev, query->xfb_query_pool[0],
+                                                 last_start, num_results,
+                                                 sizeof(xfb_results),
+                                                 xfb_results,
+                                                 2 * sizeof(uint64_t),
+                                                 flags | VK_QUERY_RESULT_64_BIT);
+         if (status != VK_SUCCESS)
+            return false;
 
-      case PIPE_QUERY_OCCLUSION_COUNTER:
-         result->u64 += results[i];
-         break;
-      case PIPE_QUERY_PRIMITIVES_GENERATED:
-         result->u32 += results[i];
-         break;
-      case PIPE_QUERY_PRIMITIVES_EMITTED:
-         /* A query pool created with this type will capture 2 integers -
-          * numPrimitivesWritten and numPrimitivesNeeded -
-          * for the specified vertex stream output from the last vertex processing stage.
-          * - from VK_EXT_transform_feedback spec
-          */
-         result->u64 += results[i];
-         i++;
-         break;
+      }
 
-      default:
-         debug_printf("unhangled query type: %s\n",
-                      util_str_query_type(query->type, true));
-         unreachable("unexpected query type");
+      check_query_results(query, result, num_results, result_size, results, xfb_results);
+   }
+
+   if (query->type == PIPE_QUERY_SO_OVERFLOW_ANY_PREDICATE && !result->b) {
+      for (unsigned i = 0; i < ARRAY_SIZE(query->xfb_query_pool) && !result->b; i++) {
+         memset(results, 0, sizeof(results));
+         VkResult status = vkGetQueryPoolResults(screen->dev, query->xfb_query_pool[i],
+                                                    query->last_start, num_results,
+                                                    sizeof(results),
+                                                    results,
+                                                    sizeof(uint64_t),
+                                                    flags);
+         if (status != VK_SUCCESS)
+            return false;
+         check_query_results(query, result, num_results, result_size, results, xfb_results);
       }
    }
-   query->last_checked_query = query->curr_query;
+
+   if (is_time_query(query))
+      timestamp_to_nanoseconds(screen, &result->u64);
 
    return TRUE;
 }
 
+static void
+force_cpu_read(struct zink_context *ctx, struct pipe_query *pquery, bool wait, enum pipe_query_value_type result_type, struct pipe_resource *pres, unsigned offset)
+{
+   unsigned result_size = result_type <= PIPE_QUERY_TYPE_U32 ? sizeof(uint32_t) : sizeof(uint64_t);
+   struct zink_query *query = (struct zink_query*)pquery;
+   union pipe_query_result result;
+   bool success = get_query_result(&ctx->base, pquery, wait, &result);
+   if (!success) {
+      debug_printf("zink: getting query result failed");
+      return;
+   }
+
+   struct pipe_transfer *transfer = NULL;
+   void *map = pipe_buffer_map_range(&ctx->base, pres, offset, result_size, PIPE_TRANSFER_WRITE, &transfer);
+   if (!transfer) {
+      debug_printf("zink: mapping result buffer failed");
+      return;
+   }
+   if (result_type <= PIPE_QUERY_TYPE_U32) {
+      uint32_t *u32 = map;
+      uint32_t limit;
+      if (result_type == PIPE_QUERY_TYPE_I32)
+         limit = INT_MAX;
+      else
+         limit = UINT_MAX;
+      if (is_so_overflow_query(query))
+         u32[0] = result.b;
+      else
+         u32[0] = result.u64 > limit ? limit : result.u64;
+   } else {
+      uint64_t *u64 = map;
+      if (is_so_overflow_query(query))
+         u64[0] = result.b;
+      else
+         u64[0] = result.u64;
+   }
+   pipe_buffer_unmap(&ctx->base, transfer);
+}
+
 static void
 reset_pool(struct zink_context *ctx, struct zink_batch *batch, struct zink_query *q)
 {
@@ -250,11 +458,20 @@ reset_pool(struct zink_context *ctx, struct zink_batch *batch, struct zink_query
     *
     * - vkCmdResetQueryPool spec
     */
-   batch = zink_batch_no_rp(ctx);
+   batch = get_batch_for_query(ctx, q, true);
 
-   get_query_result(&ctx->base, (struct pipe_query*)q, false, &q->accumulated_result);
+   if (q->type != PIPE_QUERY_TIMESTAMP)
+      get_query_result(&ctx->base, (struct pipe_query*)q, false, &q->accumulated_result);
    vkCmdResetQueryPool(batch->cmdbuf, q->query_pool, 0, q->num_queries);
-   q->last_checked_query = q->curr_query = 0;
+   if (q->type == PIPE_QUERY_PRIMITIVES_GENERATED)
+      vkCmdResetQueryPool(batch->cmdbuf, q->xfb_query_pool[0], 0, q->num_queries);
+   else if (q->type == PIPE_QUERY_SO_OVERFLOW_ANY_PREDICATE) {
+      for (unsigned i = 0; i < ARRAY_SIZE(q->xfb_query_pool); i++)
+         vkCmdResetQueryPool(batch->cmdbuf, q->xfb_query_pool[i], 0, q->num_queries);
+   }
+   memset(q->have_gs, 0, sizeof(q->have_gs));
+   memset(q->have_xfb, 0, sizeof(q->have_xfb));
+   q->last_start = q->curr_query = 0;
    q->needs_reset = false;
 }
 
@@ -266,22 +483,46 @@ begin_query(struct zink_context *ctx, struct zink_batch *batch, struct zink_quer
    if (q->needs_reset)
       reset_pool(ctx, batch, q);
    assert(q->curr_query < q->num_queries);
+   q->active = true;
+   if (q->type == PIPE_QUERY_TIME_ELAPSED)
+      vkCmdWriteTimestamp(batch->cmdbuf, VK_PIPELINE_STAGE_TOP_OF_PIPE_BIT, q->query_pool, q->curr_query++);
+   /* ignore the rest of begin_query for timestamps */
+   if (is_time_query(q))
+      return;
    if (q->precise)
       flags |= VK_QUERY_CONTROL_PRECISE_BIT;
-   if (q->vkqtype == VK_QUERY_TYPE_TRANSFORM_FEEDBACK_STREAM_EXT) {
+   if (q->type == PIPE_QUERY_PRIMITIVES_EMITTED ||
+       q->type == PIPE_QUERY_PRIMITIVES_GENERATED ||
+       q->type == PIPE_QUERY_SO_OVERFLOW_PREDICATE) {
       zink_screen(ctx->base.screen)->vk_CmdBeginQueryIndexedEXT(batch->cmdbuf,
-                                                                q->query_pool,
+                                                                q->xfb_query_pool[0] ?: q->query_pool,
                                                                 q->curr_query,
                                                                 flags,
                                                                 q->index);
       q->xfb_running = true;
-   } else
+   } else if (q->type == PIPE_QUERY_SO_OVERFLOW_ANY_PREDICATE) {
+      zink_screen(ctx->base.screen)->vk_CmdBeginQueryIndexedEXT(batch->cmdbuf,
+                                                                q->query_pool,
+                                                                q->curr_query,
+                                                                flags,
+                                                                0);
+      for (unsigned i = 0; i < ARRAY_SIZE(q->xfb_query_pool); i++)
+         zink_screen(ctx->base.screen)->vk_CmdBeginQueryIndexedEXT(batch->cmdbuf,
+                                                                   q->xfb_query_pool[i],
+                                                                   q->curr_query,
+                                                                   flags,
+                                                                   i + 1);
+      q->xfb_running = true;
+   }
+   if (q->vkqtype != VK_QUERY_TYPE_TRANSFORM_FEEDBACK_STREAM_EXT)
       vkCmdBeginQuery(batch->cmdbuf, q->query_pool, q->curr_query, flags);
-   q->active = true;
    if (!batch->active_queries)
       batch->active_queries = _mesa_set_create(NULL, _mesa_hash_pointer, _mesa_key_pointer_equal);
    assert(batch->active_queries);
+   if (q->type == PIPE_QUERY_PRIMITIVES_GENERATED)
+      list_addtail(&q->stats_list, &ctx->primitives_generated_queries);
    p_atomic_inc(&q->fences);
+   q->batch_id = batch->batch_id;
    _mesa_set_add(batch->active_queries, q);
 }
 
@@ -290,14 +531,14 @@ zink_begin_query(struct pipe_context *pctx,
                  struct pipe_query *q)
 {
    struct zink_query *query = (struct zink_query *)q;
-   struct zink_batch *batch = zink_curr_batch(zink_context(pctx));
+   struct zink_context *ctx = zink_context(pctx);
+   struct zink_batch *batch = get_batch_for_query(ctx, query, false);
+
+   query->last_start = query->curr_query;
 
-   /* ignore begin_query for timestamps */
-   if (query->type == PIPE_QUERY_TIMESTAMP)
-      return true;
    util_query_clear_result(&query->accumulated_result, query->type);
 
-   begin_query(zink_context(pctx), batch, query);
+   begin_query(ctx, batch, query);
 
    return true;
 }
@@ -306,12 +547,23 @@ static void
 end_query(struct zink_context *ctx, struct zink_batch *batch, struct zink_query *q)
 {
    struct zink_screen *screen = zink_screen(ctx->base.screen);
-   assert(q->type != PIPE_QUERY_TIMESTAMP);
-   q->active = false;
-   if (q->vkqtype == VK_QUERY_TYPE_TRANSFORM_FEEDBACK_STREAM_EXT)
-      screen->vk_CmdEndQueryIndexedEXT(batch->cmdbuf, q->query_pool, q->curr_query, q->index);
-   else
+   q->active = q->type == PIPE_QUERY_TIMESTAMP;
+   if (is_time_query(q))
+      vkCmdWriteTimestamp(batch->cmdbuf, VK_PIPELINE_STAGE_BOTTOM_OF_PIPE_BIT,
+                          q->query_pool, q->curr_query);
+   else if (q->type == PIPE_QUERY_PRIMITIVES_EMITTED ||
+            q->type == PIPE_QUERY_PRIMITIVES_GENERATED ||
+            q->type == PIPE_QUERY_SO_OVERFLOW_PREDICATE)
+      screen->vk_CmdEndQueryIndexedEXT(batch->cmdbuf, q->xfb_query_pool[0] ?: q->query_pool, q->curr_query, q->index);
+   else if (q->type == PIPE_QUERY_SO_OVERFLOW_ANY_PREDICATE) {
+      screen->vk_CmdEndQueryIndexedEXT(batch->cmdbuf, q->query_pool, q->curr_query, 0);
+      for (unsigned i = 0; i < ARRAY_SIZE(q->xfb_query_pool); i++)
+         screen->vk_CmdEndQueryIndexedEXT(batch->cmdbuf, q->xfb_query_pool[i], q->curr_query, i + 1);
+   }
+   if (q->vkqtype != VK_QUERY_TYPE_TRANSFORM_FEEDBACK_STREAM_EXT && !is_time_query(q))
       vkCmdEndQuery(batch->cmdbuf, q->query_pool, q->curr_query);
+   if (q->type == PIPE_QUERY_PRIMITIVES_GENERATED)
+      list_delinit(&q->stats_list);
    if (++q->curr_query == q->num_queries) {
       /* can't do zink_batch_no_rp here because we might already be inside a zink_batch_no_rp */
       if (batch->rp)
@@ -327,13 +579,11 @@ zink_end_query(struct pipe_context *pctx,
 {
    struct zink_context *ctx = zink_context(pctx);
    struct zink_query *query = (struct zink_query *)q;
-   struct zink_batch *batch = zink_curr_batch(ctx);
+   struct zink_batch *batch = get_batch_for_query(ctx, query, false);
 
-   if (query->type == PIPE_QUERY_TIMESTAMP) {
-      assert(query->curr_query == 0);
-      vkCmdWriteTimestamp(batch->cmdbuf, VK_PIPELINE_STAGE_BOTTOM_OF_PIPE_BIT,
-                          query->query_pool, 0);
-   } else if (query->active)
+   if (query->type == PIPE_QUERY_PRIMITIVES_GENERATED)
+      list_delinit(&query->stats_list);
+   if (query->active)
       end_query(ctx, batch, query);
 
    return true;
@@ -345,12 +595,21 @@ zink_get_query_result(struct pipe_context *pctx,
                       bool wait,
                       union pipe_query_result *result)
 {
-   struct zink_query *query = (struct zink_query *)q;
-
-   if (wait) {
-      wait_query(pctx, query);
-   } else
-      pctx->flush(pctx, NULL, 0);
+   struct zink_query *query = (void*)q;
+   struct zink_context *ctx = zink_context(pctx);
+   if (is_cs_query(query)) {
+      if (wait)
+         zink_wait_on_batch(ctx, ZINK_COMPUTE_BATCH_ID);
+      else {
+         zink_end_batch(ctx, &ctx->compute_batch);
+         zink_start_batch(ctx, &ctx->compute_batch);
+      }
+   } else {
+      if (wait) {
+         zink_fence_wait(pctx);
+      } else
+         pctx->flush(pctx, NULL, 0);
+   }
    return get_query_result(pctx, q, wait, result);
 }
 
@@ -382,6 +641,18 @@ zink_resume_queries(struct zink_context *ctx, struct zink_batch *batch)
    }
 }
 
+void
+zink_query_update_gs_states(struct zink_context *ctx)
+{
+   struct zink_query *query;
+   LIST_FOR_EACH_ENTRY(query, &ctx->primitives_generated_queries, stats_list) {
+      assert(query->curr_query < ARRAY_SIZE(query->have_gs));
+      assert(query->active);
+      query->have_gs[query->curr_query] = !!ctx->gfx_stages[PIPE_SHADER_GEOMETRY];
+      query->have_xfb[query->curr_query] = !!ctx->num_so_targets;
+   }
+}
+
 static void
 zink_set_active_query_state(struct pipe_context *pctx, bool enable)
 {
@@ -404,11 +675,12 @@ zink_render_condition(struct pipe_context *pctx,
    struct zink_context *ctx = zink_context(pctx);
    struct zink_screen *screen = zink_screen(pctx->screen);
    struct zink_query *query = (struct zink_query *)pquery;
-   struct zink_batch *batch = zink_batch_no_rp(ctx);
+   struct zink_batch *batch = get_batch_for_query(ctx, query, true);
    VkQueryResultFlagBits flags = 0;
 
    if (query == NULL) {
       screen->vk_CmdEndConditionalRenderingEXT(batch->cmdbuf);
+      ctx->conditional_render_enabled = false;
       return;
    }
 
@@ -433,11 +705,18 @@ zink_render_condition(struct pipe_context *pctx,
 
    if (query->use_64bit)
       flags |= VK_QUERY_RESULT_64_BIT;
-   int num_results = query->curr_query - query->last_checked_query;
-   vkCmdCopyQueryPoolResults(batch->cmdbuf, query->query_pool, query->last_checked_query, num_results,
-                             res->buffer, 0, 0, flags);
+   int num_results = query->curr_query - query->last_start;
+   if (query->type != PIPE_QUERY_PRIMITIVES_GENERATED &&
+       !is_so_overflow_query(query)) {
+      vkCmdCopyQueryPoolResults(batch->cmdbuf, query->query_pool, query->last_start, num_results,
+                                res->buffer, 0, 0, flags);
+      zink_batch_reference_resource_rw(batch, res, true);
+   } else {
+      /* these need special handling */
+      force_cpu_read(ctx, pquery, true, query->use_64bit, pres, 0);
+      zink_batch_reference_resource_rw(batch, res, false);
+   }
 
-   query->last_checked_query = query->curr_query;
    VkConditionalRenderingFlagsEXT begin_flags = 0;
    if (condition)
       begin_flags = VK_CONDITIONAL_RENDERING_INVERTED_BIT_EXT;
@@ -447,9 +726,97 @@ zink_render_condition(struct pipe_context *pctx,
    begin_info.flags = begin_flags;
    screen->vk_CmdBeginConditionalRenderingEXT(batch->cmdbuf, &begin_info);
 
-   zink_batch_reference_resoure(batch, res);
-
    pipe_resource_reference(&pres, NULL);
+   ctx->conditional_render_enabled = true;
+}
+
+static void
+zink_get_query_result_resource(struct pipe_context *pctx,
+                               struct pipe_query *pquery,
+                               bool wait,
+                               enum pipe_query_value_type result_type,
+                               int index,
+                               struct pipe_resource *pres,
+                               unsigned offset)
+{
+   struct zink_context *ctx = zink_context(pctx);
+   struct zink_screen *screen = zink_screen(pctx->screen);
+   struct zink_query *query = (struct zink_query*)pquery;
+   struct zink_resource *res = zink_resource(pres);
+   unsigned result_size = result_type <= PIPE_QUERY_TYPE_U32 ? sizeof(uint32_t) : sizeof(uint64_t);
+   VkQueryResultFlagBits size_flags = result_type <= PIPE_QUERY_TYPE_U32 ? 0 : VK_QUERY_RESULT_64_BIT;
+   unsigned num_queries = query->curr_query - query->last_start;
+   unsigned query_id = query->last_start;
+
+   if (index == -1) {
+      uint64_t u64[2] = {0};
+      /* TODO: this is awful. when we hook up valid regions for resources, we can at least check
+       * whether the preceding area has valid data and clobber it with a direct copy here for a
+       * big perf win
+       *
+       * VK_QUERY_RESULT_WITH_AVAILABILITY_BIT always writes result data at the specified offset,
+       * so we have to do a manual read
+       */
+      //zink_resource_buffer_barrier(batch, res, VK_ACCESS_TRANSFER_WRITE_BIT, 0);
+      //zink_batch_reference_resource_rw(batch, res, true);
+      //vkCmdCopyQueryPoolResults(batch->cmdbuf, query->query_pool, query_id, 1, res->buffer,
+                                //offset, 0, size_flags | VK_QUERY_RESULT_WITH_AVAILABILITY_BIT);
+      if (vkGetQueryPoolResults(screen->dev, query->query_pool, query_id, 1, 2 * result_size, u64,
+                                0, size_flags | VK_QUERY_RESULT_WITH_AVAILABILITY_BIT | VK_QUERY_RESULT_PARTIAL_BIT) != VK_SUCCESS) {
+         debug_printf("zink: getting query result failed");
+         return;
+      }
+      struct pipe_transfer *transfer = NULL;
+      void *map = pipe_buffer_map_range(pctx, pres, offset, result_size, PIPE_TRANSFER_WRITE, &transfer);
+      if (!transfer) {
+         debug_printf("zink: mapping result buffer failed");
+         return;
+      }
+      if (result_type <= PIPE_QUERY_TYPE_U32) {
+         uint32_t *u32_map = map;
+         u32_map[0] = u64[1];
+      } else {
+         uint64_t *u64_map = map;
+         u64_map[0] = u64[1];
+      }
+      pipe_buffer_unmap(pctx, transfer);
+      return;
+   }
+
+   if (zink_curr_batch(ctx)->batch_id == query->batch_id)
+      pctx->flush(pctx, NULL, 0);
+   unsigned fences = p_atomic_read(&query->fences);
+   if (!is_time_query(query) && (!fences || wait)) {
+      /* result happens to be ready or we're waiting */
+      if (num_queries == 1 && query->type != PIPE_QUERY_PRIMITIVES_GENERATED &&
+                              !is_so_overflow_query(query)) {
+         struct zink_batch *batch = get_batch_for_query(ctx, query, true);
+         /* if it's a single query that doesn't need special handling, we can copy it and be done */
+         zink_batch_reference_resource_rw(batch, res, true);
+         zink_resource_buffer_barrier(batch, res, VK_ACCESS_TRANSFER_WRITE_BIT, 0);
+         vkCmdCopyQueryPoolResults(batch->cmdbuf, query->query_pool, query_id, 1, res->buffer,
+                                   offset, 0, size_flags);
+         return;
+      }
+   }
+   /* unfortunately, there's no way to accumulate results from multiple queries on the gpu without either
+    * clobbering all but the last result or writing the results sequentially, so we have to manually write the result
+    */
+   force_cpu_read(ctx, pquery, wait, result_type, pres, offset);
+}
+
+static uint64_t
+zink_get_timestamp(struct pipe_context *pctx)
+{
+   struct zink_screen *screen = zink_screen(pctx->screen);
+   uint64_t timestamp, deviation;
+   assert(screen->have_EXT_calibrated_timestamps);
+   VkCalibratedTimestampInfoEXT cti = {};
+   cti.sType = VK_STRUCTURE_TYPE_CALIBRATED_TIMESTAMP_INFO_EXT;
+   cti.timeDomain = VK_TIME_DOMAIN_DEVICE_EXT;
+   screen->vk_GetCalibratedTimestampsEXT(screen->dev, 1, &cti, &timestamp, &deviation);
+   timestamp_to_nanoseconds(screen, &timestamp);
+   return timestamp;
 }
 
 void
@@ -457,12 +824,15 @@ zink_context_query_init(struct pipe_context *pctx)
 {
    struct zink_context *ctx = zink_context(pctx);
    list_inithead(&ctx->suspended_queries);
+   list_inithead(&ctx->primitives_generated_queries);
 
    pctx->create_query = zink_create_query;
    pctx->destroy_query = zink_destroy_query;
    pctx->begin_query = zink_begin_query;
    pctx->end_query = zink_end_query;
    pctx->get_query_result = zink_get_query_result;
+   pctx->get_query_result_resource = zink_get_query_result_resource;
    pctx->set_active_query_state = zink_set_active_query_state;
    pctx->render_condition = zink_render_condition;
+   pctx->get_timestamp = zink_get_timestamp;
 }
diff --git a/src/gallium/drivers/zink/zink_query.h b/src/gallium/drivers/zink/zink_query.h
index dea3562d5e8..d9606c2bdaf 100644
--- a/src/gallium/drivers/zink/zink_query.h
+++ b/src/gallium/drivers/zink/zink_query.h
@@ -38,4 +38,7 @@ zink_resume_queries(struct zink_context *ctx, struct zink_batch *batch);
 void
 zink_prune_queries(struct zink_screen *screen, struct zink_fence *fence);
 
+void
+zink_query_update_gs_states(struct zink_context *ctx);
+
 #endif
diff --git a/src/gallium/drivers/zink/zink_resource.c b/src/gallium/drivers/zink/zink_resource.c
index ee38909eab8..749bb0d2367 100644
--- a/src/gallium/drivers/zink/zink_resource.c
+++ b/src/gallium/drivers/zink/zink_resource.c
@@ -30,6 +30,7 @@
 #include "util/slab.h"
 #include "util/u_debug.h"
 #include "util/format/u_format.h"
+#include "util/u_transfer_helper.h"
 #include "util/u_inlines.h"
 #include "util/u_memory.h"
 
@@ -68,12 +69,12 @@ get_memory_type_index(struct zink_screen *screen,
    return 0;
 }
 
-static VkImageAspectFlags
-aspect_from_format(enum pipe_format fmt)
+VkImageAspectFlags
+zink_resource_aspect_from_format(const struct pipe_resource *pres)
 {
-   if (util_format_is_depth_or_stencil(fmt)) {
+   if (util_format_is_depth_or_stencil(pres->format)) {
       VkImageAspectFlags aspect = 0;
-      const struct util_format_description *desc = util_format_description(fmt);
+      const struct util_format_description *desc = util_format_description(pres->format);
       if (util_format_has_depth(desc))
          aspect |= VK_IMAGE_ASPECT_DEPTH_BIT;
       if (util_format_has_stencil(desc))
@@ -99,6 +100,8 @@ resource_create(struct pipe_screen *pscreen,
 
    VkMemoryRequirements reqs;
    VkMemoryPropertyFlags flags = 0;
+
+   res->internal_format = templ->format;
    if (templ->target == PIPE_BUFFER) {
       VkBufferCreateInfo bci = {};
       bci.sType = VK_STRUCTURE_TYPE_BUFFER_CREATE_INFO;
@@ -107,8 +110,23 @@ resource_create(struct pipe_screen *pscreen,
       bci.usage = VK_BUFFER_USAGE_TRANSFER_SRC_BIT |
                   VK_BUFFER_USAGE_TRANSFER_DST_BIT;
 
+      /* apparently gallium thinks this is the jack-of-all-trades bind type */
+      if (templ->bind & (PIPE_BIND_SAMPLER_VIEW | PIPE_BIND_QUERY_BUFFER))
+         bci.usage |= VK_BUFFER_USAGE_UNIFORM_TEXEL_BUFFER_BIT |
+                      VK_BUFFER_USAGE_STORAGE_TEXEL_BUFFER_BIT |
+                      VK_BUFFER_USAGE_STORAGE_BUFFER_BIT |
+                      VK_BUFFER_USAGE_UNIFORM_BUFFER_BIT |
+                      VK_BUFFER_USAGE_INDIRECT_BUFFER_BIT |
+                      VK_BUFFER_USAGE_INDEX_BUFFER_BIT |
+                      VK_BUFFER_USAGE_VERTEX_BUFFER_BIT |
+                      VK_BUFFER_USAGE_TRANSFORM_FEEDBACK_BUFFER_BIT_EXT;
+
       if (templ->bind & PIPE_BIND_VERTEX_BUFFER)
-         bci.usage |= VK_BUFFER_USAGE_VERTEX_BUFFER_BIT;
+         bci.usage |= VK_BUFFER_USAGE_VERTEX_BUFFER_BIT |
+                      VK_BUFFER_USAGE_INDEX_BUFFER_BIT |
+                      VK_BUFFER_USAGE_UNIFORM_TEXEL_BUFFER_BIT |
+                      VK_BUFFER_USAGE_UNIFORM_BUFFER_BIT |
+                      VK_BUFFER_USAGE_TRANSFORM_FEEDBACK_BUFFER_BIT_EXT;
 
       if (templ->bind & PIPE_BIND_INDEX_BUFFER)
          bci.usage |= VK_BUFFER_USAGE_INDEX_BUFFER_BIT;
@@ -125,7 +143,10 @@ resource_create(struct pipe_screen *pscreen,
       if (templ->bind == (PIPE_BIND_STREAM_OUTPUT | PIPE_BIND_CUSTOM)) {
          bci.usage |= VK_BUFFER_USAGE_TRANSFORM_FEEDBACK_COUNTER_BUFFER_BIT_EXT;
       } else if (templ->bind & PIPE_BIND_STREAM_OUTPUT) {
-         bci.usage |= VK_BUFFER_USAGE_VERTEX_BUFFER_BIT | VK_BUFFER_USAGE_TRANSFORM_FEEDBACK_BUFFER_BIT_EXT;
+         bci.usage |= VK_BUFFER_USAGE_VERTEX_BUFFER_BIT |
+                      VK_BUFFER_USAGE_INDEX_BUFFER_BIT |
+                      VK_BUFFER_USAGE_UNIFORM_BUFFER_BIT |
+                      VK_BUFFER_USAGE_TRANSFORM_FEEDBACK_BUFFER_BIT_EXT;
       }
 
       if (vkCreateBuffer(screen->dev, &bci, NULL, &res->buffer) !=
@@ -135,8 +156,9 @@ resource_create(struct pipe_screen *pscreen,
       }
 
       vkGetBufferMemoryRequirements(screen->dev, res->buffer, &reqs);
-      flags |= VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT;
+      flags = VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT;
    } else {
+
       res->format = zink_get_format(screen, templ->format);
 
       VkImageCreateInfo ici = {};
@@ -196,8 +218,18 @@ resource_create(struct pipe_screen *pscreen,
                   VK_IMAGE_USAGE_TRANSFER_DST_BIT |
                   VK_IMAGE_USAGE_SAMPLED_BIT;
 
-      if (templ->bind & PIPE_BIND_SHADER_IMAGE)
-         ici.usage |= VK_IMAGE_USAGE_STORAGE_BIT;
+      if ((templ->nr_samples < 2 || screen->feats.shaderStorageImageMultisample) &&
+          (templ->bind & PIPE_BIND_SHADER_IMAGE ||
+          (templ->bind & PIPE_BIND_SAMPLER_VIEW && templ->flags & PIPE_RESOURCE_FLAG_TEXTURING_MORE_LIKELY))) {
+         VkFormatProperties props;
+         vkGetPhysicalDeviceFormatProperties(screen->pdev, res->format, &props);
+         /* gallium doesn't provide any way to actually know whether this will be used as a shader image,
+          * so we have to just assume and set the bit if it's available
+          */
+         if ((ici.tiling == VK_IMAGE_TILING_LINEAR && props.linearTilingFeatures & VK_FORMAT_FEATURE_STORAGE_IMAGE_BIT) ||
+             (ici.tiling == VK_IMAGE_TILING_OPTIMAL && props.optimalTilingFeatures & VK_FORMAT_FEATURE_STORAGE_IMAGE_BIT))
+            ici.usage |= VK_IMAGE_USAGE_STORAGE_BIT;
+      }
 
       if (templ->bind & PIPE_BIND_RENDER_TARGET)
          ici.usage |= VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT;
@@ -222,15 +254,17 @@ resource_create(struct pipe_screen *pscreen,
       }
 
       res->optimial_tiling = ici.tiling != VK_IMAGE_TILING_LINEAR;
-      res->aspect = aspect_from_format(templ->format);
 
       vkGetImageMemoryRequirements(screen->dev, res->image, &reqs);
       if (templ->usage == PIPE_USAGE_STAGING || (screen->winsys && (templ->bind & (PIPE_BIND_SCANOUT|PIPE_BIND_DISPLAY_TARGET|PIPE_BIND_SHARED))))
-        flags |= VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT;
+        flags = VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT;
       else
-        flags |= VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT;
+        flags = VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT;
    }
 
+   if (templ->flags & PIPE_RESOURCE_FLAG_MAP_COHERENT)
+      flags = VK_MEMORY_PROPERTY_HOST_COHERENT_BIT;
+
    VkMemoryAllocateInfo mai = {};
    mai.sType = VK_STRUCTURE_TYPE_MEMORY_ALLOCATE_INFO;
    mai.allocationSize = reqs.size;
@@ -316,7 +350,7 @@ zink_resource_get_handle(struct pipe_screen *pscreen,
       VkImageSubresource sub_res = {};
       VkSubresourceLayout sub_res_layout = {};
 
-      sub_res.aspectMask = res->aspect;
+      sub_res.aspectMask = zink_resource_aspect_from_format(tex);
 
       vkGetImageSubresourceLayout(screen->dev, res->image, &sub_res, &sub_res_layout);
 
@@ -344,18 +378,6 @@ zink_resource_from_handle(struct pipe_screen *pscreen,
    return resource_create(pscreen, templ, whandle, usage);
 }
 
-void
-zink_screen_resource_init(struct pipe_screen *pscreen)
-{
-   pscreen->resource_create = zink_resource_create;
-   pscreen->resource_destroy = zink_resource_destroy;
-
-   if (zink_screen(pscreen)->have_KHR_external_memory_fd) {
-      pscreen->resource_get_handle = zink_resource_get_handle;
-      pscreen->resource_from_handle = zink_resource_from_handle;
-   }
-}
-
 static bool
 zink_transfer_copy_bufimage(struct zink_context *ctx,
                             struct zink_resource *res,
@@ -367,13 +389,13 @@ zink_transfer_copy_bufimage(struct zink_context *ctx,
 
    if (buf2img) {
       if (res->layout != VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL) {
-         zink_resource_barrier(batch->cmdbuf, res, res->aspect,
-                               VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL);
+         zink_resource_barrier(batch, res,
+                               VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL, 0);
       }
    } else {
       if (res->layout != VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL) {
-         zink_resource_barrier(batch->cmdbuf, res, res->aspect,
-                               VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL);
+         zink_resource_barrier(batch, res,
+                               VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL, 0);
       }
    }
 
@@ -397,14 +419,34 @@ zink_transfer_copy_bufimage(struct zink_context *ctx,
    copyRegion.imageExtent.width = trans->base.box.width;
    copyRegion.imageExtent.height = trans->base.box.height;
 
-   zink_batch_reference_resoure(batch, res);
-   zink_batch_reference_resoure(batch, staging_res);
-
-   unsigned aspects = res->aspect;
+   zink_batch_reference_resource_rw(batch, res, buf2img);
+   zink_batch_reference_resource_rw(batch, staging_res, !buf2img);
+
+   /* we're using u_transfer_helper_deinterleave, which means we'll be getting PIPE_TRANSFER_* usage
+    * to indicate whether to copy either the depth or stencil aspects
+    */
+   unsigned aspects = 0;
+   assert((trans->base.usage & (PIPE_TRANSFER_DEPTH_ONLY | PIPE_TRANSFER_STENCIL_ONLY)) !=
+          (PIPE_TRANSFER_DEPTH_ONLY | PIPE_TRANSFER_STENCIL_ONLY));
+   if (trans->base.usage & PIPE_TRANSFER_DEPTH_ONLY)
+      aspects = VK_IMAGE_ASPECT_DEPTH_BIT;
+   else if (trans->base.usage & PIPE_TRANSFER_STENCIL_ONLY)
+      aspects = VK_IMAGE_ASPECT_STENCIL_BIT;
+   else {
+      aspects = zink_resource_aspect_from_format(&res->base);
+   }
    while (aspects) {
       int aspect = 1 << u_bit_scan(&aspects);
       copyRegion.imageSubresource.aspectMask = aspect;
 
+      /* this may or may not work with multisampled depth/stencil buffers depending on the driver implementation:
+       *
+       * srcImage must have a sample count equal to VK_SAMPLE_COUNT_1_BIT
+       * - vkCmdCopyImageToBuffer spec
+       *
+       * dstImage must have a sample count equal to VK_SAMPLE_COUNT_1_BIT
+       * - vkCmdCopyBufferToImage spec
+       */
       if (buf2img)
          vkCmdCopyBufferToImage(batch->cmdbuf, staging_res->buffer, res->image, res->layout, 1, &copyRegion);
       else
@@ -414,6 +456,20 @@ zink_transfer_copy_bufimage(struct zink_context *ctx,
    return true;
 }
 
+uint32_t
+zink_get_resource_usage(struct zink_resource *res)
+{
+   uint32_t batch_uses = 0;
+   for (unsigned i = 0; i < 5; i++) {
+      uint8_t used = p_atomic_read(&res->batch_uses[i]);
+      if (used & ZINK_RESOURCE_ACCESS_READ)
+         batch_uses |= ZINK_RESOURCE_ACCESS_READ << i;
+      if (used & ZINK_RESOURCE_ACCESS_WRITE)
+         batch_uses |= ZINK_RESOURCE_ACCESS_WRITE << i;
+   }
+   return batch_uses;
+}
+
 static void *
 zink_transfer_map(struct pipe_context *pctx,
                   struct pipe_resource *pres,
@@ -425,6 +481,7 @@ zink_transfer_map(struct pipe_context *pctx,
    struct zink_context *ctx = zink_context(pctx);
    struct zink_screen *screen = zink_screen(pctx->screen);
    struct zink_resource *res = zink_resource(pres);
+   uint32_t batch_uses = zink_get_resource_usage(res);
 
    struct zink_transfer *trans = slab_alloc(&ctx->transfer_pool);
    if (!trans)
@@ -440,17 +497,17 @@ zink_transfer_map(struct pipe_context *pctx,
 
    void *ptr;
    if (pres->target == PIPE_BUFFER) {
-      if (usage & PIPE_TRANSFER_READ) {
-         /* need to wait for rendering to finish
-          * TODO: optimize/fix this to be much less obtrusive
-          * mesa/mesa#2966
-          */
-         struct pipe_fence_handle *fence = NULL;
-         pctx->flush(pctx, &fence, PIPE_FLUSH_HINT_FINISH);
-         if (fence) {
-            pctx->screen->fence_finish(pctx->screen, NULL, fence,
-                                       PIPE_TIMEOUT_INFINITE);
-            pctx->screen->fence_reference(pctx->screen, &fence, NULL);
+      if (!(usage & PIPE_TRANSFER_UNSYNCHRONIZED)) {
+         if ((usage & PIPE_TRANSFER_READ && batch_uses >= ZINK_RESOURCE_ACCESS_WRITE) ||
+             (usage & PIPE_TRANSFER_WRITE)) {
+            /* need to wait for rendering to finish
+             * TODO: optimize/fix this to be much less obtrusive
+             * mesa/mesa#2966
+             */
+            if (batch_uses & ((ZINK_RESOURCE_ACCESS_READ << ZINK_COMPUTE_BATCH_ID) |
+                              ZINK_RESOURCE_ACCESS_WRITE << ZINK_COMPUTE_BATCH_ID))
+               zink_wait_on_batch(ctx, ZINK_COMPUTE_BATCH_ID);
+            zink_fence_wait(pctx);
          }
       }
 
@@ -464,12 +521,18 @@ zink_transfer_map(struct pipe_context *pctx,
       ptr = ((uint8_t *)ptr) + box->x;
    } else {
       if (res->optimial_tiling || ((res->base.usage != PIPE_USAGE_STAGING))) {
-         trans->base.stride = util_format_get_stride(pres->format, box->width);
-         trans->base.layer_stride = util_format_get_2d_size(pres->format,
+         enum pipe_format format = pres->format;
+         if (usage & PIPE_TRANSFER_DEPTH_ONLY)
+            format = util_format_get_depth_only(pres->format);
+         else if (usage & PIPE_TRANSFER_STENCIL_ONLY)
+            format = PIPE_FORMAT_S8_UINT;
+         trans->base.stride = util_format_get_stride(format, box->width);
+         trans->base.layer_stride = util_format_get_2d_size(format,
                                                             trans->base.stride,
                                                             box->height);
 
          struct pipe_resource templ = *pres;
+         templ.format = format;
          templ.usage = PIPE_USAGE_STAGING;
          templ.target = PIPE_BUFFER;
          templ.bind = 0;
@@ -486,6 +549,11 @@ zink_transfer_map(struct pipe_context *pctx,
          struct zink_resource *staging_res = zink_resource(trans->staging_res);
 
          if (usage & PIPE_TRANSFER_READ) {
+            if (batch_uses >= ZINK_RESOURCE_ACCESS_WRITE) {
+               if (batch_uses & (ZINK_RESOURCE_ACCESS_WRITE << ZINK_COMPUTE_BATCH_ID))
+                  zink_wait_on_batch(ctx, ZINK_COMPUTE_BATCH_ID);
+               zink_fence_wait(pctx);
+            }
             struct zink_context *ctx = zink_context(pctx);
             bool ret = zink_transfer_copy_bufimage(ctx, res,
                                                    staging_res, trans,
@@ -494,13 +562,7 @@ zink_transfer_map(struct pipe_context *pctx,
                return NULL;
 
             /* need to wait for rendering to finish */
-            struct pipe_fence_handle *fence = NULL;
-            pctx->flush(pctx, &fence, PIPE_FLUSH_HINT_FINISH);
-            if (fence) {
-               pctx->screen->fence_finish(pctx->screen, NULL, fence,
-                                          PIPE_TIMEOUT_INFINITE);
-               pctx->screen->fence_reference(pctx->screen, &fence, NULL);
-            }
+            zink_fence_wait(pctx);
          }
 
          VkResult result = vkMapMemory(screen->dev, staging_res->mem,
@@ -511,11 +573,16 @@ zink_transfer_map(struct pipe_context *pctx,
 
       } else {
          assert(!res->optimial_tiling);
+         if (batch_uses >= ZINK_RESOURCE_ACCESS_WRITE) {
+            if (batch_uses & (ZINK_RESOURCE_ACCESS_WRITE << ZINK_COMPUTE_BATCH_ID))
+               zink_wait_on_batch(ctx, ZINK_COMPUTE_BATCH_ID);
+            zink_fence_wait(pctx);
+         }
          VkResult result = vkMapMemory(screen->dev, res->mem, res->offset, res->size, 0, &ptr);
          if (result != VK_SUCCESS)
             return NULL;
          VkImageSubresource isr = {
-            res->aspect,
+            zink_resource_aspect_from_format(pres),
             level,
             0
          };
@@ -528,6 +595,8 @@ zink_transfer_map(struct pipe_context *pctx,
                                   box->x;
       }
    }
+   if ((usage & PIPE_TRANSFER_PERSISTENT) && !(usage & PIPE_TRANSFER_COHERENT))
+      res->persistent_maps++;
 
    *transfer = &trans->base;
    return ptr;
@@ -547,25 +616,138 @@ zink_transfer_unmap(struct pipe_context *pctx,
 
       if (trans->base.usage & PIPE_TRANSFER_WRITE) {
          struct zink_context *ctx = zink_context(pctx);
-
+         unsigned batch_uses = zink_get_resource_usage(res);
+         if (batch_uses >= ZINK_RESOURCE_ACCESS_WRITE) {
+            if (batch_uses & (ZINK_RESOURCE_ACCESS_WRITE << ZINK_COMPUTE_BATCH_ID))
+               zink_wait_on_batch(ctx, ZINK_COMPUTE_BATCH_ID);
+            zink_fence_wait(pctx);
+         }
          zink_transfer_copy_bufimage(ctx, res, staging_res, trans, true);
       }
 
       pipe_resource_reference(&trans->staging_res, NULL);
    } else
       vkUnmapMemory(screen->dev, res->mem);
+   if ((trans->base.usage & PIPE_TRANSFER_PERSISTENT) && !(trans->base.usage & PIPE_TRANSFER_COHERENT))
+      res->persistent_maps--;
 
    pipe_resource_reference(&trans->base.resource, NULL);
    slab_free(&ctx->transfer_pool, ptrans);
 }
 
+static struct pipe_resource *
+zink_resource_get_separate_stencil(struct pipe_resource *pres)
+{
+   /* For packed depth-stencil, we treat depth as the primary resource
+    * and store S8 as the "second plane" resource.
+    */
+   if (pres->next && pres->next->format == PIPE_FORMAT_S8_UINT)
+      return pres->next;
+
+   return NULL;
+
+}
+
+void
+zink_resource_setup_layouts(struct zink_batch *batch, struct zink_resource *src, struct zink_resource *dst)
+{
+   if (src == dst) {
+      /* The Vulkan 1.1 specification says the following about valid usage
+       * of vkCmdBlitImage:
+       *
+       * "srcImageLayout must be VK_IMAGE_LAYOUT_SHARED_PRESENT_KHR,
+       *  VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL or VK_IMAGE_LAYOUT_GENERAL"
+       *
+       * and:
+       *
+       * "dstImageLayout must be VK_IMAGE_LAYOUT_SHARED_PRESENT_KHR,
+       *  VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL or VK_IMAGE_LAYOUT_GENERAL"
+       *
+       * Since we cant have the same image in two states at the same time,
+       * we're effectively left with VK_IMAGE_LAYOUT_SHARED_PRESENT_KHR or
+       * VK_IMAGE_LAYOUT_GENERAL. And since this isn't a present-related
+       * operation, VK_IMAGE_LAYOUT_GENERAL seems most appropriate.
+       */
+      if (src->layout != VK_IMAGE_LAYOUT_GENERAL)
+         zink_resource_barrier(batch, src,
+                               VK_IMAGE_LAYOUT_GENERAL, 0);
+   } else {
+      if (src->layout != VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL)
+         zink_resource_barrier(batch, src,
+                               VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL, 0);
+
+      if (dst->layout != VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL)
+         zink_resource_barrier(batch, dst,
+                               VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL, 0);
+   }
+}
+
+void
+zink_get_depth_stencil_resources(struct pipe_resource *res,
+                                 struct zink_resource **out_z,
+                                 struct zink_resource **out_s)
+{
+   if (!res) {
+      if (out_z) *out_z = NULL;
+      if (out_s) *out_s = NULL;
+      return;
+   }
+
+   if (res->format != PIPE_FORMAT_S8_UINT) {
+      if (out_z) *out_z = (void *) res;
+      if (out_s) *out_s = (void *) zink_resource_get_separate_stencil(res);
+   } else {
+      if (out_z) *out_z = NULL;
+      if (out_s) *out_s = (void *) res;
+   }
+}
+
+static void
+zink_resource_set_separate_stencil(struct pipe_resource *pres,
+                                   struct pipe_resource *stencil)
+{
+   assert(util_format_has_depth(util_format_description(pres->format)));
+   pipe_resource_reference(&pres->next, stencil);
+}
+
+static enum pipe_format
+zink_resource_get_internal_format(struct pipe_resource *pres)
+{
+   struct zink_resource *res = zink_resource(pres);
+   return res->internal_format;
+}
+
+static const struct u_transfer_vtbl transfer_vtbl = {
+   .resource_create       = zink_resource_create,
+   .resource_destroy      = zink_resource_destroy,
+   .transfer_map          = zink_transfer_map,
+   .transfer_unmap        = zink_transfer_unmap,
+   .transfer_flush_region = u_default_transfer_flush_region,
+   .get_internal_format   = zink_resource_get_internal_format,
+   .set_stencil           = zink_resource_set_separate_stencil,
+   .get_stencil           = zink_resource_get_separate_stencil,
+};
+
+void
+zink_screen_resource_init(struct pipe_screen *pscreen)
+{
+   pscreen->resource_create = zink_resource_create;
+   pscreen->resource_destroy = zink_resource_destroy;
+   pscreen->transfer_helper = u_transfer_helper_create(&transfer_vtbl, true, true, false, false);
+
+   if (zink_screen(pscreen)->have_KHR_external_memory_fd) {
+      pscreen->resource_get_handle = zink_resource_get_handle;
+      pscreen->resource_from_handle = zink_resource_from_handle;
+   }
+}
+
 void
 zink_context_resource_init(struct pipe_context *pctx)
 {
-   pctx->transfer_map = zink_transfer_map;
-   pctx->transfer_unmap = zink_transfer_unmap;
+   pctx->transfer_map = u_transfer_helper_deinterleave_transfer_map;
+   pctx->transfer_unmap = u_transfer_helper_deinterleave_transfer_unmap;
 
-   pctx->transfer_flush_region = u_default_transfer_flush_region;
+   pctx->transfer_flush_region = u_transfer_helper_transfer_flush_region;
    pctx->buffer_subdata = u_default_buffer_subdata;
    pctx->texture_subdata = u_default_texture_subdata;
 }
diff --git a/src/gallium/drivers/zink/zink_resource.h b/src/gallium/drivers/zink/zink_resource.h
index 9bca4c53a43..bb4c33ea493 100644
--- a/src/gallium/drivers/zink/zink_resource.h
+++ b/src/gallium/drivers/zink/zink_resource.h
@@ -26,21 +26,30 @@
 
 struct pipe_screen;
 struct sw_displaytarget;
+struct zink_batch;
 
 #include "util/u_transfer.h"
 
 #include <vulkan/vulkan.h>
 
+#define ZINK_RESOURCE_ACCESS_READ 1
+#define ZINK_RESOURCE_ACCESS_WRITE 32
+
 struct zink_resource {
    struct pipe_resource base;
 
+   enum pipe_format internal_format:16;
+
+   VkPipelineStageFlagBits access_stage;
    union {
-      VkBuffer buffer;
+      struct {
+         VkAccessFlags access;
+         VkBuffer buffer;
+      };
       struct {
          VkFormat format;
          VkImage image;
          VkImageLayout layout;
-         VkImageAspectFlags aspect;
          bool optimial_tiling;
       };
    };
@@ -49,8 +58,10 @@ struct zink_resource {
 
    struct sw_displaytarget *dt;
    unsigned dt_stride;
+   unsigned persistent_maps; //if nonzero, requires vkFlushMappedMemoryRanges during batch use
 
-   bool needs_xfb_barrier;
+   /* this has to be atomic for fence access, so we can't use a bitmask and make everything neat */
+   uint8_t batch_uses[5];
 };
 
 struct zink_transfer {
@@ -70,4 +81,17 @@ zink_screen_resource_init(struct pipe_screen *pscreen);
 void
 zink_context_resource_init(struct pipe_context *pctx);
 
+void
+zink_get_depth_stencil_resources(struct pipe_resource *res,
+                                 struct zink_resource **out_z,
+                                 struct zink_resource **out_s);
+
+void
+zink_resource_setup_layouts(struct zink_batch *batch, struct zink_resource *src, struct zink_resource *dst);
+
+uint32_t
+zink_get_resource_usage(struct zink_resource *res);
+
+VkImageAspectFlags
+zink_resource_aspect_from_format(const struct pipe_resource *pres);
 #endif
diff --git a/src/gallium/drivers/zink/zink_screen.c b/src/gallium/drivers/zink/zink_screen.c
index acf8e9e0bc3..f9568798d9b 100644
--- a/src/gallium/drivers/zink/zink_screen.c
+++ b/src/gallium/drivers/zink/zink_screen.c
@@ -88,6 +88,67 @@ get_video_mem(struct zink_screen *screen)
    return (int)(size >> 20);
 }
 
+static int
+zink_get_compute_param(struct pipe_screen *pscreen, enum pipe_shader_ir ir_type,
+                       enum pipe_compute_cap param, void *ret)
+{
+   struct zink_screen *screen = zink_screen(pscreen);
+#define RET(x) do {                  \
+   if (ret)                          \
+      memcpy(ret, x, sizeof(x));     \
+   return sizeof(x);                 \
+} while (0)
+
+   switch (param) {
+   case PIPE_COMPUTE_CAP_ADDRESS_BITS:
+      RET((uint32_t []){ 32 });
+
+   case PIPE_COMPUTE_CAP_IR_TARGET:
+      if (ret)
+         strcpy(ret, "nir");
+      return 4;
+
+   case PIPE_COMPUTE_CAP_GRID_DIMENSION:
+      RET((uint64_t []) { 3 });
+
+   case PIPE_COMPUTE_CAP_MAX_GRID_SIZE:
+      RET(((uint64_t []) { screen->props.limits.maxComputeWorkGroupCount[0],
+                           screen->props.limits.maxComputeWorkGroupCount[1],
+                           screen->props.limits.maxComputeWorkGroupCount[2] }));
+
+   case PIPE_COMPUTE_CAP_MAX_BLOCK_SIZE:
+      /* MaxComputeWorkGroupSize[0..2] */
+      RET(((uint64_t []) {screen->props.limits.maxComputeWorkGroupSize[0],
+                          screen->props.limits.maxComputeWorkGroupSize[1],
+                          screen->props.limits.maxComputeWorkGroupSize[2]}));
+
+   case PIPE_COMPUTE_CAP_MAX_THREADS_PER_BLOCK:
+   case PIPE_COMPUTE_CAP_MAX_VARIABLE_THREADS_PER_BLOCK:
+      RET((uint64_t []) { screen->props.limits.maxComputeWorkGroupInvocations });
+
+   case PIPE_COMPUTE_CAP_MAX_LOCAL_SIZE:
+      RET((uint64_t []) { screen->props.limits.maxComputeSharedMemorySize });
+
+   case PIPE_COMPUTE_CAP_IMAGES_SUPPORTED:
+      RET((uint32_t []) { 1 });
+
+   case PIPE_COMPUTE_CAP_SUBGROUP_SIZE:
+      RET((uint32_t []) { screen->props11.subgroupSize });
+
+   case PIPE_COMPUTE_CAP_MAX_MEM_ALLOC_SIZE:
+   case PIPE_COMPUTE_CAP_MAX_CLOCK_FREQUENCY:
+   case PIPE_COMPUTE_CAP_MAX_COMPUTE_UNITS:
+   case PIPE_COMPUTE_CAP_MAX_GLOBAL_SIZE:
+   case PIPE_COMPUTE_CAP_MAX_PRIVATE_SIZE:
+   case PIPE_COMPUTE_CAP_MAX_INPUT_SIZE:
+      // XXX: I think these are for Clover...
+      return 0;
+
+   default:
+      unreachable("unknown compute param");
+   }
+}
+
 static int
 zink_get_param(struct pipe_screen *pscreen, enum pipe_cap param)
 {
@@ -96,8 +157,48 @@ zink_get_param(struct pipe_screen *pscreen, enum pipe_cap param)
    switch (param) {
    case PIPE_CAP_NPOT_TEXTURES:
    case PIPE_CAP_TGSI_TEXCOORD:
+   case PIPE_CAP_VERTEX_ELEMENT_INSTANCE_DIVISOR:
+   case PIPE_CAP_DRAW_INDIRECT:
+   case PIPE_CAP_TEXTURE_QUERY_LOD:
+   case PIPE_CAP_GLSL_TESS_LEVELS_AS_INPUTS:
+   case PIPE_CAP_START_INSTANCE:
+   case PIPE_CAP_COPY_BETWEEN_COMPRESSED_AND_PLAIN_FORMATS:
+   case PIPE_CAP_FORCE_PERSAMPLE_INTERP:
+   case PIPE_CAP_FRAMEBUFFER_NO_ATTACHMENT:
+   case PIPE_CAP_ROBUST_BUFFER_ACCESS_BEHAVIOR:
+   case PIPE_CAP_BUFFER_MAP_PERSISTENT_COHERENT:
+   case PIPE_CAP_CLEAR_TEXTURE:
+   case PIPE_CAP_TGSI_ARRAY_COMPONENTS:
+   case PIPE_CAP_QUERY_BUFFER_OBJECT:
+   case PIPE_CAP_TEXTURE_MIRROR_CLAMP_TO_EDGE:
+   case PIPE_CAP_CONDITIONAL_RENDER_INVERTED:
+   case PIPE_CAP_CLIP_HALFZ:
+   case PIPE_CAP_TGSI_TXQS:
+   case PIPE_CAP_TEXTURE_BARRIER:
+   case PIPE_CAP_TGSI_VOTE:
+   case PIPE_CAP_DRAW_PARAMETERS:
+   case PIPE_CAP_POLYGON_OFFSET_CLAMP:
+   case PIPE_CAP_MULTI_DRAW_INDIRECT:
+   case PIPE_CAP_MULTI_DRAW_INDIRECT_PARAMS:
+   case PIPE_CAP_QUERY_SO_OVERFLOW:
+   case PIPE_CAP_ANISOTROPIC_FILTER:
+   case PIPE_CAP_QUERY_PIPELINE_STATISTICS:
+   case PIPE_CAP_QUERY_PIPELINE_STATISTICS_SINGLE:
+   case PIPE_CAP_GL_SPIRV:
+   case PIPE_CAP_TEXTURE_HALF_FLOAT_LINEAR:
       return 1;
 
+   case PIPE_CAP_BLEND_EQUATION_ADVANCED:
+      return screen->have_EXT_blend_operation_advanced;
+
+   case PIPE_CAP_MAX_VERTEX_STREAMS:
+      return screen->tf_props.maxTransformFeedbackStreams;
+
+   case PIPE_CAP_INT64:
+      return screen->feats.shaderInt64;
+   case PIPE_CAP_DOUBLES:
+      return screen->feats.shaderInt64;
+
    case PIPE_CAP_MAX_DUAL_SOURCE_RENDER_TARGETS:
       if (!screen->feats.dualSrcBlend)
          return 0;
@@ -112,12 +213,11 @@ zink_get_param(struct pipe_screen *pscreen, enum pipe_cap param)
    case PIPE_CAP_OCCLUSION_QUERY:
       return 1;
 
-#if 0 /* TODO: Enable me */
    case PIPE_CAP_QUERY_TIME_ELAPSED:
       return 1;
-#endif
 
    case PIPE_CAP_TEXTURE_MULTISAMPLE:
+   case PIPE_CAP_SAMPLE_SHADING:
       return 1;
 
    case PIPE_CAP_TEXTURE_SWIZZLE:
@@ -168,22 +268,19 @@ zink_get_param(struct pipe_screen *pscreen, enum pipe_cap param)
    case PIPE_CAP_CONDITIONAL_RENDER:
      return screen->have_EXT_conditional_rendering;
 
-   case PIPE_CAP_GLSL_FEATURE_LEVEL:
    case PIPE_CAP_GLSL_FEATURE_LEVEL_COMPATIBILITY:
       return 130;
+   case PIPE_CAP_GLSL_FEATURE_LEVEL:
+      return 460;
 
-#if 0 /* TODO: Enable me */
    case PIPE_CAP_COMPUTE:
-      return 1;
-#endif
+      return screen->compute_queue != UINT_MAX;
 
    case PIPE_CAP_CONSTANT_BUFFER_OFFSET_ALIGNMENT:
       return screen->props.limits.minUniformBufferOffsetAlignment;
 
-#if 0 /* TODO: Enable me */
    case PIPE_CAP_QUERY_TIMESTAMP:
       return 1;
-#endif
 
    case PIPE_CAP_MIN_MAP_BUFFER_ALIGNMENT:
       return screen->props.limits.minMemoryMapAlignment;
@@ -208,7 +305,7 @@ zink_get_param(struct pipe_screen *pscreen, enum pipe_cap param)
       return PIPE_ENDIAN_NATIVE; /* unsure */
 
    case PIPE_CAP_MAX_VIEWPORTS:
-      return 1; /* TODO: When GS is supported, use screen->props.limits.maxViewports */
+      return screen->props.limits.maxViewports;
 
    case PIPE_CAP_MIXED_FRAMEBUFFER_SIZES:
       return 1;
@@ -218,10 +315,8 @@ zink_get_param(struct pipe_screen *pscreen, enum pipe_cap param)
    case PIPE_CAP_MAX_GEOMETRY_TOTAL_OUTPUT_COMPONENTS:
       return screen->props.limits.maxGeometryOutputComponents;
 
-#if 0 /* TODO: Enable me. Enables ARB_texture_gather */
    case PIPE_CAP_MAX_TEXTURE_GATHER_COMPONENTS:
       return 4;
-#endif
 
    case PIPE_CAP_MIN_TEXTURE_GATHER_OFFSET:
       return screen->props.limits.minTexelGatherOffset;
@@ -246,15 +341,8 @@ zink_get_param(struct pipe_screen *pscreen, enum pipe_cap param)
    case PIPE_CAP_MAX_VERTEX_ATTRIB_STRIDE:
       return screen->props.limits.maxVertexInputBindingStride;
 
-#if 0 /* TODO: Enable me */
    case PIPE_CAP_SAMPLER_VIEW_TARGET:
       return 1;
-#endif
-
-#if 0 /* TODO: Enable me */
-   case PIPE_CAP_CLIP_HALFZ:
-      return 1;
-#endif
 
 #if 0 /* TODO: Enable me */
    case PIPE_CAP_TEXTURE_FLOAT_LINEAR:
@@ -265,10 +353,8 @@ zink_get_param(struct pipe_screen *pscreen, enum pipe_cap param)
    case PIPE_CAP_SHAREABLE_SHADERS:
       return 1;
 
-#if 0 /* TODO: Enable me. Enables GL_ARB_shader_storage_buffer_object */
    case PIPE_CAP_SHADER_BUFFER_OFFSET_ALIGNMENT:
       return screen->props.limits.minStorageBufferOffsetAlignment;
-#endif
 
    case PIPE_CAP_PCI_GROUP:
    case PIPE_CAP_PCI_BUS:
@@ -286,13 +372,13 @@ zink_get_param(struct pipe_screen *pscreen, enum pipe_cap param)
       return 0; /* not sure */
 
    case PIPE_CAP_MAX_GS_INVOCATIONS:
-      return 0; /* not implemented */
+      return screen->props.limits.maxGeometryShaderInvocations;
 
    case PIPE_CAP_MAX_COMBINED_SHADER_BUFFERS:
       return screen->props.limits.maxDescriptorSetStorageBuffers;
 
    case PIPE_CAP_MAX_SHADER_BUFFER_SIZE:
-      return screen->props.limits.maxStorageBufferRange; /* unsure */
+      return 65535;
 
    case PIPE_CAP_TGSI_FS_COORD_ORIGIN_UPPER_LEFT:
    case PIPE_CAP_TGSI_FS_COORD_PIXEL_CENTER_HALF_INTEGER:
@@ -302,9 +388,6 @@ zink_get_param(struct pipe_screen *pscreen, enum pipe_cap param)
    case PIPE_CAP_TGSI_FS_COORD_PIXEL_CENTER_INTEGER:
       return 0;
 
-   case PIPE_CAP_BUFFER_MAP_PERSISTENT_COHERENT:
-      return 0;
-
    case PIPE_CAP_NIR_COMPACT_ARRAYS:
       return 1;
 
@@ -368,6 +451,20 @@ zink_get_shader_param(struct pipe_screen *pscreen,
 
    switch (param) {
    case PIPE_SHADER_CAP_MAX_INSTRUCTIONS:
+      /* iris seems to have the most restrictive values, so copy that */
+      switch (shader) {
+      case PIPE_SHADER_FRAGMENT:
+         return 1024;
+      case PIPE_SHADER_VERTEX:
+      case PIPE_SHADER_TESS_CTRL:
+      case PIPE_SHADER_TESS_EVAL:
+      case PIPE_SHADER_GEOMETRY:
+      case PIPE_SHADER_COMPUTE:
+         return 16384;
+      default:
+         break;
+      }
+      return 0;
    case PIPE_SHADER_CAP_MAX_ALU_INSTRUCTIONS:
    case PIPE_SHADER_CAP_MAX_TEX_INSTRUCTIONS:
    case PIPE_SHADER_CAP_MAX_TEX_INDIRECTIONS:
@@ -382,6 +479,15 @@ zink_get_shader_param(struct pipe_screen *pscreen,
       case PIPE_SHADER_VERTEX:
          return MIN2(screen->props.limits.maxVertexInputAttributes,
                      PIPE_MAX_SHADER_INPUTS);
+      case PIPE_SHADER_TESS_CTRL:
+         return MIN2(screen->props.limits.maxTessellationControlPerVertexInputComponents / 4,
+                     PIPE_MAX_SHADER_INPUTS);
+      case PIPE_SHADER_TESS_EVAL:
+         return MIN2(screen->props.limits.maxTessellationEvaluationInputComponents / 4,
+                     PIPE_MAX_SHADER_INPUTS);
+      case PIPE_SHADER_GEOMETRY:
+         return MIN2(screen->props.limits.maxGeometryInputComponents,
+                     PIPE_MAX_SHADER_INPUTS);
       case PIPE_SHADER_FRAGMENT:
          return MIN2(screen->props.limits.maxFragmentInputComponents / 4,
                      PIPE_MAX_SHADER_INPUTS);
@@ -394,6 +500,15 @@ zink_get_shader_param(struct pipe_screen *pscreen,
       case PIPE_SHADER_VERTEX:
          return MIN2(screen->props.limits.maxVertexOutputComponents / 4,
                      PIPE_MAX_SHADER_OUTPUTS);
+      case PIPE_SHADER_TESS_CTRL:
+         return MIN2(screen->props.limits.maxTessellationControlPerVertexOutputComponents / 4,
+                     PIPE_MAX_SHADER_OUTPUTS);
+      case PIPE_SHADER_TESS_EVAL:
+         return MIN2(screen->props.limits.maxTessellationEvaluationOutputComponents / 4,
+                     PIPE_MAX_SHADER_OUTPUTS);
+      case PIPE_SHADER_GEOMETRY:
+         return MIN2(screen->props.limits.maxGeometryOutputComponents / 4,
+                     PIPE_MAX_SHADER_OUTPUTS);
       case PIPE_SHADER_FRAGMENT:
          return MIN2(screen->props.limits.maxColorAttachments,
                 PIPE_MAX_SHADER_OUTPUTS);
@@ -405,6 +520,9 @@ zink_get_shader_param(struct pipe_screen *pscreen,
       switch (shader) {
       case PIPE_SHADER_VERTEX:
       case PIPE_SHADER_FRAGMENT:
+      case PIPE_SHADER_GEOMETRY:
+      case PIPE_SHADER_TESS_EVAL:
+      case PIPE_SHADER_COMPUTE:
          /* this might be a bit simplistic... */
          return MIN2(screen->props.limits.maxPerStageDescriptorSamplers,
                      PIPE_MAX_SAMPLERS);
@@ -413,7 +531,7 @@ zink_get_shader_param(struct pipe_screen *pscreen,
       }
 
    case PIPE_SHADER_CAP_MAX_CONST_BUFFER_SIZE:
-      return MIN2(screen->props.limits.maxUniformBufferRange, INT_MAX);
+      return 65535;
 
    case PIPE_SHADER_CAP_MAX_CONST_BUFFERS:
       return screen->props.limits.maxPerStageDescriptorUniformBuffers;
@@ -424,10 +542,12 @@ zink_get_shader_param(struct pipe_screen *pscreen,
    case PIPE_SHADER_CAP_INTEGERS:
       return 1;
 
+   case PIPE_SHADER_CAP_INDIRECT_CONST_ADDR:
+      return 1;
+
    case PIPE_SHADER_CAP_INDIRECT_INPUT_ADDR:
    case PIPE_SHADER_CAP_INDIRECT_OUTPUT_ADDR:
    case PIPE_SHADER_CAP_INDIRECT_TEMP_ADDR:
-   case PIPE_SHADER_CAP_INDIRECT_CONST_ADDR:
    case PIPE_SHADER_CAP_SUBROUTINES:
    case PIPE_SHADER_CAP_INT64_ATOMICS:
    case PIPE_SHADER_CAP_FP16:
@@ -465,12 +585,8 @@ zink_get_shader_param(struct pipe_screen *pscreen,
       return (1 << PIPE_SHADER_IR_NIR) | (1 << PIPE_SHADER_IR_TGSI);
 
    case PIPE_SHADER_CAP_MAX_SHADER_IMAGES:
-#if 0 /* TODO: needs compiler support */
       return MIN2(screen->props.limits.maxPerStageDescriptorStorageImages,
                   PIPE_MAX_SHADER_IMAGES);
-#else
-      return 0;
-#endif
 
    case PIPE_SHADER_CAP_LOWER_IF_THRESHOLD:
    case PIPE_SHADER_CAP_TGSI_SKIP_MERGE_REGISTERS:
@@ -558,6 +674,10 @@ zink_is_format_supported(struct pipe_screen *pscreen,
              !(screen->props.limits.sampledImageColorSampleCounts & sample_mask))
             return false;
       }
+      if (bind & PIPE_BIND_SHADER_IMAGE) {
+          if (!screen->feats.shaderStorageImageMultisample)
+             return false;
+      }
    }
 
    VkFormatProperties props;
@@ -578,8 +698,15 @@ zink_is_format_supported(struct pipe_screen *pscreen,
         return false;
 
       if (bind & PIPE_BIND_SAMPLER_VIEW &&
-          !(props.optimalTilingFeatures & VK_FORMAT_FEATURE_SAMPLED_IMAGE_BIT))
-         return false;
+         !(props.optimalTilingFeatures & VK_FORMAT_FEATURE_SAMPLED_IMAGE_BIT))
+            return false;
+
+      if ((bind & PIPE_BIND_SAMPLER_VIEW) || (bind & PIPE_BIND_RENDER_TARGET)) {
+         /* if this is a 3-component texture, force gallium to give us 4 components by rejecting this one */
+         unsigned bpb = util_format_get_blocksizebits(format);
+         if (bpb == 24 || bpb == 48 || bpb == 96)
+            return false;
+      }
 
       if (bind & PIPE_BIND_DEPTH_STENCIL &&
           !(props.optimalTilingFeatures & VK_FORMAT_FEATURE_DEPTH_STENCIL_ATTACHMENT_BIT))
@@ -617,7 +744,7 @@ create_instance()
       ai.pApplicationName = "unknown";
 
    ai.pEngineName = "mesa zink";
-   ai.apiVersion = VK_API_VERSION_1_0;
+   ai.apiVersion = VK_API_VERSION_1_2;
 
    const char *extensions[] = {
       VK_KHR_GET_PHYSICAL_DEVICE_PROPERTIES_2_EXTENSION_NAME,
@@ -678,7 +805,11 @@ update_queue_props(struct zink_screen *screen)
          screen->gfx_queue = i;
          screen->timestamp_valid_bits = props[i].timestampValidBits;
          assert(screen->timestamp_valid_bits);
-         break;
+      }
+      if (props[i].queueFlags & VK_QUEUE_COMPUTE_BIT) {
+         screen->compute_queue = i;
+         screen->compute_timestamp_valid_bits = props[i].timestampValidBits;
+         assert(screen->compute_timestamp_valid_bits);
       }
    }
    free(props);
@@ -701,7 +832,7 @@ zink_flush_frontbuffer(struct pipe_screen *pscreen,
 
    if (map) {
       VkImageSubresource isr = {};
-      isr.aspectMask = res->aspect;
+      isr.aspectMask = zink_resource_aspect_from_format(&res->base);
       isr.mipLevel = level;
       isr.arrayLayer = layer;
       VkSubresourceLayout layout;
@@ -796,15 +927,19 @@ zink_internal_create_screen(struct sw_winsys *winsys, int fd)
    struct zink_screen *screen = CALLOC_STRUCT(zink_screen);
    bool have_tf_ext = false, have_cond_render_ext = false, have_EXT_index_type_uint8 = false,
       have_EXT_robustness2_features = false, have_EXT_vertex_attribute_divisor = false,
-      have_EXT_calibrated_timestamps = false;
+      have_EXT_calibrated_timestamps = false, have_VK_KHR_vulkan_memory_model = false;
+   bool have_EXT_custom_border_color = false, have_EXT_blend_operation_advanced = false;
    if (!screen)
       return NULL;
 
+   screen->compute_queue = UINT_MAX;
+   screen->gfx_queue = UINT_MAX;
    zink_debug = debug_get_option_zink_debug();
 
    screen->instance = create_instance();
    screen->pdev = choose_pdev(screen->instance);
    update_queue_props(screen);
+   assert(screen->gfx_queue < UINT_MAX);
 
    vkGetPhysicalDeviceMemoryProperties(screen->pdev, &screen->mem_props);
 
@@ -847,6 +982,15 @@ zink_internal_create_screen(struct sw_winsys *winsys, int fd)
             if (!strcmp(extensions[i].extensionName,
                         VK_EXT_CALIBRATED_TIMESTAMPS_EXTENSION_NAME))
                have_EXT_calibrated_timestamps = true;
+            if (!strcmp(extensions[i].extensionName,
+                        VK_KHR_VULKAN_MEMORY_MODEL_EXTENSION_NAME))
+               have_VK_KHR_vulkan_memory_model = true;
+            if (!strcmp(extensions[i].extensionName,
+                        VK_EXT_CUSTOM_BORDER_COLOR_EXTENSION_NAME))
+               have_EXT_custom_border_color = true;
+            if (!strcmp(extensions[i].extensionName,
+                        VK_EXT_BLEND_OPERATION_ADVANCED_EXTENSION_NAME))
+               have_EXT_blend_operation_advanced = true;
 
          }
          FREE(extensions);
@@ -856,8 +1000,16 @@ zink_internal_create_screen(struct sw_winsys *winsys, int fd)
    VkPhysicalDeviceTransformFeedbackFeaturesEXT tf_feats = {};
    VkPhysicalDeviceConditionalRenderingFeaturesEXT cond_render_feats = {};
    VkPhysicalDeviceIndexTypeUint8FeaturesEXT index_uint8_feats = {};
+   VkPhysicalDeviceVulkanMemoryModelFeatures mem_feats = {};
+   VkPhysicalDeviceBlendOperationAdvancedFeaturesEXT blend_feats = {};
 
    feats.sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_FEATURES_2;
+   screen->feats11.sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_VULKAN_1_1_FEATURES;
+   screen->feats11.pNext = feats.pNext;
+   feats.pNext = &screen->feats11;
+   screen->feats12.sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_VULKAN_1_2_FEATURES;
+   screen->feats12.pNext = feats.pNext;
+   feats.pNext = &screen->feats12;
    if (have_tf_ext) {
       tf_feats.sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_TRANSFORM_FEEDBACK_FEATURES_EXT;
       tf_feats.pNext = feats.pNext;
@@ -883,6 +1035,21 @@ zink_internal_create_screen(struct sw_winsys *winsys, int fd)
       screen->vdiv_feats.pNext = feats.pNext;
       feats.pNext = &screen->vdiv_feats;
    }
+   if (have_VK_KHR_vulkan_memory_model) {
+      mem_feats.sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_VULKAN_MEMORY_MODEL_FEATURES;
+      mem_feats.pNext = feats.pNext;
+      feats.pNext = &mem_feats;
+   }
+   if (have_EXT_custom_border_color) {
+      screen->border_color_feats.sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_CUSTOM_BORDER_COLOR_FEATURES_EXT;
+      screen->border_color_feats.pNext = feats.pNext;
+      feats.pNext = &screen->border_color_feats;
+   }
+   if (have_EXT_blend_operation_advanced) {
+      blend_feats.sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_BLEND_OPERATION_ADVANCED_FEATURES_EXT;
+      blend_feats.pNext = feats.pNext;
+      feats.pNext = &blend_feats;
+   }
    vkGetPhysicalDeviceFeatures2(screen->pdev, &feats);
    memcpy(&screen->feats, &feats.features, sizeof(screen->feats));
    if (have_tf_ext && tf_feats.transformFeedback)
@@ -895,10 +1062,19 @@ zink_internal_create_screen(struct sw_winsys *winsys, int fd)
    if (have_EXT_vertex_attribute_divisor && screen->vdiv_feats.vertexAttributeInstanceRateDivisor)
       screen->have_EXT_vertex_attribute_divisor = true;
    screen->have_EXT_calibrated_timestamps = have_EXT_calibrated_timestamps;
+   if (have_EXT_custom_border_color && screen->border_color_feats.customBorderColors)
+      screen->have_EXT_custom_border_color = true;
 
    VkPhysicalDeviceProperties2 props = {};
    VkPhysicalDeviceVertexAttributeDivisorPropertiesEXT vdiv_props = {};
+   VkPhysicalDeviceCustomBorderColorPropertiesEXT border_color_props = {};
    props.sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_PROPERTIES_2;
+   screen->props11.sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_VULKAN_1_1_PROPERTIES;
+   screen->props11.pNext = props.pNext;
+   props.pNext = &screen->props11;
+   screen->props12.sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_VULKAN_1_2_PROPERTIES;
+   screen->props12.pNext = props.pNext;
+   props.pNext = &screen->props12;
    if (screen->have_EXT_transform_feedback) {
       screen->tf_props.sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_TRANSFORM_FEEDBACK_PROPERTIES_EXT;
       screen->tf_props.pNext = props.pNext;
@@ -914,31 +1090,57 @@ zink_internal_create_screen(struct sw_winsys *winsys, int fd)
       vdiv_props.pNext = props.pNext;
       props.pNext = &vdiv_props;
    }
+   if (have_EXT_custom_border_color) {
+      border_color_props.sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_CUSTOM_BORDER_COLOR_PROPERTIES_EXT;
+      border_color_props.pNext = props.pNext;
+      props.pNext = &border_color_props;
+   }
+   if (have_EXT_blend_operation_advanced) {
+      screen->blend_props.sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_BLEND_OPERATION_ADVANCED_PROPERTIES_EXT;
+      screen->blend_props.pNext = props.pNext;
+      props.pNext = &screen->blend_props;
+   }
    vkGetPhysicalDeviceProperties2(screen->pdev, &props);
+   /* TODO: we can probably support non-premul here with some work? */
+   if (have_EXT_blend_operation_advanced &&
+       screen->blend_props.advancedBlendNonPremultipliedSrcColor &&
+       screen->blend_props.advancedBlendNonPremultipliedDstColor) {
+      screen->have_EXT_blend_operation_advanced = true;
+      screen->advancedBlendCoherentOperations = blend_feats.advancedBlendCoherentOperations;
+   }
    memcpy(&screen->props, &props.properties, sizeof(screen->props));
    screen->max_vertex_attrib_divisor = vdiv_props.maxVertexAttribDivisor;
+   screen->max_custom_border_color_samplers = border_color_props.maxCustomBorderColorSamplers;
 
    if (!screen->have_KHR_maintenance1) {
       debug_printf("ZINK: VK_KHR_maintenance1 required!\n");
       goto fail;
    }
 
-   VkDeviceQueueCreateInfo qci = {};
-   float dummy = 0.0f;
-   qci.sType = VK_STRUCTURE_TYPE_DEVICE_QUEUE_CREATE_INFO;
-   qci.queueFamilyIndex = screen->gfx_queue;
-   qci.queueCount = 1;
-   qci.pQueuePriorities = &dummy;
-
    VkDeviceCreateInfo dci = {};
    dci.sType = VK_STRUCTURE_TYPE_DEVICE_CREATE_INFO;
    dci.queueCreateInfoCount = 1;
-   dci.pQueueCreateInfos = &qci;
+
+   VkDeviceQueueCreateInfo qci[2] = {};
+   float dummy = 0.0f;
+   qci[0].sType = VK_STRUCTURE_TYPE_DEVICE_QUEUE_CREATE_INFO;
+   qci[0].queueFamilyIndex = screen->gfx_queue;
+   qci[0].queueCount = 1;
+   qci[0].pQueuePriorities = &dummy;
+   if (screen->compute_queue != UINT_MAX && screen->compute_queue != screen->gfx_queue) {
+      qci[1].sType = VK_STRUCTURE_TYPE_DEVICE_QUEUE_CREATE_INFO;
+      qci[1].queueFamilyIndex = screen->compute_queue;
+      qci[1].queueCount = 1;
+      qci[1].pQueuePriorities = &dummy;
+      dci.queueCreateInfoCount++;
+   }
+
+   dci.pQueueCreateInfos = &qci[0];
    /* extensions don't have bool members in pEnabledFeatures.
     * this requires us to pass the whole VkPhysicalDeviceFeatures2 struct
     */
    dci.pNext = &feats;
-   const char *extensions[9] = {
+   const char *extensions[12] = {
       VK_KHR_MAINTENANCE1_EXTENSION_NAME,
    };
    num_extensions = 1;
@@ -967,6 +1169,12 @@ zink_internal_create_screen(struct sw_winsys *winsys, int fd)
       extensions[num_extensions++] = VK_EXT_VERTEX_ATTRIBUTE_DIVISOR_EXTENSION_NAME;
    if (screen->have_EXT_calibrated_timestamps)
       extensions[num_extensions++] = VK_EXT_CALIBRATED_TIMESTAMPS_EXTENSION_NAME;
+   if (have_VK_KHR_vulkan_memory_model)
+      extensions[num_extensions++] = VK_KHR_VULKAN_MEMORY_MODEL_EXTENSION_NAME;
+   if (have_EXT_custom_border_color)
+      extensions[num_extensions++] = VK_EXT_CUSTOM_BORDER_COLOR_EXTENSION_NAME;
+   if (have_EXT_blend_operation_advanced)
+      extensions[num_extensions++] = VK_EXT_BLEND_OPERATION_ADVANCED_EXTENSION_NAME;
    assert(num_extensions <= ARRAY_SIZE(extensions));
 
    dci.ppEnabledExtensionNames = extensions;
@@ -982,6 +1190,7 @@ zink_internal_create_screen(struct sw_winsys *winsys, int fd)
    screen->base.get_name = zink_get_name;
    screen->base.get_vendor = zink_get_vendor;
    screen->base.get_device_vendor = zink_get_device_vendor;
+   screen->base.get_compute_param = zink_get_compute_param;
    screen->base.get_param = zink_get_param;
    screen->base.get_paramf = zink_get_paramf;
    screen->base.get_shader_param = zink_get_shader_param;
diff --git a/src/gallium/drivers/zink/zink_screen.h b/src/gallium/drivers/zink/zink_screen.h
index a65a053c927..08719a5f0dc 100644
--- a/src/gallium/drivers/zink/zink_screen.h
+++ b/src/gallium/drivers/zink/zink_screen.h
@@ -42,17 +42,28 @@ struct zink_screen {
 
    struct slab_parent_pool transfer_pool;
 
+   unsigned shader_id;
+
    VkInstance instance;
    VkPhysicalDevice pdev;
 
    VkPhysicalDeviceProperties props;
+   VkPhysicalDeviceVulkan11Properties props11;
+   VkPhysicalDeviceVulkan12Properties props12;
    VkPhysicalDeviceFeatures feats;
+   VkPhysicalDeviceVulkan11Features feats11;
+   VkPhysicalDeviceVulkan12Features feats12;
    VkPhysicalDeviceMemoryProperties mem_props;
    VkPhysicalDeviceTransformFeedbackPropertiesEXT tf_props;
    VkPhysicalDeviceRobustness2PropertiesEXT rb2_props;
    VkPhysicalDeviceRobustness2FeaturesEXT rb2_feats;
    VkPhysicalDeviceVertexAttributeDivisorFeaturesEXT vdiv_feats;
    uint32_t max_vertex_attrib_divisor;
+   VkPhysicalDeviceCustomBorderColorFeaturesEXT border_color_feats;
+   uint32_t max_custom_border_color_samplers;
+   uint32_t cur_custom_border_color_samplers;
+   VkPhysicalDeviceBlendOperationAdvancedPropertiesEXT blend_props;
+   bool advancedBlendCoherentOperations;
 
    bool have_KHR_maintenance1;
    bool have_KHR_external_memory_fd;
@@ -62,12 +73,16 @@ struct zink_screen {
    bool have_EXT_robustness2_features;
    bool have_EXT_vertex_attribute_divisor;
    bool have_EXT_calibrated_timestamps;
+   bool have_EXT_custom_border_color;
+   bool have_EXT_blend_operation_advanced;
 
    bool have_X8_D24_UNORM_PACK32;
    bool have_D24_UNORM_S8_UINT;
 
    uint32_t gfx_queue;
+   uint32_t compute_queue;
    uint32_t timestamp_valid_bits;
+   uint32_t compute_timestamp_valid_bits;
    VkDevice dev;
 
    PFN_vkGetMemoryFdKHR vk_GetMemoryFdKHR;
diff --git a/src/gallium/drivers/zink/zink_shader_keys.h b/src/gallium/drivers/zink/zink_shader_keys.h
new file mode 100644
index 00000000000..47131cd1681
--- /dev/null
+++ b/src/gallium/drivers/zink/zink_shader_keys.h
@@ -0,0 +1,74 @@
+/*
+ * Copyright 2020 Mike Blumenkrantz
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * on the rights to use, copy, modify, merge, publish, distribute, sub
+ * license, and/or sell copies of the Software, and to permit persons to whom
+ * the Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
+ * THE AUTHOR(S) AND/OR THEIR SUPPLIERS BE LIABLE FOR ANY CLAIM,
+ * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
+ * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
+ * USE OR OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+
+/** this file exists solely to be included in nir_to_spirv/ without pulling in extra deps */
+#ifndef ZINK_SHADER_KEYS_H
+# define ZINK_SHADER_KEYS_H
+
+struct zink_vs_key {
+   unsigned shader_id;
+   bool clip_halfz;
+};
+
+struct zink_fs_key {
+   unsigned shader_id;
+   //bool flat_shade;
+   unsigned char nr_cbufs;
+   bool samples;
+};
+
+struct zink_tcs_key {
+   unsigned shader_id;
+   unsigned vertices_per_patch;
+   uint64_t vs_outputs_written;
+};
+
+/* a shader key is used for swapping out shader modules based on pipeline states,
+ * e.g., if sampleCount changes, we must verify that the fs doesn't need a recompile
+ *       to account for GL ignoring gl_SampleMask in some cases when VK will not
+ * which allows us to avoid recompiling shaders when the pipeline state changes repeatedly
+ */
+struct zink_shader_key {
+   union {
+      /* reuse vs key for now with tes/gs since we only use clip_halfz */
+      struct zink_vs_key vs;
+      struct zink_fs_key fs;
+      struct zink_tcs_key tcs;
+   } key;
+   uint32_t size;
+};
+
+static inline const struct zink_fs_key *zink_fs_key(const struct zink_shader_key *key)
+{
+   return &key->key.fs;
+}
+
+static inline const struct zink_vs_key *zink_vs_key(const struct zink_shader_key *key)
+{
+   return &key->key.vs;
+}
+
+
+
+#endif
diff --git a/src/gallium/drivers/zink/zink_state.c b/src/gallium/drivers/zink/zink_state.c
index 95f23c273ba..1be3c53b840 100644
--- a/src/gallium/drivers/zink/zink_state.c
+++ b/src/gallium/drivers/zink/zink_state.c
@@ -26,6 +26,7 @@
 #include "zink_context.h"
 #include "zink_screen.h"
 
+#include "compiler/shader_enums.h"
 #include "util/u_memory.h"
 
 #include <math.h>
@@ -48,7 +49,6 @@ zink_create_vertex_elements_state(struct pipe_context *pctx,
    int num_bindings = 0;
    for (i = 0; i < num_elements; ++i) {
       const struct pipe_vertex_element *elem = elements + i;
-      assert(!elem->instance_divisor);
 
       int binding = elem->vertex_buffer_index;
       if (buffer_map[binding] < 0) {
@@ -59,7 +59,11 @@ zink_create_vertex_elements_state(struct pipe_context *pctx,
 
 
       ves->bindings[binding].binding = binding;
-      ves->bindings[binding].inputRate = VK_VERTEX_INPUT_RATE_VERTEX;
+      ves->bindings[binding].inputRate = elem->instance_divisor ? VK_VERTEX_INPUT_RATE_INSTANCE : VK_VERTEX_INPUT_RATE_VERTEX;
+
+      assert(!elem->instance_divisor || zink_screen(pctx->screen)->have_EXT_vertex_attribute_divisor);
+      ves->divisor[binding] = elem->instance_divisor;
+      assert(elem->instance_divisor <= screen->max_vertex_attrib_divisor);
 
       ves->hw_state.attribs[i].binding = binding;
       ves->hw_state.attribs[i].location = i; // TODO: unsure
@@ -82,12 +86,18 @@ zink_bind_vertex_elements_state(struct pipe_context *pctx,
    struct zink_gfx_pipeline_state *state = &ctx->gfx_pipeline_state;
    ctx->element_state = cso;
    state->hash = 0;
+   state->divisors_present = 0;
    if (cso) {
       state->element_state = &ctx->element_state->hw_state;
       struct zink_vertex_elements_state *ves = cso;
       for (int i = 0; i < state->element_state->num_bindings; ++i) {
          state->bindings[i].binding = ves->bindings[i].binding;
          state->bindings[i].inputRate = ves->bindings[i].inputRate;
+         if (ves->divisor[i]) {
+            state->divisors[state->divisors_present].divisor = ves->divisor[i];
+            state->divisors[state->divisors_present].binding = state->bindings[i].binding;
+            state->divisors_present++;
+         }
       }
    } else
      state->element_state = NULL;
@@ -167,6 +177,30 @@ blend_op(enum pipe_blend_func func)
    unreachable("unexpected blend function");
 }
 
+static VkBlendOp
+advanced_blend_op(enum gl_advanced_blend_mode mode)
+{
+   switch (mode) {
+   case BLEND_MULTIPLY: return VK_BLEND_OP_MULTIPLY_EXT;
+   case BLEND_SCREEN: return VK_BLEND_OP_SCREEN_EXT;
+   case BLEND_OVERLAY: return VK_BLEND_OP_OVERLAY_EXT;
+   case BLEND_DARKEN: return VK_BLEND_OP_DARKEN_EXT;
+   case BLEND_LIGHTEN: return VK_BLEND_OP_LIGHTEN_EXT;
+   case BLEND_COLORDODGE: return VK_BLEND_OP_COLORDODGE_EXT;
+   case BLEND_COLORBURN: return VK_BLEND_OP_COLORBURN_EXT;
+   case BLEND_HARDLIGHT: return VK_BLEND_OP_HARDLIGHT_EXT;
+   case BLEND_SOFTLIGHT: return VK_BLEND_OP_SOFTLIGHT_EXT;
+   case BLEND_DIFFERENCE: return VK_BLEND_OP_DIFFERENCE_EXT;
+   case BLEND_EXCLUSION: return VK_BLEND_OP_EXCLUSION_EXT;
+   case BLEND_HSL_HUE: return VK_BLEND_OP_HSL_HUE_EXT;
+   case BLEND_HSL_SATURATION: return VK_BLEND_OP_HSL_SATURATION_EXT;
+   case BLEND_HSL_COLOR: return VK_BLEND_OP_HSL_COLOR_EXT;
+   case BLEND_HSL_LUMINOSITY: return VK_BLEND_OP_HSL_LUMINOSITY_EXT;
+   default:
+      unreachable("unknown advanced blend mode");
+   }
+}
+
 static VkLogicOp
 logic_op(enum pipe_logicop func)
 {
@@ -191,10 +225,26 @@ logic_op(enum pipe_logicop func)
    unreachable("unexpected logicop function");
 }
 
+/* from iris */
+static enum pipe_blendfactor
+fix_blendfactor(enum pipe_blendfactor f, bool alpha_to_one)
+{
+   if (alpha_to_one) {
+      if (f == PIPE_BLENDFACTOR_SRC1_ALPHA)
+         return PIPE_BLENDFACTOR_ONE;
+
+      if (f == PIPE_BLENDFACTOR_INV_SRC1_ALPHA)
+         return PIPE_BLENDFACTOR_ZERO;
+   }
+
+   return f;
+}
+
 static void *
 zink_create_blend_state(struct pipe_context *pctx,
                         const struct pipe_blend_state *blend_state)
 {
+   struct zink_screen *screen = zink_screen(pctx->screen);
    struct zink_blend_state *cso = CALLOC_STRUCT(zink_blend_state);
    if (!cso)
       return NULL;
@@ -215,6 +265,11 @@ zink_create_blend_state(struct pipe_context *pctx,
    cso->alpha_to_one = blend_state->alpha_to_one;
 
    cso->need_blend_constants = false;
+   cso->advanced_blend = blend_state->advanced_blend_func && screen->have_EXT_blend_operation_advanced;
+   if (cso->advanced_blend) {
+      cso->logicop_enable = VK_TRUE;
+      cso->logicop_func = VK_LOGIC_OP_COPY;
+   }
 
    for (int i = 0; i < PIPE_MAX_COLOR_BUFS; ++i) {
       const struct pipe_rt_blend_state *rt = blend_state->rt;
@@ -223,13 +278,24 @@ zink_create_blend_state(struct pipe_context *pctx,
 
       VkPipelineColorBlendAttachmentState att = { };
 
-      if (rt->blend_enable) {
+      if (cso->advanced_blend) {
+         assert(i < screen->blend_props.advancedBlendMaxColorAttachments);
          att.blendEnable = VK_TRUE;
-         att.srcColorBlendFactor = blend_factor(rt->rgb_src_factor);
-         att.dstColorBlendFactor = blend_factor(rt->rgb_dst_factor);
+         att.srcColorBlendFactor = VK_BLEND_FACTOR_SRC_COLOR;
+         att.dstColorBlendFactor = VK_BLEND_FACTOR_DST_COLOR;
+         att.colorBlendOp = advanced_blend_op(blend_state->advanced_blend_func);
+         att.srcAlphaBlendFactor = VK_BLEND_FACTOR_SRC_ALPHA;
+         att.dstAlphaBlendFactor = VK_BLEND_FACTOR_DST_ALPHA;
+         att.alphaBlendOp = att.colorBlendOp;
+      } else if (rt->blend_enable) {
+         if (blend_state->advanced_blend_func)
+            debug_printf("ignoring advanced blend mode due to missing EXT_blend_operation_advanced extension");
+         att.blendEnable = VK_TRUE;
+         att.srcColorBlendFactor = blend_factor(fix_blendfactor(rt->rgb_src_factor, cso->alpha_to_one));
+         att.dstColorBlendFactor = blend_factor(fix_blendfactor(rt->rgb_dst_factor, cso->alpha_to_one));
          att.colorBlendOp = blend_op(rt->rgb_func);
-         att.srcAlphaBlendFactor = blend_factor(rt->alpha_src_factor);
-         att.dstAlphaBlendFactor = blend_factor(rt->alpha_dst_factor);
+         att.srcAlphaBlendFactor = blend_factor(fix_blendfactor(rt->alpha_src_factor, cso->alpha_to_one));
+         att.dstAlphaBlendFactor = blend_factor(fix_blendfactor(rt->alpha_dst_factor, cso->alpha_to_one));
          att.alphaBlendOp = blend_op(rt->alpha_func);
 
          if (need_blend_constants(rt->rgb_src_factor) ||
@@ -402,6 +468,7 @@ zink_create_rasterizer_state(struct pipe_context *pctx,
    assert(rs_state->depth_clip_far == rs_state->depth_clip_near);
    state->hw_state.depth_clamp = rs_state->depth_clip_near == 0;
    state->hw_state.rasterizer_discard = rs_state->rasterizer_discard;
+   state->hw_state.force_persample_interp = rs_state->force_persample_interp;
 
    assert(rs_state->fill_front <= PIPE_POLYGON_MODE_POINT);
    if (rs_state->fill_back != rs_state->fill_front)
@@ -431,6 +498,7 @@ static void
 zink_bind_rasterizer_state(struct pipe_context *pctx, void *cso)
 {
    struct zink_context *ctx = zink_context(pctx);
+   bool clip_halfz = ctx->rast_state ? ctx->rast_state->base.clip_halfz : false;
    ctx->rast_state = cso;
 
    if (ctx->rast_state) {
@@ -439,6 +507,11 @@ zink_bind_rasterizer_state(struct pipe_context *pctx, void *cso)
          ctx->gfx_pipeline_state.hash = 0;
       }
 
+      if (clip_halfz != ctx->rast_state->base.clip_halfz)
+         ctx->dirty_shader_stages |= BITFIELD64_BIT(PIPE_SHADER_VERTEX) |
+                                     BITFIELD64_BIT(PIPE_SHADER_TESS_EVAL) |
+                                     BITFIELD64_BIT(PIPE_SHADER_GEOMETRY);
+
       if (ctx->line_width != ctx->rast_state->line_width) {
          ctx->line_width = ctx->rast_state->line_width;
          ctx->gfx_pipeline_state.hash = 0;
diff --git a/src/gallium/drivers/zink/zink_state.h b/src/gallium/drivers/zink/zink_state.h
index ef5e18176d4..7d0996fc2ff 100644
--- a/src/gallium/drivers/zink/zink_state.h
+++ b/src/gallium/drivers/zink/zink_state.h
@@ -38,6 +38,7 @@ struct zink_vertex_elements_state {
       uint32_t binding;
       VkVertexInputRate inputRate;
    } bindings[PIPE_MAX_ATTRIBS];
+   uint32_t divisor[PIPE_MAX_ATTRIBS];
    uint8_t binding_map[PIPE_MAX_ATTRIBS];
    struct zink_vertex_elements_hw_state hw_state;
 };
@@ -48,6 +49,7 @@ struct zink_rasterizer_hw_state {
    VkFrontFace front_face;
    VkPolygonMode polygon_mode;
    VkCullModeFlags cull_mode;
+   bool force_persample_interp;
 };
 
 struct zink_rasterizer_state {
@@ -68,6 +70,7 @@ struct zink_blend_state {
    VkBool32 alpha_to_one;
 
    bool need_blend_constants;
+   bool advanced_blend;
 };
 
 struct zink_depth_stencil_alpha_state {
diff --git a/src/gallium/drivers/zink/zink_surface.c b/src/gallium/drivers/zink/zink_surface.c
index d5d0a3c6dbc..17fae5203ac 100644
--- a/src/gallium/drivers/zink/zink_surface.c
+++ b/src/gallium/drivers/zink/zink_surface.c
@@ -102,15 +102,17 @@ zink_create_surface(struct pipe_context *pctx,
    ivci.components.b = VK_COMPONENT_SWIZZLE_B;
    ivci.components.a = VK_COMPONENT_SWIZZLE_A;
 
-   ivci.subresourceRange.aspectMask = res->aspect;
+   ivci.subresourceRange.aspectMask = zink_resource_aspect_from_format(pres);
    ivci.subresourceRange.baseMipLevel = templ->u.tex.level;
    ivci.subresourceRange.levelCount = 1;
    ivci.subresourceRange.baseArrayLayer = templ->u.tex.first_layer;
    ivci.subresourceRange.layerCount = 1 + templ->u.tex.last_layer - templ->u.tex.first_layer;
 
    if (pres->target == PIPE_TEXTURE_CUBE ||
-       pres->target == PIPE_TEXTURE_CUBE_ARRAY)
-      ivci.subresourceRange.layerCount *= 6;
+       pres->target == PIPE_TEXTURE_CUBE_ARRAY) {
+      if (ivci.subresourceRange.layerCount != 6)
+         ivci.subresourceRange.layerCount = VK_REMAINING_ARRAY_LAYERS;
+   }
 
    if (vkCreateImageView(screen->dev, &ivci, NULL,
                          &surface->image_view) != VK_SUCCESS) {
