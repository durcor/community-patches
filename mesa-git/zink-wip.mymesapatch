diff --git a/.gitlab-ci.yml b/.gitlab-ci.yml
index 25d758053eb..e3c5c7411b2 100644
--- a/.gitlab-ci.yml
+++ b/.gitlab-ci.yml
@@ -362,13 +362,13 @@ x86_test-base:
 x86_test-gl:
   extends: .use-x86_test-base
   variables:
-    MESA_IMAGE_TAG: &x86_test-gl "2020-01-13-piglit"
+    MESA_IMAGE_TAG: &x86_test-gl "2020-12-22-runner"
 
 # Debian 10 based x86 test image for VK
 x86_test-vk:
   extends: .use-x86_test-base
   variables:
-    MESA_IMAGE_TAG: &x86_test-vk "2020-01-13-piglit"
+    MESA_IMAGE_TAG: &x86_test-vk "2020-12-22-runner"
 
 # Debian 10 based ARM build image
 arm_build:
@@ -408,7 +408,7 @@ arm64_test:
   extends:
     - .use-arm_test-base
   variables:
-    MESA_IMAGE_TAG: &arm64_test "2020-01-13-piglit"
+    MESA_IMAGE_TAG: &arm64_test "2020-12-23-piglit-2"
 
 .use-arm64_test:
   variables:
@@ -422,7 +422,7 @@ armhf_test:
   extends:
     - .use-arm_test-base
   variables:
-    MESA_IMAGE_TAG: &armhf_test "2020-01-13-piglit"
+    MESA_IMAGE_TAG: &armhf_test "2020-12-23-piglit-2"
 
 .use-armhf_test:
   variables:
diff --git a/.gitlab-ci/build-piglit.sh b/.gitlab-ci/build-piglit.sh
index 63c26a0e010..99e09913ca6 100644
--- a/.gitlab-ci/build-piglit.sh
+++ b/.gitlab-ci/build-piglit.sh
@@ -8,7 +8,7 @@ fi
 
 git clone https://gitlab.freedesktop.org/mesa/piglit.git --single-branch --no-checkout /piglit
 pushd /piglit
-git checkout 7e168a62b336dd106f685cd5a4a457ad7c28d501
+git checkout c702d2bbf28b01a18ce613f386a4ffef03f6c0c9
 patch -p1 <$OLDPWD/.gitlab-ci/piglit/disable-vs_in.diff
 cmake -S . -B . -G Ninja -DCMAKE_BUILD_TYPE=Release $PIGLIT_OPTS $EXTRA_CMAKE_ARGS
 ninja $PIGLIT_BUILD_TARGETS
diff --git a/.gitlab-ci/deqp-freedreno-a307-fails.txt b/.gitlab-ci/deqp-freedreno-a307-fails.txt
index 018ee6b663e..01ea1204669 100644
--- a/.gitlab-ci/deqp-freedreno-a307-fails.txt
+++ b/.gitlab-ci/deqp-freedreno-a307-fails.txt
@@ -28,6 +28,5 @@ dEQP-GLES3.functional.occlusion_query.stencil_clear,Fail
 dEQP-GLES3.functional.shaders.derivate.dfdx.fbo_msaa4.vec3_mediump,Fail
 dEQP-GLES3.functional.shaders.derivate.dfdy.fbo_msaa4.vec4_highp,Fail
 dEQP-GLES3.functional.shaders.derivate.fwidth.fbo_msaa2.float_mediump,Fail
-dEQP-GLES3.functional.shaders.texture_functions.texturegradoffset.sampler3d_float_vertex,Fail
 dEQP-GLES3.functional.transform_feedback.random_full_array_capture.interleaved.lines.8,Fail
 dEQP-GLES3.functional.transform_feedback.random_full_array_capture.separate.triangles.3,Fail
diff --git a/.gitlab-ci/deqp-freedreno-a307-flakes.txt b/.gitlab-ci/deqp-freedreno-a307-flakes.txt
index 6802525b14c..3b119dec616 100644
--- a/.gitlab-ci/deqp-freedreno-a307-flakes.txt
+++ b/.gitlab-ci/deqp-freedreno-a307-flakes.txt
@@ -1,7 +1,15 @@
 # Note: flakes lists for CI are just a list of lines that, when
 # non-zero-length and not starting with '#', will regex match to
 # delete lines from the test list.  Be careful.
-dEQP-GLES3.functional.shaders.texture_functions.texturegradoffset.sampler3d_float_vertex
+dEQP-GLES3.functional.occlusion_query.stencil_write
+dEQP-GLES3.functional.rasterization.fbo.rbo_.*
+dEQP-GLES3.functional.rasterization.fbo.texture_2d.interpolation.triangles
+dEQP-GLES3.functional.rasterization.fbo.texture_2d.primitives.points
+dEQP-GLES3.functional.rasterization.flatshading.lines_wide
+dEQP-GLES3.functional.rasterization.flatshading.triangles
+dEQP-GLES3.functional.shaders.linkage.varying.interpolation.centroid
+dEQP-GLES3.functional.shaders.texture_functions.texturegradoffset.*
+dEQP-GLES3.functional.shaders.texture_functions.textureprojgradoffset.*
 dEQP-GLES3.functional.texture.units.4_units.only_3d.*
 dEQP-GLES3.functional.transform_feedback.random.interleaved.triangles.8
 dEQP-GLES3.functional.vertex_arrays.single_attribute.*
diff --git a/.gitlab-ci/deqp-freedreno-a530-fails.txt b/.gitlab-ci/deqp-freedreno-a530-fails.txt
index 518a1ba4f7d..1c18b061a43 100644
--- a/.gitlab-ci/deqp-freedreno-a530-fails.txt
+++ b/.gitlab-ci/deqp-freedreno-a530-fails.txt
@@ -39,10 +39,7 @@ dEQP-GLES31.functional.image_load_store.cube.load_store.rgba32i_single_layer,Fai
 dEQP-GLES31.functional.image_load_store.cube.load_store.rgba8_snorm_single_layer,Fail
 dEQP-GLES31.functional.image_load_store.early_fragment_tests.early_fragment_tests_stencil_fbo,Crash
 dEQP-GLES31.functional.separate_shader.random.59,Fail
-dEQP-GLES31.functional.separate_shader.random.69,Fail
 dEQP-GLES31.functional.separate_shader.random.79,Fail
-dEQP-GLES31.functional.separate_shader.random.99,Fail
-dEQP-GLES31.functional.separate_shader.random.119,Fail
 dEQP-GLES31.functional.texture.border_clamp.formats.compressed_rgba8_etc2_eac.nearest_size_tile_multiple,Fail
 dEQP-GLES31.functional.texture.texture_buffer.modify.bufferdata.buffer_size_131071,Fail
 dEQP-GLES31.functional.texture.texture_buffer.render.as_index_array_as_fragment_texture.offset_7_alignments,Fail
diff --git a/.gitlab-ci/deqp-freedreno-a530-flakes.txt b/.gitlab-ci/deqp-freedreno-a530-flakes.txt
index 071342b79c0..71715301b52 100644
--- a/.gitlab-ci/deqp-freedreno-a530-flakes.txt
+++ b/.gitlab-ci/deqp-freedreno-a530-flakes.txt
@@ -1,10 +1,19 @@
-# Note: flakes lists for CI are just a list of lines that, when
+# Note: skips lists for CI are just a list of lines that, when
 # non-zero-length and not starting with '#', will regex match to
 # delete lines from the test list.  Be careful.
 
+# Skip the perf/stress tests to keep runtime manageable
+dEQP-GLES[0-9]*.performance.*
+dEQP-GLES[0-9]*.stress.*
+
+# These are really slow on tiling architectures (including llvmpipe).
+dEQP-GLES[0-9]*.functional.flush_finish.*
+
 # unstable results (probably related to the iommu faults).
 dEQP-GLES3.functional.texture.filtering.3d.*
 dEQP-GLES3.functional.texture.vertex.3d.filtering.*
+dEQP-GLES3.functional.fbo.invalidate.sub.unbind_blit_msaa_stencil
+dEQP-GLES3.functional.fbo.invalidate.whole.unbind_blit_msaa_stencil
 dEQP-GLES31.functional.ubo.2_level_struct_array.single_buffer.packed_instance_array_fragment
 
 # These are in the xfails list (they usually do), but the random
diff --git a/.gitlab-ci/deqp-freedreno-a630-bypass-fails.txt b/.gitlab-ci/deqp-freedreno-a630-bypass-fails.txt
index 76965a3d99c..b62278475c1 100644
--- a/.gitlab-ci/deqp-freedreno-a630-bypass-fails.txt
+++ b/.gitlab-ci/deqp-freedreno-a630-bypass-fails.txt
@@ -11,3 +11,4 @@ dEQP-GLES31.functional.blend_equation_advanced.srgb.colordodge,Fail
 dEQP-GLES31.functional.blend_equation_advanced.srgb.exclusion,Fail
 dEQP-GLES31.functional.blend_equation_advanced.srgb.multiply,Fail
 dEQP-GLES31.functional.draw_buffers_indexed.overwrite_common.common_blend_eq_buffer_advanced_blend_eq,Fail
+dEQP-VK.renderpass2.depth_stencil_resolve.image_2d_16_64_6.samples_2.d24_unorm_s8_uint.depth_zero,Fail
diff --git a/.gitlab-ci/deqp-freedreno-a630-fails.txt b/.gitlab-ci/deqp-freedreno-a630-fails.txt
index fa763fc36df..3ead6bc1220 100644
--- a/.gitlab-ci/deqp-freedreno-a630-fails.txt
+++ b/.gitlab-ci/deqp-freedreno-a630-fails.txt
@@ -25,12 +25,19 @@ dEQP-VK.image.subresource_layout.2d_array.all_levels.r8_snorm,Fail
 dEQP-VK.image.subresource_layout.3d.2_levels.r16g16_snorm,Fail
 dEQP-VK.image.subresource_layout.3d.2_levels.r8g8b8a8_snorm,Fail
 dEQP-VK.image.subresource_layout.3d.4_levels.r8g8_snorm,Fail
+dEQP-VK.memory_model.message_passing.core11.u32.coherent.fence_fence.atomicwrite.device.payload_local.image.guard_local.image.frag,Crash
+dEQP-VK.memory_model.message_passing.core11.u32.coherent.fence_fence.atomicwrite.workgroup.payload_local.buffer.guard_local.image.comp,Crash
+dEQP-VK.memory_model.write_after_read.core11.u32.coherent.fence_fence.atomicwrite.device.payload_local.buffer.guard_local.image.comp,Crash
+dEQP-VK.memory_model.write_after_read.core11.u32.coherent.fence_fence.atomicwrite.workgroup.payload_local.image.guard_local.image.comp,Crash
+dEQP-VK.memory_model.write_after_read.core11.u32.coherent.fence_fence.atomicwrite.workgroup.payload_nonlocal.workgroup.guard_local.image.comp,Crash
 dEQP-VK.multiview.masks.max_multi_view_view_count,Fail
 dEQP-VK.multiview.renderpass2.masks.max_multi_view_view_count,Fail
 dEQP-VK.pipeline.extended_dynamic_state.after_pipelines.depth_compare_greater_equal_greater,Fail
 dEQP-VK.pipeline.extended_dynamic_state.before_draw.depth_compare_always_greater,Fail
 dEQP-VK.pipeline.multisample.alpha_to_coverage_unused_attachment.samples_4.alpha_invisible,Fail
 dEQP-VK.pipeline.push_descriptor.compute.binding3_numcalls2_sampler,Crash
+dEQP-VK.renderpass2.depth_stencil_resolve.image_2d_16_64_6.samples_2.d24_unorm_s8_uint.depth_zero,Fail
+dEQP-VK.renderpass2.depth_stencil_resolve.image_2d_16_64_6.samples_4.x8_d24_unorm_pack32.depth_zero,Fail
 dEQP-VK.renderpass2.suballocation.attachment_allocation.input_output.7,Fail
 dEQP-VK.spirv_assembly.instruction.graphics.opquantize.carry_to_exponent_tesse,Fail
 dEQP-VK.spirv_assembly.instruction.graphics.opquantize.negative_round_up_or_round_down_tesse,Fail
@@ -38,3 +45,15 @@ dEQP-VK.spirv_assembly.instruction.graphics.opquantize.negative_too_small_tesse,
 dEQP-VK.spirv_assembly.instruction.graphics.opquantize.round_to_inf_tesse,Fail
 dEQP-VK.spirv_assembly.instruction.graphics.opquantize.spec_const_carry_to_exponent_tesse,Fail
 dEQP-VK.spirv_assembly.instruction.graphics.opquantize.spec_const_negative_round_up_or_round_down_tesse,Fail
+dEQP-VK.tessellation.invariance.inner_triangle_set.triangles_equal_spacing,Fail
+dEQP-VK.tessellation.invariance.outer_edge_division.triangles_fractional_even_spacing,Fail
+dEQP-VK.tessellation.invariance.outer_edge_index_independence.triangles_equal_spacing_ccw,Fail
+dEQP-VK.tessellation.invariance.outer_edge_index_independence.triangles_fractional_even_spacing_cw,Fail
+dEQP-VK.tessellation.invariance.outer_edge_index_independence.quads_fractional_even_spacing_ccw,Fail
+dEQP-VK.tessellation.invariance.outer_edge_symmetry.isolines_equal_spacing_cw,Fail
+dEQP-VK.tessellation.invariance.outer_edge_symmetry.quads_fractional_odd_spacing_ccw,Fail
+dEQP-VK.tessellation.invariance.outer_edge_symmetry.triangles_fractional_odd_spacing_cw,Fail
+dEQP-VK.tessellation.invariance.outer_triangle_set.quads_fractional_odd_spacing,Fail
+dEQP-VK.tessellation.invariance.primitive_set.isolines_fractional_odd_spacing_ccw,Fail
+dEQP-VK.tessellation.invariance.primitive_set.quads_fractional_odd_spacing_cw,Fail
+dEQP-VK.tessellation.invariance.primitive_set.triangles_fractional_even_spacing_ccw,Fail
diff --git a/.gitlab-ci/deqp-freedreno-a630-flakes.txt b/.gitlab-ci/deqp-freedreno-a630-flakes.txt
index bd88d6b46ec..ffaf1cfd71c 100644
--- a/.gitlab-ci/deqp-freedreno-a630-flakes.txt
+++ b/.gitlab-ci/deqp-freedreno-a630-flakes.txt
@@ -22,6 +22,9 @@ dEQP-GLES31.functional.ssbo.atomic.compswap.lowp_uint
 dEQP-GLES31.functional.ssbo.atomic.compswap.mediump_int
 dEQP-GLES31.functional.ssbo.atomic.compswap.mediump_uint
 
+# Non-sysmem flakes
+dEQP-VK.pipeline.spec_constant.compute.composite.matrix.mat3x2
+
 # Fails NIR_VALIDATE so probably flaky
 dEQP-VK.memory_model.write_after_read.core11.u32.coherent.fence_fence.atomicwrite.workgroup.payload_nonlocal.workgroup.guard_local.buffer.comp
 
@@ -36,3 +39,5 @@ dEQP-GLES31.functional.layout_binding.ssbo.fragment_binding_array
 dEQP-VK.subgroups.quad.framebuffer.subgroupquadswapvertical_ivec2_tess_eval
 dEQP-VK.api.copy_and_blit.core.blit_image.all_formats.color.r8g8b8a8_snorm.r32_sfloat.general_optimal_nearest
 dEQP-GLES31.functional.tessellation.invariance.primitive_set.quads_equal_spacing_cw
+dEQP-GLES31.functional.ssbo.layout.3_level_unsized_array.std140.mat2
+dEQP-GLES31.functional.draw_indirect.compute_interop.large.drawelements_combined_grid_1200x1200_drawcount_8
diff --git a/.gitlab-ci/deqp-freedreno-a630-skips.txt b/.gitlab-ci/deqp-freedreno-a630-skips.txt
index cb45101287c..c222d2c1828 100644
--- a/.gitlab-ci/deqp-freedreno-a630-skips.txt
+++ b/.gitlab-ci/deqp-freedreno-a630-skips.txt
@@ -26,6 +26,3 @@ dEQP-VK.geometry.layered.cube_array.64_64_12.readback
 
 # Crashes likely caused by https://gitlab.khronos.org/Tracker/vk-gl-cts/-/issues/2701
 dEQP-VK.synchronization.cross_instance.*binary_semaphore_fence_fd
-
-# Timeouts, passes otherwise
-dEQP-VK.tessellation.invariance.outer_triangle_set.quads_fractional_odd_spacing
diff --git a/.gitlab-ci/deqp-radv-default-skips.txt b/.gitlab-ci/deqp-radv-default-skips.txt
index 521123c7738..4129192fa68 100644
--- a/.gitlab-ci/deqp-radv-default-skips.txt
+++ b/.gitlab-ci/deqp-radv-default-skips.txt
@@ -4,6 +4,3 @@ dEQP-VK.info.device_extensions
 # Exclude WSI related tests.
 dEQP-VK.image.swapchain_mutable.*
 dEQP-VK.wsi.*
-
-# Exclude this test which timeout most of the time.
-dEQP-VK.memory.pipeline_barrier.transfer_src_transfer_dst.1048576
diff --git a/.gitlab-ci/deqp-radv-fiji-aco-fails.txt b/.gitlab-ci/deqp-radv-fiji-aco-fails.txt
index 99ceff8a585..a5f85203642 100644
--- a/.gitlab-ci/deqp-radv-fiji-aco-fails.txt
+++ b/.gitlab-ci/deqp-radv-fiji-aco-fails.txt
@@ -16,3 +16,112 @@ dEQP-VK.renderpass2.depth_stencil_resolve.image_2d_16_64_6.samples_8.d32_sfloat_
 dEQP-VK.renderpass2.depth_stencil_resolve.image_2d_16_64_6.samples_8.d32_sfloat_s8_uint_separate_layouts.stencil_max,Fail
 dEQP-VK.renderpass2.depth_stencil_resolve.image_2d_16_64_6.samples_8.d32_sfloat_s8_uint_separate_layouts.stencil_min,Fail
 dEQP-VK.renderpass2.depth_stencil_resolve.image_2d_16_64_6.samples_8.d32_sfloat_s8_uint_separate_layouts.stencil_zero,Fail
+
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_12.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_20.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_252.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_260.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_31.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_36.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_39.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_4.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_12.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_20.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_252.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_260.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_31.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_36.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_39.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_4.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_12.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_20.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_252.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_260.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_31.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_36.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_39.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_4.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_12.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_20.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_252.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_260.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_31.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_36.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_39.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_4.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_12.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_20.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_252.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_260.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_31.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_36.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_39.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_4.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_12.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_20.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_252.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_260.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_31.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_36.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_39.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_4.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_12.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_20.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_252.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_260.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_31.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_36.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_39.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_4.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_41.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_8.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_12.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_20.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_252.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_260.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_31.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_36.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_39.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_4.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_41.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_8.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_12.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_20.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_252.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_260.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_31.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_36.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_39.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_4.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_41.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_8.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_12.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_20.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_252.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_260.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_31.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_36.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_39.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_4.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_41.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_8.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_12.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_20.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_252.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_260.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_31.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_36.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_39.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_4.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_41.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_8.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_12.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_20.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_252.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_260.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_31.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_36.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_39.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_4.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_41.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_8.samples_1.1d.vert,Fail
diff --git a/.gitlab-ci/deqp-radv-polaris10-aco-fails.txt b/.gitlab-ci/deqp-radv-polaris10-aco-fails.txt
index e69de29bb2d..6ef5fef9fc7 100644
--- a/.gitlab-ci/deqp-radv-polaris10-aco-fails.txt
+++ b/.gitlab-ci/deqp-radv-polaris10-aco-fails.txt
@@ -0,0 +1,108 @@
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_12.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_20.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_252.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_260.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_31.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_36.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_39.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_4.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_12.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_20.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_252.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_260.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_31.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_36.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_39.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_4.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_12.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_20.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_252.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_260.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_31.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_36.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_39.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_4.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_12.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_20.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_252.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_260.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_31.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_36.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_39.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_4.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_12.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_20.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_252.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_260.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_31.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_36.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_39.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_4.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_12.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_20.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_252.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_260.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_31.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_36.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_39.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rg32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_4.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_12.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_20.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_252.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_260.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_31.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_36.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_39.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_4.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_41.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_8.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_12.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_20.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_252.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_260.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_31.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_36.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_39.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_4.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_41.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32f.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_8.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_12.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_20.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_252.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_260.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_31.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_36.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_39.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_4.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_41.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_8.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_12.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_20.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_252.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_260.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_31.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_36.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_39.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_4.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_41.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32i.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_8.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_12.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_20.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_252.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_260.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_31.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_36.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_39.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_4.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_41.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.dontunroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_8.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_12.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_20.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_252.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_260.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_31.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_36.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_39.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_4.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_41.samples_1.1d.vert,Fail
+dEQP-VK.robustness.robustness2.bind.notemplate.rgba32ui.unroll.nonvolatile.vertex_attribute_fetch.no_fmt_qual.len_8.samples_1.1d.vert,Fail
diff --git a/.gitlab-ci/deqp-radv-raven-aco-skips.txt b/.gitlab-ci/deqp-radv-raven-aco-skips.txt
index 77f7620934f..e49406dfa88 100644
--- a/.gitlab-ci/deqp-radv-raven-aco-skips.txt
+++ b/.gitlab-ci/deqp-radv-raven-aco-skips.txt
@@ -8,6 +8,3 @@ dEQP-VK.wsi.*
 # This subset of CTS seems to randomly hangs on RAVEN only.
 # This needs to be investigated and fixed!
 dEQP-VK.synchronization.*
-
-# Exclude this test which timeout most of the time.
-dEQP-VK.memory.pipeline_barrier.transfer_src_transfer_dst.1048576
diff --git a/.gitlab-ci/deqp-radv-vega10-aco-fails.txt b/.gitlab-ci/deqp-radv-vega10-aco-fails.txt
index e69de29bb2d..b37496a86b2 100644
--- a/.gitlab-ci/deqp-radv-vega10-aco-fails.txt
+++ b/.gitlab-ci/deqp-radv-vega10-aco-fails.txt
@@ -0,0 +1,804 @@
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.r16.503_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.r16.512_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.r16_snorm.503_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.r16_snorm.512_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.r16i.503_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.r16i.512_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.r16ui.503_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.r16ui.512_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.r32i.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.r32i.503_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.r32i.512_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.r32ui.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.r32ui.503_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.r32ui.512_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.r8.503_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.r8.512_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.r8_snorm.503_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.r8_snorm.512_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.r8i.503_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.r8i.512_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.r8ui.503_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.r8ui.512_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rg16.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rg16.503_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rg16.512_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rg16_snorm.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rg16_snorm.503_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rg16_snorm.512_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rg16i.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rg16i.503_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rg16i.512_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rg16ui.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rg16ui.503_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rg16ui.512_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rg32i.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rg32i.503_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rg32i.512_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rg32ui.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rg32ui.503_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rg32ui.512_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rg8.503_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rg8.512_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rg8_snorm.503_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rg8_snorm.512_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rg8i.503_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rg8i.512_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rg8ui.503_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rg8ui.512_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rgba16.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rgba16.503_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rgba16.512_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rgba16_snorm.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rgba16_snorm.503_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rgba16_snorm.512_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rgba16i.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rgba16i.503_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rgba16i.512_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rgba16ui.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rgba16ui.503_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rgba16ui.512_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rgba32i.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rgba32i.503_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rgba32i.512_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rgba32ui.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rgba32ui.503_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rgba32ui.512_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rgba8.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rgba8.503_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rgba8.512_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rgba8_snorm.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rgba8_snorm.503_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rgba8_snorm.512_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rgba8i.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rgba8i.503_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rgba8i.512_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rgba8ui.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rgba8ui.503_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d.rgba8ui.512_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.r16.503_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.r16.512_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.r16_snorm.503_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.r16_snorm.512_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.r16i.503_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.r16i.512_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.r16ui.503_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.r16ui.512_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.r32i.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.r32i.503_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.r32i.512_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.r32ui.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.r32ui.503_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.r32ui.512_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.r8.503_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.r8.512_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.r8_snorm.503_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.r8_snorm.512_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.r8i.503_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.r8i.512_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.r8ui.503_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.r8ui.512_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rg16.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rg16.503_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rg16.512_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rg16_snorm.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rg16_snorm.503_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rg16_snorm.512_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rg16i.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rg16i.503_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rg16i.512_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rg16ui.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rg16ui.503_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rg16ui.512_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rg32i.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rg32i.503_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rg32i.512_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rg32ui.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rg32ui.503_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rg32ui.512_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rg8.503_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rg8.512_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rg8_snorm.503_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rg8_snorm.512_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rg8i.503_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rg8i.512_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rg8ui.503_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rg8ui.512_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rgba16.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rgba16.503_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rgba16.512_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rgba16_snorm.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rgba16_snorm.503_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rgba16_snorm.512_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rgba16i.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rgba16i.503_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rgba16i.512_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rgba16ui.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rgba16ui.503_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rgba16ui.512_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rgba32i.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rgba32i.503_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rgba32i.512_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rgba32ui.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rgba32ui.503_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rgba32ui.512_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rgba8.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rgba8.503_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rgba8.512_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rgba8_snorm.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rgba8_snorm.503_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rgba8_snorm.512_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rgba8i.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rgba8i.503_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rgba8i.512_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rgba8ui.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rgba8ui.503_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.2d_array.rgba8ui.512_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.r16.137_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.r16.256_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.r16_snorm.137_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.r16_snorm.256_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.r16i.137_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.r16i.256_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.r16ui.137_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.r16ui.256_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.r32i.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.r32i.137_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.r32i.256_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.r32ui.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.r32ui.137_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.r32ui.256_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.r8.137_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.r8.256_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.r8_snorm.137_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.r8_snorm.256_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.r8i.137_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.r8i.256_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.r8ui.137_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.r8ui.256_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rg16.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rg16.137_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rg16.256_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rg16_snorm.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rg16_snorm.137_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rg16_snorm.256_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rg16i.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rg16i.137_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rg16i.256_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rg16ui.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rg16ui.137_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rg16ui.256_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rg32i.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rg32i.137_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rg32i.256_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rg32ui.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rg32ui.137_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rg32ui.256_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rg8.137_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rg8.256_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rg8_snorm.137_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rg8_snorm.256_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rg8i.137_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rg8i.256_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rg8ui.137_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rg8ui.256_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rgba16.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rgba16.137_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rgba16.256_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rgba16_snorm.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rgba16_snorm.137_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rgba16_snorm.256_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rgba16i.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rgba16i.137_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rgba16i.256_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rgba16ui.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rgba16ui.137_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rgba16ui.256_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rgba32i.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rgba32i.137_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rgba32i.256_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rgba32ui.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rgba32ui.137_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rgba32ui.256_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rgba8.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rgba8.137_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rgba8.256_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rgba8_snorm.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rgba8_snorm.137_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rgba8_snorm.256_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rgba8i.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rgba8i.137_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rgba8i.256_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rgba8ui.128_128_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rgba8ui.137_137_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube.rgba8ui.256_256_1,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.r16.137_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.r16.256_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.r16_snorm.137_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.r16_snorm.256_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.r16i.137_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.r16i.256_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.r16ui.137_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.r16ui.256_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.r32i.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.r32i.137_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.r32i.256_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.r32ui.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.r32ui.137_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.r32ui.256_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.r8.137_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.r8.256_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.r8_snorm.137_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.r8_snorm.256_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.r8i.137_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.r8i.256_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.r8ui.137_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.r8ui.256_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rg16.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rg16.137_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rg16.256_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rg16_snorm.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rg16_snorm.137_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rg16_snorm.256_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rg16i.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rg16i.137_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rg16i.256_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rg16ui.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rg16ui.137_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rg16ui.256_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rg32i.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rg32i.137_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rg32i.256_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rg32ui.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rg32ui.137_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rg32ui.256_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rg8.137_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rg8.256_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rg8_snorm.137_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rg8_snorm.256_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rg8i.137_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rg8i.256_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rg8ui.137_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rg8ui.256_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rgba16.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rgba16.137_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rgba16.256_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rgba16_snorm.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rgba16_snorm.137_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rgba16_snorm.256_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rgba16i.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rgba16i.137_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rgba16i.256_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rgba16ui.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rgba16ui.137_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rgba16ui.256_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rgba32i.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rgba32i.137_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rgba32i.256_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rgba32ui.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rgba32ui.137_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rgba32ui.256_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rgba8.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rgba8.137_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rgba8.256_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rgba8_snorm.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rgba8_snorm.137_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rgba8_snorm.256_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rgba8i.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rgba8i.137_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rgba8i.256_256_6,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rgba8ui.128_128_8,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rgba8ui.137_137_3,Fail
+dEQP-VK.sparse_resources.image_sparse_memory_aliasing.cube_array.rgba8ui.256_256_6,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.r16.1024_128_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.r16_snorm.1024_128_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.r16i.1024_128_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.r16ui.1024_128_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.r32i.1024_128_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.r32i.512_256_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.r32ui.1024_128_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.r32ui.512_256_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.r8.1024_128_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.r8_snorm.1024_128_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.r8i.1024_128_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.r8ui.1024_128_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rg16.1024_128_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rg16.512_256_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rg16_snorm.1024_128_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rg16_snorm.512_256_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rg16i.1024_128_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rg16i.512_256_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rg16ui.1024_128_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rg16ui.512_256_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rg32i.1024_128_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rg32i.512_256_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rg32ui.1024_128_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rg32ui.512_256_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rg8.1024_128_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rg8_snorm.1024_128_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rg8i.1024_128_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rg8ui.1024_128_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rgba16.1024_128_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rgba16.512_256_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rgba16_snorm.1024_128_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rgba16_snorm.512_256_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rgba16i.1024_128_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rgba16i.512_256_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rgba16ui.1024_128_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rgba16ui.512_256_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rgba32i.1024_128_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rgba32i.512_256_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rgba32ui.1024_128_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rgba32ui.512_256_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rgba8.1024_128_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rgba8.512_256_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rgba8_snorm.1024_128_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rgba8_snorm.512_256_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rgba8i.1024_128_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rgba8i.512_256_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rgba8ui.1024_128_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d.rgba8ui.512_256_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.r16.1024_128_8,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.r16_snorm.1024_128_8,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.r16i.1024_128_8,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.r16ui.1024_128_8,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.r32i.1024_128_8,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.r32i.512_256_6,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.r32ui.1024_128_8,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.r32ui.512_256_6,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.r8.1024_128_8,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.r8_snorm.1024_128_8,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.r8i.1024_128_8,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.r8ui.1024_128_8,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rg16.1024_128_8,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rg16.512_256_6,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rg16_snorm.1024_128_8,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rg16_snorm.512_256_6,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rg16i.1024_128_8,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rg16i.512_256_6,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rg16ui.1024_128_8,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rg16ui.512_256_6,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rg32i.1024_128_8,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rg32i.512_256_6,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rg32ui.1024_128_8,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rg32ui.512_256_6,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rg8.1024_128_8,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rg8_snorm.1024_128_8,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rg8i.1024_128_8,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rg8ui.1024_128_8,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rgba16.1024_128_8,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rgba16.512_256_6,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rgba16_snorm.1024_128_8,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rgba16_snorm.512_256_6,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rgba16i.1024_128_8,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rgba16i.512_256_6,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rgba16ui.1024_128_8,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rgba16ui.512_256_6,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rgba32i.1024_128_8,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rgba32i.512_256_6,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rgba32ui.1024_128_8,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rgba32ui.512_256_6,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rgba8.1024_128_8,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rgba8.512_256_6,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rgba8_snorm.1024_128_8,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rgba8_snorm.512_256_6,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rgba8i.1024_128_8,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rgba8i.512_256_6,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rgba8ui.1024_128_8,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.2d_array.rgba8ui.512_256_6,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.cube.rgba32i.256_256_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.cube.rgba32ui.256_256_1,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.cube_array.rgba32i.256_256_6,Fail
+dEQP-VK.sparse_resources.mipmap_sparse_residency.cube_array.rgba32ui.256_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.r32i.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.r32i.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.r32ui.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.r32ui.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.rg16.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.rg16.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.rg16_snorm.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.rg16_snorm.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.rg16i.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.rg16i.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.rg16ui.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.rg16ui.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.rg32i.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.rg32i.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.rg32ui.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.rg32ui.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.rgba16.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.rgba16.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.rgba16_snorm.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.rgba16_snorm.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.rgba16i.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.rgba16i.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.rgba16ui.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.rgba16ui.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.rgba32i.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.rgba32i.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.rgba32ui.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.rgba32ui.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.rgba8.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.rgba8.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.rgba8_snorm.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.rgba8_snorm.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.rgba8i.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.rgba8i.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.rgba8ui.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_fetch.rgba8ui.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.r32i.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.r32i.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.r32ui.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.r32ui.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.rg16.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.rg16.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.rg16_snorm.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.rg16_snorm.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.rg16i.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.rg16i.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.rg16ui.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.rg16ui.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.rg32i.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.rg32i.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.rg32ui.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.rg32ui.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.rgba16.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.rgba16.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.rgba16_snorm.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.rgba16_snorm.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.rgba16i.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.rgba16i.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.rgba16ui.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.rgba16ui.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.rgba32i.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.rgba32i.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.rgba32ui.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.rgba32ui.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.rgba8.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.rgba8.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.rgba8_snorm.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.rgba8_snorm.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.rgba8i.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.rgba8i.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.rgba8ui.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_gather.rgba8ui.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.r32i.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.r32i.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.r32ui.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.r32ui.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.rg16.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.rg16.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.rg16_snorm.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.rg16_snorm.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.rg16i.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.rg16i.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.rg16ui.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.rg16ui.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.rg32i.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.rg32i.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.rg32ui.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.rg32ui.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.rgba16.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.rgba16.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.rgba16_snorm.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.rgba16_snorm.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.rgba16i.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.rgba16i.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.rgba16ui.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.rgba16ui.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.rgba32i.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.rgba32i.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.rgba32ui.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.rgba32ui.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.rgba8.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.rgba8.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.rgba8_snorm.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.rgba8_snorm.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.rgba8i.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.rgba8i.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.rgba8ui.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_read.rgba8ui.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.r32i.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.r32i.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.r32ui.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.r32ui.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.rg16.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.rg16.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.rg16_snorm.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.rg16_snorm.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.rg16i.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.rg16i.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.rg16ui.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.rg16ui.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.rg32i.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.rg32i.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.rg32ui.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.rg32ui.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.rgba16.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.rgba16.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.rgba16_snorm.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.rgba16_snorm.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.rgba16i.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.rgba16i.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.rgba16ui.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.rgba16ui.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.rgba32i.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.rgba32i.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.rgba32ui.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.rgba32ui.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.rgba8.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.rgba8.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.rgba8_snorm.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.rgba8_snorm.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.rgba8i.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.rgba8i.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.rgba8ui.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_explicit_lod.rgba8ui.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.r32i.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.r32i.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.r32ui.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.r32ui.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.rg16.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.rg16.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.rg16_snorm.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.rg16_snorm.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.rg16i.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.rg16i.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.rg16ui.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.rg16ui.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.rg32i.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.rg32i.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.rg32ui.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.rg32ui.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.rgba16.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.rgba16.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.rgba16_snorm.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.rgba16_snorm.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.rgba16i.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.rgba16i.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.rgba16ui.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.rgba16ui.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.rgba32i.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.rgba32i.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.rgba32ui.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.rgba32ui.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.rgba8.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.rgba8.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.rgba8_snorm.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.rgba8_snorm.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.rgba8i.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.rgba8i.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.rgba8ui.503_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_array_sparse_sample_implicit_lod.rgba8ui.512_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.r32i.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.r32i.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.r32ui.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.r32ui.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.rg16.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.rg16.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.rg16_snorm.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.rg16_snorm.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.rg16i.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.rg16i.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.rg16ui.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.rg16ui.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.rg32i.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.rg32i.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.rg32ui.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.rg32ui.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.rgba16.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.rgba16.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.rgba16_snorm.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.rgba16_snorm.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.rgba16i.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.rgba16i.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.rgba16ui.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.rgba16ui.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.rgba32i.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.rgba32i.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.rgba32ui.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.rgba32ui.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.rgba8.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.rgba8.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.rgba8_snorm.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.rgba8_snorm.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.rgba8i.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.rgba8i.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.rgba8ui.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_fetch.rgba8ui.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.r32i.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.r32i.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.r32ui.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.r32ui.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.rg16.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.rg16.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.rg16_snorm.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.rg16_snorm.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.rg16i.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.rg16i.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.rg16ui.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.rg16ui.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.rg32i.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.rg32i.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.rg32ui.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.rg32ui.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.rgba16.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.rgba16.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.rgba16_snorm.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.rgba16_snorm.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.rgba16i.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.rgba16i.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.rgba16ui.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.rgba16ui.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.rgba32i.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.rgba32i.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.rgba32ui.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.rgba32ui.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.rgba8.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.rgba8.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.rgba8_snorm.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.rgba8_snorm.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.rgba8i.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.rgba8i.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.rgba8ui.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_gather.rgba8ui.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.r32i.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.r32i.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.r32ui.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.r32ui.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.rg16.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.rg16.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.rg16_snorm.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.rg16_snorm.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.rg16i.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.rg16i.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.rg16ui.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.rg16ui.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.rg32i.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.rg32i.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.rg32ui.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.rg32ui.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.rgba16.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.rgba16.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.rgba16_snorm.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.rgba16_snorm.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.rgba16i.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.rgba16i.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.rgba16ui.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.rgba16ui.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.rgba32i.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.rgba32i.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.rgba32ui.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.rgba32ui.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.rgba8.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.rgba8.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.rgba8_snorm.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.rgba8_snorm.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.rgba8i.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.rgba8i.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.rgba8ui.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_read.rgba8ui.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.r32i.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.r32i.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.r32ui.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.r32ui.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.rg16.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.rg16.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.rg16_snorm.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.rg16_snorm.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.rg16i.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.rg16i.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.rg16ui.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.rg16ui.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.rg32i.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.rg32i.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.rg32ui.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.rg32ui.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.rgba16.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.rgba16.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.rgba16_snorm.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.rgba16_snorm.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.rgba16i.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.rgba16i.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.rgba16ui.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.rgba16ui.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.rgba32i.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.rgba32i.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.rgba32ui.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.rgba32ui.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.rgba8.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.rgba8.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.rgba8_snorm.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.rgba8_snorm.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.rgba8i.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.rgba8i.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.rgba8ui.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_explicit_lod.rgba8ui.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.r32i.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.r32i.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.r32ui.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.r32ui.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.rg16.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.rg16.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.rg16_snorm.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.rg16_snorm.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.rg16i.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.rg16i.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.rg16ui.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.rg16ui.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.rg32i.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.rg32i.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.rg32ui.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.rg32ui.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.rgba16.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.rgba16.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.rgba16_snorm.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.rgba16_snorm.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.rgba16i.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.rgba16i.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.rgba16ui.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.rgba16ui.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.rgba32i.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.rgba32i.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.rgba32ui.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.rgba32ui.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.rgba8.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.rgba8.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.rgba8_snorm.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.rgba8_snorm.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.rgba8i.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.rgba8i.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.rgba8ui.503_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.2d_sparse_sample_implicit_lod.rgba8ui.512_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.cube_array_sparse_read.rg32i.137_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.cube_array_sparse_read.rg32i.256_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.cube_array_sparse_read.rg32ui.137_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.cube_array_sparse_read.rg32ui.256_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.cube_array_sparse_read.rgba16.137_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.cube_array_sparse_read.rgba16.256_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.cube_array_sparse_read.rgba16_snorm.137_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.cube_array_sparse_read.rgba16_snorm.256_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.cube_array_sparse_read.rgba16i.137_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.cube_array_sparse_read.rgba16i.256_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.cube_array_sparse_read.rgba16ui.137_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.cube_array_sparse_read.rgba16ui.256_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.cube_array_sparse_read.rgba32i.137_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.cube_array_sparse_read.rgba32i.256_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.cube_array_sparse_read.rgba32ui.137_137_3,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.cube_array_sparse_read.rgba32ui.256_256_6,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.cube_sparse_read.rg32i.137_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.cube_sparse_read.rg32i.256_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.cube_sparse_read.rg32ui.137_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.cube_sparse_read.rg32ui.256_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.cube_sparse_read.rgba16.137_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.cube_sparse_read.rgba16.256_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.cube_sparse_read.rgba16_snorm.137_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.cube_sparse_read.rgba16_snorm.256_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.cube_sparse_read.rgba16i.137_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.cube_sparse_read.rgba16i.256_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.cube_sparse_read.rgba16ui.137_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.cube_sparse_read.rgba16ui.256_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.cube_sparse_read.rgba32i.137_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.cube_sparse_read.rgba32i.256_256_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.cube_sparse_read.rgba32ui.137_137_1,Fail
+dEQP-VK.sparse_resources.shader_intrinsics.cube_sparse_read.rgba32ui.256_256_1,Fail
diff --git a/.gitlab-ci/lava-gitlab-ci.yml b/.gitlab-ci/lava-gitlab-ci.yml
index befd9243ef9..ed11f0a0098 100644
--- a/.gitlab-ci/lava-gitlab-ci.yml
+++ b/.gitlab-ci/lava-gitlab-ci.yml
@@ -1,5 +1,5 @@
 variables:
-  DISTRIBUTION_TAG: "2020-01-13-piglit"
+  DISTRIBUTION_TAG: "2020-12-23-piglit"
 
 .kernel+rootfs:
   stage: container-2
diff --git a/.gitlab-ci/meson-build.bat b/.gitlab-ci/meson-build.bat
new file mode 100644
index 00000000000..5982a4059df
--- /dev/null
+++ b/.gitlab-ci/meson-build.bat
@@ -0,0 +1,13 @@
+call "C:\Program Files (x86)\Microsoft Visual Studio\%VERSION%\Common7\Tools\VsDevCmd.bat" -arch=%ARCH%
+
+del /Q /S _build
+meson _build ^
+        -Dbuild-tests=true ^
+        -Db_vscrt=mtd ^
+        -Dbuildtype=release ^
+        -Dllvm=false ^
+        -Dgallium-drivers=swrast ^
+        -Dosmesa=gallium
+meson configure _build
+ninja -C _build
+ninja -C _build test
diff --git a/.gitlab-ci/meson-build.sh b/.gitlab-ci/meson-build.sh
index ce912a74e6b..c6affea46da 100755
--- a/.gitlab-ci/meson-build.sh
+++ b/.gitlab-ci/meson-build.sh
@@ -58,6 +58,6 @@ meson _build --native-file=native.file \
 cd _build
 meson configure
 ninja
-LC_ALL=C.UTF-8 meson test --num-processes ${FDO_CI_CONCURRENT:-4}
+LC_ALL=C.UTF-8 ninja test
 ninja install
 cd ..
diff --git a/.gitlab-ci/piglit/freedreno-a530-gl.txt b/.gitlab-ci/piglit/freedreno-a530-gl.txt
index 5be19dbe341..63818be152c 100644
--- a/.gitlab-ci/piglit/freedreno-a530-gl.txt
+++ b/.gitlab-ci/piglit/freedreno-a530-gl.txt
@@ -179,6 +179,11 @@ spec/!opengl 1.0/gl-1.0-edgeflag: crash
 spec/!opengl 1.0/gl-1.0-edgeflag-quads: crash
 spec/!opengl 1.0/gl-1.0-no-op-paths: fail
 spec/!opengl 1.0/gl-1.0-ortho-pos: fail
+spec/!opengl 1.0/gl-1.0-rendermode-feedback/gl_2d: crash
+spec/!opengl 1.0/gl-1.0-rendermode-feedback/gl_3d: notrun
+spec/!opengl 1.0/gl-1.0-rendermode-feedback/gl_3d_color: notrun
+spec/!opengl 1.0/gl-1.0-rendermode-feedback/gl_3d_color_texture: notrun
+spec/!opengl 1.0/gl-1.0-rendermode-feedback/gl_4d_color_texture: notrun
 spec/!opengl 1.0/gl-1.0-scissor-copypixels: fail
 spec/!opengl 1.0/gl-1.0-scissor-offscreen: fail
 spec/!opengl 1.0/gl-1.0-spot-light: fail
@@ -251,6 +256,11 @@ spec/!opengl 1.1/draw-pixels samples=4: skip
 spec/!opengl 1.1/draw-pixels samples=6: skip
 spec/!opengl 1.1/draw-pixels samples=8: skip
 spec/!opengl 1.1/gl-1.1-xor-copypixels: fail
+spec/!opengl 1.1/gl_select - alpha-test enabled: crash
+spec/!opengl 1.1/gl_select - depth-test enabled: crash
+spec/!opengl 1.1/gl_select - no test function: crash
+spec/!opengl 1.1/gl_select - scissor-test enabled: crash
+spec/!opengl 1.1/gl_select - stencil-test enabled: crash
 spec/!opengl 1.1/linestipple/factor 2x: fail
 spec/!opengl 1.1/linestipple/factor 3x: fail
 spec/!opengl 1.1/linestipple/line loop: fail
@@ -506,7 +516,6 @@ spec/!opengl 2.0/vertex-program-two-side/vs, tcs, tes and fs: skip
 spec/!opengl 2.1/pbo/test_polygon_stip: fail
 spec/!opengl 2.1/polygon-stipple-fs: fail
 spec/!opengl 3.0/clearbuffer-depth: fail
-spec/!opengl 3.0/clearbuffer-depth-cs-probe: skip
 spec/!opengl 3.0/clearbuffer-stencil: fail
 spec/!opengl 3.0/viewport-clamp: crash
 spec/!opengl 3.1/minmax: fail
@@ -1898,6 +1907,7 @@ spec/glsl-1.20/execution/tex-miplevel-selection gl2:textureproj(bias) 2d: fail
 spec/glsl-1.20/execution/tex-miplevel-selection gl2:textureproj(bias) 2d_projvec4: fail
 spec/glsl-1.20/execution/tex-miplevel-selection gl2:textureproj(bias) 2dshadow: fail
 spec/glsl-1.20/execution/tex-miplevel-selection gl2:textureproj(bias) 3d: fail
+spec/glsl-1.30/execution/clipping/clip-plane-transformation pos: fail
 spec/glsl-1.30/execution/tex-miplevel-selection texture() 1d: fail
 spec/glsl-1.30/execution/tex-miplevel-selection texture() 1darray: fail
 spec/glsl-1.30/execution/tex-miplevel-selection texture() 1darrayshadow: fail
@@ -2049,6 +2059,7 @@ spec/nv_alpha_to_coverage_dither_control/nv_alpha_to_coverage_dither_control 8 1
 spec/nv_alpha_to_coverage_dither_control/nv_alpha_to_coverage_dither_control 8 1 1: skip
 spec/nv_alpha_to_coverage_dither_control/nv_alpha_to_coverage_dither_control 9: skip
 spec/nv_alpha_to_coverage_dither_control/nv_alpha_to_coverage_dither_control-error: skip
+spec/nv_conditional_render/copypixels: fail
 spec/nv_conditional_render/copyteximage: fail
 spec/nv_conditional_render/copytexsubimage: fail
 spec/nv_copy_depth_to_color/nv_copy_depth_to_color: crash
@@ -2169,10 +2180,10 @@ wgl/wgl-sanity: skip
 summary:
        name:  results
        ----  --------
-       pass:    16395
-       fail:      491
-      crash:       28
-       skip:     1606
+       pass:    16347
+       fail:      498
+      crash:       34
+       skip:     1605
     timeout:        0
        warn:        0
  incomplete:        0
@@ -2181,4 +2192,4 @@ summary:
     changes:        0
       fixes:        0
 regressions:        0
-      total:    18563
+      total:    18531
diff --git a/.gitlab-ci/piglit/freedreno-a530-shader.txt b/.gitlab-ci/piglit/freedreno-a530-shader.txt
index 46b4c7ab926..f2bd9011a37 100644
--- a/.gitlab-ci/piglit/freedreno-a530-shader.txt
+++ b/.gitlab-ci/piglit/freedreno-a530-shader.txt
@@ -34,17 +34,15 @@ spec/amd_shader_trinary_minmax/execution/built-in-functions/cs-min3-uvec4-uvec4-
 spec/amd_shader_trinary_minmax/execution/built-in-functions/cs-min3-vec2-vec2-vec2: skip
 spec/amd_shader_trinary_minmax/execution/built-in-functions/cs-min3-vec3-vec3-vec3: skip
 spec/amd_shader_trinary_minmax/execution/built-in-functions/cs-min3-vec4-vec4-vec4: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/fs-max3-int-int-int: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/fs-max3-uint-uint-uint: skip
 spec/amd_shader_trinary_minmax/execution/built-in-functions/fs-max3-vec4-vec4-vec4: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/fs-mid3-int-int-int: skip
 spec/amd_shader_trinary_minmax/execution/built-in-functions/fs-mid3-ivec2-ivec2-ivec2: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/fs-mid3-ivec4-ivec4-ivec4: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/fs-mid3-ivec3-ivec3-ivec3: skip
 spec/amd_shader_trinary_minmax/execution/built-in-functions/fs-mid3-uvec2-uvec2-uvec2: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/fs-mid3-uvec4-uvec4-uvec4: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/fs-mid3-vec4-vec4-vec4: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/fs-min3-float-float-float: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/fs-min3-ivec4-ivec4-ivec4: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/fs-mid3-vec2-vec2-vec2: skip
 spec/amd_shader_trinary_minmax/execution/built-in-functions/fs-min3-uvec2-uvec2-uvec2: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/fs-min3-vec3-vec3-vec3: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/fs-min3-vec2-vec2-vec2: skip
 spec/amd_shader_trinary_minmax/execution/built-in-functions/gs-max3-float-float-float: skip
 spec/amd_shader_trinary_minmax/execution/built-in-functions/gs-max3-int-int-int: skip
 spec/amd_shader_trinary_minmax/execution/built-in-functions/gs-max3-ivec2-ivec2-ivec2: skip
@@ -117,28 +115,33 @@ spec/amd_shader_trinary_minmax/execution/built-in-functions/tcs-min3-uvec4-uvec4
 spec/amd_shader_trinary_minmax/execution/built-in-functions/tcs-min3-vec2-vec2-vec2: skip
 spec/amd_shader_trinary_minmax/execution/built-in-functions/tcs-min3-vec3-vec3-vec3: skip
 spec/amd_shader_trinary_minmax/execution/built-in-functions/tcs-min3-vec4-vec4-vec4: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/vs-max3-float-float-float: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/vs-max3-ivec4-ivec4-ivec4: skip
 spec/amd_shader_trinary_minmax/execution/built-in-functions/vs-max3-uvec2-uvec2-uvec2: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/vs-max3-uvec4-uvec4-uvec4: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/vs-max3-vec4-vec4-vec4: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/vs-mid3-float-float-float: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/vs-max3-uvec3-uvec3-uvec3: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/vs-mid3-ivec2-ivec2-ivec2: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/vs-mid3-uint-uint-uint: skip
 spec/amd_shader_trinary_minmax/execution/built-in-functions/vs-mid3-uvec2-uvec2-uvec2: skip
 spec/amd_shader_trinary_minmax/execution/built-in-functions/vs-mid3-uvec4-uvec4-uvec4: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/vs-mid3-vec2-vec2-vec2: skip
 spec/amd_shader_trinary_minmax/execution/built-in-functions/vs-mid3-vec3-vec3-vec3: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/vs-min3-int-int-int: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/vs-min3-ivec2-ivec2-ivec2: skip
 spec/arb_arrays_of_arrays/execution/image_store/basic-imagestore-const-uniform-index: skip
 spec/arb_arrays_of_arrays/execution/image_store/basic-imagestore-mixed-const-non-const-uniform-index: skip
 spec/arb_arrays_of_arrays/execution/image_store/basic-imagestore-mixed-const-non-const-uniform-index2: skip
 spec/arb_arrays_of_arrays/execution/image_store/basic-imagestore-non-const-uniform-index: skip
+spec/arb_arrays_of_arrays/execution/sampler/fs-const-index: skip
 spec/arb_arrays_of_arrays/execution/sampler/fs-initializer-non-const-index: skip
 spec/arb_arrays_of_arrays/execution/sampler/fs-mixed-const-and-non-const-index: skip
 spec/arb_arrays_of_arrays/execution/sampler/fs-mixed-const-and-non-const-index2: skip
 spec/arb_arrays_of_arrays/execution/sampler/fs-nested-struct-arrays-nonconst-nested-array: skip
 spec/arb_arrays_of_arrays/execution/sampler/fs-non-const-index: skip
 spec/arb_arrays_of_arrays/execution/sampler/fs-struct-const-index: skip
+spec/arb_arrays_of_arrays/execution/sampler/fs-struct-const-index-sampler-const-index: skip
 spec/arb_arrays_of_arrays/execution/sampler/fs-struct-non-const-index: skip
 spec/arb_arrays_of_arrays/execution/sampler/fs-struct-non-const-index-const-index: skip
 spec/arb_arrays_of_arrays/execution/sampler/fs-struct-non-const-index-sampler-non-const-index: skip
-spec/arb_arrays_of_arrays/execution/sampler/vs-const-index-three-dimensions: skip
+spec/arb_arrays_of_arrays/execution/sampler/vs-const-index: skip
 spec/arb_arrays_of_arrays/execution/sampler/vs-non-const-index: skip
 spec/arb_arrays_of_arrays/execution/sampler/vs-struct-nonconst: skip
 spec/arb_arrays_of_arrays/execution/sampler/vs-struct-nonconst-non-opaque-members: skip
@@ -155,9 +158,6 @@ spec/arb_arrays_of_arrays/linker/intrastage-interface-field: skip
 spec/arb_arrays_of_arrays/linker/uniform-block-array-instance-name-mismatch: skip
 spec/arb_arrays_of_arrays/linker/uniform-block-array-size-and-instance-name-mismatch: skip
 spec/arb_arrays_of_arrays/linker/vs-to-fs-atomic-counter: skip
-spec/arb_arrays_of_arrays/linker/vs-to-fs-atomic-counter-mismatch: skip
-spec/arb_arrays_of_arrays/linker/vs-to-fs-dimension-size-mismatch: skip
-spec/arb_arrays_of_arrays/linker/vs-to-fs-dimensions-mismatch: skip
 spec/arb_arrays_of_arrays/linker/vs-to-fs-interface: skip
 spec/arb_arrays_of_arrays/linker/vs-to-fs-interface-field: skip
 spec/arb_arrays_of_arrays/linker/vs-to-fs-interface-field-mismatch: skip
@@ -166,6 +166,7 @@ spec/arb_arrays_of_arrays/linker/vs-to-fs-interface-field-unsized: skip
 spec/arb_arrays_of_arrays/linker/vs-to-fs-interface-field-unsized-mismatch: skip
 spec/arb_arrays_of_arrays/linker/vs-to-fs-interface-field-unsized-mismatch2: skip
 spec/arb_arrays_of_arrays/linker/vs-to-fs-interface-field-unsized2: skip
+spec/arb_arrays_of_arrays/linker/vs-to-fs-unsized-mismatch: skip
 spec/arb_arrays_of_arrays/linker/vs-to-gs: skip
 spec/arb_arrays_of_arrays/linker/vs-to-gs-explicit-location: skip
 spec/arb_arrays_of_arrays/linker/vs-to-gs-invalid-dimensions: skip
@@ -371,7 +372,6 @@ spec/arb_gl_spirv/execution/ssbo/two-ssbo: skip
 spec/arb_gl_spirv/execution/ssbo/two-ssbo-different-layouts: skip
 spec/arb_gl_spirv/execution/ssbo/two-stages: skip
 spec/arb_gl_spirv/execution/ssbo/unsized-array: skip
-spec/arb_gl_spirv/execution/ssbo/unsized-array-length: skip
 spec/arb_gl_spirv/execution/ubo/aoa: skip
 spec/arb_gl_spirv/execution/ubo/aoa-2: skip
 spec/arb_gl_spirv/execution/ubo/array: skip
@@ -940,6 +940,7 @@ spec/glsl-1.10/execution/loops/glsl-fs-unroll-explosion: crash
 spec/glsl-1.10/execution/loops/glsl-vs-unroll-explosion: crash
 spec/glsl-1.10/preprocessor/extension-defined-test: skip
 spec/glsl-1.10/preprocessor/extension-if-1: skip
+spec/glsl-1.10/preprocessor/if-eq: skip
 spec/glsl-1.20/execution/clipping/fixed-clip-enables: fail
 spec/glsl-1.20/execution/clipping/vs-clip-vertex-const-accept: fail
 spec/glsl-1.20/execution/clipping/vs-clip-vertex-different-from-position: fail
@@ -1064,17 +1065,17 @@ spec/glsl-3.30/execution/built-in-functions/vs-intbitstofloat-neg: skip
 spec/glsl-3.30/execution/built-in-functions/vs-intbitstofloat-neg_abs: skip
 spec/glsl-3.30/execution/built-in-functions/vs-uintbitstofloat: skip
 spec/glsl-3.30/execution/glsl-bug-109601: skip
-spec/glsl-es-3.00/execution/built-in-functions/const-packhalf2x16: skip
-spec/glsl-es-3.00/execution/built-in-functions/const-packunorm2x16: skip
-spec/glsl-es-3.00/execution/built-in-functions/const-unpacksnorm2x16: skip
-spec/glsl-es-3.00/execution/built-in-functions/const-unpackunorm2x16: skip
+spec/glsl-es-3.00/execution/built-in-functions/const-packsnorm2x16: skip
+spec/glsl-es-3.00/execution/built-in-functions/fs-packsnorm2x16: skip
 spec/glsl-es-3.00/execution/built-in-functions/fs-unpackhalf2x16: skip
-spec/glsl-es-3.00/execution/built-in-functions/vs-packhalf2x16: skip
+spec/glsl-es-3.00/execution/built-in-functions/fs-unpacksnorm2x16: skip
+spec/glsl-es-3.00/execution/built-in-functions/vs-packsnorm2x16: skip
+spec/glsl-es-3.00/execution/built-in-functions/vs-packunorm2x16: skip
+spec/glsl-es-3.00/execution/sanity: skip
 spec/glsl-es-3.00/execution/varying-struct-copy-local-fs: skip
-spec/glsl-es-3.00/execution/varying-struct-copy-out-vs: skip
-spec/glsl-es-3.00/execution/varying-struct-copy-return-vs: skip
-spec/glsl-es-3.00/linker/glsl-mismatched-uniform-precision-used: skip
-spec/glsl-es-3.10/execution/fs-simple-atomic-counter-inc-dec-read: skip
+spec/glsl-es-3.00/execution/varying-struct-copy-uniform-vs: skip
+spec/glsl-es-3.00/linker/glsl-mismatched-uniform-precision-unused: skip
+spec/glsl-es-3.10/execution/cs-image-atomic-if-else: skip
 spec/glsl-es-3.10/execution/vs-simple-atomic-counter-inc-dec-read: skip
 spec/intel_shader_atomic_float_minmax/execution/shared-atomiccompswap-float: skip
 spec/intel_shader_atomic_float_minmax/execution/shared-atomicexchange-float: skip
@@ -1201,10 +1202,10 @@ spec/oes_viewport_array/viewport-gs-writes-out-of-range: skip
 summary:
        name:  results
        ----  --------
-       pass:     5344
+       pass:     5342
        fail:       71
       crash:       39
-       skip:     1088
+       skip:     1089
     timeout:        0
        warn:        2
  incomplete:        0
@@ -1213,4 +1214,4 @@ summary:
     changes:        0
       fixes:        0
 regressions:        0
-      total:     6544
+      total:     6543
diff --git a/.gitlab-ci/piglit/freedreno-a630-shader.txt b/.gitlab-ci/piglit/freedreno-a630-shader.txt
index 345093e343d..c5f0dd7f431 100644
--- a/.gitlab-ci/piglit/freedreno-a630-shader.txt
+++ b/.gitlab-ci/piglit/freedreno-a630-shader.txt
@@ -34,35 +34,38 @@ spec/amd_shader_trinary_minmax/execution/built-in-functions/cs-min3-uvec4-uvec4-
 spec/amd_shader_trinary_minmax/execution/built-in-functions/cs-min3-vec2-vec2-vec2: skip
 spec/amd_shader_trinary_minmax/execution/built-in-functions/cs-min3-vec3-vec3-vec3: skip
 spec/amd_shader_trinary_minmax/execution/built-in-functions/cs-min3-vec4-vec4-vec4: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/fs-max3-vec4-vec4-vec4: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/fs-mid3-ivec4-ivec4-ivec4: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/fs-mid3-uvec2-uvec2-uvec2: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/fs-mid3-uvec4-uvec4-uvec4: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/fs-mid3-vec4-vec4-vec4: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/fs-min3-ivec4-ivec4-ivec4: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/gs-max3-int-int-int: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/gs-max3-ivec3-ivec3-ivec3: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/gs-mid3-ivec3-ivec3-ivec3: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/gs-mid3-ivec4-ivec4-ivec4: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/fs-max3-uint-uint-uint: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/fs-mid3-ivec2-ivec2-ivec2: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/fs-min3-uvec2-uvec2-uvec2: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/fs-min3-vec2-vec2-vec2: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/gs-max3-ivec2-ivec2-ivec2: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/gs-max3-ivec4-ivec4-ivec4: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/gs-max3-uvec3-uvec3-uvec3: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/gs-mid3-ivec2-ivec2-ivec2: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/gs-min3-vec3-vec3-vec3: skip
 spec/amd_shader_trinary_minmax/execution/built-in-functions/gs-min3-vec4-vec4-vec4: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/tcs-max3-ivec2-ivec2-ivec2: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/tcs-max3-vec4-vec4-vec4: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/tcs-mid3-ivec4-ivec4-ivec4: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/tcs-mid3-uvec4-uvec4-uvec4: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/tcs-min3-float-float-float: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/vs-mid3-float-float-float: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/tcs-max3-uvec4-uvec4-uvec4: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/tcs-mid3-float-float-float: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/tcs-min3-uvec3-uvec3-uvec3: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/tcs-min3-vec2-vec2-vec2: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/vs-max3-ivec4-ivec4-ivec4: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/vs-max3-uvec3-uvec3-uvec3: skip
 spec/amd_shader_trinary_minmax/execution/built-in-functions/vs-mid3-uvec2-uvec2-uvec2: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/vs-mid3-vec3-vec3-vec3: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/vs-min3-int-int-int: skip
 spec/arb_arrays_of_arrays/execution/atomic_counters/vs-indirect-index: skip
 spec/arb_arrays_of_arrays/execution/atomic_counters/vs-simple-inc-dec-read: skip
 spec/arb_arrays_of_arrays/execution/image_store/basic-imagestore-mixed-const-non-const-uniform-index: crash
 spec/arb_arrays_of_arrays/execution/image_store/basic-imagestore-mixed-const-non-const-uniform-index2: crash
 spec/arb_arrays_of_arrays/execution/image_store/basic-imagestore-non-const-uniform-index: crash
+spec/arb_arrays_of_arrays/execution/sampler/fs-const-index: skip
 spec/arb_arrays_of_arrays/execution/sampler/fs-initializer-non-const-index: skip
 spec/arb_arrays_of_arrays/execution/sampler/fs-mixed-const-and-non-const-index: skip
 spec/arb_arrays_of_arrays/execution/sampler/fs-mixed-const-and-non-const-index2: skip
 spec/arb_arrays_of_arrays/execution/sampler/fs-nested-struct-arrays-nonconst-nested-array: skip
 spec/arb_arrays_of_arrays/execution/sampler/fs-non-const-index: skip
-spec/arb_arrays_of_arrays/execution/sampler/fs-struct-const-index-sampler-const-index: crash
+spec/arb_arrays_of_arrays/execution/sampler/fs-struct-const-index: skip
+spec/arb_arrays_of_arrays/execution/sampler/fs-struct-const-index-sampler-const-index: skip
 spec/arb_arrays_of_arrays/execution/sampler/fs-struct-non-const-index: skip
 spec/arb_arrays_of_arrays/execution/sampler/fs-struct-non-const-index-const-index: skip
 spec/arb_arrays_of_arrays/execution/sampler/fs-struct-non-const-index-sampler-non-const-index: skip
@@ -71,6 +74,7 @@ spec/arb_arrays_of_arrays/execution/sampler/vs-struct-nonconst: skip
 spec/arb_arrays_of_arrays/execution/sampler/vs-struct-nonconst-non-opaque-members: skip
 spec/arb_arrays_of_arrays/execution/sampler/vs-struct-nonconst-sampler-const: skip
 spec/arb_arrays_of_arrays/execution/sampler/vs-struct-nonconst-sampler-nonconst: skip
+spec/arb_arrays_of_arrays/execution/ubo/fs-const-explicit-binding: skip
 spec/arb_arrays_of_arrays/execution/ubo/fs-mixed-const-nonconst: skip
 spec/arb_arrays_of_arrays/execution/ubo/fs-nonconst: skip
 spec/arb_arrays_of_arrays/linker/vs-to-fs-atomic-counter: skip
@@ -102,6 +106,7 @@ spec/arb_bindless_texture/linker/global_bindless_sampler_and_bound_image: skip
 spec/arb_bindless_texture/linker/global_bindless_sampler_and_bound_sampler: skip
 spec/arb_bindless_texture/linker/global_bound_sampler_and_bound_image: skip
 spec/arb_compute_shader/execution/border-color: fail
+spec/arb_compute_shader/execution/simple-barrier-atomics: fail
 spec/arb_derivative_control/execution/dfdx-coarse: skip
 spec/arb_derivative_control/execution/dfdx-dfdy: skip
 spec/arb_derivative_control/execution/dfdx-fine: skip
@@ -221,7 +226,6 @@ spec/arb_gl_spirv/execution/ssbo/two-ssbo: skip
 spec/arb_gl_spirv/execution/ssbo/two-ssbo-different-layouts: skip
 spec/arb_gl_spirv/execution/ssbo/two-stages: skip
 spec/arb_gl_spirv/execution/ssbo/unsized-array: skip
-spec/arb_gl_spirv/execution/ssbo/unsized-array-length: skip
 spec/arb_gl_spirv/execution/ubo/aoa: skip
 spec/arb_gl_spirv/execution/ubo/aoa-2: skip
 spec/arb_gl_spirv/execution/ubo/array: skip
@@ -461,7 +465,7 @@ spec/arb_gpu_shader5/linker/stream-different-zero-gs-fs: skip
 spec/arb_gpu_shader5/linker/stream-invalid-prim-output: skip
 spec/arb_gpu_shader5/linker/stream-negative-value: skip
 spec/arb_separate_shader_objects/linker/large-number-of-unused-varyings: skip
-spec/arb_separate_shader_objects/linker/pervertex-position-tcs-out-tes: skip
+spec/arb_separate_shader_objects/linker/pervertex-culldistance-vs-out-gs: skip
 spec/arb_separate_shader_objects/linker/vs-to-fs-explicit-location-mismatch-array: skip
 spec/arb_shader_atomic_counters/execution/vs-simple-inc-dec-read: skip
 spec/arb_shader_atomic_counters/linker/different-bindings-atomic-counter: skip
@@ -486,7 +490,9 @@ spec/arb_shader_group_vote/vs-any-uniform: skip
 spec/arb_shader_group_vote/vs-eq-const: skip
 spec/arb_shader_group_vote/vs-eq-uniform: skip
 spec/arb_shader_image_load_store/execution/gl45-imageatomicexchange-float: skip
+spec/arb_shader_image_load_store/execution/image_checkerboard: skip
 spec/arb_shader_image_load_store/execution/imagestore-array: skip
+spec/arb_shader_image_load_store/execution/write-to-rendered-image: skip
 spec/arb_shader_precision/fs-degrees-float: skip
 spec/arb_shader_precision/fs-degrees-vec2: skip
 spec/arb_shader_precision/fs-degrees-vec3: skip
@@ -805,8 +811,9 @@ spec/arb_shader_precision/vs-sqrt-float: skip
 spec/arb_shader_precision/vs-sqrt-vec2: skip
 spec/arb_shader_precision/vs-sqrt-vec3: skip
 spec/arb_shader_precision/vs-sqrt-vec4: skip
-spec/arb_shader_storage_buffer_object/execution/indirect: skip
+spec/arb_shader_storage_buffer_object/execution/indirect: fail
 spec/arb_shader_storage_buffer_object/execution/ssbo-atomiccompswap-int: skip
+spec/arb_shader_storage_buffer_object/execution/ssbo-atomicmax-int: fail
 spec/arb_shader_storage_buffer_object/linker/instance-matching-shader-storage-blocks-array-size-mismatch: skip
 spec/arb_shader_storage_buffer_object/linker/instance-matching-shader-storage-blocks-binding-qualifier-mismatch: skip
 spec/arb_shader_storage_buffer_object/linker/instance-matching-shader-storage-blocks-member-array-size-mismatch: fail
@@ -817,505 +824,504 @@ spec/arb_shader_storage_buffer_object/linker/instance-matching-shader-storage-bl
 spec/arb_shader_storage_buffer_object/linker/instance-matching-shader-storage-blocks-member-type-mismatch: skip
 spec/arb_shader_storage_buffer_object/linker/instance-matching-shader-storage-blocks-memory-qualifier-mismatch: skip
 spec/arb_shader_storage_buffer_object/linker/instance-matching-shader-storage-blocks-packaging-qualifier-mismatch: skip
-spec/arb_shading_language_420pack/linker/different-bindings-image2d: skip
 spec/arb_shading_language_420pack/linker/different-bindings-shader-storage-blocks: skip
 spec/arb_shading_language_420pack/linker/different-bindings-shader-storage-blocks-instanced: skip
 spec/arb_tessellation_shader/execution/16in-1out: crash
 spec/arb_tessellation_shader/execution/1in-1out: skip
-spec/arb_tessellation_shader/execution/barrier-patch: crash
+spec/arb_tessellation_shader/execution/barrier-patch: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-abs-ivec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-abs-vec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-acosh-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-acosh-vec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-acosh-vec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-all-bvec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-all-bvec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-any-bvec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-any-bvec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-asin-vec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-abs-vec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-acos-vec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-acos-vec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-all-bvec3-using-if: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-any-bvec2-using-if: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-any-bvec3-using-if: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-asin-vec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-asin-vec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-asinh-vec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-asinh-vec4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-atan-float: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-atan-vec2: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-atan-vec2-vec2: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-atan-vec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-atan-vec3-vec3: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-atan-vec4-vec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-atanh-vec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-ceil-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-clamp-int-int-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-ceil-vec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-clamp-ivec2-int-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-clamp-ivec2-ivec2-ivec2: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-clamp-ivec3-ivec3-ivec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-clamp-uint-uint-uint: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-clamp-uvec2-uint-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-clamp-uvec3-uint-uint: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-clamp-uvec2-uvec2-uvec2: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-clamp-uvec4-uint-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-clamp-vec2-float-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-clamp-vec4-vec4-vec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-cos-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-cosh-vec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-clamp-vec2-vec2-vec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-cosh-vec2: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-cross-vec3-vec3: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-degrees-vec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-determinant-mat3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-distance-float-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-dot-vec3-vec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-dot-vec4-vec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-determinant-mat4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-distance-vec2-vec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-distance-vec3-vec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-dot-vec2-vec2: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-equal-bvec3-bvec3: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-equal-bvec4-bvec4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-equal-ivec4-ivec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-equal-uvec3-uvec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-exp-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-exp-vec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-equal-uvec2-uvec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-equal-vec4-vec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-exp-vec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-exp2-vec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-exp2-vec4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-faceforward-vec2-vec2-vec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-fract-vec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-faceforward-vec4-vec4-vec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-floor-vec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-floor-vec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-fract-float: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-fract-vec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-greaterthan-vec4-vec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-greaterthanequal-ivec3-ivec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-greaterthanequal-uvec2-uvec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-greaterthan-uvec3-uvec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-greaterthanequal-ivec4-ivec4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-greaterthanequal-uvec3-uvec3: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-greaterthanequal-uvec4-uvec4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-inverse-mat2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-inverse-mat3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-inversesqrt-vec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-length-vec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-lessthan-ivec3-ivec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-inverse-mat4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-inversesqrt-float: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-inversesqrt-vec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-length-vec4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-lessthan-ivec4-ivec4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-lessthan-vec3-vec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-lessthanequal-ivec3-ivec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-lessthanequal-uvec3-uvec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-lessthanequal-uvec2-uvec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-lessthanequal-uvec4-uvec4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-lessthanequal-vec3-vec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-lessthanequal-vec4-vec4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-log-vec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-log-vec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-log-vec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-log2-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-log2-vec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-matrixcompmult-mat4x2-mat4x2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-matrixcompmult-mat2-mat2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-matrixcompmult-mat2x3-mat2x3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-matrixcompmult-mat2x4-mat2x4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-matrixcompmult-mat3-mat3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-matrixcompmult-mat3x4-mat3x4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-matrixcompmult-mat4x3-mat4x3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-max-ivec3-int: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-max-ivec4-int: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-max-ivec4-ivec4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-max-uint-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-max-uvec2-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-max-uvec3-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-max-uvec4-uvec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-max-vec3-vec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-min-float-float: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-max-vec2-vec2: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-min-ivec2-ivec2: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-min-ivec3-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-min-ivec3-ivec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-min-ivec4-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-min-ivec4-ivec4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-min-uvec2-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-min-vec3-vec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-min-uvec4-uvec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-min-vec3-float: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-mix-float-float-bool: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-mix-vec2-vec2-bvec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-mix-vec3-vec3-bvec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-mix-vec2-vec2-float: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-mix-vec4-vec4-bvec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-mix-vec4-vec4-float: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-mod-float-float: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-mod-vec2-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-mod-vec2-vec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-mod-vec4-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-normalize-vec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-mod-vec4-vec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-normalize-vec4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-not-bvec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-notequal-bvec4-bvec4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-notequal-ivec3-ivec3: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-notequal-ivec4-ivec4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-notequal-uvec2-uvec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-notequal-uvec3-uvec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-float-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-float-vec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-int-ivec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-int-ivec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-int-ivec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-float-mat2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-float-mat2x3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-float-mat2x4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-float-mat3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-float-mat3x2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-float-mat4x2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-float-vec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-int-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-ivec2-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-ivec2-ivec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-mat2-float: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-mat2-mat2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-mat2x3-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-mat2x3-mat2x3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-mat2x4-mat2x4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-mat3-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-mat3-mat3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-mat2x4-float: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-mat3x2-mat3x2: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-mat3x4-float: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-mat3x4-mat3x4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-mat4-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-mat4x2-mat4x2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-uint-uvec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-vec4-vec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-and-bool-bool-using-if: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-mat4x2-float: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-uint-uvec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-uint-uvec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-uvec2-uvec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-add-uvec4-uint: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-add-ivec2-int: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-add-ivec2-ivec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-add-ivec3-ivec3: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-add-mat2x3-float: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-add-mat2x3-mat2x3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-add-mat2x4-float: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-add-mat2x4-mat2x4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-add-mat3x2-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-add-mat3x2-mat3x2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-add-mat3x4-mat3x4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-add-mat4x2-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-add-mat4x3-mat4x3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-add-uvec4-uint: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-add-mat4-float: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-add-mat4x3-float: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-add-uvec2-uint: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-add-uvec2-uvec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-add-uvec4-uvec4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-add-vec2-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-bitand-ivec2-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-add-vec2-vec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-add-vec4-float: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-add-vec4-vec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-bitand-int-int: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-bitand-ivec3-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-bitand-ivec3-ivec3: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-bitand-uvec2-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-bitand-uvec3-uvec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-bitand-uvec4-uint: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-bitand-uvec2-uvec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-bitor-int-int: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-bitor-ivec2-ivec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-bitor-ivec3-int: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-bitor-ivec4-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-bitor-uint-uint: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-bitor-uvec3-uint: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-bitor-uvec4-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-bitxor-ivec2-ivec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-bitxor-ivec3-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-bitxor-int-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-bitxor-ivec3-ivec3: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-bitxor-ivec4-ivec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-bitxor-uvec3-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-div-ivec2-ivec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-div-ivec3-ivec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-bitxor-uvec4-uvec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-div-ivec4-int: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-div-ivec4-ivec4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-div-large-uint-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-div-mat2-mat2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-div-mat2x4-mat2x4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-div-mat3-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-div-mat3x2-mat3x2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-div-mat2x3-mat2x3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-div-mat2x4-float: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-div-mat3x4-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-div-mat4-float: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-div-mat3x4-mat3x4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-div-mat4-mat4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-div-mat4x3-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-div-mat4x3-mat4x3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-div-uvec2-uint: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-div-mat4x2-float: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-div-uvec3-uvec3: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-div-uvec4-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-lshift-int-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-lshift-int-uint: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-div-uvec4-uvec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-div-vec4-vec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-lshift-ivec2-ivec2: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-lshift-ivec2-uvec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-lshift-uint-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-lshift-uvec2-ivec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-lshift-ivec3-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-lshift-ivec4-ivec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-lshift-ivec4-uvec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-lshift-uint-uint: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-lshift-uvec2-uvec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-lshift-uvec3-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-lshift-uvec3-ivec3: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-lshift-uvec3-uint: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-lshift-uvec3-uvec3: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-lshift-uvec4-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-lshift-uvec4-ivec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-lshift-uvec4-uvec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-lshift-uvec4-uint: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-mod-ivec2-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-mod-uvec3-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-mult-int-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-mod-ivec3-ivec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-mod-ivec4-ivec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-mod-uvec2-uint: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-mod-uvec2-uvec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-mod-uvec4-uint: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-mod-uvec4-uvec4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-mult-ivec3-ivec3: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-mult-ivec4-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-mult-mat2-float: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-mult-ivec4-ivec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-mult-mat2x3-mat2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-mult-mat2x4-float: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-mult-mat2x4-mat2: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-mult-mat3x2-float: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-mult-mat3x4-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-mult-mat3x4-mat3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-mult-mat4-float: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-mult-mat4x2-mat4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-mult-uvec2-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-mult-uvec3-uvec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-mult-uvec2-uvec2: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-mult-uvec4-uvec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-mult-vec3-mat3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-mult-vec2-mat2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-mult-vec3-float: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-mult-vec4-float: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-mult-vec4-vec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-rshift-ivec2-int: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-rshift-ivec2-ivec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-rshift-ivec3-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-rshift-ivec3-uvec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-rshift-ivec2-uint: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-rshift-ivec4-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-rshift-ivec4-uvec4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-rshift-uvec2-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-rshift-uvec3-uvec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-rshift-uvec2-uint: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-rshift-uvec4-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-rshift-uvec4-uvec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-sub-mat2-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-sub-mat3-mat3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-rshift-uvec4-ivec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-rshift-uvec4-uint: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-sub-int-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-sub-mat2x4-float: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-sub-mat3x2-mat3x2: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-sub-mat3x4-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-sub-mat3x4-mat3x4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-sub-mat4-float: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-sub-mat4x3-mat4x3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-sub-uvec2-uint: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-sub-uint-uint: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-sub-uvec3-uint: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-sub-uvec3-uvec3: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-sub-uvec4-uint: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-sub-uvec4-uvec4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-sub-vec2-float: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-sub-vec2-vec2: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-sub-vec3-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-abs-neg-int-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-abs-neg-int-ivec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-abs-neg-ivec2-ivec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-abs-not-int-ivec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-abs-not-int-ivec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-abs-not-ivec3-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-abs-not-ivec4-ivec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-int-ivec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-ivec2-ivec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-ivec3-ivec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-ivec4-ivec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-neg-abs-int-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-sub-vec3-vec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-assign-sub-vec4-vec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-abs-neg-int-ivec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-abs-neg-int-ivec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-abs-neg-ivec3-ivec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-abs-neg-ivec4-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-abs-not-ivec3-ivec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-ivec2-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-ivec4-int: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-neg-abs-ivec2-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-neg-abs-ivec3-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-neg-int-ivec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-neg-ivec2-ivec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-neg-abs-ivec2-ivec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-neg-abs-ivec4-ivec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-neg-int-int: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-neg-ivec4-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-neg-uint-uint: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-neg-uint-uvec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-neg-uvec4-uint: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-neg-uvec3-uvec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-not-abs-int-ivec2: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-not-abs-int-ivec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-not-abs-ivec2-ivec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-not-abs-ivec3-int: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-not-abs-ivec4-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-not-int-int: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-not-int-ivec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-not-ivec3-ivec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-not-ivec4-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-not-ivec2-ivec2: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-not-uint-uvec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-not-uvec2-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-not-uvec3-uint: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-not-uvec2-uvec2: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-not-uvec3-uvec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-not-uvec4-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-uint-uvec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-uint-uint: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-uint-uvec3: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-uvec2-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-uvec4-uvec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-abs-neg-int-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-abs-neg-int-ivec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-abs-neg-int-ivec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-abs-neg-ivec4-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-uvec2-uvec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-uvec3-uvec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitand-uvec4-uint: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-abs-neg-int-ivec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-abs-neg-ivec2-int: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-abs-not-int-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-abs-not-int-ivec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-abs-not-int-ivec4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-abs-not-ivec2-int: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-abs-not-ivec2-ivec2: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-abs-not-ivec3-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-int-ivec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-ivec2-ivec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-int-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-int-ivec2: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-ivec3-ivec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-ivec4-int: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-ivec4-ivec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-neg-abs-int-ivec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-neg-abs-ivec2-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-neg-int-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-neg-int-ivec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-neg-abs-int-ivec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-neg-abs-int-ivec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-neg-abs-ivec3-ivec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-neg-int-ivec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-neg-ivec3-ivec3: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-neg-ivec4-int: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-neg-uint-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-neg-uvec3-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-neg-uvec4-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-not-abs-int-ivec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-not-abs-ivec2-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-not-abs-ivec2-ivec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-neg-uvec4-uvec4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-not-abs-ivec3-ivec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-not-ivec2-ivec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-not-ivec4-ivec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-not-uint-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-not-uint-uvec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-not-uint-uvec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-not-abs-ivec4-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-not-ivec4-int: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-not-uint-uvec4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-not-uvec2-uint: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-not-uvec4-uvec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-uint-uvec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-uint-uvec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-uvec2-uint: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-uvec3-uint: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-uvec3-uvec3: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-uvec4-uint: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitor-uvec4-uvec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-abs-neg-int-ivec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-abs-neg-int-int: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-abs-neg-int-ivec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-abs-neg-ivec3-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-abs-neg-ivec4-ivec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-abs-not-int-ivec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-abs-not-ivec2-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-abs-neg-ivec2-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-abs-not-ivec4-ivec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-int-ivec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-int-ivec3: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-ivec2-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-ivec3-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-neg-abs-int-ivec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-neg-abs-ivec2-ivec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-ivec3-ivec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-neg-abs-int-ivec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-neg-abs-ivec2-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-neg-abs-ivec3-int: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-neg-abs-ivec3-ivec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-neg-int-ivec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-neg-int-ivec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-neg-abs-ivec4-int: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-neg-ivec2-int: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-neg-ivec3-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-neg-uint-uvec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-neg-uvec3-uvec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-not-abs-int-ivec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-not-abs-ivec2-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-not-uvec2-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-not-uvec3-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-uint-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-uint-uvec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-uvec2-uvec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-uvec3-uvec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-uvec4-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-complement-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-complement-ivec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-complement-ivec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-complement-uvec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-neg-ivec3-ivec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-neg-ivec4-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-neg-uint-uvec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-neg-uvec4-uint: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-neg-uvec4-uvec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-not-abs-ivec2-ivec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-not-abs-ivec3-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-not-ivec3-ivec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-not-ivec4-ivec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-not-uvec4-uint: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-uint-uvec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-uint-uvec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-bitxor-uvec2-uint: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-complement-uint: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-complement-uvec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-div-float-mat2x4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-div-float-mat3x4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-div-float-mat4x2: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-div-float-mat4x3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-div-float-vec2: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-div-int-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-div-ivec2-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-div-int-ivec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-div-ivec2-ivec2: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-div-ivec3-ivec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-div-ivec4-ivec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-div-mat2x3-float: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-div-ivec4-int: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-div-mat2x3-mat2x3: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-div-mat2x4-mat2x4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-div-mat3x2-mat3x2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-div-mat3-mat3: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-div-mat3x4-mat3x4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-div-mat4x2-float: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-div-mat4x2-mat4x2: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-div-mat4x3-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-div-mat4x3-mat4x3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-div-uvec3-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-div-uvec4-uvec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-div-uint-uvec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-div-uint-uvec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-div-uvec2-uvec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-div-vec2-vec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-div-vec3-float: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-div-vec3-vec3: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-div-vec4-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-eq-bool-bool: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-eq-bvec3-bvec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-eq-int-int-using-if: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-eq-ivec4-ivec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-eq-mat2x4-mat2x4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-div-vec4-vec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-eq-bool-bool-using-if: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-eq-float-float-using-if: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-eq-int-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-eq-mat2-mat2-using-if: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-eq-mat2x4-mat2x4-using-if: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-eq-mat3-mat3: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-eq-mat3-mat3-using-if: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-eq-mat3x2-mat3x2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-eq-mat3x4-mat3x4-using-if: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-eq-mat4-mat4-using-if: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-eq-mat4x2-mat4x2: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-eq-mat4x3-mat4x3: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-eq-uint-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-eq-uvec2-uvec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-eq-uvec2-uvec2-using-if: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-eq-uint-uint-using-if: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-eq-uvec3-uvec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-eq-uvec3-uvec3-using-if: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-eq-vec2-vec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-eq-vec2-vec2-using-if: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-eq-vec3-vec3-using-if: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-eq-vec4-vec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-ge-float-float: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-eq-vec4-vec4-using-if: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-ge-int-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-gt-uint-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-lshift-int-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-lshift-ivec2-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-lshift-ivec2-ivec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-lshift-ivec2-uint: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-gt-int-int-using-if: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-gt-uint-uint-using-if: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-le-float-float: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-le-uint-uint: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-lshift-int-uint: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-lshift-ivec3-ivec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-lshift-ivec3-uvec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-lshift-ivec4-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-lshift-ivec4-uvec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-lshift-uint-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-lshift-uvec3-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-lshift-uvec3-uint: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-lshift-ivec4-uint: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-lshift-uvec2-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-lshift-uvec3-ivec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-lshift-uvec3-uvec3: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-lshift-uvec4-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-lshift-uvec4-ivec4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-lshift-uvec4-uint: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-lshift-uvec4-uvec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-lt-float-float-using-if: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-lt-int-int-using-if: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-lt-uint-uint: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-lt-uint-uint-using-if: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mod-int-ivec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mod-ivec3-ivec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mod-uint-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mod-uint-uvec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mod-int-ivec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mod-ivec2-ivec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mod-ivec4-ivec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mod-uint-uvec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mod-uint-uvec3: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mod-uvec2-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mod-uvec3-uint: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mod-uvec2-uvec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mod-uvec3-uvec3: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-float-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-float-mat2x3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-float-mat2x4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-float-mat3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-float-mat4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-float-vec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-int-ivec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-ivec2-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-ivec2-ivec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-float-mat3x4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-float-vec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-ivec3-int: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-ivec4-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-mat2-mat4x2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-mat2-mat2: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-mat2-vec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-mat2x3-vec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-mat2x4-mat4x2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-mat2x3-float: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-mat2x3-mat2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-mat2x4-float: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-mat2x4-mat2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-mat3-float: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-mat3-mat2x3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-mat3-mat3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-mat3-vec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-mat3x2-mat2x3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-mat3x2-vec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-mat3x4-mat3: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-mat3x4-mat4x3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-mat4-mat4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-mat4x2-float: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-mat3x4-vec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-mat4-float: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-mat4-vec4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-mat4x2-mat2x4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-mat4x2-vec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-mat4x3-vec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-uint-uvec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-mat4x2-mat4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-mat4x3-mat4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-uint-uvec4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-uvec4-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-vec2-mat3x2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-vec3-mat2x3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-vec3-mat4x3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-vec4-mat2x4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-vec4-mat3x4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-vec4-vec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-vec2-vec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-mult-vec4-mat4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-ne-bool-bool-using-if: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-ne-bvec3-bvec3: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-ne-bvec3-bvec3-using-if: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-ne-bvec4-bvec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-ne-float-float-using-if: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-ne-ivec2-ivec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-ne-mat2x3-mat2x3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-ne-float-float: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-ne-mat2x3-mat2x3-using-if: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-ne-mat3-mat3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-ne-mat3-mat3-using-if: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-ne-mat3x2-mat3x2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-ne-mat4x2-mat4x2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-ne-mat3x2-mat3x2-using-if: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-ne-mat3x4-mat3x4-using-if: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-ne-mat4x3-mat4x3-using-if: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-ne-uint-uint: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-ne-uint-uint-using-if: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-ne-uvec2-uvec2-using-if: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-ne-uvec3-uvec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-ne-uvec4-uvec4-using-if: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-ne-vec2-vec2: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-ne-vec2-vec2-using-if: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-neg-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-neg-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-ne-vec4-vec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-neg-mat2: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-neg-mat2x3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-neg-mat4x2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-neg-uvec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-neg-vec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-not-bool-using-if: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-neg-mat3x2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-neg-mat4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-neg-uint: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-neg-uvec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-neg-vec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-not-bool: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-or-bool-bool: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-or-bool-bool-using-if: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-rshift-ivec2-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-rshift-ivec2-ivec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-rshift-ivec2-uint: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-rshift-ivec3-ivec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-rshift-ivec4-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-rshift-ivec3-uvec3: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-rshift-ivec4-ivec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-rshift-ivec4-uvec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-rshift-uint-uint: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-rshift-uvec2-ivec2: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-rshift-uvec2-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-rshift-uvec2-uvec2: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-rshift-uvec3-int: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-rshift-uvec3-ivec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-rshift-uvec3-uvec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-rshift-uvec4-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-rshift-uvec3-uint: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-rshift-uvec4-ivec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-selection-bool-bool-bool-using-if: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-selection-bool-bvec3-bvec3: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-selection-bool-float-float: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-selection-bool-ivec3-ivec3: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-selection-bool-ivec4-ivec4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-selection-bool-mat2-mat2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-selection-bool-mat2x3-mat2x3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-selection-bool-mat2x4-mat2x4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-selection-bool-mat3x2-mat3x2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-selection-bool-mat3x4-mat3x4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-selection-bool-mat4-mat4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-selection-bool-mat4x3-mat4x3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-selection-bool-uvec3-uvec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-selection-bool-vec2-vec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-float-mat3x4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-float-mat4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-selection-bool-uint-uint: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-selection-bool-uvec2-uvec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-selection-bool-uvec4-uvec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-selection-bool-vec3-vec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-float-mat2x3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-float-mat2x4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-float-mat3: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-float-mat4x2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-float-mat4x3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-float-vec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-float-vec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-int-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-int-ivec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-mat2x3-float: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-float-vec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-int-ivec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-ivec2-ivec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-ivec3-int: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-ivec3-ivec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-mat2-float: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-mat2x3-mat2x3: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-mat3-mat3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-mat3x2-mat3x2: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-mat3x4-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-mat3x4-mat3x4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-mat4-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-uint-uvec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-uvec2-uint: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-mat4x2-float: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-mat4x2-mat4x2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-mat4x3-float: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-mat4x3-mat4x3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-uint-uvec2: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-uvec2-uvec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-uvec3-uvec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-vec2-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-sub-vec4-vec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-uplus-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-uplus-int: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-uplus-ivec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-uplus-mat2x4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-uplus-mat3x4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-uplus-uint: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-outerproduct-vec2-vec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-outerproduct-vec2-vec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-outerproduct-vec3-vec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-outerproduct-vec4-vec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-pow-float-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-pow-vec3-vec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-pow-vec4-vec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-uplus-vec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-uplus-vec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-op-xor-bool-bool-using-if: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-radians-float: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-radians-vec2: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-radians-vec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-reflect-vec3-vec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-refract-float-float-float: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-reflect-float-float: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-reflect-vec2-vec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-reflect-vec4-vec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-round-vec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-round-vec3: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-roundeven-vec4: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-sign-ivec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-sign-vec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-sign-vec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-sin-vec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-sign-ivec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-sign-vec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-sin-float: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-sinh-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-smoothstep-float-float-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-smoothstep-float-float-vec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-smoothstep-float-float-vec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-sqrt-vec4: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-step-vec3-vec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-sinh-vec2: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-sinh-vec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-sqrt-vec3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-tan-float: skip
 spec/arb_tessellation_shader/execution/built-in-functions/tcs-tanh-float: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-transpose-mat3x2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-trunc-vec2: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-trunc-vec3: skip
-spec/arb_tessellation_shader/execution/built-in-functions/tcs-trunc-vec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-tanh-vec4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-transpose-mat2x4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-transpose-mat3: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-transpose-mat3x4: skip
+spec/arb_tessellation_shader/execution/built-in-functions/tcs-trunc-float: skip
 spec/arb_tessellation_shader/execution/compatibility/tcs-tes-ff-frag: skip
 spec/arb_tessellation_shader/execution/compatibility/tes-clamp-vertex-color: skip
 spec/arb_tessellation_shader/execution/compatibility/tes-clip-vertex-different-from-position: skip
@@ -1328,83 +1334,82 @@ spec/arb_tessellation_shader/execution/double-vs-tcs-tes: skip
 spec/arb_tessellation_shader/execution/dvec2-vs-tcs-tes: skip
 spec/arb_tessellation_shader/execution/dvec3-vs-tcs-tes: skip
 spec/arb_tessellation_shader/execution/fs-primitiveid-instanced: skip
-spec/arb_tessellation_shader/execution/gs-primitiveid-instanced: skip
+spec/arb_tessellation_shader/execution/gs-primitiveid-instanced: fail
 spec/arb_tessellation_shader/execution/invocation-counting-even: fail
 spec/arb_tessellation_shader/execution/invocation-counting-odd: fail
+spec/arb_tessellation_shader/execution/nop: skip
+spec/arb_tessellation_shader/execution/patch-pair-verts: skip
 spec/arb_tessellation_shader/execution/patch-partial-write: crash
-spec/arb_tessellation_shader/execution/quads: skip
-spec/arb_tessellation_shader/execution/sanity: skip
-spec/arb_tessellation_shader/execution/tcs-input-read-mat: crash
-spec/arb_tessellation_shader/execution/tcs-input-read-nonconst-interface: skip
-spec/arb_tessellation_shader/execution/tcs-output-unmatched: fail
+spec/arb_tessellation_shader/execution/patch-single-vert: skip
+spec/arb_tessellation_shader/execution/sanity2: skip
+spec/arb_tessellation_shader/execution/tcs-input-read-mat: skip
+spec/arb_tessellation_shader/execution/tcs-input-read-nonconst: skip
+spec/arb_tessellation_shader/execution/tcs-input-read-simple: fail
+spec/arb_tessellation_shader/execution/tcs-output-unmatched: skip
 spec/arb_tessellation_shader/execution/tcs-primitiveid: fail
-spec/arb_tessellation_shader/execution/tcs-read-texture: skip
 spec/arb_tessellation_shader/execution/tcs-tes-max-in-out-patch-components: fail
-spec/arb_tessellation_shader/execution/tcs-tes-patch-array: skip
 spec/arb_tessellation_shader/execution/tcs-tes-vertex: skip
 spec/arb_tessellation_shader/execution/tcs-tes-vertex-dlist: skip
+spec/arb_tessellation_shader/execution/tes-fs: skip
 spec/arb_tessellation_shader/execution/tes-no-tcs-primitiveid-instanced: fail
 spec/arb_tessellation_shader/execution/tes-primitiveid: fail
-spec/arb_tessellation_shader/execution/tes-primitiveid-instanced: skip
 spec/arb_tessellation_shader/execution/tes-read-texture: skip
 spec/arb_tessellation_shader/execution/tess-instance-id: skip
 spec/arb_tessellation_shader/execution/tess_with_geometry: fail
-spec/arb_tessellation_shader/execution/trivial: skip
-spec/arb_tessellation_shader/execution/trivial-tess-gs: skip
-spec/arb_tessellation_shader/execution/trivial-tess-gs_no-gs-inputs: skip
-spec/arb_tessellation_shader/execution/trivial-tess-gs_no-tes-inputs: skip
+spec/arb_tessellation_shader/execution/trivial-tess-gs: fail
+spec/arb_tessellation_shader/execution/trivial-tess-gs_no-gs-inputs: fail
+spec/arb_tessellation_shader/execution/trivial-tess-gs_no-tes-inputs: fail
 spec/arb_tessellation_shader/execution/variable-indexing/tcs-input-array-dvec4-index-rd: skip
-spec/arb_tessellation_shader/execution/variable-indexing/tcs-input-array-float-index-rd: crash
+spec/arb_tessellation_shader/execution/variable-indexing/tcs-input-array-float-index-rd: skip
 spec/arb_tessellation_shader/execution/variable-indexing/tcs-input-array-vec2-index-rd: crash
 spec/arb_tessellation_shader/execution/variable-indexing/tcs-input-array-vec3-index-rd: crash
-spec/arb_tessellation_shader/execution/variable-indexing/tcs-input-array-vec4-index-rd: crash
+spec/arb_tessellation_shader/execution/variable-indexing/tcs-input-array-vec4-index-rd: skip
 spec/arb_tessellation_shader/execution/variable-indexing/tcs-output-array-dvec4-index-wr: skip
 spec/arb_tessellation_shader/execution/variable-indexing/tcs-output-array-float-index-rd-after-barrier: crash
-spec/arb_tessellation_shader/execution/variable-indexing/tcs-output-array-float-index-wr: crash
+spec/arb_tessellation_shader/execution/variable-indexing/tcs-output-array-float-index-wr: skip
 spec/arb_tessellation_shader/execution/variable-indexing/tcs-output-array-float-index-wr-before-barrier: crash
-spec/arb_tessellation_shader/execution/variable-indexing/tcs-output-array-vec2-index-rd-after-barrier: skip
+spec/arb_tessellation_shader/execution/variable-indexing/tcs-output-array-vec2-index-rd-after-barrier: crash
 spec/arb_tessellation_shader/execution/variable-indexing/tcs-output-array-vec2-index-wr: crash
 spec/arb_tessellation_shader/execution/variable-indexing/tcs-output-array-vec2-index-wr-before-barrier: crash
 spec/arb_tessellation_shader/execution/variable-indexing/tcs-output-array-vec3-index-rd-after-barrier: crash
-spec/arb_tessellation_shader/execution/variable-indexing/tcs-output-array-vec3-index-wr: skip
+spec/arb_tessellation_shader/execution/variable-indexing/tcs-output-array-vec3-index-wr: crash
 spec/arb_tessellation_shader/execution/variable-indexing/tcs-output-array-vec3-index-wr-before-barrier: crash
-spec/arb_tessellation_shader/execution/variable-indexing/tcs-output-array-vec4-index-rd-after-barrier: skip
+spec/arb_tessellation_shader/execution/variable-indexing/tcs-output-array-vec4-index-rd-after-barrier: crash
 spec/arb_tessellation_shader/execution/variable-indexing/tcs-output-array-vec4-index-wr: crash
 spec/arb_tessellation_shader/execution/variable-indexing/tcs-output-array-vec4-index-wr-before-barrier: crash
 spec/arb_tessellation_shader/execution/variable-indexing/tcs-patch-output-array-float-index-wr: skip
-spec/arb_tessellation_shader/execution/variable-indexing/tcs-patch-output-array-vec2-index-wr: skip
-spec/arb_tessellation_shader/execution/variable-indexing/tcs-patch-output-array-vec3-index-wr: skip
+spec/arb_tessellation_shader/execution/variable-indexing/tcs-patch-output-array-vec2-index-wr: crash
+spec/arb_tessellation_shader/execution/variable-indexing/tcs-patch-output-array-vec3-index-wr: crash
 spec/arb_tessellation_shader/execution/variable-indexing/tcs-patch-output-array-vec4-index-wr: skip
 spec/arb_tessellation_shader/execution/variable-indexing/tcs-patch-vec4-index-wr: crash
-spec/arb_tessellation_shader/execution/variable-indexing/tcs-patch-vec4-swiz-index-wr: crash
+spec/arb_tessellation_shader/execution/variable-indexing/tcs-patch-vec4-swiz-index-wr: skip
 spec/arb_tessellation_shader/execution/variable-indexing/tcs-tes-array-in-struct: skip
 spec/arb_tessellation_shader/execution/variable-indexing/tes-both-input-array-float-index-rd: crash
 spec/arb_tessellation_shader/execution/variable-indexing/tes-both-input-array-vec2-index-rd: crash
 spec/arb_tessellation_shader/execution/variable-indexing/tes-both-input-array-vec3-index-rd: crash
-spec/arb_tessellation_shader/execution/variable-indexing/tes-both-input-array-vec4-index-rd: crash
+spec/arb_tessellation_shader/execution/variable-indexing/tes-both-input-array-vec4-index-rd: skip
 spec/arb_tessellation_shader/execution/variable-indexing/tes-input-array-dvec4-index-rd: skip
 spec/arb_tessellation_shader/execution/variable-indexing/tes-input-array-float-index-rd: skip
-spec/arb_tessellation_shader/execution/variable-indexing/tes-input-array-vec2-index-rd: skip
+spec/arb_tessellation_shader/execution/variable-indexing/tes-input-array-vec2-index-rd: crash
 spec/arb_tessellation_shader/execution/variable-indexing/tes-input-array-vec3-index-rd: skip
 spec/arb_tessellation_shader/execution/variable-indexing/tes-input-array-vec4-index-rd: crash
-spec/arb_tessellation_shader/execution/variable-indexing/tes-patch-input-array-float-index-rd: crash
+spec/arb_tessellation_shader/execution/variable-indexing/tes-patch-input-array-float-index-rd: skip
 spec/arb_tessellation_shader/execution/variable-indexing/tes-patch-input-array-vec2-index-invalid-rd: crash
 spec/arb_tessellation_shader/execution/variable-indexing/tes-patch-input-array-vec2-index-rd: crash
 spec/arb_tessellation_shader/execution/variable-indexing/tes-patch-input-array-vec3-index-rd: crash
-spec/arb_tessellation_shader/execution/variable-indexing/tes-patch-input-array-vec4-index-rd: crash
+spec/arb_tessellation_shader/execution/variable-indexing/tes-patch-input-array-vec4-index-rd: skip
 spec/arb_tessellation_shader/execution/variable-indexing/vs-output-array-dvec4-index-wr-before-tcs: skip
-spec/arb_tessellation_shader/execution/variable-indexing/vs-output-array-float-index-wr-before-tcs: skip
-spec/arb_tessellation_shader/execution/variable-indexing/vs-output-array-vec4-index-wr-before-tcs: skip
+spec/arb_tessellation_shader/execution/variable-indexing/vs-output-array-vec3-index-wr-before-tcs: skip
 spec/arb_tessellation_shader/execution/vertex-partial-write: crash
 spec/arb_tessellation_shader/execution/vs-tcs-max-in-out-components: skip
+spec/arb_tessellation_shader/execution/vs-tcs-tes-tessinner-tessouter-inputs-quads: skip
 spec/arb_tessellation_shader/execution/vs-tcs-tes-tessinner-tessouter-inputs-tris: skip
-spec/arb_tessellation_shader/execution/vs-tcs-tes-vertex: fail
 spec/arb_tessellation_shader/execution/vs-tes-max-in-out-components: fail
-spec/arb_tessellation_shader/execution/vs-tes-tessinner-tessouter-inputs-quads: skip
+spec/arb_tessellation_shader/execution/vs-tes-tessinner-tessouter-inputs-quads: fail
 spec/arb_tessellation_shader/execution/vs-tes-tessinner-tessouter-inputs-quads-dlist: skip
 spec/arb_tessellation_shader/execution/vs-tes-tessinner-tessouter-inputs-tris: fail
 spec/arb_tessellation_shader/execution/vs-tes-vertex: fail
-spec/arb_tessellation_shader/linker/no-vs: skip
+spec/arb_tessellation_shader/linker/tcs-no-vs: skip
 spec/arb_vertex_attrib_64bit/execution/vs-fp64-input-trunc: skip
 spec/arb_vertex_attrib_64bit/execution/vs-fs-pass-vertex-attrib: skip
 spec/arb_vertex_attrib_64bit/execution/vs-test-attrib-location: skip
@@ -1478,6 +1483,7 @@ spec/glsl-1.10/execution/loops/glsl-vs-unroll-explosion: crash
 spec/glsl-1.10/execution/vsfs-unused-array-member: crash
 spec/glsl-1.10/preprocessor/extension-defined-test: skip
 spec/glsl-1.10/preprocessor/extension-if-1: skip
+spec/glsl-1.10/preprocessor/if-eq: skip
 spec/glsl-1.20/execution/clipping/fixed-clip-enables: fail
 spec/glsl-1.20/execution/clipping/vs-clip-vertex-enables: fail
 spec/glsl-1.30/execution/clipping/vs-clip-distance-deadcode: crash
@@ -1675,10 +1681,10 @@ spec/oes_viewport_array/viewport-gs-writes-out-of-range: skip
 summary:
        name:  results
        ----  --------
-       pass:     8353
-       fail:       29
-      crash:       97
-       skip:     1546
+       pass:     8346
+       fail:       36
+      crash:       93
+       skip:     1549
     timeout:        0
        warn:        2
  incomplete:        0
@@ -1687,4 +1693,4 @@ summary:
     changes:        0
       fixes:        0
 regressions:        0
-      total:    10027
+      total:    10026
diff --git a/.gitlab-ci/piglit/llvmpipe-quick_gl.txt b/.gitlab-ci/piglit/llvmpipe-quick_gl.txt
index 973da3a8a10..e15e5b7ac5e 100644
--- a/.gitlab-ci/piglit/llvmpipe-quick_gl.txt
+++ b/.gitlab-ci/piglit/llvmpipe-quick_gl.txt
@@ -387,7 +387,6 @@ spec/!opengl 2.0/vertex-program-two-side/tcs-out, tes and fs: skip
 spec/!opengl 2.0/vertex-program-two-side/tes-out and fs: skip
 spec/!opengl 2.0/vertex-program-two-side/vs, gs and fs: skip
 spec/!opengl 2.0/vertex-program-two-side/vs, tcs, tes and fs: skip
-spec/!opengl 3.0/clearbuffer-depth-cs-probe: fail
 spec/!opengl 3.1/draw-buffers-errors: skip
 spec/!opengl 3.2/gl-3.2-adj-prims pv-first: fail
 spec/!opengl 3.2/layered-rendering/clear-color-mismatched-layer-count: fail
@@ -1662,8 +1661,8 @@ wgl/wgl-sanity: skip
 summary:
        name:  results
        ----  --------
-       pass:    23132
-       fail:      198
+       pass:    23099
+       fail:      197
       crash:        1
        skip:     1438
     timeout:        0
@@ -1674,4 +1673,4 @@ summary:
     changes:        0
       fixes:        0
 regressions:        0
-      total:    24793
+      total:    24759
diff --git a/.gitlab-ci/piglit/llvmpipe-quick_shader.txt b/.gitlab-ci/piglit/llvmpipe-quick_shader.txt
index dd9c7be9b1d..c7ebbb19b5d 100644
--- a/.gitlab-ci/piglit/llvmpipe-quick_shader.txt
+++ b/.gitlab-ci/piglit/llvmpipe-quick_shader.txt
@@ -73,7 +73,6 @@ spec/arb_gl_spirv/execution/ssbo/two-ssbo: skip
 spec/arb_gl_spirv/execution/ssbo/two-ssbo-different-layouts: skip
 spec/arb_gl_spirv/execution/ssbo/two-stages: skip
 spec/arb_gl_spirv/execution/ssbo/unsized-array: skip
-spec/arb_gl_spirv/execution/ssbo/unsized-array-length: skip
 spec/arb_gl_spirv/execution/ubo/aoa: skip
 spec/arb_gl_spirv/execution/ubo/aoa-2: skip
 spec/arb_gl_spirv/execution/ubo/array: skip
@@ -602,7 +601,7 @@ summary:
        pass:    15819
        fail:       83
       crash:      170
-       skip:      345
+       skip:      344
     timeout:        0
        warn:        0
  incomplete:        0
@@ -611,4 +610,4 @@ summary:
     changes:        0
       fixes:        0
 regressions:        0
-      total:    16417
+      total:    16416
diff --git a/.gitlab-ci/piglit/softpipe-quick.txt b/.gitlab-ci/piglit/softpipe-quick.txt
index 6bc0e4fda39..d500fb01306 100644
--- a/.gitlab-ci/piglit/softpipe-quick.txt
+++ b/.gitlab-ci/piglit/softpipe-quick.txt
@@ -373,7 +373,6 @@ spec/!opengl 2.0/vertex-program-two-side front/vs, tcs, tes and fs: skip
 spec/!opengl 2.0/vertex-program-two-side/tcs-out, tes and fs: skip
 spec/!opengl 2.0/vertex-program-two-side/tes-out and fs: skip
 spec/!opengl 2.0/vertex-program-two-side/vs, tcs, tes and fs: skip
-spec/!opengl 3.0/clearbuffer-depth-cs-probe: fail
 spec/!opengl 3.0/gl30basic: fail
 spec/!opengl 3.0/minmax: fail
 spec/!opengl 3.1/minmax: fail
@@ -448,14 +447,16 @@ spec/amd_shader_trinary_minmax/execution/built-in-functions/cs-min3-uvec4-uvec4-
 spec/amd_shader_trinary_minmax/execution/built-in-functions/cs-min3-vec2-vec2-vec2: skip
 spec/amd_shader_trinary_minmax/execution/built-in-functions/cs-min3-vec3-vec3-vec3: skip
 spec/amd_shader_trinary_minmax/execution/built-in-functions/cs-min3-vec4-vec4-vec4: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/fs-max3-int-int-int: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/fs-max3-vec2-vec2-vec2: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/fs-mid3-ivec4-ivec4-ivec4: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/fs-min3-int-int-int: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/fs-max3-uint-uint-uint: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/fs-mid3-ivec2-ivec2-ivec2: skip
 spec/amd_shader_trinary_minmax/execution/built-in-functions/fs-min3-uvec2-uvec2-uvec2: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/gs-min3-int-int-int: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/gs-min3-ivec2-ivec2-ivec2: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/gs-min3-vec2-vec2-vec2: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/fs-min3-vec2-vec2-vec2: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/gs-max3-ivec2-ivec2-ivec2: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/gs-max3-ivec4-ivec4-ivec4: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/gs-max3-uvec3-uvec3-uvec3: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/gs-mid3-ivec2-ivec2-ivec2: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/gs-min3-vec3-vec3-vec3: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/gs-min3-vec4-vec4-vec4: skip
 spec/amd_shader_trinary_minmax/execution/built-in-functions/tcs-max3-float-float-float: skip
 spec/amd_shader_trinary_minmax/execution/built-in-functions/tcs-max3-int-int-int: skip
 spec/amd_shader_trinary_minmax/execution/built-in-functions/tcs-max3-ivec2-ivec2-ivec2: skip
@@ -492,9 +493,11 @@ spec/amd_shader_trinary_minmax/execution/built-in-functions/tcs-min3-uvec4-uvec4
 spec/amd_shader_trinary_minmax/execution/built-in-functions/tcs-min3-vec2-vec2-vec2: skip
 spec/amd_shader_trinary_minmax/execution/built-in-functions/tcs-min3-vec3-vec3-vec3: skip
 spec/amd_shader_trinary_minmax/execution/built-in-functions/tcs-min3-vec4-vec4-vec4: skip
-spec/amd_shader_trinary_minmax/execution/built-in-functions/vs-mid3-int-int-int: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/vs-max3-ivec4-ivec4-ivec4: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/vs-max3-uvec3-uvec3-uvec3: skip
 spec/amd_shader_trinary_minmax/execution/built-in-functions/vs-mid3-uvec2-uvec2-uvec2: skip
 spec/amd_shader_trinary_minmax/execution/built-in-functions/vs-mid3-vec3-vec3-vec3: skip
+spec/amd_shader_trinary_minmax/execution/built-in-functions/vs-min3-int-int-int: skip
 spec/apple_object_purgeable/object_purgeable-api-pbo: skip
 spec/apple_object_purgeable/object_purgeable-api-texture: skip
 spec/apple_object_purgeable/object_purgeable-api-vbo: skip
@@ -612,7 +615,9 @@ spec/arb_enhanced_layouts/arb_enhanced_layouts-transform-feedback-layout-qualifi
 spec/arb_enhanced_layouts/arb_enhanced_layouts-transform-feedback-layout-qualifiers_vs_interface: crash
 spec/arb_enhanced_layouts/arb_enhanced_layouts-transform-feedback-layout-qualifiers_vs_named_interface: crash
 spec/arb_enhanced_layouts/arb_enhanced_layouts-transform-feedback-layout-qualifiers_vs_struct: crash
-spec/arb_enhanced_layouts/execution/component-layout/vs-fs: skip
+spec/arb_enhanced_layouts/execution/component-layout/vs-attribs-array: skip
+spec/arb_enhanced_layouts/execution/component-layout/vs-gs-fs: skip
+spec/arb_enhanced_layouts/execution/component-layout/vs-gs-fs-double: skip
 spec/arb_enhanced_layouts/execution/component-layout/vs-tcs-load-output: skip
 spec/arb_enhanced_layouts/execution/component-layout/vs-tcs-load-output-indirect: skip
 spec/arb_enhanced_layouts/execution/component-layout/vs-tcs-tes-fs: skip
@@ -691,7 +696,6 @@ spec/arb_gl_spirv/execution/ssbo/two-ssbo: skip
 spec/arb_gl_spirv/execution/ssbo/two-ssbo-different-layouts: skip
 spec/arb_gl_spirv/execution/ssbo/two-stages: skip
 spec/arb_gl_spirv/execution/ssbo/unsized-array: skip
-spec/arb_gl_spirv/execution/ssbo/unsized-array-length: skip
 spec/arb_gl_spirv/execution/ubo/aoa: skip
 spec/arb_gl_spirv/execution/ubo/aoa-2: skip
 spec/arb_gl_spirv/execution/ubo/array: skip
@@ -858,11 +862,13 @@ spec/arb_sample_shading/samplemask 6: skip
 spec/arb_sample_shading/samplemask 6 all: skip
 spec/arb_sample_shading/samplemask 8: skip
 spec/arb_sample_shading/samplemask 8 all: skip
+spec/arb_separate_shader_objects/linker/large-number-of-unused-varyings: skip
 spec/arb_separate_shader_objects/linker/pervertex-clipdistance-tcs-out-tes: skip
 spec/arb_separate_shader_objects/linker/pervertex-clipdistance-tes-out-gs: skip
 spec/arb_separate_shader_objects/linker/pervertex-clipdistance-vs-out-tcs: skip
 spec/arb_separate_shader_objects/linker/pervertex-culldistance-tcs-out-tes: skip
 spec/arb_separate_shader_objects/linker/pervertex-culldistance-tes-out-gs: skip
+spec/arb_separate_shader_objects/linker/pervertex-culldistance-vs-out-gs: skip
 spec/arb_separate_shader_objects/linker/pervertex-culldistance-vs-out-tcs: skip
 spec/arb_separate_shader_objects/linker/pervertex-pointsize-tcs-out-tes: skip
 spec/arb_separate_shader_objects/linker/pervertex-pointsize-tes-out-gs: skip
@@ -870,6 +876,7 @@ spec/arb_separate_shader_objects/linker/pervertex-pointsize-vs-out-tcs: skip
 spec/arb_separate_shader_objects/linker/pervertex-position-tcs-out-tes: skip
 spec/arb_separate_shader_objects/linker/pervertex-position-tes-out-gs: skip
 spec/arb_separate_shader_objects/linker/pervertex-position-vs-out-tcs: skip
+spec/arb_separate_shader_objects/linker/vs-to-fs-explicit-location-mismatch-array: skip
 spec/arb_separate_shader_objects/mix-and-match-tcs-tes: skip
 spec/arb_separate_shader_objects/rendezvous by location (5 stages): skip
 spec/arb_separate_shader_objects/validateprogrampipeline/only tes from tes/tcs program: skip
@@ -921,6 +928,7 @@ spec/arb_shader_group_vote/vs-eq-const: skip
 spec/arb_shader_group_vote/vs-eq-uniform: skip
 spec/arb_shader_image_load_store/execution/disable_early_z: fail
 spec/arb_shader_image_load_store/execution/gl45-imageatomicexchange-float: skip
+spec/arb_shader_image_load_store/execution/image_checkerboard: skip
 spec/arb_shader_image_load_store/max-size/imagecube max size test/4096x4096x6x1: skip
 spec/arb_shader_image_load_store/max-size/imagecubearray max size test/4096x4096x6x1: skip
 spec/arb_shader_precision/fs-degrees-float: skip
@@ -2710,7 +2718,7 @@ spec/glsl-1.10/execution/loops/glsl-fs-unroll-explosion: crash
 spec/glsl-1.10/execution/loops/glsl-vs-unroll-explosion: crash
 spec/glsl-1.10/preprocessor/extension-defined-test: skip
 spec/glsl-1.10/preprocessor/extension-if-1: skip
-spec/glsl-1.10/preprocessor/ifndef: skip
+spec/glsl-1.10/preprocessor/if-eq: skip
 spec/glsl-1.30/execution/tex-miplevel-selection texturegrad 1d: fail
 spec/glsl-1.30/execution/tex-miplevel-selection texturegrad 1darray: fail
 spec/glsl-1.30/execution/tex-miplevel-selection texturegrad 1darrayshadow: fail
@@ -3075,10 +3083,10 @@ wgl/wgl-sanity: skip
 summary:
        name:  results
        ----  --------
-       pass:    26844
-       fail:      400
+       pass:    26801
+       fail:      399
       crash:       42
-       skip:     2603
+       skip:     2612
     timeout:        0
        warn:        4
  incomplete:        0
@@ -3087,4 +3095,4 @@ summary:
     changes:        0
       fixes:        0
 regressions:        0
-      total:    29918
+      total:    29883
diff --git a/.gitlab-ci/windows/mesa_build.ps1 b/.gitlab-ci/windows/mesa_build.ps1
index db1fdb5b1b4..e290060a112 100644
--- a/.gitlab-ci/windows/mesa_build.ps1
+++ b/.gitlab-ci/windows/mesa_build.ps1
@@ -7,7 +7,7 @@ Write-Host "Compiling Mesa"
 $builddir = New-Item -ItemType Directory -Name "_build"
 $installdir = New-Item -ItemType Directory -Name "_install"
 Push-Location $builddir.FullName
-cmd.exe /C "C:\BuildTools\Common7\Tools\VsDevCmd.bat -host_arch=amd64 -arch=amd64 && meson --default-library=shared -Dzlib:default_library=static --buildtype=release -Db_ndebug=false -Db_vscrt=mt --cmake-prefix-path=`"C:\llvm-10`" --pkg-config-path=`"C:\llvm-10\lib\pkgconfig;C:\llvm-10\share\pkgconfig;C:\spirv-tools\lib\pkgconfig`" --prefix=`"$installdir`" -Dllvm=enabled -Dshared-llvm=disabled -Dgallium-drivers=swrast,d3d12 -Dmicrosoft-clc=enabled -Dstatic-libclc=all -Dbuild-tests=true && ninja -j32 install && meson test --num-processes 32"
+cmd.exe /C "C:\BuildTools\Common7\Tools\VsDevCmd.bat -host_arch=amd64 -arch=amd64 && meson --default-library=shared -Dzlib:default_library=static --buildtype=release -Db_ndebug=false -Db_vscrt=mt --cmake-prefix-path=`"C:\llvm-10`" --pkg-config-path=`"C:\llvm-10\lib\pkgconfig;C:\llvm-10\share\pkgconfig;C:\spirv-tools\lib\pkgconfig`" --prefix=`"$installdir`" -Dllvm=enabled -Dshared-llvm=disabled -Dgallium-drivers=swrast,d3d12 -Dmicrosoft-clc=enabled -Dstatic-libclc=all -Dbuild-tests=true && ninja -j32 install test"
 $buildstatus = $?
 Pop-Location
 
diff --git a/docs/drivers/freedreno/ir3-notes.rst b/docs/drivers/freedreno/ir3-notes.rst
index ef7ceb0e144..bded59e0e93 100644
--- a/docs/drivers/freedreno/ir3-notes.rst
+++ b/docs/drivers/freedreno/ir3-notes.rst
@@ -409,7 +409,7 @@ In the grouping pass, instructions which need to be grouped (for ``fanin``\s, et
 Depth
 ~~~~~
 
-In the depth pass, a depth is calculated for each instruction node within its basic block.  The depth is the sum of the required cycles (delay slots needed between two instructions plus one) of each instruction plus the max depth of any of its source instructions.  (meta_ instructions don't add to the depth).  As an instruction's depth is calculated, it is inserted into a per block list sorted by deepest instruction.  Unreachable instructions and inputs are marked.
+In the depth pass, a depth is calculated for each instruction node within it's basic block.  The depth is the sum of the required cycles (delay slots needed between two instructions plus one) of each instruction plus the max depth of any of it's source instructions.  (meta_ instructions don't add to the depth).  As an instruction's depth is calculated, it is inserted into a per block list sorted by deepest instruction.  Unreachable instructions and inputs are marked.
 
     TODO: we should probably calculate both hard and soft depths (?) to
     try to coax additional instructions to fit in places where we need
@@ -420,7 +420,7 @@ In the depth pass, a depth is calculated for each instruction node within its ba
 Scheduling
 ~~~~~~~~~~
 
-After the grouping_ pass, there are no more instructions to insert or remove.  Start scheduling each basic block from the deepest node in the depth sorted list created by the depth_ pass, recursively trying to schedule each instruction after its source instructions plus delay slots.  Insert ``nop``\s as required.
+After the grouping_ pass, there are no more instructions to insert or remove.  Start scheduling each basic block from the deepest node in the depth sorted list created by the depth_ pass, recursively trying to schedule each instruction after it's source instructions plus delay slots.  Insert ``nop``\s as required.
 
 .. _`register assignment`:
 
diff --git a/docs/drivers/freedreno/isaspec.rst b/docs/drivers/freedreno/isaspec.rst
index a1dffe9e0ee..df67c71effb 100644
--- a/docs/drivers/freedreno/isaspec.rst
+++ b/docs/drivers/freedreno/isaspec.rst
@@ -9,8 +9,8 @@ underlying instruction encoding to simplify dealing with instruction
 encoding differences between generations of GPU.
 
 Benefits of a formal ISA description, compared to hand-coded assemblers
-and disassemblers, include easier detection of new bit combinations that
-were not seen before in previous generations due to more rigorous
+and disassemblers, include easier detection of new bit combintions that
+were not seen before in previous generations due to more rigerous
 description of bits that are expect to be '0' or '1' or 'x' (dontcare)
 and verification that different encodings don't have conflicting bits
 (ie. that the specification cannot result in more than one valid
@@ -276,4 +276,4 @@ with the specified prefix prepended to uppercase'd leaf node name.  Ie. in
 this case, "add.f" becomes ``OPC_ADD_F``.
 
 Individual ``<map>`` elements teach the encoder how to map from the encode
-source to fields in the encoded instruction.
+source to fields in the encoded instruction.
\ No newline at end of file
diff --git a/docs/gallium/screen.rst b/docs/gallium/screen.rst
index cbd5dcb85dd..2ea87676e0c 100644
--- a/docs/gallium/screen.rst
+++ b/docs/gallium/screen.rst
@@ -596,6 +596,8 @@ The integer capabilities:
 * ``PIPE_CAP_MAX_TEXTURE_MB``: Maximum texture size in MB (default is 1024)
 * ``PIPE_CAP_DEVICE_PROTECTED_CONTENT``: Whether the device support protected / encrypted content.
 * ``PIPE_CAP_PREFER_REAL_BUFFER_IN_CONSTBUF0``: The state tracker is encouraged to upload constants into a real buffer and bind it into constant buffer 0 instead of binding a user pointer. This may enable a faster codepath in a gallium frontend for drivers that really prefer a real buffer.
+* ``PIPE_CAP_NO_DITHERING``: Driver doesn't want new blend states if dither state changes
+* ``PIPE_CAP_EMULATE_ARGB``: Driver doesn't support ARGB/ABGR natively and requires format swizzles from RGBA/BGRA
 
 .. _pipe_capf:
 
diff --git a/docs/gallium/tgsi.rst b/docs/gallium/tgsi.rst
index 0d6e036d91f..79d10076918 100644
--- a/docs/gallium/tgsi.rst
+++ b/docs/gallium/tgsi.rst
@@ -23,29 +23,26 @@ When an instruction has a scalar result, the result is usually copied into
 each of the components of *dst*. When this happens, the result is said to be
 *replicated* to *dst*. :opcode:`RCP` is one such instruction.
 
-Source Modifiers
-^^^^^^^^^^^^^^^^
-
-TGSI supports 32-bit negate and absolute value modifiers on floating-point
-inputs, and 32-bit integer negates on some drivers.  The negate applies after
-absolute value if both are present.
-
-The type of an input can be found by ``tgsi_opcode_infer_src_type()``, and
-TGSI_OPCODE_MOV and the second and third operands of TGSI_OPCODE_UCMP (which
-return TGSI_TYPE_UNTYPED) are also considered floats for the purpose of source
-modifiers.
-
-
-Other Modifiers
+Modifiers
 ^^^^^^^^^^^^^^^
 
-The saturate modifier clamps 32-bit destination stores to [0.0, 1.0].
+TGSI supports modifiers on inputs (as well as saturate and precise modifier
+on instructions).
 
 For arithmetic instruction having a precise modifier certain optimizations
 which may alter the result are disallowed. Example: *add(mul(a,b),c)* can't be
 optimized to TGSI_OPCODE_MAD, because some hardware only supports the fused
 MAD instruction.
 
+For inputs which have a floating point type, both absolute value and
+negation modifiers are supported (with absolute value being applied
+first).  The only source of TGSI_OPCODE_MOV and the second and third
+sources of TGSI_OPCODE_UCMP are considered to have float type for
+applying modifiers.
+
+For inputs which have signed or unsigned type only the negate modifier is
+supported.
+
 Instruction Set
 ---------------
 
diff --git a/meson.build b/meson.build
index 96571c9df25..932eb136681 100644
--- a/meson.build
+++ b/meson.build
@@ -1408,9 +1408,7 @@ if dep_thread.found() and host_machine.system() != 'windows'
     pre_args += '-DHAVE_PTHREAD_SETAFFINITY'
   endif
 endif
-if host_machine.system() == 'darwin'
-  dep_expat = meson.get_compiler('c').find_library('expat')
-elif host_machine.system() != 'windows'
+if host_machine.system() != 'windows'
   dep_expat = dependency('expat', fallback : ['expat', 'expat_dep'],
                          required: not with_platform_android or with_any_broadcom or with_any_intel)
 else
@@ -1735,11 +1733,7 @@ elif _libunwind == 'false'
   warning('libunwind option "false" deprecated, please use "disabled" instead.')
 endif
 if _libunwind != 'disabled' and not with_platform_android
-  if host_machine.system() == 'darwin'
-    dep_unwind = meson.get_compiler('c').find_library('System')
-  else
-    dep_unwind = dependency('libunwind', required : _libunwind == 'enabled')
-  endif
+  dep_unwind = dependency('libunwind', required : _libunwind == 'enabled')
   if dep_unwind.found()
     pre_args += '-DHAVE_LIBUNWIND'
   endif
diff --git a/src/amd/common/ac_shadowed_regs.h b/src/amd/common/ac_shadowed_regs.h
index 8a183927177..df2a7b72fc4 100644
--- a/src/amd/common/ac_shadowed_regs.h
+++ b/src/amd/common/ac_shadowed_regs.h
@@ -47,10 +47,6 @@ enum ac_reg_range_type
    SI_NUM_ALL_REG_RANGES,
 };
 
-#ifdef __cplusplus
-extern "C" {
-#endif
-
 typedef void (*set_context_reg_seq_array_fn)(struct radeon_cmdbuf *cs, unsigned reg, unsigned num,
                                              const uint32_t *values);
 
@@ -63,9 +59,4 @@ void ac_check_shadowed_regs(enum chip_class chip_class, enum radeon_family famil
                             unsigned reg_offset, unsigned count);
 void ac_print_shadowed_regs(const struct radeon_info *info);
 
-#ifdef __cplusplus
-}
-#endif
-
-
 #endif
diff --git a/src/amd/common/ac_surface.c b/src/amd/common/ac_surface.c
index e534c16b69d..49ec8f5685d 100644
--- a/src/amd/common/ac_surface.c
+++ b/src/amd/common/ac_surface.c
@@ -1684,12 +1684,8 @@ static int gfx9_compute_miptree(struct ac_addrlib *addrlib, const struct radeon_
       }
 
       for (unsigned i = 0; i < in->numMipLevels; i++) {
-         surf->u.gfx9.prt_level_offset[i] = mip_info[i].macroBlockOffset + mip_info[i].mipTailOffset;
-
-         if (info->chip_class >= GFX10)
-            surf->u.gfx9.prt_level_pitch[i] = mip_info[i].pitch;
-         else
-            surf->u.gfx9.prt_level_pitch[i] = out.mipChainPitch;
+         surf->u.gfx9.prt_level_offset[i] = mip_info[i].offset;
+         surf->u.gfx9.prt_level_pitch[i] = mip_info[i].pitch;
       }
    }
 
diff --git a/src/amd/compiler/aco_assembler.cpp b/src/amd/compiler/aco_assembler.cpp
index 05ec485a2cf..aed70afb28f 100644
--- a/src/amd/compiler/aco_assembler.cpp
+++ b/src/amd/compiler/aco_assembler.cpp
@@ -428,15 +428,6 @@ void emit_instruction(asm_context& ctx, std::vector<uint32_t>& out, Instruction*
       break;
    }
    case Format::MIMG: {
-      unsigned use_nsa = false;
-      unsigned addr_dwords = instr->operands.size() - 3;
-      for (unsigned i = 1; i < addr_dwords; i++) {
-         if (instr->operands[3 + i].physReg() != instr->operands[3].physReg().advance(i * 4))
-            use_nsa = true;
-      }
-      assert(!use_nsa || ctx.chip_class >= GFX10);
-      unsigned nsa_dwords = use_nsa ? DIV_ROUND_UP(addr_dwords - 1, 4) : 0;
-
       MIMG_instruction* mimg = static_cast<MIMG_instruction*>(instr);
       uint32_t encoding = (0b111100 << 26);
       encoding |= mimg->slc ? 1 << 25 : 0;
@@ -452,17 +443,16 @@ void emit_instruction(asm_context& ctx, std::vector<uint32_t>& out, Instruction*
          encoding |= mimg->da ? 1 << 14 : 0;
       } else {
          encoding |= mimg->r128 ? 1 << 15 : 0; /* GFX10: A16 moved to 2nd word, R128 replaces it in 1st word */
-         encoding |= nsa_dwords << 1;
          encoding |= mimg->dim << 3; /* GFX10: dimensionality instead of declare array */
          encoding |= mimg->dlc ? 1 << 7 : 0;
       }
       encoding |= (0xF & mimg->dmask) << 8;
       out.push_back(encoding);
-      encoding = (0xFF & instr->operands[3].physReg()); /* VADDR */
+      encoding = (0xFF & instr->operands[2].physReg()); /* VADDR */
       if (!instr->definitions.empty()) {
          encoding |= (0xFF & instr->definitions[0].physReg()) << 8; /* VDATA */
-      } else if (!instr->operands[2].isUndefined()) {
-         encoding |= (0xFF & instr->operands[2].physReg()) << 8; /* VDATA */
+      } else if (instr->operands.size() >= 4) {
+         encoding |= (0xFF & instr->operands[3].physReg()) << 8; /* VDATA */
       }
       encoding |= (0x1F & (instr->operands[0].physReg() >> 2)) << 16; /* T# (resource) */
       if (!instr->operands[1].isUndefined())
@@ -475,13 +465,6 @@ void emit_instruction(asm_context& ctx, std::vector<uint32_t>& out, Instruction*
       }
 
       out.push_back(encoding);
-
-      if (nsa_dwords) {
-         out.resize(out.size() + nsa_dwords);
-         std::vector<uint32_t>::iterator nsa = std::prev(out.end(), nsa_dwords);
-         for (unsigned i = 0; i < addr_dwords - 1; i++)
-            nsa[i / 4] |= (0xFF & instr->operands[4 + i].physReg().reg()) << (i % 4 * 8);
-      }
       break;
    }
    case Format::FLAT:
diff --git a/src/amd/compiler/aco_insert_waitcnt.cpp b/src/amd/compiler/aco_insert_waitcnt.cpp
index 765e1f5c45c..5adbc1fbd26 100644
--- a/src/amd/compiler/aco_insert_waitcnt.cpp
+++ b/src/amd/compiler/aco_insert_waitcnt.cpp
@@ -850,10 +850,10 @@ void gen(Instruction* instr, wait_ctx& ctx)
          insert_wait_entry(ctx, instr->operands[3], event_vmem_gpr_lock);
       } else if (ctx.chip_class == GFX6 &&
                  instr->format == Format::MIMG &&
-                 !instr->operands[2].isUndefined()) {
+                 instr->operands.size() >= 4) {
          ctx.exp_cnt++;
          update_counters(ctx, event_vmem_gpr_lock);
-         insert_wait_entry(ctx, instr->operands[2], event_vmem_gpr_lock);
+         insert_wait_entry(ctx, instr->operands[3], event_vmem_gpr_lock);
       }
 
       break;
diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 7e614071bb0..1c47c45eacb 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -158,14 +158,16 @@ Temp emit_mbcnt(isel_context *ctx, Temp dst, Operand mask = Operand(), Operand b
       return bld.vop3(aco_opcode::v_mbcnt_hi_u32_b32_e64, Definition(dst), mask_hi, mbcnt_lo);
 }
 
-Temp emit_wqm(Builder& bld, Temp src, Temp dst=Temp(0, s1), bool program_needs_wqm = false)
+Temp emit_wqm(isel_context *ctx, Temp src, Temp dst=Temp(0, s1), bool program_needs_wqm = false)
 {
+   Builder bld(ctx->program, ctx->block);
+
    if (!dst.id())
       dst = bld.tmp(src.regClass());
 
    assert(src.size() == dst.size());
 
-   if (bld.program->stage != fragment_fs) {
+   if (ctx->stage != fragment_fs) {
       if (!dst.id())
          return src;
 
@@ -174,7 +176,7 @@ Temp emit_wqm(Builder& bld, Temp src, Temp dst=Temp(0, s1), bool program_needs_w
    }
 
    bld.pseudo(aco_opcode::p_wqm, Definition(dst), src);
-   bld.program->needs_wqm |= program_needs_wqm;
+   ctx->program->needs_wqm |= program_needs_wqm;
    return dst;
 }
 
@@ -552,7 +554,7 @@ Temp bool_to_scalar_condition(isel_context *ctx, Temp val, Temp dst = Temp(0, s1
    /* if we're currently in WQM mode, ensure that the source is also computed in WQM */
    Temp tmp = bld.tmp(s1);
    bld.sop2(Builder::s_and, bld.def(bld.lm), bld.scc(Definition(tmp)), val, Operand(exec, bld.lm));
-   return emit_wqm(bld, tmp, dst);
+   return emit_wqm(ctx, tmp, dst);
 }
 
 Temp convert_int(isel_context *ctx, Builder& bld, Temp src, unsigned src_bits, unsigned dst_bits, bool is_signed, Temp dst=Temp())
@@ -3123,7 +3125,7 @@ void visit_alu_instr(isel_context *ctx, nir_alu_instr *instr)
          Temp tr = bld.ds(aco_opcode::ds_swizzle_b32, bld.def(v1), src, (1 << 15) | dpp_ctrl2);
          tmp = bld.vop2(aco_opcode::v_sub_f32, bld.def(v1), tr, tl);
       }
-      emit_wqm(bld, tmp, dst, true);
+      emit_wqm(ctx, tmp, dst, true);
       break;
    }
    default:
@@ -4697,7 +4699,7 @@ void emit_load_frag_coord(isel_context *ctx, Temp dst, unsigned num_components)
       /* dFdx fine */
       Temp tl = bld.vop1_dpp(aco_opcode::v_mov_b32, bld.def(v1), frag_z, dpp_quad_perm(0, 0, 2, 2));
       tmp = bld.vop2_dpp(aco_opcode::v_sub_f32, bld.def(v1), frag_z, tl, dpp_quad_perm(1, 1, 3, 3));
-      emit_wqm(bld, tmp, adjusted_frag_z, true);
+      emit_wqm(ctx, tmp, adjusted_frag_z, true);
 
       /* adjusted_frag_z * 0.0625 + frag_z */
       adjusted_frag_z = bld.vop3(aco_opcode::v_fma_f32, bld.def(v1), adjusted_frag_z,
@@ -4948,18 +4950,12 @@ void visit_load_input(isel_context *ctx, nir_intrinsic_instr *instr)
          unsigned fetch_offset = attrib_offset + channel_start * vtx_info->chan_byte_size;
          bool expanded = false;
 
-         /* Use MUBUF when possible to avoid possible alignment issues.
-          * We don't use MUBUF for multi-component loads because
-          * robustBufferAccess2 requires that bounds checking is per-attribute,
-          * but MUBUF is per-dword. Other generations get around this by doing
-          * bounds checking with the index, instead of the offset like GFX8. */
+         /* use MUBUF when possible to avoid possible alignment issues */
          /* TODO: we could use SDWA to unpack 8/16-bit attributes without extra instructions */
          bool use_mubuf = (nfmt == V_008F0C_BUF_NUM_FORMAT_FLOAT ||
                            nfmt == V_008F0C_BUF_NUM_FORMAT_UINT ||
                            nfmt == V_008F0C_BUF_NUM_FORMAT_SINT) &&
-                          vtx_info->chan_byte_size == 4 &&
-                          (fetch_component == 1 || ctx->options->chip_class != GFX8 ||
-                           !ctx->options->robust_buffer_access2);
+                          vtx_info->chan_byte_size == 4;
          unsigned fetch_dfmt = V_008F0C_BUF_DATA_FORMAT_INVALID;
          if (!use_mubuf) {
             fetch_dfmt = get_fetch_data_format(ctx, vtx_info, fetch_offset, attrib_stride, &fetch_component);
@@ -5843,62 +5839,6 @@ static int image_type_to_components_count(enum glsl_sampler_dim dim, bool array)
 }
 
 
-static MIMG_instruction *emit_mimg(Builder& bld, aco_opcode op,
-                                   Definition dst,
-                                   Temp rsrc,
-                                   Operand samp,
-                                   std::vector<Temp> coords,
-                                   unsigned num_wqm_coords=0,
-                                   Operand vdata=Operand(v1))
-{
-   if (bld.program->chip_class < GFX10) {
-      Temp coord = coords[0];
-      if (coords.size() > 1) {
-         coord = bld.tmp(RegType::vgpr, coords.size());
-
-         aco_ptr<Pseudo_instruction> vec{create_instruction<Pseudo_instruction>(aco_opcode::p_create_vector, Format::PSEUDO, coords.size(), 1)};
-         for (unsigned i = 0; i < coords.size(); i++)
-            vec->operands[i] = Operand(coords[i]);
-         vec->definitions[0] = Definition(coord);
-         bld.insert(std::move(vec));
-      } else if (coord.type() == RegType::sgpr) {
-         coord = bld.copy(bld.def(v1), coord);
-      }
-
-      if (num_wqm_coords) {
-         /* We don't need the bias, sample index, compare value or offset to be
-          * computed in WQM but if the p_create_vector copies the coordinates, then it
-          * needs to be in WQM. */
-         coord = emit_wqm(bld, coord, bld.tmp(coord.regClass()), true);
-      }
-
-      coords[0] = coord;
-      coords.resize(1);
-   } else {
-      for (unsigned i = 0; i < num_wqm_coords; i++)
-         coords[i] = emit_wqm(bld, coords[i], bld.tmp(coords[i].regClass()), true);
-
-      for (Temp& coord : coords) {
-         if (coord.type() == RegType::sgpr)
-            coord = bld.copy(bld.def(v1), coord);
-      }
-   }
-
-   aco_ptr<MIMG_instruction> mimg{create_instruction<MIMG_instruction>(
-      op, Format::MIMG, 3 + coords.size(), dst.isTemp())};
-   if (dst.isTemp())
-      mimg->definitions[0] = dst;
-   mimg->operands[0] = Operand(rsrc);
-   mimg->operands[1] = samp;
-   mimg->operands[2] = vdata;
-   for (unsigned i = 0; i < coords.size(); i++)
-      mimg->operands[3 + i] = Operand(coords[i]);
-
-   MIMG_instruction *res = mimg.get();
-   bld.insert(std::move(mimg));
-   return res;
-}
-
 /* Adjust the sample index according to FMASK.
  *
  * For uncompressed MSAA surfaces, FMASK should return 0x76543210,
@@ -5921,15 +5861,20 @@ static Temp adjust_sample_index_using_fmask(isel_context *ctx, bool da, std::vec
                   ? ac_get_sampler_dim(ctx->options->chip_class, GLSL_SAMPLER_DIM_2D, da)
                   : 0;
 
-   MIMG_instruction *load = emit_mimg(bld, aco_opcode::image_load,
-                                      Definition(fmask), fmask_desc_ptr,
-                                      Operand(s4), coords);
+   Temp coord = da ? bld.pseudo(aco_opcode::p_create_vector, bld.def(v3), coords[0], coords[1], coords[2]) :
+                     bld.pseudo(aco_opcode::p_create_vector, bld.def(v2), coords[0], coords[1]);
+   aco_ptr<MIMG_instruction> load{create_instruction<MIMG_instruction>(aco_opcode::image_load, Format::MIMG, 3, 1)};
+   load->operands[0] = Operand(fmask_desc_ptr);
+   load->operands[1] = Operand(s4); /* no sampler */
+   load->operands[2] = Operand(coord);
+   load->definitions[0] = Definition(fmask);
    load->glc = false;
    load->dlc = false;
    load->dmask = 0x1;
    load->unrm = true;
    load->da = da;
    load->dim = dim;
+   ctx->block->instructions.emplace_back(std::move(load));
 
    Operand sample_index4;
    if (sample_index.isConstant()) {
@@ -5966,7 +5911,7 @@ static Temp adjust_sample_index_using_fmask(isel_context *ctx, bool da, std::vec
    return bld.vop2(aco_opcode::v_cndmask_b32, bld.def(v1), sample_index_v, final_sample, compare);
 }
 
-static std::vector<Temp> get_image_coords(isel_context *ctx, const nir_intrinsic_instr *instr, const struct glsl_type *type)
+static Temp get_image_coords(isel_context *ctx, const nir_intrinsic_instr *instr, const struct glsl_type *type)
 {
 
    Temp src0 = get_ssa_temp(ctx, instr->src[1].ssa);
@@ -6020,7 +5965,13 @@ static std::vector<Temp> get_image_coords(isel_context *ctx, const nir_intrinsic
          coords.emplace_back(get_ssa_temp(ctx, instr->src[lod_index].ssa));
    }
 
-   return coords;
+   aco_ptr<Pseudo_instruction> vec{create_instruction<Pseudo_instruction>(aco_opcode::p_create_vector, Format::PSEUDO, coords.size(), 1)};
+   for (unsigned i = 0; i < coords.size(); i++)
+      vec->operands[i] = Operand(coords[i]);
+   Temp res = ctx->program->allocateTmp(RegClass(RegType::vgpr, coords.size()));
+   vec->definitions[0] = Definition(res);
+   ctx->block->instructions.emplace_back(std::move(vec));
+   return res;
 }
 
 
@@ -6132,14 +6083,16 @@ void visit_image_load(isel_context *ctx, nir_intrinsic_instr *instr)
          load->operands[3] = emit_tfe_init(bld, tmp);
       ctx->block->instructions.emplace_back(std::move(load));
    } else {
-      std::vector<Temp> coords = get_image_coords(ctx, instr, type);
+      Temp coords = get_image_coords(ctx, instr, type);
 
       bool level_zero = nir_src_is_const(instr->src[3]) && nir_src_as_uint(instr->src[3]) == 0;
       aco_opcode opcode = level_zero ? aco_opcode::image_load : aco_opcode::image_load_mip;
 
-      Operand vdata = is_sparse ? emit_tfe_init(bld, tmp) : Operand(v1);
-      MIMG_instruction *load = emit_mimg(bld, opcode, Definition(tmp), resource,
-                                         Operand(s4), coords, 0, vdata);
+      aco_ptr<MIMG_instruction> load{create_instruction<MIMG_instruction>(opcode, Format::MIMG, 3 + is_sparse, 1)};
+      load->operands[0] = Operand(resource);
+      load->operands[1] = Operand(s4); /* no sampler */
+      load->operands[2] = Operand(coords);
+      load->definitions[0] = Definition(tmp);
       load->glc = access & (ACCESS_VOLATILE | ACCESS_COHERENT) ? 1 : 0;
       load->dlc = load->glc && ctx->options->chip_class >= GFX10;
       load->dim = ac_get_image_dim(ctx->options->chip_class, dim, is_array);
@@ -6148,6 +6101,9 @@ void visit_image_load(isel_context *ctx, nir_intrinsic_instr *instr)
       load->da = should_declare_array(ctx, dim, glsl_sampler_type_is_array(type));
       load->sync = sync;
       load->tfe = is_sparse;
+      if (load->tfe)
+         load->operands[3] = emit_tfe_init(bld, tmp);
+      ctx->block->instructions.emplace_back(std::move(load));
    }
 
    if (is_sparse && instr->dest.ssa.bit_size == 64) {
@@ -6213,15 +6169,17 @@ void visit_image_store(isel_context *ctx, nir_intrinsic_instr *instr)
    }
 
    assert(data.type() == RegType::vgpr);
-   std::vector<Temp> coords = get_image_coords(ctx, instr, type);
+   Temp coords = get_image_coords(ctx, instr, type);
    Temp resource = get_sampler_desc(ctx, nir_instr_as_deref(instr->src[0].ssa->parent_instr), ACO_DESC_IMAGE, nullptr, true, true);
 
    bool level_zero = nir_src_is_const(instr->src[4]) && nir_src_as_uint(instr->src[4]) == 0;
    aco_opcode opcode = level_zero ? aco_opcode::image_store : aco_opcode::image_store_mip;
 
-   Builder bld(ctx->program, ctx->block);
-   MIMG_instruction *store = emit_mimg(bld, opcode, Definition(), resource,
-                                       Operand(s4), coords, 0, Operand(data));
+   aco_ptr<MIMG_instruction> store{create_instruction<MIMG_instruction>(opcode, Format::MIMG, 4, 0)};
+   store->operands[0] = Operand(resource);
+   store->operands[1] = Operand(s4); /* no sampler */
+   store->operands[2] = Operand(coords);
+   store->operands[3] = Operand(data);
    store->glc = glc;
    store->dlc = false;
    store->dim = ac_get_image_dim(ctx->options->chip_class, dim, is_array);
@@ -6231,6 +6189,7 @@ void visit_image_store(isel_context *ctx, nir_intrinsic_instr *instr)
    store->disable_wqm = true;
    store->sync = sync;
    ctx->program->needs_exact = true;
+   ctx->block->instructions.emplace_back(std::move(store));
    return;
 }
 
@@ -6342,11 +6301,15 @@ void visit_image_atomic(isel_context *ctx, nir_intrinsic_instr *instr)
       return;
    }
 
-   std::vector<Temp> coords = get_image_coords(ctx, instr, type);
+   Temp coords = get_image_coords(ctx, instr, type);
    Temp resource = get_sampler_desc(ctx, nir_instr_as_deref(instr->src[0].ssa->parent_instr), ACO_DESC_IMAGE, nullptr, true, true);
-   Definition def = return_previous ? Definition(dst) : Definition();
-   MIMG_instruction *mimg = emit_mimg(bld, image_op, def, resource,
-                                      Operand(s4), coords, 0, Operand(data));
+   aco_ptr<MIMG_instruction> mimg{create_instruction<MIMG_instruction>(image_op, Format::MIMG, 4, return_previous ? 1 : 0)};
+   mimg->operands[0] = Operand(resource);
+   mimg->operands[1] = Operand(s4); /* no sampler */
+   mimg->operands[2] = Operand(coords);
+   mimg->operands[3] = Operand(data);
+   if (return_previous)
+      mimg->definitions[0] = Definition(dst);
    mimg->glc = return_previous;
    mimg->dlc = false; /* Not needed for atomics */
    mimg->dim = ac_get_image_dim(ctx->options->chip_class, dim, is_array);
@@ -6356,6 +6319,7 @@ void visit_image_atomic(isel_context *ctx, nir_intrinsic_instr *instr)
    mimg->disable_wqm = true;
    mimg->sync = sync;
    ctx->program->needs_exact = true;
+   ctx->block->instructions.emplace_back(std::move(mimg));
    return;
 }
 
@@ -6403,26 +6367,30 @@ void visit_image_size(isel_context *ctx, nir_intrinsic_instr *instr)
 
    /* LOD */
    assert(nir_src_as_uint(instr->src[1]) == 0);
-   std::vector<Temp> lod{bld.copy(bld.def(v1), Operand(0u))};
+   Temp lod = bld.copy(bld.def(v1), Operand(0u));
 
    /* Resource */
    Temp resource = get_sampler_desc(ctx, nir_instr_as_deref(instr->src[0].ssa->parent_instr), ACO_DESC_IMAGE, NULL, true, false);
 
    Temp dst = get_ssa_temp(ctx, &instr->dest.ssa);
 
-   MIMG_instruction *mimg = emit_mimg(bld, aco_opcode::image_get_resinfo,
-                                      Definition(dst), resource, Operand(s4), lod);
+   aco_ptr<MIMG_instruction> mimg{create_instruction<MIMG_instruction>(aco_opcode::image_get_resinfo, Format::MIMG, 3, 1)};
+   mimg->operands[0] = Operand(resource);
+   mimg->operands[1] = Operand(s4); /* no sampler */
+   mimg->operands[2] = Operand(lod);
    uint8_t& dmask = mimg->dmask;
    mimg->dim = ac_get_image_dim(ctx->options->chip_class, dim, is_array);
    mimg->dmask = (1 << instr->dest.ssa.num_components) - 1;
    mimg->da = glsl_sampler_type_is_array(type);
+   Definition& def = mimg->definitions[0];
+   ctx->block->instructions.emplace_back(std::move(mimg));
 
    if (glsl_get_sampler_dim(type) == GLSL_SAMPLER_DIM_CUBE &&
        glsl_sampler_type_is_array(type)) {
 
       assert(instr->dest.ssa.num_components == 3);
       Temp tmp = ctx->program->allocateTmp(v3);
-      mimg->definitions[0] = Definition(tmp);
+      def = Definition(tmp);
       emit_split_vector(ctx, tmp, 3);
 
       /* divide 3rd value by 6 by multiplying with magic number */
@@ -6438,7 +6406,10 @@ void visit_image_size(isel_context *ctx, nir_intrinsic_instr *instr)
               glsl_get_sampler_dim(type) == GLSL_SAMPLER_DIM_1D &&
               glsl_sampler_type_is_array(type)) {
       assert(instr->dest.ssa.num_components == 2);
+      def = Definition(dst);
       dmask = 0x5;
+   } else {
+      def = Definition(dst);
    }
 
    emit_split_vector(ctx, dst, instr->dest.ssa.num_components);
@@ -6600,6 +6571,7 @@ void visit_atomic_ssbo(isel_context *ctx, nir_intrinsic_instr *instr)
 void visit_get_ssbo_size(isel_context *ctx, nir_intrinsic_instr *instr) {
 
    Temp rsrc = get_ssa_temp(ctx, instr->src[0].ssa);
+   rsrc = emit_extract_vector(ctx, rsrc, 0, RegClass(rsrc.type(), 1));
    Temp index = convert_pointer_to_64_bit(ctx, rsrc);
 
    Builder bld(ctx->program, ctx->block);
@@ -7401,7 +7373,7 @@ Temp emit_boolean_reduce(isel_context *ctx, nir_op op, unsigned cluster_size, Te
    } else if (op == nir_op_iand && cluster_size == ctx->program->wave_size) {
       //subgroupAnd(val) -> (exec & ~val) == 0
       Temp tmp = bld.sop2(Builder::s_andn2, bld.def(bld.lm), bld.def(s1, scc), Operand(exec, bld.lm), src).def(1).getTemp();
-      Temp cond = bool_to_vector_condition(ctx, emit_wqm(bld, tmp));
+      Temp cond = bool_to_vector_condition(ctx, emit_wqm(ctx, tmp));
       return bld.sop1(Builder::s_not, bld.def(bld.lm), bld.def(s1, scc), cond);
    } else if (op == nir_op_ior && cluster_size == ctx->program->wave_size) {
       //subgroupOr(val) -> (val & exec) != 0
@@ -7775,9 +7747,9 @@ void emit_interp_center(isel_context *ctx, Temp dst, Temp pos1, Temp pos2)
    tmp1 = bld.vop3(mad, bld.def(v1), ddy_1, pos2, tmp1);
    tmp2 = bld.vop3(mad, bld.def(v1), ddy_2, pos2, tmp2);
    Temp wqm1 = bld.tmp(v1);
-   emit_wqm(bld, tmp1, wqm1, true);
+   emit_wqm(ctx, tmp1, wqm1, true);
    Temp wqm2 = bld.tmp(v1);
-   emit_wqm(bld, tmp2, wqm2, true);
+   emit_wqm(ctx, tmp2, wqm2, true);
    bld.pseudo(aco_opcode::p_create_vector, Definition(dst), wqm1, wqm2);
    return;
 }
@@ -8176,7 +8148,7 @@ void visit_intrinsic(isel_context *ctx, nir_intrinsic_instr *instr)
          /* Wave32 with ballot size set to 64 */
          bld.pseudo(aco_opcode::p_create_vector, Definition(tmp), lanemask_tmp.getTemp(), Operand(0u));
       }
-      emit_wqm(bld, tmp.getTemp(), dst);
+      emit_wqm(ctx, tmp.getTemp(), dst);
       break;
    }
    case nir_intrinsic_shuffle:
@@ -8191,24 +8163,24 @@ void visit_intrinsic(isel_context *ctx, nir_intrinsic_instr *instr)
          Temp dst = get_ssa_temp(ctx, &instr->dest.ssa);
          if (src.regClass() == v1b || src.regClass() == v2b) {
             Temp tmp = bld.tmp(v1);
-            tmp = emit_wqm(bld, emit_bpermute(ctx, bld, tid, src), tmp);
+            tmp = emit_wqm(ctx, emit_bpermute(ctx, bld, tid, src), tmp);
             if (dst.type() == RegType::vgpr)
                bld.pseudo(aco_opcode::p_split_vector, Definition(dst), bld.def(src.regClass() == v1b ? v3b : v2b), tmp);
             else
                bld.pseudo(aco_opcode::p_as_uniform, Definition(dst), tmp);
          } else if (src.regClass() == v1) {
-            emit_wqm(bld, emit_bpermute(ctx, bld, tid, src), dst);
+            emit_wqm(ctx, emit_bpermute(ctx, bld, tid, src), dst);
          } else if (src.regClass() == v2) {
             Temp lo = bld.tmp(v1), hi = bld.tmp(v1);
             bld.pseudo(aco_opcode::p_split_vector, Definition(lo), Definition(hi), src);
-            lo = emit_wqm(bld, emit_bpermute(ctx, bld, tid, lo));
-            hi = emit_wqm(bld, emit_bpermute(ctx, bld, tid, hi));
+            lo = emit_wqm(ctx, emit_bpermute(ctx, bld, tid, lo));
+            hi = emit_wqm(ctx, emit_bpermute(ctx, bld, tid, hi));
             bld.pseudo(aco_opcode::p_create_vector, Definition(dst), lo, hi);
             emit_split_vector(ctx, dst, 2);
          } else if (instr->dest.ssa.bit_size == 1 && tid.regClass() == s1) {
             assert(src.regClass() == bld.lm);
             Temp tmp = bld.sopc(Builder::s_bitcmp1, bld.def(s1, scc), src, tid);
-            bool_to_vector_condition(ctx, emit_wqm(bld, tmp), dst);
+            bool_to_vector_condition(ctx, emit_wqm(ctx, tmp), dst);
          } else if (instr->dest.ssa.bit_size == 1 && tid.regClass() == v1) {
             assert(src.regClass() == bld.lm);
             Temp tmp;
@@ -8220,7 +8192,7 @@ void visit_intrinsic(isel_context *ctx, nir_intrinsic_instr *instr)
                tmp = bld.vop2_e64(aco_opcode::v_lshrrev_b32, bld.def(v1), tid, src);
             tmp = emit_extract_vector(ctx, tmp, 0, v1);
             tmp = bld.vop2(aco_opcode::v_and_b32, bld.def(v1), Operand(1u), tmp);
-            emit_wqm(bld, bld.vopc(aco_opcode::v_cmp_lg_u32, bld.def(bld.lm), Operand(0u), tmp), dst);
+            emit_wqm(ctx, bld.vopc(aco_opcode::v_cmp_lg_u32, bld.def(bld.lm), Operand(0u), tmp), dst);
          } else {
             isel_err(&instr->instr, "Unimplemented NIR instr bit size");
          }
@@ -8240,21 +8212,21 @@ void visit_intrinsic(isel_context *ctx, nir_intrinsic_instr *instr)
       Temp src = get_ssa_temp(ctx, instr->src[0].ssa);
       Temp dst = get_ssa_temp(ctx, &instr->dest.ssa);
       if (src.regClass() == v1b || src.regClass() == v2b || src.regClass() == v1) {
-         emit_wqm(bld,
+         emit_wqm(ctx,
                   bld.vop1(aco_opcode::v_readfirstlane_b32, bld.def(s1), src),
                   dst);
       } else if (src.regClass() == v2) {
          Temp lo = bld.tmp(v1), hi = bld.tmp(v1);
          bld.pseudo(aco_opcode::p_split_vector, Definition(lo), Definition(hi), src);
-         lo = emit_wqm(bld, bld.vop1(aco_opcode::v_readfirstlane_b32, bld.def(s1), lo));
-         hi = emit_wqm(bld, bld.vop1(aco_opcode::v_readfirstlane_b32, bld.def(s1), hi));
+         lo = emit_wqm(ctx, bld.vop1(aco_opcode::v_readfirstlane_b32, bld.def(s1), lo));
+         hi = emit_wqm(ctx, bld.vop1(aco_opcode::v_readfirstlane_b32, bld.def(s1), hi));
          bld.pseudo(aco_opcode::p_create_vector, Definition(dst), lo, hi);
          emit_split_vector(ctx, dst, 2);
       } else if (instr->dest.ssa.bit_size == 1) {
          assert(src.regClass() == bld.lm);
          Temp tmp = bld.sopc(Builder::s_bitcmp1, bld.def(s1, scc), src,
                              bld.sop1(Builder::s_ff1_i32, bld.def(s1), Operand(exec, bld.lm)));
-         bool_to_vector_condition(ctx, emit_wqm(bld, tmp), dst);
+         bool_to_vector_condition(ctx, emit_wqm(ctx, tmp), dst);
       } else {
          bld.copy(Definition(dst), src);
       }
@@ -8267,7 +8239,7 @@ void visit_intrinsic(isel_context *ctx, nir_intrinsic_instr *instr)
       assert(dst.regClass() == bld.lm);
 
       Temp tmp = bld.sop2(Builder::s_andn2, bld.def(bld.lm), bld.def(s1, scc), Operand(exec, bld.lm), src).def(1).getTemp();
-      Temp cond = bool_to_vector_condition(ctx, emit_wqm(bld, tmp));
+      Temp cond = bool_to_vector_condition(ctx, emit_wqm(ctx, tmp));
       bld.sop1(Builder::s_not, Definition(dst), bld.def(s1, scc), cond);
       break;
    }
@@ -8278,7 +8250,7 @@ void visit_intrinsic(isel_context *ctx, nir_intrinsic_instr *instr)
       assert(dst.regClass() == bld.lm);
 
       Temp tmp = bool_to_scalar_condition(ctx, src);
-      bool_to_vector_condition(ctx, emit_wqm(bld, tmp), dst);
+      bool_to_vector_condition(ctx, emit_wqm(ctx, tmp), dst);
       break;
    }
    case nir_intrinsic_reduce:
@@ -8319,13 +8291,13 @@ void visit_intrinsic(isel_context *ctx, nir_intrinsic_instr *instr)
 
          switch (instr->intrinsic) {
          case nir_intrinsic_reduce:
-            emit_wqm(bld, emit_boolean_reduce(ctx, op, cluster_size, src), dst);
+            emit_wqm(ctx, emit_boolean_reduce(ctx, op, cluster_size, src), dst);
             break;
          case nir_intrinsic_exclusive_scan:
-            emit_wqm(bld, emit_boolean_exclusive_scan(ctx, op, src), dst);
+            emit_wqm(ctx, emit_boolean_exclusive_scan(ctx, op, src), dst);
             break;
          case nir_intrinsic_inclusive_scan:
-            emit_wqm(bld, emit_boolean_inclusive_scan(ctx, op, src), dst);
+            emit_wqm(ctx, emit_boolean_inclusive_scan(ctx, op, src), dst);
             break;
          default:
             assert(false);
@@ -8349,7 +8321,7 @@ void visit_intrinsic(isel_context *ctx, nir_intrinsic_instr *instr)
          }
 
          Temp tmp_dst = emit_reduction_instr(ctx, aco_op, reduce_op, cluster_size, bld.def(dst.regClass()), src);
-         emit_wqm(bld, tmp_dst, dst);
+         emit_wqm(ctx, tmp_dst, dst);
       }
       break;
    }
@@ -8371,35 +8343,35 @@ void visit_intrinsic(isel_context *ctx, nir_intrinsic_instr *instr)
             bld.sop1(Builder::s_wqm, Definition(tmp),
                      bld.sop2(Builder::s_and, bld.def(bld.lm), bld.def(s1, scc), mask_tmp,
                               bld.sop2(Builder::s_and, bld.def(bld.lm), bld.def(s1, scc), src, Operand(exec, bld.lm))));
-            emit_wqm(bld, tmp, dst);
+            emit_wqm(ctx, tmp, dst);
          } else if (instr->dest.ssa.bit_size == 8) {
             Temp tmp = bld.tmp(v1);
             if (ctx->program->chip_class >= GFX8)
-               emit_wqm(bld, bld.vop1_dpp(aco_opcode::v_mov_b32, bld.def(v1), src, dpp_ctrl), tmp);
+               emit_wqm(ctx, bld.vop1_dpp(aco_opcode::v_mov_b32, bld.def(v1), src, dpp_ctrl), tmp);
             else
-               emit_wqm(bld, bld.ds(aco_opcode::ds_swizzle_b32, bld.def(v1), src, (1 << 15) | dpp_ctrl), tmp);
+               emit_wqm(ctx, bld.ds(aco_opcode::ds_swizzle_b32, bld.def(v1), src, (1 << 15) | dpp_ctrl), tmp);
             bld.pseudo(aco_opcode::p_split_vector, Definition(dst), bld.def(v3b), tmp);
          } else if (instr->dest.ssa.bit_size == 16) {
             Temp tmp = bld.tmp(v1);
             if (ctx->program->chip_class >= GFX8)
-               emit_wqm(bld, bld.vop1_dpp(aco_opcode::v_mov_b32, bld.def(v1), src, dpp_ctrl), tmp);
+               emit_wqm(ctx, bld.vop1_dpp(aco_opcode::v_mov_b32, bld.def(v1), src, dpp_ctrl), tmp);
             else
-               emit_wqm(bld, bld.ds(aco_opcode::ds_swizzle_b32, bld.def(v1), src, (1 << 15) | dpp_ctrl), tmp);
+               emit_wqm(ctx, bld.ds(aco_opcode::ds_swizzle_b32, bld.def(v1), src, (1 << 15) | dpp_ctrl), tmp);
             bld.pseudo(aco_opcode::p_split_vector, Definition(dst), bld.def(v2b), tmp);
          } else if (instr->dest.ssa.bit_size == 32) {
             if (ctx->program->chip_class >= GFX8)
-               emit_wqm(bld, bld.vop1_dpp(aco_opcode::v_mov_b32, bld.def(v1), src, dpp_ctrl), dst);
+               emit_wqm(ctx, bld.vop1_dpp(aco_opcode::v_mov_b32, bld.def(v1), src, dpp_ctrl), dst);
             else
-               emit_wqm(bld, bld.ds(aco_opcode::ds_swizzle_b32, bld.def(v1), src, (1 << 15) | dpp_ctrl), dst);
+               emit_wqm(ctx, bld.ds(aco_opcode::ds_swizzle_b32, bld.def(v1), src, (1 << 15) | dpp_ctrl), dst);
          } else if (instr->dest.ssa.bit_size == 64) {
             Temp lo = bld.tmp(v1), hi = bld.tmp(v1);
             bld.pseudo(aco_opcode::p_split_vector, Definition(lo), Definition(hi), src);
             if (ctx->program->chip_class >= GFX8) {
-               lo = emit_wqm(bld, bld.vop1_dpp(aco_opcode::v_mov_b32, bld.def(v1), lo, dpp_ctrl));
-               hi = emit_wqm(bld, bld.vop1_dpp(aco_opcode::v_mov_b32, bld.def(v1), hi, dpp_ctrl));
+               lo = emit_wqm(ctx, bld.vop1_dpp(aco_opcode::v_mov_b32, bld.def(v1), lo, dpp_ctrl));
+               hi = emit_wqm(ctx, bld.vop1_dpp(aco_opcode::v_mov_b32, bld.def(v1), hi, dpp_ctrl));
             } else {
-               lo = emit_wqm(bld, bld.ds(aco_opcode::ds_swizzle_b32, bld.def(v1), lo, (1 << 15) | dpp_ctrl));
-               hi = emit_wqm(bld, bld.ds(aco_opcode::ds_swizzle_b32, bld.def(v1), hi, (1 << 15) | dpp_ctrl));
+               lo = emit_wqm(ctx, bld.ds(aco_opcode::ds_swizzle_b32, bld.def(v1), lo, (1 << 15) | dpp_ctrl));
+               hi = emit_wqm(ctx, bld.ds(aco_opcode::ds_swizzle_b32, bld.def(v1), hi, (1 << 15) | dpp_ctrl));
             }
             bld.pseudo(aco_opcode::p_create_vector, Definition(dst), lo, hi);
             emit_split_vector(ctx, dst, 2);
@@ -8447,20 +8419,20 @@ void visit_intrinsic(isel_context *ctx, nir_intrinsic_instr *instr)
          else
             src = bld.ds(aco_opcode::ds_swizzle_b32, bld.def(v1), src, dpp_ctrl);
          Temp tmp = bld.vopc(aco_opcode::v_cmp_lg_u32, bld.def(bld.lm), Operand(0u), src);
-         emit_wqm(bld, tmp, dst);
+         emit_wqm(ctx, tmp, dst);
       } else if (instr->dest.ssa.bit_size == 8) {
          Temp tmp = bld.tmp(v1);
          if (ctx->program->chip_class >= GFX8)
-            emit_wqm(bld, bld.vop1_dpp(aco_opcode::v_mov_b32, bld.def(v1), src, dpp_ctrl), tmp);
+            emit_wqm(ctx, bld.vop1_dpp(aco_opcode::v_mov_b32, bld.def(v1), src, dpp_ctrl), tmp);
          else
-            emit_wqm(bld, bld.ds(aco_opcode::ds_swizzle_b32, bld.def(v1), src, dpp_ctrl), tmp);
+            emit_wqm(ctx, bld.ds(aco_opcode::ds_swizzle_b32, bld.def(v1), src, dpp_ctrl), tmp);
          bld.pseudo(aco_opcode::p_split_vector, Definition(dst), bld.def(v3b), tmp);
       } else if (instr->dest.ssa.bit_size == 16) {
          Temp tmp = bld.tmp(v1);
          if (ctx->program->chip_class >= GFX8)
-            emit_wqm(bld, bld.vop1_dpp(aco_opcode::v_mov_b32, bld.def(v1), src, dpp_ctrl), tmp);
+            emit_wqm(ctx, bld.vop1_dpp(aco_opcode::v_mov_b32, bld.def(v1), src, dpp_ctrl), tmp);
          else
-            emit_wqm(bld, bld.ds(aco_opcode::ds_swizzle_b32, bld.def(v1), src, dpp_ctrl), tmp);
+            emit_wqm(ctx, bld.ds(aco_opcode::ds_swizzle_b32, bld.def(v1), src, dpp_ctrl), tmp);
          bld.pseudo(aco_opcode::p_split_vector, Definition(dst), bld.def(v2b), tmp);
       } else if (instr->dest.ssa.bit_size == 32) {
          Temp tmp;
@@ -8468,16 +8440,16 @@ void visit_intrinsic(isel_context *ctx, nir_intrinsic_instr *instr)
             tmp = bld.vop1_dpp(aco_opcode::v_mov_b32, bld.def(v1), src, dpp_ctrl);
          else
             tmp = bld.ds(aco_opcode::ds_swizzle_b32, bld.def(v1), src, dpp_ctrl);
-         emit_wqm(bld, tmp, dst);
+         emit_wqm(ctx, tmp, dst);
       } else if (instr->dest.ssa.bit_size == 64) {
          Temp lo = bld.tmp(v1), hi = bld.tmp(v1);
          bld.pseudo(aco_opcode::p_split_vector, Definition(lo), Definition(hi), src);
          if (ctx->program->chip_class >= GFX8) {
-            lo = emit_wqm(bld, bld.vop1_dpp(aco_opcode::v_mov_b32, bld.def(v1), lo, dpp_ctrl));
-            hi = emit_wqm(bld, bld.vop1_dpp(aco_opcode::v_mov_b32, bld.def(v1), hi, dpp_ctrl));
+            lo = emit_wqm(ctx, bld.vop1_dpp(aco_opcode::v_mov_b32, bld.def(v1), lo, dpp_ctrl));
+            hi = emit_wqm(ctx, bld.vop1_dpp(aco_opcode::v_mov_b32, bld.def(v1), hi, dpp_ctrl));
          } else {
-            lo = emit_wqm(bld, bld.ds(aco_opcode::ds_swizzle_b32, bld.def(v1), lo, dpp_ctrl));
-            hi = emit_wqm(bld, bld.ds(aco_opcode::ds_swizzle_b32, bld.def(v1), hi, dpp_ctrl));
+            lo = emit_wqm(ctx, bld.ds(aco_opcode::ds_swizzle_b32, bld.def(v1), lo, dpp_ctrl));
+            hi = emit_wqm(ctx, bld.ds(aco_opcode::ds_swizzle_b32, bld.def(v1), hi, dpp_ctrl));
          }
          bld.pseudo(aco_opcode::p_create_vector, Definition(dst), lo, hi);
          emit_split_vector(ctx, dst, 2);
@@ -8499,20 +8471,20 @@ void visit_intrinsic(isel_context *ctx, nir_intrinsic_instr *instr)
          src = bld.vop2_e64(aco_opcode::v_cndmask_b32, bld.def(v1), Operand(0u), Operand((uint32_t)-1), src);
          src = emit_masked_swizzle(ctx, bld, src, mask);
          Temp tmp = bld.vopc(aco_opcode::v_cmp_lg_u32, bld.def(bld.lm), Operand(0u), src);
-         emit_wqm(bld, tmp, dst);
+         emit_wqm(ctx, tmp, dst);
       } else if (dst.regClass() == v1b) {
-         Temp tmp = emit_wqm(bld, emit_masked_swizzle(ctx, bld, src, mask));
+         Temp tmp = emit_wqm(ctx, emit_masked_swizzle(ctx, bld, src, mask));
          emit_extract_vector(ctx, tmp, 0, dst);
       } else if (dst.regClass() == v2b) {
-         Temp tmp = emit_wqm(bld, emit_masked_swizzle(ctx, bld, src, mask));
+         Temp tmp = emit_wqm(ctx, emit_masked_swizzle(ctx, bld, src, mask));
          emit_extract_vector(ctx, tmp, 0, dst);
       } else if (dst.regClass() == v1) {
-         emit_wqm(bld, emit_masked_swizzle(ctx, bld, src, mask), dst);
+         emit_wqm(ctx, emit_masked_swizzle(ctx, bld, src, mask), dst);
       } else if (dst.regClass() == v2) {
          Temp lo = bld.tmp(v1), hi = bld.tmp(v1);
          bld.pseudo(aco_opcode::p_split_vector, Definition(lo), Definition(hi), src);
-         lo = emit_wqm(bld, emit_masked_swizzle(ctx, bld, lo, mask));
-         hi = emit_wqm(bld, emit_masked_swizzle(ctx, bld, hi, mask));
+         lo = emit_wqm(ctx, emit_masked_swizzle(ctx, bld, lo, mask));
+         hi = emit_wqm(ctx, emit_masked_swizzle(ctx, bld, hi, mask));
          bld.pseudo(aco_opcode::p_create_vector, Definition(dst), lo, hi);
          emit_split_vector(ctx, dst, 2);
       } else {
@@ -8527,14 +8499,14 @@ void visit_intrinsic(isel_context *ctx, nir_intrinsic_instr *instr)
       Temp dst = get_ssa_temp(ctx, &instr->dest.ssa);
       if (dst.regClass() == v1) {
          /* src2 is ignored for writelane. RA assigns the same reg for dst */
-         emit_wqm(bld, bld.writelane(bld.def(v1), val, lane, src), dst);
+         emit_wqm(ctx, bld.writelane(bld.def(v1), val, lane, src), dst);
       } else if (dst.regClass() == v2) {
          Temp src_lo = bld.tmp(v1), src_hi = bld.tmp(v1);
          Temp val_lo = bld.tmp(s1), val_hi = bld.tmp(s1);
          bld.pseudo(aco_opcode::p_split_vector, Definition(src_lo), Definition(src_hi), src);
          bld.pseudo(aco_opcode::p_split_vector, Definition(val_lo), Definition(val_hi), val);
-         Temp lo = emit_wqm(bld, bld.writelane(bld.def(v1), val_lo, lane, src_hi));
-         Temp hi = emit_wqm(bld, bld.writelane(bld.def(v1), val_hi, lane, src_hi));
+         Temp lo = emit_wqm(ctx, bld.writelane(bld.def(v1), val_lo, lane, src_hi));
+         Temp hi = emit_wqm(ctx, bld.writelane(bld.def(v1), val_hi, lane, src_hi));
          bld.pseudo(aco_opcode::p_create_vector, Definition(dst), lo, hi);
          emit_split_vector(ctx, dst, 2);
       } else {
@@ -8548,7 +8520,7 @@ void visit_intrinsic(isel_context *ctx, nir_intrinsic_instr *instr)
       /* Fit 64-bit mask for wave32 */
       src = emit_extract_vector(ctx, src, 0, RegClass(src.type(), bld.lm.size()));
       Temp wqm_tmp = emit_mbcnt(ctx, bld.tmp(v1), Operand(src));
-      emit_wqm(bld, wqm_tmp, dst);
+      emit_wqm(ctx, wqm_tmp, dst);
       break;
    }
    case nir_intrinsic_load_helper_invocation: {
@@ -8586,7 +8558,7 @@ void visit_intrinsic(isel_context *ctx, nir_intrinsic_instr *instr)
       break;
    }
    case nir_intrinsic_first_invocation: {
-      emit_wqm(bld, bld.sop1(Builder::s_ff1_i32, bld.def(s1), Operand(exec, bld.lm)),
+      emit_wqm(ctx, bld.sop1(Builder::s_ff1_i32, bld.def(s1), Operand(exec, bld.lm)),
                get_ssa_temp(ctx, &instr->dest.ssa));
       break;
    }
@@ -8594,12 +8566,12 @@ void visit_intrinsic(isel_context *ctx, nir_intrinsic_instr *instr)
       Temp flbit = bld.sop1(Builder::s_flbit_i32, bld.def(s1), Operand(exec, bld.lm));
       Temp last = bld.sop2(aco_opcode::s_sub_i32, bld.def(s1), bld.def(s1, scc),
                            Operand(ctx->program->wave_size - 1u), flbit);
-      emit_wqm(bld, last, get_ssa_temp(ctx, &instr->dest.ssa));
+      emit_wqm(ctx, last, get_ssa_temp(ctx, &instr->dest.ssa));
       break;
    }
    case nir_intrinsic_elect: {
       Temp first = bld.sop1(Builder::s_ff1_i32, bld.def(s1), Operand(exec, bld.lm));
-      emit_wqm(bld, bld.sop2(Builder::s_lshl, bld.def(bld.lm), bld.def(s1, scc), Operand(1u), first),
+      emit_wqm(ctx, bld.sop2(Builder::s_lshl, bld.def(bld.lm), bld.def(s1, scc), Operand(1u), first),
                get_ssa_temp(ctx, &instr->dest.ssa));
       break;
    }
@@ -9175,6 +9147,7 @@ void visit_tex(isel_context *ctx, nir_tex_instr *instr)
       tmp_dst = bld.tmp(RegClass(RegType::vgpr, util_bitcount(dmask)));
    }
 
+   aco_ptr<MIMG_instruction> tex;
    if (instr->op == nir_texop_txs || instr->op == nir_texop_query_levels) {
       if (!has_lod)
          lod = bld.copy(bld.def(v1), Operand(0u));
@@ -9186,9 +9159,10 @@ void visit_tex(isel_context *ctx, nir_tex_instr *instr)
       if (tmp_dst.id() == dst.id() && div_by_6)
          tmp_dst = bld.tmp(tmp_dst.regClass());
 
-      MIMG_instruction *tex = emit_mimg(bld, aco_opcode::image_get_resinfo,
-                                        Definition(tmp_dst), resource, Operand(s4),
-                                        std::vector<Temp>{lod});
+      tex.reset(create_instruction<MIMG_instruction>(aco_opcode::image_get_resinfo, Format::MIMG, 3, 1));
+      tex->operands[0] = Operand(resource);
+      tex->operands[1] = Operand(s4); /* no sampler */
+      tex->operands[2] = Operand(as_vgpr(ctx,lod));
       if (ctx->options->chip_class == GFX9 &&
           instr->op == nir_texop_txs &&
           instr->sampler_dim == GLSL_SAMPLER_DIM_1D &&
@@ -9200,7 +9174,9 @@ void visit_tex(isel_context *ctx, nir_tex_instr *instr)
          tex->dmask = dmask;
       }
       tex->da = da;
+      tex->definitions[0] = Definition(tmp_dst);
       tex->dim = dim;
+      ctx->block->instructions.emplace_back(std::move(tex));
 
       if (div_by_6) {
          /* divide 3rd value by 6 by multiplying with magic number */
@@ -9223,14 +9199,16 @@ void visit_tex(isel_context *ctx, nir_tex_instr *instr)
    Temp tg4_compare_cube_wa64 = Temp();
 
    if (tg4_integer_workarounds) {
-      Temp tg4_lod = bld.copy(bld.def(v1), Operand(0u));
-      Temp size = bld.tmp(v2);
-      MIMG_instruction *tex = emit_mimg(bld, aco_opcode::image_get_resinfo,
-                                        Definition(size), resource, Operand(s4),
-                                        std::vector<Temp>{tg4_lod});
+      tex.reset(create_instruction<MIMG_instruction>(aco_opcode::image_get_resinfo, Format::MIMG, 3, 1));
+      tex->operands[0] = Operand(resource);
+      tex->operands[1] = Operand(s4); /* no sampler */
+      tex->operands[2] = bld.copy(bld.def(v1), Operand(0u));
       tex->dim = dim;
       tex->dmask = 0x3;
       tex->da = da;
+      Temp size = bld.tmp(v2);
+      tex->definitions[0] = Definition(size);
+      ctx->block->instructions.emplace_back(std::move(tex));
       emit_split_vector(ctx, size, size.size());
 
       Temp half_texel[2];
@@ -9353,6 +9331,13 @@ void visit_tex(isel_context *ctx, nir_tex_instr *instr)
    if (has_clamped_lod)
       args.emplace_back(clamped_lod);
 
+   Temp arg = bld.tmp(RegClass(RegType::vgpr, args.size()));
+   aco_ptr<Instruction> vec{create_instruction<Pseudo_instruction>(aco_opcode::p_create_vector, Format::PSEUDO, args.size(), 1)};
+   vec->definitions[0] = Definition(arg);
+   for (unsigned i = 0; i < args.size(); i++)
+      vec->operands[i] = Operand(args[i]);
+   ctx->block->instructions.emplace_back(std::move(vec));
+
 
    if (instr->op == nir_texop_txf ||
        instr->op == nir_texop_txf_ms ||
@@ -9360,14 +9345,19 @@ void visit_tex(isel_context *ctx, nir_tex_instr *instr)
        instr->op == nir_texop_fragment_fetch ||
        instr->op == nir_texop_fragment_mask_fetch) {
       aco_opcode op = level_zero || instr->sampler_dim == GLSL_SAMPLER_DIM_MS || instr->sampler_dim == GLSL_SAMPLER_DIM_SUBPASS_MS ? aco_opcode::image_load : aco_opcode::image_load_mip;
-      Operand vdata = instr->is_sparse ? emit_tfe_init(bld, tmp_dst) : Operand(v1);
-      MIMG_instruction *tex = emit_mimg(bld, op, Definition(tmp_dst), resource,
-                                        Operand(s4), args, 0, vdata);
+      tex.reset(create_instruction<MIMG_instruction>(op, Format::MIMG, 3 + instr->is_sparse, 1));
+      tex->operands[0] = Operand(resource);
+      tex->operands[1] = Operand(s4); /* no sampler */
+      tex->operands[2] = Operand(arg);
       tex->dim = dim;
       tex->dmask = dmask & 0xf;
       tex->unrm = true;
       tex->da = da;
       tex->tfe = instr->is_sparse;
+      tex->definitions[0] = Definition(tmp_dst);
+      if (tex->tfe)
+         tex->operands[3] = emit_tfe_init(bld, tmp_dst);
+      ctx->block->instructions.emplace_back(std::move(tex));
 
       if (instr->op == nir_texop_samples_identical) {
          assert(dmask == 1 && dst.regClass() == bld.lm);
@@ -9490,19 +9480,27 @@ void visit_tex(isel_context *ctx, nir_tex_instr *instr)
       opcode = aco_opcode::image_get_lod;
    }
 
-   bool implicit_derivs = bld.program->stage == fragment_fs &&
-                          !has_derivs && !has_lod && !level_zero &&
-                          instr->sampler_dim != GLSL_SAMPLER_DIM_MS &&
-                          instr->sampler_dim != GLSL_SAMPLER_DIM_SUBPASS_MS;
+   /* we don't need the bias, sample index, compare value or offset to be
+    * computed in WQM but if the p_create_vector copies the coordinates, then it
+    * needs to be in WQM */
+   if (ctx->stage == fragment_fs &&
+       !has_derivs && !has_lod && !level_zero &&
+       instr->sampler_dim != GLSL_SAMPLER_DIM_MS &&
+       instr->sampler_dim != GLSL_SAMPLER_DIM_SUBPASS_MS)
+      arg = emit_wqm(ctx, arg, bld.tmp(arg.regClass()), true);
 
-   Operand vdata = instr->is_sparse ? emit_tfe_init(bld, tmp_dst) : Operand(v1);
-   unsigned num_wqm_coords = implicit_derivs ? coords.size() : 0;
-   MIMG_instruction *tex = emit_mimg(bld, opcode, Definition(tmp_dst), resource,
-                                     Operand(sampler), args, num_wqm_coords, vdata);
+   tex.reset(create_instruction<MIMG_instruction>(opcode, Format::MIMG, 3 + instr->is_sparse, 1));
+   tex->operands[0] = Operand(resource);
+   tex->operands[1] = Operand(sampler);
+   tex->operands[2] = Operand(arg);
    tex->dim = dim;
    tex->dmask = dmask & 0xf;
    tex->da = da;
    tex->tfe = instr->is_sparse;
+   tex->definitions[0] = Definition(tmp_dst);
+   if (tex->tfe)
+      tex->operands[3] = emit_tfe_init(bld, tmp_dst);
+   ctx->block->instructions.emplace_back(std::move(tex));
 
    if (tg4_integer_cube_workaround) {
       assert(tmp_dst.id() != dst.id());
diff --git a/src/amd/compiler/aco_ir.cpp b/src/amd/compiler/aco_ir.cpp
index 676a047c8b4..83ef05e7ebd 100644
--- a/src/amd/compiler/aco_ir.cpp
+++ b/src/amd/compiler/aco_ir.cpp
@@ -99,12 +99,10 @@ void init_program(Program *program, Stage stage, struct radv_shader_info *info,
    program->has_16bank_lds = family == CHIP_KABINI || family == CHIP_STONEY;
 
    program->vgpr_limit = 256;
-   program->physical_vgprs = 256;
    program->vgpr_alloc_granule = 3;
 
    if (chip_class >= GFX10) {
       program->physical_sgprs = 2560; /* doesn't matter as long as it's at least 128 * 20 */
-      program->physical_vgprs = 512;
       program->sgpr_alloc_granule = 127;
       program->sgpr_limit = 106;
       if (chip_class >= GFX10_3)
diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index 5beca44ff2b..69a9d977595 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -1290,8 +1290,8 @@ static_assert(sizeof(MTBUF_instruction) == sizeof(Instruction) + 8, "Unexpected
  * Vector Memory Image Instructions
  * Operand(0) SRSRC - Scalar GPR that specifies the resource constant.
  * Operand(1): SSAMP - Scalar GPR that specifies sampler constant.
- * Operand(2): VDATA - Vector GPR for write data or zero if TFE/LWE=1.
- * Operand(3): VADDR - Address source. Can carry an offset or an index.
+ * Operand(2): VADDR - Address source. Can carry an offset or an index.
+ * Operand(3): VDATA - Vector GPR for write data or zero if TFE/LWE=1.
  * Definition(0): VDATA - Vector GPR for read result.
  *
  */
@@ -1735,7 +1735,6 @@ public:
    uint16_t vgpr_limit;
    uint16_t sgpr_limit;
    uint16_t physical_sgprs;
-   uint16_t physical_vgprs;
    uint16_t sgpr_alloc_granule; /* minus one. must be power of two */
    uint16_t vgpr_alloc_granule; /* minus one. must be power of two */
    unsigned workgroup_size; /* if known; otherwise UINT_MAX */
diff --git a/src/amd/compiler/aco_live_var_analysis.cpp b/src/amd/compiler/aco_live_var_analysis.cpp
index 1c041c9dbbb..bc713a1a188 100644
--- a/src/amd/compiler/aco_live_var_analysis.cpp
+++ b/src/amd/compiler/aco_live_var_analysis.cpp
@@ -314,7 +314,7 @@ uint16_t get_addr_sgpr_from_waves(Program *program, uint16_t max_waves)
 
 uint16_t get_addr_vgpr_from_waves(Program *program, uint16_t max_waves)
 {
-    uint16_t vgprs = program->physical_vgprs / max_waves & ~program->vgpr_alloc_granule;
+    uint16_t vgprs = 256 / max_waves & ~program->vgpr_alloc_granule;
     return std::min(vgprs, program->vgpr_limit);
 }
 
@@ -325,7 +325,7 @@ void calc_min_waves(Program* program)
    if (program->wave_size == 32)
       waves_per_workgroup = DIV_ROUND_UP(waves_per_workgroup, 2);
 
-   unsigned simd_per_cu = program->chip_class >= GFX10 ? 2 : 4;
+   unsigned simd_per_cu = 4; /* TODO: different on Navi */
    bool wgp = program->chip_class >= GFX10; /* assume WGP is used on Navi */
    unsigned simd_per_cu_wgp = wgp ? simd_per_cu * 2 : simd_per_cu;
 
@@ -334,12 +334,11 @@ void calc_min_waves(Program* program)
 
 void update_vgpr_sgpr_demand(Program* program, const RegisterDemand new_demand)
 {
-   unsigned max_waves_per_simd = program->chip_class == GFX10 ? 20 : 10;
-   if (program->chip_class >= GFX10_3)
-      max_waves_per_simd = 16;
-   else if (program->family >= CHIP_POLARIS10 && program->family <= CHIP_VEGAM)
+   /* TODO: max_waves_per_simd, simd_per_cu and the number of physical vgprs for Navi */
+   unsigned max_waves_per_simd = 10;
+   if ((program->family >= CHIP_POLARIS10 && program->family <= CHIP_VEGAM) || program->chip_class >= GFX10_3)
       max_waves_per_simd = 8;
-   unsigned simd_per_cu = program->chip_class >= GFX10 ? 2 : 4;
+   unsigned simd_per_cu = 4;
 
    bool wgp = program->chip_class >= GFX10; /* assume WGP is used on Navi */
    unsigned simd_per_cu_wgp = wgp ? simd_per_cu * 2 : simd_per_cu;
@@ -351,7 +350,7 @@ void update_vgpr_sgpr_demand(Program* program, const RegisterDemand new_demand)
       program->max_reg_demand = new_demand;
    } else {
       program->num_waves = program->physical_sgprs / get_sgpr_alloc(program, new_demand.sgpr);
-      program->num_waves = std::min<uint16_t>(program->num_waves, program->physical_vgprs / get_vgpr_alloc(program, new_demand.vgpr));
+      program->num_waves = std::min<uint16_t>(program->num_waves, 256 / get_vgpr_alloc(program, new_demand.vgpr));
       program->max_waves = max_waves_per_simd;
 
       /* adjust max_waves for workgroup and LDS limits */
diff --git a/src/amd/compiler/aco_lower_to_hw_instr.cpp b/src/amd/compiler/aco_lower_to_hw_instr.cpp
index 56da58bd333..a4e2d5b6404 100644
--- a/src/amd/compiler/aco_lower_to_hw_instr.cpp
+++ b/src/amd/compiler/aco_lower_to_hw_instr.cpp
@@ -1023,53 +1023,49 @@ void copy_constant(lower_context *ctx, Builder& bld, Definition dst, Operand op)
       }
    } else if (dst.regClass() == v1) {
       bld.vop1(aco_opcode::v_mov_b32, dst, op);
-   } else {
-      assert(dst.regClass() == v1b || dst.regClass() == v2b);
-
-      if (dst.regClass() == v1b && ctx->program->chip_class >= GFX9) {
-         uint8_t val = op.constantValue();
-         Operand op32((uint32_t)val | (val & 0x80u ? 0xffffff00u : 0u));
-         if (op32.isLiteral()) {
-            uint32_t a = (uint32_t)int8_mul_table[val * 2];
-            uint32_t b = (uint32_t)int8_mul_table[val * 2 + 1];
-            bld.vop2_sdwa(aco_opcode::v_mul_u32_u24, dst,
-                          Operand(a | (a & 0x80u ? 0xffffff00u : 0x0u)),
-                          Operand(b | (b & 0x80u ? 0xffffff00u : 0x0u)));
-         } else {
-            bld.vop1_sdwa(aco_opcode::v_mov_b32, dst, op32);
-         }
-      } else if (dst.regClass() == v2b && ctx->program->chip_class >= GFX9 && !op.isLiteral()) {
-         if (op.constantValue() >= 0xfff0 || op.constantValue() <= 64) {
-            /* use v_mov_b32 to avoid possible issues with denormal flushing or
-             * NaN. v_add_f16 is still needed for float constants. */
-            uint32_t val32 = (int32_t)(int16_t)op.constantValue();
-            bld.vop1_sdwa(aco_opcode::v_mov_b32, dst, Operand(val32));
-         } else {
-            bld.vop2_sdwa(aco_opcode::v_add_f16, dst, op, Operand(0u));
-         }
-      } else if (dst.regClass() == v2b && ctx->program->chip_class >= GFX10 &&
-                 (ctx->block->fp_mode.denorm16_64 & fp_denorm_keep_in)) {
-         if (dst.physReg().byte() == 2) {
-            Operand def_lo(dst.physReg().advance(-2), v2b);
-            Instruction* instr = bld.vop3(aco_opcode::v_pack_b32_f16, dst, def_lo, op);
-            static_cast<VOP3A_instruction*>(instr)->opsel = 0;
-         } else {
-            assert(dst.physReg().byte() == 0);
-            Operand def_hi(dst.physReg().advance(2), v2b);
-            Instruction* instr = bld.vop3(aco_opcode::v_pack_b32_f16, dst, op, def_hi);
-            static_cast<VOP3A_instruction*>(instr)->opsel = 2;
-         }
+   } else if (dst.regClass() == v1b) {
+      assert(ctx->program->chip_class >= GFX8);
+      uint8_t val = op.constantValue();
+      Operand op32((uint32_t)val | (val & 0x80u ? 0xffffff00u : 0u));
+      aco_ptr<SDWA_instruction> sdwa;
+      if (op32.isLiteral()) {
+         uint32_t a = (uint32_t)int8_mul_table[val * 2];
+         uint32_t b = (uint32_t)int8_mul_table[val * 2 + 1];
+         bld.vop2_sdwa(aco_opcode::v_mul_u32_u24, dst,
+                       Operand(a | (a & 0x80u ? 0xffffff00u : 0x0u)),
+                       Operand(b | (b & 0x80u ? 0xffffff00u : 0x0u)));
+      } else {
+         bld.vop1_sdwa(aco_opcode::v_mov_b32, dst, op32);
+      }
+   } else if (dst.regClass() == v2b && op.isConstant() && !op.isLiteral()) {
+      assert(ctx->program->chip_class >= GFX8);
+      if (op.constantValue() >= 0xfff0 || op.constantValue() <= 64) {
+         /* use v_mov_b32 to avoid possible issues with denormal flushing or
+          * NaN. v_add_f16 is still needed for float constants. */
+         uint32_t val32 = (int32_t)(int16_t)op.constantValue();
+         bld.vop1_sdwa(aco_opcode::v_mov_b32, dst, Operand(val32));
       } else {
-         uint32_t offset = dst.physReg().byte() * 8u;
-         uint32_t mask = ((1u << (dst.bytes() * 8)) - 1) << offset;
-         uint32_t val = (op.constantValue() << offset) & mask;
+         bld.vop2_sdwa(aco_opcode::v_add_f16, dst, op, Operand(0u));
+      }
+   } else if (dst.regClass() == v2b && op.isLiteral()) {
+      if (ctx->program->chip_class < GFX10 || !(ctx->block->fp_mode.denorm16_64 & fp_denorm_keep_in)) {
+         unsigned offset = dst.physReg().byte() * 8u;
          dst = Definition(PhysReg(dst.physReg().reg()), v1);
          Operand def_op(dst.physReg(), v1);
-         if (val != mask)
-            bld.vop2(aco_opcode::v_and_b32, dst, Operand(~mask), def_op);
-         if (val != 0)
-            bld.vop2(aco_opcode::v_or_b32, dst, Operand(val), def_op);
+         bld.vop2(aco_opcode::v_and_b32, dst, Operand(~(0xffffu << offset)), def_op);
+         bld.vop2(aco_opcode::v_or_b32, dst, Operand(op.constantValue() << offset), def_op);
+      } else if (dst.physReg().byte() == 2) {
+         Operand def_lo(dst.physReg().advance(-2), v2b);
+         Instruction* instr = bld.vop3(aco_opcode::v_pack_b32_f16, dst, def_lo, op);
+         static_cast<VOP3A_instruction*>(instr)->opsel = 0;
+      } else {
+         assert(dst.physReg().byte() == 0);
+         Operand def_hi(dst.physReg().advance(2), v2b);
+         Instruction* instr = bld.vop3(aco_opcode::v_pack_b32_f16, dst, op, def_hi);
+         static_cast<VOP3A_instruction*>(instr)->opsel = 2;
       }
+   } else {
+      unreachable("unsupported copy");
    }
 }
 
diff --git a/src/amd/compiler/aco_optimizer.cpp b/src/amd/compiler/aco_optimizer.cpp
index b1e786408a1..646f06e9793 100644
--- a/src/amd/compiler/aco_optimizer.cpp
+++ b/src/amd/compiler/aco_optimizer.cpp
@@ -627,67 +627,6 @@ bool can_use_VOP3(opt_ctx& ctx, const aco_ptr<Instruction>& instr)
           instr->opcode != aco_opcode::v_readfirstlane_b32;
 }
 
-bool pseudo_propagate_temp(opt_ctx& ctx, aco_ptr<Instruction>& instr,
-                           Temp temp, unsigned index)
-{
-   if (instr->definitions.empty())
-      return false;
-
-   const bool vgpr = instr->opcode == aco_opcode::p_as_uniform ||
-                     std::all_of(instr->definitions.begin(), instr->definitions.end(),
-                                 [] (const Definition& def) { return def.regClass().type() == RegType::vgpr;});
-
-   /* don't propagate VGPRs into SGPR instructions */
-   if (temp.type() == RegType::vgpr && !vgpr)
-      return false;
-
-   bool can_accept_sgpr = ctx.program->chip_class >= GFX9 ||
-                          std::none_of(instr->definitions.begin(), instr->definitions.end(),
-                                       [] (const Definition& def) { return def.regClass().is_subdword();});
-
-   switch (instr->opcode) {
-   case aco_opcode::p_phi:
-   case aco_opcode::p_linear_phi:
-   case aco_opcode::p_parallelcopy:
-   case aco_opcode::p_create_vector:
-      if (temp.bytes() != instr->operands[index].bytes())
-         return false;
-      break;
-   case aco_opcode::p_extract_vector:
-      if (temp.type() == RegType::sgpr && !can_accept_sgpr)
-         return false;
-      break;
-   case aco_opcode::p_split_vector: {
-      if (temp.type() == RegType::sgpr && !can_accept_sgpr)
-         return false;
-      /* don't increase the vector size */
-      if (temp.bytes() > instr->operands[index].bytes())
-         return false;
-      /* We can decrease the vector size as smaller temporaries are only
-       * propagated by p_as_uniform instructions.
-       * If this propagation leads to invalid IR or hits the assertion below,
-       * it means that some undefined bytes within a dword are begin accessed
-       * and a bug in instruction_selection is likely. */
-      int decrease = instr->operands[index].bytes() - temp.bytes();
-      while (decrease > 0) {
-         decrease -= instr->definitions.back().bytes();
-         instr->definitions.pop_back();
-      }
-      assert(decrease == 0);
-      break;
-   }
-   case aco_opcode::p_as_uniform:
-      if (temp.regClass() == instr->definitions[0].regClass())
-         instr->opcode = aco_opcode::p_parallelcopy;
-      break;
-   default:
-      return false;
-   }
-
-   instr->operands[index].setTemp(temp);
-   return true;
-}
-
 bool can_apply_sgprs(opt_ctx& ctx, aco_ptr<Instruction>& instr)
 {
    if (instr->isSDWA() && ctx.program->chip_class < GFX9)
@@ -900,21 +839,47 @@ void label_instruction(opt_ctx &ctx, Block& block, aco_ptr<Instruction>& instr)
       if (info.is_undefined() && is_phi(instr))
          instr->operands[i] = Operand(instr->operands[i].regClass());
       /* propagate reg->reg of same type */
-      while (info.is_temp() && info.temp.regClass() == instr->operands[i].getTemp().regClass()) {
+      if (info.is_temp() && info.temp.regClass() == instr->operands[i].getTemp().regClass()) {
          instr->operands[i].setTemp(ctx.info[instr->operands[i].tempId()].temp);
          info = ctx.info[info.temp.id()];
       }
 
-      /* PSEUDO: propagate temporaries */
-      if (instr->format == Format::PSEUDO) {
-         while (info.is_temp()) {
-            pseudo_propagate_temp(ctx, instr, info.temp, i);
-            info = ctx.info[info.temp.id()];
-         }
-      }
-
       /* SALU / PSEUDO: propagate inline constants */
       if (instr->isSALU() || instr->format == Format::PSEUDO) {
+         bool is_subdword = false;
+         // TODO: optimize SGPR propagation for subdword pseudo instructions on gfx9+
+         if (instr->format == Format::PSEUDO) {
+            is_subdword = std::any_of(instr->definitions.begin(), instr->definitions.end(),
+                                      [] (const Definition& def) { return def.regClass().is_subdword();});
+            is_subdword = is_subdword || std::any_of(instr->operands.begin(), instr->operands.end(),
+                                                     [] (const Operand& op) { return op.bytes() % 4;});
+            if (is_subdword && ctx.program->chip_class < GFX9)
+               continue;
+         }
+
+         if (info.is_temp() && info.temp.type() == RegType::sgpr) {
+            instr->operands[i].setTemp(info.temp);
+            info = ctx.info[info.temp.id()];
+         } else if (info.is_temp() && info.temp.type() == RegType::vgpr) {
+            /* propagate vgpr if it can take it */
+            switch (instr->opcode) {
+            case aco_opcode::p_create_vector:
+            case aco_opcode::p_split_vector:
+            case aco_opcode::p_extract_vector:
+            case aco_opcode::p_phi:
+            case aco_opcode::p_parallelcopy: {
+               const bool all_vgpr = std::none_of(instr->definitions.begin(), instr->definitions.end(),
+                                                  [] (const Definition& def) { return def.getTemp().type() != RegType::vgpr;});
+               if (all_vgpr) {
+                  instr->operands[i] = Operand(info.temp);
+                  info = ctx.info[info.temp.id()];
+               }
+               break;
+            }
+            default:
+               break;
+            }
+         }
          unsigned bits = get_operand_size(instr, i);
          if ((info.is_constant(bits) || (info.is_literal(bits) && instr->format == Format::PSEUDO)) &&
              !instr->operands[i].isFixed() && alu_can_accept_constant(instr->opcode, i)) {
@@ -1146,16 +1111,28 @@ void label_instruction(opt_ctx &ctx, Block& block, aco_ptr<Instruction>& instr)
       }
 
       /* expand vector operands */
+      bool accept_subdword = instr->definitions[0].regClass().type() == RegType::vgpr &&
+                             std::all_of(instr->operands.begin(), instr->operands.end(),
+                             [&] (const Operand& op) { return !op.isLiteral() &&
+                             (ctx.program->chip_class >= GFX9 || (op.hasRegClass() && op.regClass().type() == RegType::vgpr));});
+
       std::vector<Operand> ops;
-      unsigned offset = 0;
       for (const Operand& op : instr->operands) {
-         /* ensure that any expanded operands are properly aligned */
-         bool aligned = offset % 4 == 0 || op.bytes() < 4;
-         offset += op.bytes();
-         if (aligned && op.isTemp() && ctx.info[op.tempId()].is_vec()) {
-            Instruction* vec = ctx.info[op.tempId()].instr;
-            for (const Operand& vec_op : vec->operands)
+         if (!op.isTemp() || !ctx.info[op.tempId()].is_vec()) {
+            ops.emplace_back(op);
+            continue;
+         }
+         Instruction* vec = ctx.info[op.tempId()].instr;
+         bool is_subdword = std::any_of(vec->operands.begin(), vec->operands.end(),
+                               [&] (const Operand& op2) { return op2.bytes() % 4; } );
+
+         if (accept_subdword || !is_subdword) {
+            for (const Operand& vec_op : vec->operands) {
                ops.emplace_back(vec_op);
+               if (op.isLiteral() || (ctx.program->chip_class <= GFX8 &&
+                                      (!op.hasRegClass() || op.regClass().type() == RegType::sgpr)))
+                  accept_subdword = false;
+            }
          } else {
             ops.emplace_back(op);
          }
@@ -1168,7 +1145,7 @@ void label_instruction(opt_ctx &ctx, Block& block, aco_ptr<Instruction>& instr)
          instr.reset(create_instruction<Pseudo_instruction>(aco_opcode::p_create_vector, Format::PSEUDO, ops.size(), 1));
          for (unsigned i = 0; i < ops.size(); i++) {
             if (ops[i].isTemp() && ctx.info[ops[i].tempId()].is_temp() &&
-                ops[i].regClass() == ctx.info[ops[i].tempId()].temp.regClass())
+                ctx.info[ops[i].tempId()].temp.type() == def.regClass().type())
                ops[i].setTemp(ctx.info[ops[i].tempId()].temp);
             instr->operands[i] = ops[i];
          }
@@ -1224,37 +1201,42 @@ void label_instruction(opt_ctx &ctx, Block& block, aco_ptr<Instruction>& instr)
       const unsigned index = instr->operands[1].constantValue();
       const unsigned dst_offset = index * instr->definitions[0].bytes();
 
-      if (info.is_vec()) {
-         /* check if we index directly into a vector element */
-         Instruction* vec = info.instr;
-         unsigned offset = 0;
+      if (info.is_constant_or_literal(32)) {
+         uint32_t mask = u_bit_consecutive(0, instr->definitions[0].bytes() * 8u);
+         ctx.info[instr->definitions[0].tempId()].set_constant(ctx.program->chip_class, (info.val >> (dst_offset * 8u)) & mask);
+         break;
+      } else if (!info.is_vec()) {
+         break;
+      }
+
+      /* check if we index directly into a vector element */
+      Instruction* vec = info.instr;
+      unsigned offset = 0;
 
-         for (const Operand& op : vec->operands) {
-            if (offset < dst_offset) {
-               offset += op.bytes();
-               continue;
-            } else if (offset != dst_offset || op.bytes() != instr->definitions[0].bytes()) {
-               break;
-            }
-            instr->operands[0] = op;
+      for (const Operand& op : vec->operands) {
+         if (offset < dst_offset) {
+            offset += op.bytes();
+            continue;
+         } else if (offset != dst_offset || op.bytes() != instr->definitions[0].bytes()) {
             break;
          }
-      } else if (info.is_constant_or_literal(32)) {
-         /* propagate constants */
-         uint32_t mask = u_bit_consecutive(0, instr->definitions[0].bytes() * 8u);
-         uint32_t val = (info.val >> (dst_offset * 8u)) & mask;
-         instr->operands[0] = Operand::get_const(ctx.program->chip_class, val, instr->definitions[0].bytes());;
-      } else if (index == 0 && instr->operands[0].size() == instr->definitions[0].size()) {
-         ctx.info[instr->definitions[0].tempId()].set_temp(instr->operands[0].getTemp());
-      }
 
-      if (instr->operands[0].bytes() != instr->definitions[0].bytes())
-         break;
+         /* convert this extract into a copy instruction */
+         instr->opcode = aco_opcode::p_parallelcopy;
+         instr->operands.pop_back();
+         instr->operands[0] = op;
 
-      /* convert this extract into a copy instruction */
-      instr->opcode = aco_opcode::p_parallelcopy;
-      instr->operands.pop_back();
-      FALLTHROUGH;
+         if (op.isConstant()) {
+            ctx.info[instr->definitions[0].tempId()].set_constant(ctx.program->chip_class, op.constantValue64());
+         } else if (op.isUndefined()) {
+            ctx.info[instr->definitions[0].tempId()].set_undefined();
+         } else {
+            assert(op.isTemp());
+            ctx.info[instr->definitions[0].tempId()].set_temp(op.getTemp());
+         }
+         break;
+      }
+      break;
    }
    case aco_opcode::p_parallelcopy: /* propagate */
       if (instr->operands[0].isTemp() && ctx.info[instr->operands[0].tempId()].is_vec() &&
@@ -1406,6 +1388,7 @@ void label_instruction(opt_ctx &ctx, Block& block, aco_ptr<Instruction>& instr)
           instr->operands[1].isTemp() && ctx.info[instr->operands[1].tempId()].is_vcc())
          ctx.info[instr->definitions[0].tempId()].set_temp(ctx.info[instr->operands[1].tempId()].temp);
       break;
+   case aco_opcode::p_phi:
    case aco_opcode::p_linear_phi: {
       /* lower_bool_phis() can create phis like this */
       bool all_same_temp = instr->operands[0].isTemp();
diff --git a/src/amd/compiler/aco_register_allocation.cpp b/src/amd/compiler/aco_register_allocation.cpp
index 8b59317c51b..2459e4489fb 100644
--- a/src/amd/compiler/aco_register_allocation.cpp
+++ b/src/amd/compiler/aco_register_allocation.cpp
@@ -43,7 +43,7 @@ struct ra_ctx;
 unsigned get_subdword_operand_stride(chip_class chip, const aco_ptr<Instruction>& instr, unsigned idx, RegClass rc);
 void add_subdword_operand(ra_ctx& ctx, aco_ptr<Instruction>& instr, unsigned idx, unsigned byte, RegClass rc);
 std::pair<unsigned, unsigned> get_subdword_definition_info(Program *program, const aco_ptr<Instruction>& instr, RegClass rc);
-void add_subdword_definition(Program *program, aco_ptr<Instruction>& instr, unsigned idx, PhysReg reg);
+void add_subdword_definition(Program *program, aco_ptr<Instruction>& instr, unsigned idx, PhysReg reg, bool is_partial);
 
 struct assignment {
    PhysReg reg;
@@ -601,16 +601,17 @@ std::pair<unsigned, unsigned> get_subdword_definition_info(Program *program, con
    return std::make_pair(4u, bytes_written);
 }
 
-void add_subdword_definition(Program *program, aco_ptr<Instruction>& instr, unsigned idx, PhysReg reg)
+void add_subdword_definition(Program *program, aco_ptr<Instruction>& instr, unsigned idx, PhysReg reg, bool is_partial)
 {
    RegClass rc = instr->definitions[idx].regClass();
    chip_class chip = program->chip_class;
 
+   instr->definitions[idx].setFixed(reg);
+
    if (instr->format == Format::PSEUDO) {
       return;
    } else if (can_use_SDWA(chip, instr)) {
-      unsigned def_size = instr_info.definition_size[(int)instr->opcode];
-      if (reg.byte() || chip < GFX10 || def_size > rc.bytes() * 8u)
+      if (reg.byte() || (is_partial && chip < GFX10))
          convert_to_SDWA(chip, instr);
       return;
    } else if (reg.byte() && rc.bytes() == 2 && can_use_opsel(chip, instr->opcode, -1, reg.byte() / 2)) {
@@ -1253,35 +1254,6 @@ void increase_register_file(ra_ctx& ctx, RegType type) {
    }
 }
 
-bool is_mimg_vaddr_intact(ra_ctx& ctx, RegisterFile& reg_file, Instruction *instr)
-{
-   PhysReg first{512};
-   for (unsigned i = 0; i < instr->operands.size() - 3u; i++) {
-      Operand op = instr->operands[i + 3];
-
-      if (ctx.assignments[op.tempId()].assigned) {
-         PhysReg reg = ctx.assignments[op.tempId()].reg;
-
-         if (first.reg() != 512 && reg != first.advance(i * 4))
-            return false; /* not at the best position */
-
-         if ((reg.reg() - 256) < i)
-            return false; /* no space for previous operands */
-
-         first = reg.advance(i * -4);
-      } else if (first.reg() != 512) {
-         /* If there's an unexpected temporary, this operand is unlikely to be
-          * placed in the best position.
-          */
-         unsigned id = reg_file.get_id(first.advance(i * 4));
-         if (id && id != op.tempId())
-            return false;
-      }
-   }
-
-   return true;
-}
-
 PhysReg get_reg(ra_ctx& ctx,
                 RegisterFile& reg_file,
                 Temp temp,
@@ -1313,42 +1285,35 @@ PhysReg get_reg(ra_ctx& ctx,
 
    if (ctx.vectors.find(temp.id()) != ctx.vectors.end()) {
       Instruction* vec = ctx.vectors[temp.id()];
-      unsigned first_operand = vec->format == Format::MIMG ? 3 : 0;
       unsigned byte_offset = 0;
-      for (unsigned i = first_operand; i < vec->operands.size(); i++) {
-         Operand& op = vec->operands[i];
+      for (const Operand& op : vec->operands) {
          if (op.isTemp() && op.tempId() == temp.id())
             break;
          else
             byte_offset += op.bytes();
       }
-
-      if (vec->format != Format::MIMG || is_mimg_vaddr_intact(ctx, reg_file, vec)) {
-         unsigned k = 0;
-         for (unsigned i = first_operand; i < vec->operands.size(); i++) {
-            Operand& op = vec->operands[i];
-            if (op.isTemp() &&
-                op.tempId() != temp.id() &&
-                op.getTemp().type() == temp.type() &&
-                ctx.assignments[op.tempId()].assigned) {
-               PhysReg reg = ctx.assignments[op.tempId()].reg;
-               reg.reg_b += (byte_offset - k);
-               if (get_reg_specified(ctx, reg_file, temp.regClass(), instr, reg))
-                  return reg;
-            }
-            k += op.bytes();
-         }
-
-         RegClass vec_rc = RegClass::get(temp.type(), k);
-         DefInfo info(ctx, ctx.pseudo_dummy, vec_rc, -1);
-         std::pair<PhysReg, bool> res = get_reg_simple(ctx, reg_file, info);
-         PhysReg reg = res.first;
-         if (res.second) {
-            reg.reg_b += byte_offset;
-            /* make sure to only use byte offset if the instruction supports it */
+      unsigned k = 0;
+      for (const Operand& op : vec->operands) {
+         if (op.isTemp() &&
+             op.tempId() != temp.id() &&
+             op.getTemp().type() == temp.type() &&
+             ctx.assignments[op.tempId()].assigned) {
+            PhysReg reg = ctx.assignments[op.tempId()].reg;
+            reg.reg_b += (byte_offset - k);
             if (get_reg_specified(ctx, reg_file, temp.regClass(), instr, reg))
                return reg;
          }
+         k += op.bytes();
+      }
+
+      DefInfo info(ctx, ctx.pseudo_dummy, vec->definitions[0].regClass(), -1);
+      std::pair<PhysReg, bool> res = get_reg_simple(ctx, reg_file, info);
+      PhysReg reg = res.first;
+      if (res.second) {
+         reg.reg_b += byte_offset;
+         /* make sure to only use byte offset if the instruction supports it */
+         if (get_reg_specified(ctx, reg_file, temp.regClass(), instr, reg))
+            return reg;
       }
    }
 
@@ -1866,9 +1831,6 @@ void register_allocation(Program *program, std::vector<IDSet>& live_out_per_bloc
                   if (op.isTemp() && op.isFirstKill() && op.getTemp().type() == instr->definitions[0].getTemp().type())
                      ctx.vectors[op.tempId()] = instr.get();
                }
-            } else if (instr->format == Format::MIMG && instr->operands.size() > 4) {
-               for (unsigned i = 3; i < instr->operands.size(); i++)
-                  ctx.vectors[instr->operands[i].tempId()] = instr.get();
             }
 
             if (instr->opcode == aco_opcode::p_split_vector && instr->operands[0].isFirstKillBeforeDef())
@@ -2238,8 +2200,8 @@ void register_allocation(Program *program, std::vector<IDSet>& live_out_per_bloc
             instr->definitions[0].setFixed(instr->operands[3].physReg());
          } else if (instr->format == Format::MIMG &&
                     instr->definitions.size() == 1 &&
-                    !instr->operands[2].isUndefined()) {
-            instr->definitions[0].setFixed(instr->operands[2].physReg());
+                    instr->operands.size() >= 4) {
+            instr->definitions[0].setFixed(instr->operands[3].physReg());
          }
 
          ctx.defs_done.reset();
@@ -2325,11 +2287,9 @@ void register_allocation(Program *program, std::vector<IDSet>& live_out_per_bloc
                Temp tmp = definition->getTemp();
                if (definition->regClass().is_subdword() && definition->bytes() < 4) {
                   PhysReg reg = get_reg(ctx, register_file, tmp, parallelcopy, instr);
-                  definition->setFixed(reg);
-                  if (reg.byte() || register_file.test(reg, 4)) {
-                     add_subdword_definition(program, instr, i, reg);
-                     definition = &instr->definitions[i]; /* add_subdword_definition can invalidate the reference */
-                  }
+                  bool partial = !(tmp.bytes() <= 4 && reg.byte() == 0 && !register_file.test(reg, 4));
+                  add_subdword_definition(program, instr, i, reg, partial);
+                  definition = &instr->definitions[i]; /* add_subdword_definition can invalidate the reference */
                } else {
                   definition->setFixed(get_reg(ctx, register_file, tmp, parallelcopy, instr));
                }
diff --git a/src/amd/compiler/aco_scheduler.cpp b/src/amd/compiler/aco_scheduler.cpp
index fc9be0e0212..0ae110f3209 100644
--- a/src/amd/compiler/aco_scheduler.cpp
+++ b/src/amd/compiler/aco_scheduler.cpp
@@ -902,25 +902,20 @@ void schedule_program(Program *program, live& live_vars)
    /* Allowing the scheduler to reduce the number of waves to as low as 5
     * improves performance of Thrones of Britannia significantly and doesn't
     * seem to hurt anything else. */
-   //TODO: account for possible uneven num_waves on GFX10+
-   unsigned wave_fac = program->physical_vgprs / 256;
-   if (program->num_waves <= 5 * wave_fac)
+   if (program->num_waves <= 5)
       ctx.num_waves = program->num_waves;
    else if (demand.vgpr >= 29)
-      ctx.num_waves = 5 * wave_fac;
+      ctx.num_waves = 5;
    else if (demand.vgpr >= 25)
-      ctx.num_waves = 6 * wave_fac;
+      ctx.num_waves = 6;
    else
-      ctx.num_waves = 7 * wave_fac;
+      ctx.num_waves = 7;
    ctx.num_waves = std::max<uint16_t>(ctx.num_waves, program->min_waves);
    ctx.num_waves = std::min<uint16_t>(ctx.num_waves, program->num_waves);
 
-   /* VMEM_MAX_MOVES and such assume pre-GFX10 wave count */
-   ctx.num_waves = std::max<uint16_t>(ctx.num_waves / wave_fac, 1);
-
    assert(ctx.num_waves > 0);
-   ctx.mv.max_registers = { int16_t(get_addr_vgpr_from_waves(program, ctx.num_waves * wave_fac) - 2),
-                            int16_t(get_addr_sgpr_from_waves(program, ctx.num_waves * wave_fac))};
+   ctx.mv.max_registers = { int16_t(get_addr_vgpr_from_waves(program, ctx.num_waves) - 2),
+                            int16_t(get_addr_sgpr_from_waves(program, ctx.num_waves))};
 
    for (Block& block : program->blocks)
       schedule_block(ctx, program, &block, live_vars);
diff --git a/src/amd/compiler/aco_validate.cpp b/src/amd/compiler/aco_validate.cpp
index b7f9fef2b36..bf45cc7cb0e 100644
--- a/src/amd/compiler/aco_validate.cpp
+++ b/src/amd/compiler/aco_validate.cpp
@@ -182,6 +182,10 @@ bool validate_ir(Program* program)
                      "SDWA can't be used with this opcode", instr.get());
             }
 
+            for (unsigned i = 0; i < MIN2(instr->operands.size(), 2); i++) {
+               if (instr->operands[i].hasRegClass() && instr->operands[i].regClass().is_subdword())
+                  check((sdwa->sel[i] & sdwa_asuint) == (sdwa_isra | instr->operands[i].bytes()), "Unexpected SDWA sel for sub-dword operand", instr.get());
+            }
             if (instr->definitions[0].regClass().is_subdword())
                check((sdwa->dst_sel & sdwa_asuint) == (sdwa_isra | instr->definitions[0].bytes()), "Unexpected SDWA sel for sub-dword definition", instr.get());
          }
@@ -207,7 +211,7 @@ bool validate_ir(Program* program)
                bool can_be_undef = is_phi(instr) || instr->format == Format::EXP ||
                                    instr->format == Format::PSEUDO_REDUCTION ||
                                    instr->opcode == aco_opcode::p_create_vector ||
-                                   (flat && i == 1) || (instr->format == Format::MIMG && (i == 1 || i == 2)) ||
+                                   (flat && i == 1) || (instr->format == Format::MIMG && i == 1) ||
                                    ((instr->format == Format::MUBUF || instr->format == Format::MTBUF) && i == 1);
                check(can_be_undef, "Undefs can only be used in certain operands", instr.get());
             } else {
@@ -324,10 +328,37 @@ bool validate_ir(Program* program)
 
          switch (instr->format) {
          case Format::PSEUDO: {
+            if (instr->opcode == aco_opcode::p_parallelcopy) {
+               for (unsigned i = 0; i < instr->operands.size(); i++) {
+                  if (!instr->definitions[i].regClass().is_subdword())
+                     continue;
+                  Operand op = instr->operands[i];
+                  check(program->chip_class >= GFX9 || !op.isLiteral(), "Sub-dword copies cannot take literals", instr.get());
+                  if (op.isConstant() || (op.hasRegClass() && op.regClass().type() == RegType::sgpr))
+                     check(program->chip_class >= GFX9, "Sub-dword pseudo instructions can only take constants or SGPRs on GFX9+", instr.get());
+               }
+            } else {
+               bool is_subdword = false;
+               bool has_const_sgpr = false;
+               bool has_literal = false;
+               for (Definition def : instr->definitions)
+                  is_subdword |= def.regClass().is_subdword();
+               for (unsigned i = 0; i < instr->operands.size(); i++) {
+                  if (instr->opcode == aco_opcode::p_extract_vector && i == 1)
+                     continue;
+                  Operand op = instr->operands[i];
+                  is_subdword |= op.hasRegClass() && op.regClass().is_subdword();
+                  has_const_sgpr |= op.isConstant() || (op.hasRegClass() && op.regClass().type() == RegType::sgpr);
+                  has_literal |= op.isLiteral();
+               }
+
+               check(!is_subdword || !has_const_sgpr || program->chip_class >= GFX9 || instr->opcode == aco_opcode::p_unit_test,
+                     "Sub-dword pseudo instructions can only take constants or SGPRs on GFX9+", instr.get());
+            }
+
             if (instr->opcode == aco_opcode::p_create_vector) {
                unsigned size = 0;
                for (const Operand& op : instr->operands) {
-                  check(op.bytes() < 4 || size % 4 == 0, "Operand is not aligned", instr.get());
                   size += op.bytes();
                }
                check(size == instr->definitions[0].bytes(), "Definition size does not match operand sizes", instr.get());
@@ -342,8 +373,6 @@ bool validate_ir(Program* program)
                check((instr->operands[1].constantValue() + 1) * instr->definitions[0].bytes() <= instr->operands[0].bytes(), "Index out of range", instr.get());
                check(instr->definitions[0].getTemp().type() == RegType::vgpr || instr->operands[0].regClass().type() == RegType::sgpr,
                      "Cannot extract SGPR value from VGPR vector", instr.get());
-               check(program->chip_class >= GFX9 || !instr->definitions[0].regClass().is_subdword() ||
-                     instr->operands[0].regClass().type() == RegType::vgpr, "Cannot extract subdword from SGPR before GFX9+", instr.get());
             } else if (instr->opcode == aco_opcode::p_split_vector) {
                check(instr->operands[0].isTemp(), "Operand must be a temporary", instr.get());
                unsigned size = 0;
@@ -354,14 +383,10 @@ bool validate_ir(Program* program)
                if (instr->operands[0].getTemp().type() == RegType::vgpr) {
                   for (const Definition& def : instr->definitions)
                      check(def.regClass().type() == RegType::vgpr, "Wrong Definition type for VGPR split_vector", instr.get());
-               } else {
-                  for (const Definition& def : instr->definitions)
-                     check(program->chip_class >= GFX9 || !def.regClass().is_subdword(), "Cannot split SGPR into subdword VGPRs before GFX9+", instr.get());
                }
             } else if (instr->opcode == aco_opcode::p_parallelcopy) {
                check(instr->definitions.size() == instr->operands.size(), "Number of Operands does not match number of Definitions", instr.get());
                for (unsigned i = 0; i < instr->operands.size(); i++) {
-                  check(instr->definitions[i].bytes() == instr->operands[i].bytes(), "Operand and Definition size must match", instr.get());
                   if (instr->operands[i].isTemp())
                      check((instr->definitions[i].getTemp().type() == instr->operands[i].regClass().type()) ||
                            (instr->definitions[i].getTemp().type() == RegType::vgpr && instr->operands[i].regClass().type() == RegType::sgpr),
@@ -411,26 +436,20 @@ bool validate_ir(Program* program)
             break;
          }
          case Format::MIMG: {
-            check(instr->operands.size() >= 4, "MIMG instructions must have at least 4 operands", instr.get());
+            check(instr->operands.size() >= 3, "MIMG instructions must have 3 or 4 operands", instr.get());
+            check(instr->operands.size() <= 4, "MIMG instructions must have 3 or 4 operands", instr.get());
             check(instr->operands[0].hasRegClass() && (instr->operands[0].regClass() == s4 || instr->operands[0].regClass() == s8),
                   "MIMG operands[0] (resource constant) must be in 4 or 8 SGPRs", instr.get());
             if (instr->operands[1].hasRegClass())
                check(instr->operands[1].regClass() == s4, "MIMG operands[1] (sampler constant) must be 4 SGPRs", instr.get());
-            if (!instr->operands[2].isUndefined()) {
+            if (instr->operands.size() >= 4) {
                bool is_cmpswap = instr->opcode == aco_opcode::image_atomic_cmpswap ||
                                  instr->opcode == aco_opcode::image_atomic_fcmpswap;
-               check(instr->definitions.empty() || (instr->definitions[0].regClass() == instr->operands[2].regClass() || is_cmpswap),
-                     "MIMG operands[2] (VDATA) must be the same as definitions[0] for atomics and TFE/LWE loads", instr.get());
-            }
-            check(instr->operands.size() == 4 || program->chip_class >= GFX10, "NSA is only supported on GFX10+", instr.get());
-            for (unsigned i = 3; i < instr->operands.size(); i++) {
-               if (instr->operands.size() == 4) {
-                  check(instr->operands[i].hasRegClass() && instr->operands[i].regClass().type() == RegType::vgpr,
-                        "MIMG operands[3] (VADDR) must be VGPR", instr.get());
-               } else {
-                  check(instr->operands[i].regClass() == v1, "MIMG VADDR must be v1 if NSA is used", instr.get());
-               }
+               check(instr->definitions.empty() || (instr->definitions[0].regClass() == instr->operands[3].regClass() || is_cmpswap),
+                     "MIMG operands[3] (VDATA) must be the same as definitions[0] for atomics and TFE/LWE loads", instr.get());
             }
+            check(instr->operands[2].hasRegClass() && instr->operands[2].regClass().type() == RegType::vgpr,
+                  "MIMG operands[2] (VADDR) must be VGPR", instr.get());
             check(instr->definitions.empty() || (instr->definitions[0].isTemp() && instr->definitions[0].regClass().type() == RegType::vgpr),
                   "MIMG definitions[0] (VDATA) must be VGPR", instr.get());
             break;
diff --git a/src/amd/compiler/tests/test_isel.cpp b/src/amd/compiler/tests/test_isel.cpp
index 208833c54fc..676ff2c83c2 100644
--- a/src/amd/compiler/tests/test_isel.cpp
+++ b/src/amd/compiler/tests/test_isel.cpp
@@ -149,18 +149,18 @@ BEGIN_TEST(isel.sparse.clause)
          };
          void main() {
             //>> v5: (noCSE)%zero0 = p_create_vector 0, 0, 0, 0, 0
-            //>> v5: %_ = image_sample_lz_o %_, %_, %zero0, %_, %_, %_ dmask:xyzw 2d tfe storage: semantics: scope:invocation
+            //>> v5: %_ = image_sample_lz_o %_, %_, %_, %zero0 dmask:xyzw 2d tfe storage: semantics: scope:invocation
             //>> v5: (noCSE)%zero1 = p_create_vector 0, 0, 0, 0, 0
-            //>> v5: %_ = image_sample_lz_o %_, %_, %zero1, %_, %_, %_ dmask:xyzw 2d tfe storage: semantics: scope:invocation
+            //>> v5: %_ = image_sample_lz_o %_, %_, %_, %zero1 dmask:xyzw 2d tfe storage: semantics: scope:invocation
             //>> v5: (noCSE)%zero2 = p_create_vector 0, 0, 0, 0, 0
-            //>> v5: %_ = image_sample_lz_o %_, %_, %zero2, %_, %_, %_ dmask:xyzw 2d tfe storage: semantics: scope:invocation
+            //>> v5: %_ = image_sample_lz_o %_, %_, %_, %zero2 dmask:xyzw 2d tfe storage: semantics: scope:invocation
             //>> v5: (noCSE)%zero3 = p_create_vector 0, 0, 0, 0, 0
-            //>> v5: %_ = image_sample_lz_o %_, %_, %zero3, %_, %_, %_ dmask:xyzw 2d tfe storage: semantics: scope:invocation
+            //>> v5: %_ = image_sample_lz_o %_, %_, %_, %zero3 dmask:xyzw 2d tfe storage: semantics: scope:invocation
             //>> s_clause 0x3
-            //! image_sample_lz_o v#_, [v#_, v#_, v#_, v#_], @s256(img), @s128(samp) dmask:0xf dim:SQ_RSRC_IMG_2D tfe
-            //! image_sample_lz_o v#_, [v#_, v#_, v#_, v#_], @s256(img), @s128(samp) dmask:0xf dim:SQ_RSRC_IMG_2D tfe
-            //! image_sample_lz_o v#_, [v#_, v#_, v#_, v#_], @s256(img), @s128(samp) dmask:0xf dim:SQ_RSRC_IMG_2D tfe
-            //! image_sample_lz_o v#_, [v#_, v#_, v#_, v#_], @s256(img), @s128(samp) dmask:0xf dim:SQ_RSRC_IMG_2D tfe
+            //! image_sample_lz_o v#_, v[#_:#_], @s256(img), @s128(samp) dmask:0xf dim:SQ_RSRC_IMG_2D tfe
+            //! image_sample_lz_o v#_, v[#_:#_], @s256(img), @s128(samp) dmask:0xf dim:SQ_RSRC_IMG_2D tfe
+            //! image_sample_lz_o v#_, v[#_:#_], @s256(img), @s128(samp) dmask:0xf dim:SQ_RSRC_IMG_2D tfe
+            //! image_sample_lz_o v#_, v[#_:#_], @s256(img), @s128(samp) dmask:0xf dim:SQ_RSRC_IMG_2D tfe
             code[0] = sparseTextureOffsetARB(tex, vec2(0.5), ivec2(1, 0), res[0]);
             code[1] = sparseTextureOffsetARB(tex, vec2(0.5), ivec2(2, 0), res[1]);
             code[2] = sparseTextureOffsetARB(tex, vec2(0.5), ivec2(3, 0), res[2]);
diff --git a/src/amd/compiler/tests/test_optimizer.cpp b/src/amd/compiler/tests/test_optimizer.cpp
index 679812faac8..e885001c614 100644
--- a/src/amd/compiler/tests/test_optimizer.cpp
+++ b/src/amd/compiler/tests/test_optimizer.cpp
@@ -676,7 +676,8 @@ BEGIN_TEST(optimize.const_comparison_ordering)
    writeout(9, bld.sop2(aco_opcode::s_or_b64, bld.def(bld.lm), bld.def(s1, scc), src0, src1));
 
    /* bit sizes */
-   //! s2: %res10 = v_cmp_nge_f16 4.0, %b
+   //! v2b: %b16 = p_extract_vector %b, 0
+   //! s2: %res10 = v_cmp_nge_f16 4.0, %b16
    //! p_unit_test 10, %res10
    Temp input1_16 = bld.pseudo(aco_opcode::p_extract_vector, bld.def(v2b), inputs[1], Operand(0u));
    writeout(10, bld.sop2(aco_opcode::s_or_b64, bld.def(bld.lm), bld.def(s1, scc),
diff --git a/src/amd/compiler/tests/test_regalloc.cpp b/src/amd/compiler/tests/test_regalloc.cpp
index b6f07c06ae4..ef93f991dab 100644
--- a/src/amd/compiler/tests/test_regalloc.cpp
+++ b/src/amd/compiler/tests/test_regalloc.cpp
@@ -58,22 +58,3 @@ BEGIN_TEST(regalloc.subdword_alloc.reuse_16bit_operands)
       }
    }
 END_TEST
-
-BEGIN_TEST(regalloc.32bit_partial_write)
-   //>> v1: %_:v[0], s2: %_:exec = p_startpgm
-   if (!setup_cs("v1", GFX10))
-      return;
-
-   /* ensure high 16 bits are occupied */
-   //! v2b: %_:v[0][0:16], v2b: %_:v[0][16:32] = p_split_vector %_:v[0]
-   Temp hi = bld.pseudo(aco_opcode::p_split_vector, bld.def(v2b), bld.def(v2b), inputs[0]).def(1).getTemp();
-
-   /* This test checks if this instruction uses SDWA. */
-   //! v2b: %_:v[0][0:16] = v_not_b32 0 dst_preserve
-   Temp lo = bld.vop1(aco_opcode::v_not_b32, bld.def(v2b), Operand(0u));
-
-   //! v1: %_:v[0] = p_create_vector %_:v[0][0:16], %_:v[0][16:32]
-   bld.pseudo(aco_opcode::p_create_vector, bld.def(v1), lo, hi);
-
-   finish_ra_test(ra_test_policy());
-END_TEST
diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index f3ecc948be6..bddbba79dea 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -3267,62 +3267,11 @@ static void radv_stage_flush(struct radv_cmd_buffer *cmd_buffer,
 	}
 }
 
-/* Determine if the image is affected by the pipe misaligned metadata issue
- * which requires to invalidate L2.
- */
-static bool
-radv_image_is_pipe_misaligned(const struct radv_device *device,
-			      const struct radv_image *image)
-{
-	struct radeon_info *rad_info = &device->physical_device->rad_info;
-	unsigned log2_samples = util_logbase2(image->info.samples);
-	unsigned log2_bpp = util_logbase2(vk_format_get_blocksize(image->vk_format));
-	unsigned log2_bpp_and_samples;
-
-	assert(rad_info->chip_class >= GFX10);
-
-	if (rad_info->chip_class >= GFX10_3) {
-		log2_bpp_and_samples = log2_bpp + log2_samples;
-	} else {
-		if (vk_format_is_depth(image->vk_format) &&
-		     image->info.array_size >= 8) {
-			log2_bpp = 2;
-		}
-
-		log2_bpp_and_samples = MIN2(6, log2_bpp + log2_samples);
-	}
-
-	unsigned num_pipes = G_0098F8_NUM_PIPES(rad_info->gb_addr_config);
-	int overlap = MAX2(0, log2_bpp_and_samples + num_pipes - 8);
-
-	if (vk_format_is_depth(image->vk_format)) {
-		if (radv_image_is_tc_compat_htile(image) && overlap) {
-			return true;
-		}
-	} else {
-		unsigned max_compressed_frags = G_0098F8_MAX_COMPRESSED_FRAGS(rad_info->gb_addr_config);
-		int log2_samples_frag_diff = MAX2(0, log2_samples - max_compressed_frags);
-		int samples_overlap = MIN2(log2_samples, overlap);
-
-		/* TODO: It shouldn't be necessary if the image has DCC but
-		 * not readable by shader.
-		 */
-		if ((radv_image_has_dcc(image) ||
-		     radv_image_is_tc_compat_cmask(image)) &&
-		     (samples_overlap > log2_samples_frag_diff)) {
-			return true;
-		}
-	}
-
-	return false;
-}
-
 static bool
 radv_image_is_l2_coherent(const struct radv_device *device, const struct radv_image *image)
 {
 	if (device->physical_device->rad_info.chip_class >= GFX10) {
-		return !device->physical_device->rad_info.tcc_harvested &&
-			(image && !radv_image_is_pipe_misaligned(device, image));
+		return !device->physical_device->rad_info.tcc_harvested;
 	} else if (device->physical_device->rad_info.chip_class == GFX9 && image) {
 		if (image->info.samples == 1 &&
 		    (image->usage & (VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT |
@@ -3512,28 +3461,11 @@ radv_dst_access_flush(struct radv_cmd_buffer *cmd_buffer,
 void radv_subpass_barrier(struct radv_cmd_buffer *cmd_buffer,
 			  const struct radv_subpass_barrier *barrier)
 {
-	struct radv_framebuffer *fb = cmd_buffer->state.framebuffer;
-	if (fb && !fb->imageless) {
-		for (int i = 0; i < fb->attachment_count; ++i) {
-			cmd_buffer->state.flush_bits |= radv_src_access_flush(cmd_buffer, barrier->src_access_mask,
-									      fb->attachments[i]->image);
-		}
-	} else {
-		cmd_buffer->state.flush_bits |= radv_src_access_flush(cmd_buffer, barrier->src_access_mask,
-								      NULL);
-	}
-
+	cmd_buffer->state.flush_bits |= radv_src_access_flush(cmd_buffer, barrier->src_access_mask,
+							      NULL);
 	radv_stage_flush(cmd_buffer, barrier->src_stage_mask);
-
-	if (fb && !fb->imageless) {
-		for (int i = 0; i < fb->attachment_count; ++i) {
-			cmd_buffer->state.flush_bits |= radv_dst_access_flush(cmd_buffer, barrier->dst_access_mask,
-									      fb->attachments[i]->image);
-		}
-	} else {
-		cmd_buffer->state.flush_bits |= radv_dst_access_flush(cmd_buffer, barrier->dst_access_mask,
-		                                                      NULL);
-	}
+	cmd_buffer->state.flush_bits |= radv_dst_access_flush(cmd_buffer, barrier->dst_access_mask,
+	                                                      NULL);
 }
 
 uint32_t
diff --git a/src/amd/vulkan/radv_device.c b/src/amd/vulkan/radv_device.c
index 323676674c2..7c10d24ed7d 100644
--- a/src/amd/vulkan/radv_device.c
+++ b/src/amd/vulkan/radv_device.c
@@ -38,7 +38,6 @@
 #include "radv_shader.h"
 #include "radv_cs.h"
 #include "util/disk_cache.h"
-#include "vk_deferred_operation.h"
 #include "vk_util.h"
 #include <xf86drm.h>
 #include <amdgpu.h>
@@ -2710,7 +2709,6 @@ VkResult radv_CreateDevice(
 
 	bool keep_shader_info = false;
 	bool robust_buffer_access = false;
-	bool robust_buffer_access2 = false;
 	bool overallocation_disallowed = false;
 	bool custom_border_colors = false;
 	bool vrs_enabled = false;
@@ -2757,12 +2755,6 @@ VkResult radv_CreateDevice(
 				      vrs->attachmentFragmentShadingRate;
 			break;
 		}
-		case VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_ROBUSTNESS_2_FEATURES_EXT: {
-			const VkPhysicalDeviceRobustness2FeaturesEXT *features = (const void *)ext;
-			if (features->robustBufferAccess2)
-				robust_buffer_access2 = true;
-			break;
-		}
 		default:
 			break;
 		}
@@ -2806,8 +2798,7 @@ VkResult radv_CreateDevice(
 		device->enabled_extensions.EXT_buffer_device_address ||
 		device->enabled_extensions.KHR_buffer_device_address;
 
-	device->robust_buffer_access = robust_buffer_access || robust_buffer_access2;
-	device->robust_buffer_access2 = robust_buffer_access2;
+	device->robust_buffer_access = robust_buffer_access;
 
 	device->adjust_frag_coord_z = (vrs_enabled ||
 				       device->enabled_extensions.KHR_fragment_shading_rate) &&
@@ -7340,8 +7331,6 @@ VkResult radv_CreateFramebuffer(
 	framebuffer->width = pCreateInfo->width;
 	framebuffer->height = pCreateInfo->height;
 	framebuffer->layers = pCreateInfo->layers;
-	framebuffer->imageless = !!imageless_create_info;
-
 	if (imageless_create_info) {
 		for (unsigned i = 0; i < imageless_create_info->attachmentImageInfoCount; ++i) {
 			const VkFramebufferAttachmentImageInfo *attachment =
@@ -8374,42 +8363,3 @@ void radv_GetPrivateDataEXT(
 	vk_object_base_get_private_data(&device->vk, objectType, objectHandle,
 					privateDataSlot, pData);
 }
-
-VkResult radv_CreateDeferredOperationKHR(VkDevice _device,
-                                         const VkAllocationCallbacks* pAllocator,
-                                         VkDeferredOperationKHR* pDeferredOperation)
-{
-	RADV_FROM_HANDLE(radv_device, device, _device);
-	return vk_create_deferred_operation(&device->vk, pAllocator,
-	                                    pDeferredOperation);
-}
-
-void radv_DestroyDeferredOperationKHR(VkDevice _device,
-                                      VkDeferredOperationKHR operation,
-                                      const VkAllocationCallbacks* pAllocator)
-{
-	RADV_FROM_HANDLE(radv_device, device, _device);
-	vk_destroy_deferred_operation(&device->vk, operation, pAllocator);
-}
-
-uint32_t radv_GetDeferredOperationMaxConcurrencyKHR(VkDevice _device,
-                                                    VkDeferredOperationKHR operation)
-{
-	RADV_FROM_HANDLE(radv_device, device, _device);
-	return vk_get_deferred_operation_max_concurrency(&device->vk, operation);
-}
-
-VkResult radv_GetDeferredOperationResultKHR(VkDevice _device,
-                                            VkDeferredOperationKHR operation)
-{
-	RADV_FROM_HANDLE(radv_device, device, _device);
-	return vk_get_deferred_operation_result(&device->vk, operation);
-}
-
-VkResult radv_DeferredOperationJoinKHR(VkDevice _device,
-                                       VkDeferredOperationKHR operation)
-{
-	RADV_FROM_HANDLE(radv_device, device, _device);
-	return vk_deferred_operation_join(&device->vk, operation);
-}
-
diff --git a/src/amd/vulkan/radv_extensions.py b/src/amd/vulkan/radv_extensions.py
index 30e88c2dc74..8452935e262 100644
--- a/src/amd/vulkan/radv_extensions.py
+++ b/src/amd/vulkan/radv_extensions.py
@@ -62,7 +62,6 @@ EXTENSIONS = [
     Extension('VK_KHR_copy_commands2',                    1, True),
     Extension('VK_KHR_create_renderpass2',                1, True),
     Extension('VK_KHR_dedicated_allocation',              3, True),
-    Extension('VK_KHR_deferred_host_operations',          1, True),
     Extension('VK_KHR_depth_stencil_resolve',             1, True),
     Extension('VK_KHR_descriptor_update_template',        1, True),
     Extension('VK_KHR_device_group',                      4, True),
diff --git a/src/amd/vulkan/radv_image.c b/src/amd/vulkan/radv_image.c
index a69f6d09864..90409792089 100644
--- a/src/amd/vulkan/radv_image.c
+++ b/src/amd/vulkan/radv_image.c
@@ -93,23 +93,20 @@ radv_use_tc_compat_htile_for_image(struct radv_device *device,
 				    VK_IMAGE_USAGE_TRANSFER_SRC_BIT)))
 		return false;
 
-	if (device->physical_device->rad_info.chip_class < GFX9) {
-		/* FIXME: for some reason TC compat with 2/4/8 samples breaks
-		 * some cts tests - disable for now.
-		 */
-		if (pCreateInfo->samples >= 2 && format == VK_FORMAT_D32_SFLOAT_S8_UINT)
-			return false;
+	/* FIXME: for some reason TC compat with 2/4/8 samples breaks some cts
+	 * tests - disable for now.
+	 */
+	if (pCreateInfo->samples >= 2 && format == VK_FORMAT_D32_SFLOAT_S8_UINT)
+		return false;
 
-		/* GFX9+ supports compression for both 32-bit and 16-bit depth
-		 * surfaces, while GFX8 only supports 32-bit natively. Though,
-		 * the driver allows TC-compat HTILE for 16-bit depth surfaces
-		 * with no Z planes compression.
-		 */
-		if (format != VK_FORMAT_D32_SFLOAT_S8_UINT &&
-		    format != VK_FORMAT_D32_SFLOAT &&
-		    format != VK_FORMAT_D16_UNORM)
-			return false;
-	}
+	/* GFX9 supports both 32-bit and 16-bit depth surfaces, while GFX8 only
+	 * supports 32-bit. Though, it's possible to enable TC-compat for
+	 * 16-bit depth surfaces if no Z planes are compressed.
+	 */
+	if (format != VK_FORMAT_D32_SFLOAT_S8_UINT &&
+	    format != VK_FORMAT_D32_SFLOAT &&
+	    format != VK_FORMAT_D16_UNORM)
+		return false;
 
 	return true;
 }
@@ -1308,7 +1305,7 @@ radv_image_reset_layout(struct radv_image *image)
 	image->size = 0;
 	image->alignment = 1;
 
-	image->tc_compatible_cmask = 0;
+	image->tc_compatible_cmask = image->tc_compatible_htile = 0;
 	image->fce_pred_offset = image->dcc_pred_offset = 0;
 	image->clear_value_offset = image->tc_compat_zrange_offset = 0;
 
@@ -1381,6 +1378,9 @@ radv_image_create_layout(struct radv_device *device,
 	image->tc_compatible_cmask = radv_image_has_cmask(image) &&
 	                             radv_use_tc_compat_cmask_for_image(device, image);
 
+	image->tc_compatible_htile = radv_image_has_htile(image) &&
+	                             image->planes[0].surface.flags & RADEON_SURF_TC_COMPATIBLE_HTILE;
+
 	radv_image_alloc_values(device, image);
 
 	assert(image->planes[0].surface.surf_size);
@@ -1520,8 +1520,7 @@ radv_image_create(VkDevice _device,
 		                     EXTERNAL_MEMORY_IMAGE_CREATE_INFO) ;
 
 	image->shareable = external_info;
-	if (!vk_format_is_depth_or_stencil(format) && !image->shareable &&
-	    !(image->flags & VK_IMAGE_CREATE_SPARSE_ALIASED_BIT)) {
+	if (!vk_format_is_depth_or_stencil(format) && !image->shareable) {
 		image->info.surf_index = &device->image_mrt_offset_counter;
 	}
 
diff --git a/src/amd/vulkan/radv_meta_buffer.c b/src/amd/vulkan/radv_meta_buffer.c
index 9a8bb31c21e..a08b6e408c5 100644
--- a/src/amd/vulkan/radv_meta_buffer.c
+++ b/src/amd/vulkan/radv_meta_buffer.c
@@ -368,7 +368,6 @@ uint32_t radv_fill_buffer(struct radv_cmd_buffer *cmd_buffer,
 		fill_buffer_shader(cmd_buffer, bo, offset, size, value);
 
 		flush_bits = RADV_CMD_FLAG_CS_PARTIAL_FLUSH |
-			     RADV_CMD_FLAG_INV_VCACHE |
 			     radv_src_access_flush(cmd_buffer, VK_ACCESS_SHADER_WRITE_BIT, image);
 	} else if (size) {
 		uint64_t va = radv_buffer_get_va(bo);
diff --git a/src/amd/vulkan/radv_meta_clear.c b/src/amd/vulkan/radv_meta_clear.c
index 62c7215c7cb..421855839e6 100644
--- a/src/amd/vulkan/radv_meta_clear.c
+++ b/src/amd/vulkan/radv_meta_clear.c
@@ -699,7 +699,8 @@ static bool depth_view_can_fast_clear(struct radv_cmd_buffer *cmd_buffer,
 	      clear_value.depth != 1.0) ||
 	     ((aspects & VK_IMAGE_ASPECT_STENCIL_BIT) && clear_value.stencil != 0)))
 		return false;
-	if (iview->base_mip == 0 &&
+	if (radv_image_has_htile(iview->image) &&
+	    iview->base_mip == 0 &&
 	    iview->base_layer == 0 &&
 	    iview->layer_count == iview->image->info.array_size &&
 	    radv_layout_is_htile_compressed(cmd_buffer->device, iview->image, layout, in_render_loop, queue_mask) &&
@@ -939,7 +940,6 @@ clear_htile_mask(struct radv_cmd_buffer *cmd_buffer, const struct radv_image *im
 	radv_meta_restore(&saved_state, cmd_buffer);
 
 	return RADV_CMD_FLAG_CS_PARTIAL_FLUSH |
-	       RADV_CMD_FLAG_INV_VCACHE |
 	       radv_src_access_flush(cmd_buffer, VK_ACCESS_SHADER_WRITE_BIT, image);
 }
 
diff --git a/src/amd/vulkan/radv_meta_decompress.c b/src/amd/vulkan/radv_meta_decompress.c
index 011c353b355..4ac3c25f506 100644
--- a/src/amd/vulkan/radv_meta_decompress.c
+++ b/src/amd/vulkan/radv_meta_decompress.c
@@ -481,6 +481,9 @@ static void radv_process_depth_stencil(struct radv_cmd_buffer *cmd_buffer,
 	VkCommandBuffer cmd_buffer_h = radv_cmd_buffer_to_handle(cmd_buffer);
 	VkPipeline *pipeline;
 
+	if (!radv_image_has_htile(image))
+		return;
+
 	radv_meta_save(&saved_state, cmd_buffer,
 		       RADV_META_SAVE_GRAPHICS_PIPELINE |
 		       RADV_META_SAVE_SAMPLE_LOCATIONS |
diff --git a/src/amd/vulkan/radv_meta_fast_clear.c b/src/amd/vulkan/radv_meta_fast_clear.c
index 1b2bab0fd84..b65e175e3f3 100644
--- a/src/amd/vulkan/radv_meta_fast_clear.c
+++ b/src/amd/vulkan/radv_meta_fast_clear.c
@@ -949,7 +949,6 @@ radv_decompress_dcc_compute(struct radv_cmd_buffer *cmd_buffer,
 	radv_meta_restore(&saved_state, cmd_buffer);
 
 	cmd_buffer->state.flush_bits |= RADV_CMD_FLAG_CS_PARTIAL_FLUSH |
-					RADV_CMD_FLAG_INV_VCACHE |
 			radv_src_access_flush(cmd_buffer, VK_ACCESS_SHADER_WRITE_BIT, image);
 
 	/* Initialize the DCC metadata as "fully expanded". */
diff --git a/src/amd/vulkan/radv_meta_resolve.c b/src/amd/vulkan/radv_meta_resolve.c
index 8258b9a04e8..5a02dd5bb28 100644
--- a/src/amd/vulkan/radv_meta_resolve.c
+++ b/src/amd/vulkan/radv_meta_resolve.c
@@ -378,10 +378,6 @@ static void radv_pick_resolve_method_images(struct radv_device *device,
 			*method = RESOLVE_FRAGMENT;
 		} else if (dest_image->planes[0].surface.micro_tile_mode !=
 		           src_image->planes[0].surface.micro_tile_mode) {
-			/* The micro tile mode only needs to match for the HW
-			 * resolve path which is the default path for non-DCC
-			 * resolves.
-			 */
 			*method = RESOLVE_COMPUTE;
 		}
 
diff --git a/src/amd/vulkan/radv_meta_resolve_cs.c b/src/amd/vulkan/radv_meta_resolve_cs.c
index 9a30a94ec6f..6aa8765a02f 100644
--- a/src/amd/vulkan/radv_meta_resolve_cs.c
+++ b/src/amd/vulkan/radv_meta_resolve_cs.c
@@ -943,7 +943,6 @@ radv_cmd_buffer_resolve_subpass_cs(struct radv_cmd_buffer *cmd_buffer)
 
 	cmd_buffer->state.flush_bits |=
 		RADV_CMD_FLAG_CS_PARTIAL_FLUSH |
-		RADV_CMD_FLAG_INV_VCACHE |
 		radv_src_access_flush(cmd_buffer, VK_ACCESS_SHADER_WRITE_BIT, NULL);
 }
 
@@ -1023,7 +1022,6 @@ radv_depth_stencil_resolve_subpass_cs(struct radv_cmd_buffer *cmd_buffer,
 
 	cmd_buffer->state.flush_bits |=
 		RADV_CMD_FLAG_CS_PARTIAL_FLUSH |
-		RADV_CMD_FLAG_INV_VCACHE |
 		radv_src_access_flush(cmd_buffer, VK_ACCESS_SHADER_WRITE_BIT, NULL);
 
 	VkImageLayout layout =
diff --git a/src/amd/vulkan/radv_pipeline.c b/src/amd/vulkan/radv_pipeline.c
index db3c36850e3..a910cfc16b5 100644
--- a/src/amd/vulkan/radv_pipeline.c
+++ b/src/amd/vulkan/radv_pipeline.c
@@ -3181,10 +3181,10 @@ opt_vectorize_callback(const nir_instr *instr, void *_)
    case nir_op_imax:
    case nir_op_umin:
    case nir_op_umax:
-      return true;
-   case nir_op_ishl: /* TODO: in NIR, these have 32bit shift operands */
-   case nir_op_ishr: /* while Radeon needs 16bit operands when vectorized */
+   case nir_op_ishl:
+   case nir_op_ishr:
    case nir_op_ushr:
+      return true;
    default:
       return false;
    }
diff --git a/src/amd/vulkan/radv_private.h b/src/amd/vulkan/radv_private.h
index 6eef02e7172..be4920d73ff 100644
--- a/src/amd/vulkan/radv_private.h
+++ b/src/amd/vulkan/radv_private.h
@@ -826,9 +826,8 @@ struct radv_device {
 	struct radv_device_extension_table enabled_extensions;
 	struct radv_device_dispatch_table dispatch;
 
-	/* Whether the app has enabled the robustBufferAccess/robustBufferAccess2 features. */
+	/* Whether the app has enabled the robustBufferAccess feature. */
 	bool robust_buffer_access;
-	bool robust_buffer_access2;
 
 	/* Whether gl_FragCoord.z should be adjusted for VRS due to a hw bug
 	 * on some GFX10.3 chips.
@@ -1905,6 +1904,7 @@ struct radv_image {
 	/* Set when bound */
 	struct radeon_winsys_bo *bo;
 	VkDeviceSize offset;
+	bool tc_compatible_htile;
 	bool tc_compatible_cmask;
 
 	uint64_t clear_value_offset;
@@ -2031,8 +2031,7 @@ radv_htile_enabled(const struct radv_image *image, unsigned level)
 static inline bool
 radv_image_is_tc_compat_htile(const struct radv_image *image)
 {
-	return radv_image_has_htile(image) &&
-	       (image->planes[0].surface.flags & RADEON_SURF_TC_COMPATIBLE_HTILE);
+	return radv_image_has_htile(image) && image->tc_compatible_htile;
 }
 
 /**
@@ -2327,8 +2326,6 @@ struct radv_framebuffer {
 	uint32_t                                     height;
 	uint32_t                                     layers;
 
-	bool                                         imageless;
-
 	uint32_t                                     attachment_count;
 	struct radv_image_view                       *attachments[0];
 };
diff --git a/src/amd/vulkan/radv_query.c b/src/amd/vulkan/radv_query.c
index b4298c9f477..8b9fbf29a2b 100644
--- a/src/amd/vulkan/radv_query.c
+++ b/src/amd/vulkan/radv_query.c
@@ -1542,14 +1542,13 @@ static void emit_begin_query(struct radv_cmd_buffer *cmd_buffer,
 
 			va += 8 * idx;
 
-			radeon_emit(cs, PKT3(PKT3_COPY_DATA, 4, 0));
-			radeon_emit(cs, COPY_DATA_SRC_SEL(COPY_DATA_GDS) |
-					COPY_DATA_DST_SEL(COPY_DATA_DST_MEM) |
-					COPY_DATA_WR_CONFIRM);
-			radeon_emit(cs, 0);
-			radeon_emit(cs, 0);
-			radeon_emit(cs, va);
-			radeon_emit(cs, va >> 32);
+			si_cs_emit_write_event_eop(cs,
+						   cmd_buffer->device->physical_device->rad_info.chip_class,
+						   radv_cmd_buffer_uses_mec(cmd_buffer),
+						   V_028A90_PS_DONE, 0,
+						   EOP_DST_SEL_TC_L2,
+						   EOP_DATA_SEL_GDS,
+						   va, EOP_DATA_GDS(0, 1), 0);
 
 			/* Record that the command buffer needs GDS. */
 			cmd_buffer->gds_needed = true;
@@ -1633,14 +1632,13 @@ static void emit_end_query(struct radv_cmd_buffer *cmd_buffer,
 
 			va += 8 * idx;
 
-			radeon_emit(cs, PKT3(PKT3_COPY_DATA, 4, 0));
-			radeon_emit(cs, COPY_DATA_SRC_SEL(COPY_DATA_GDS) |
-					COPY_DATA_DST_SEL(COPY_DATA_DST_MEM) |
-					COPY_DATA_WR_CONFIRM);
-			radeon_emit(cs, 0);
-			radeon_emit(cs, 0);
-			radeon_emit(cs, va);
-			radeon_emit(cs, va >> 32);
+			si_cs_emit_write_event_eop(cs,
+						   cmd_buffer->device->physical_device->rad_info.chip_class,
+						   radv_cmd_buffer_uses_mec(cmd_buffer),
+						   V_028A90_PS_DONE, 0,
+						   EOP_DST_SEL_TC_L2,
+						   EOP_DATA_SEL_GDS,
+						   va, EOP_DATA_GDS(0, 1), 0);
 
 			cmd_buffer->state.active_pipeline_gds_queries--;
 		}
diff --git a/src/amd/vulkan/radv_shader.c b/src/amd/vulkan/radv_shader.c
index 973e2a33e5d..e25ac04c29e 100644
--- a/src/amd/vulkan/radv_shader.c
+++ b/src/amd/vulkan/radv_shader.c
@@ -500,7 +500,6 @@ radv_shader_compile_to_nir(struct radv_device *device,
 			.push_const_addr_format = nir_address_format_logical,
 			.shared_addr_format = nir_address_format_32bit_offset,
 			.frag_coord_is_sysval = true,
-			.use_deref_buffer_array_length = true,
 			.debug = {
 				.func = radv_spirv_nir_debug,
 				.private_data = &spirv_debug_data,
@@ -1421,7 +1420,6 @@ radv_shader_variant_compile(struct radv_device *device,
 
 	options.explicit_scratch_args = !radv_use_llvm_for_stage(device, stage);
 	options.robust_buffer_access = device->robust_buffer_access;
-	options.robust_buffer_access2 = device->robust_buffer_access2;
 	options.disable_optimizations = disable_optimizations;
 
 	return shader_variant_compile(device, module, shaders, shader_count, stage, info,
@@ -1580,8 +1578,6 @@ radv_get_max_waves(struct radv_device *device,
 
 	if (conf->num_vgprs) {
 		unsigned vgprs = align(conf->num_vgprs, wave_size == 32 ? 8 : 4);
-		if (chip_class >= GFX10_3)
-		   vgprs = align(vgprs, wave_size == 32 ? 16 : 8);
 		max_simd_waves =
 			MIN2(max_simd_waves,
 			     device->physical_device->rad_info.num_physical_wave64_vgprs_per_simd / vgprs);
diff --git a/src/amd/vulkan/radv_shader.h b/src/amd/vulkan/radv_shader.h
index 10370353da8..44f5a2785f0 100644
--- a/src/amd/vulkan/radv_shader.h
+++ b/src/amd/vulkan/radv_shader.h
@@ -135,7 +135,6 @@ struct radv_nir_compiler_options {
 	bool explicit_scratch_args;
 	bool clamp_shadow_reference;
 	bool robust_buffer_access;
-	bool robust_buffer_access2;
 	bool adjust_frag_coord_z;
 	bool dump_shader;
 	bool dump_preoptir;
diff --git a/src/amd/vulkan/si_cmd_buffer.c b/src/amd/vulkan/si_cmd_buffer.c
index 9be773468d5..4ba23c5b8d2 100644
--- a/src/amd/vulkan/si_cmd_buffer.c
+++ b/src/amd/vulkan/si_cmd_buffer.c
@@ -1353,11 +1353,18 @@ si_cs_emit_cache_flush(struct radeon_cmdbuf *cs,
 		*sqtt_flush_bits |= RGP_FLUSH_CS_PARTIAL_FLUSH;
 	}
 
-	if (chip_class == GFX9 && flush_cb_db) {
+	if (chip_class == GFX9 &&
+	    (flush_cb_db || (flush_bits & RADV_CMD_FLAG_INV_L2_METADATA))) {
 		unsigned cb_db_event, tc_flags;
 
 		/* Set the CB/DB flush event. */
-		cb_db_event = V_028A90_CACHE_FLUSH_AND_INV_TS_EVENT;
+		if (flush_cb_db) {
+			cb_db_event = V_028A90_CACHE_FLUSH_AND_INV_TS_EVENT;
+		} else {
+			/* Besides the CB the only other thing writing HTILE
+			 * or DCC metadata are our meta compute shaders. */
+			cb_db_event = V_028A90_CS_DONE;
+		}
 
 		/* These are the only allowed combinations. If you need to
 		 * do multiple operations at once, do them separately.
@@ -1371,11 +1378,12 @@ si_cs_emit_cache_flush(struct radeon_cmdbuf *cs,
 		 * TC    | TC_MD         = writeback & invalidate L2 metadata (DCC, etc.)
 		 * TCL1                  = invalidate L1
 		 */
-		tc_flags = EVENT_TC_ACTION_ENA |
-			   EVENT_TC_MD_ACTION_ENA;
+		tc_flags = 0;
 
-		*sqtt_flush_bits |= RGP_FLUSH_FLUSH_CB | RGP_FLUSH_INVAL_CB |
-			            RGP_FLUSH_FLUSH_DB | RGP_FLUSH_INVAL_DB;
+		if (flush_cb_db) {
+			*sqtt_flush_bits |= RGP_FLUSH_FLUSH_CB | RGP_FLUSH_INVAL_CB |
+			                    RGP_FLUSH_FLUSH_DB | RGP_FLUSH_INVAL_DB;
+		}
 
 		/* Ideally flush TC together with CB/DB. */
 		if (flush_bits & RADV_CMD_FLAG_INV_L2) {
@@ -1390,6 +1398,9 @@ si_cs_emit_cache_flush(struct radeon_cmdbuf *cs,
 					 RADV_CMD_FLAG_INV_VCACHE);
 
 			*sqtt_flush_bits |= RGP_FLUSH_INVAL_L2;
+		} else if (flush_bits & RADV_CMD_FLAG_INV_L2_METADATA) {
+			tc_flags = EVENT_TC_ACTION_ENA |
+			           EVENT_TC_MD_ACTION_ENA;
 		}
 
 		assert(flush_cnt);
@@ -1496,7 +1507,6 @@ si_emit_cache_flush(struct radv_cmd_buffer *cmd_buffer)
 	                                          RADV_CMD_FLAG_FLUSH_AND_INV_CB_META |
 	                                          RADV_CMD_FLAG_FLUSH_AND_INV_DB |
 	                                          RADV_CMD_FLAG_FLUSH_AND_INV_DB_META |
-	                                          RADV_CMD_FLAG_INV_L2_METADATA |
 	                                          RADV_CMD_FLAG_PS_PARTIAL_FLUSH |
 	                                          RADV_CMD_FLAG_VS_PARTIAL_FLUSH |
 	                                          RADV_CMD_FLAG_VGT_FLUSH |
diff --git a/src/broadcom/compiler/nir_to_vir.c b/src/broadcom/compiler/nir_to_vir.c
index ef043609b00..4e924350a1d 100644
--- a/src/broadcom/compiler/nir_to_vir.c
+++ b/src/broadcom/compiler/nir_to_vir.c
@@ -3218,7 +3218,6 @@ nir_to_vir(struct v3d_compile *c)
 }
 
 const nir_shader_compiler_options v3d_nir_options = {
-        .lower_add_sat = true,
         .lower_all_io_to_temps = true,
         .lower_extract_byte = true,
         .lower_extract_word = true,
diff --git a/src/broadcom/vulkan/v3dv_bo.c b/src/broadcom/vulkan/v3dv_bo.c
index 459032990b2..87e495d0f48 100644
--- a/src/broadcom/vulkan/v3dv_bo.c
+++ b/src/broadcom/vulkan/v3dv_bo.c
@@ -319,7 +319,8 @@ v3dv_bo_map(struct v3dv_device *device, struct v3dv_bo *bo, uint32_t size)
    if (!ok)
       return false;
 
-   ok = v3dv_bo_wait(device, bo, PIPE_TIMEOUT_INFINITE);
+   const uint64_t infinite = 0xffffffffffffffffull;
+   ok = v3dv_bo_wait(device, bo, infinite);
    if (!ok) {
       fprintf(stderr, "memory wait for map failed\n");
       return false;
diff --git a/src/broadcom/vulkan/v3dv_cmd_buffer.c b/src/broadcom/vulkan/v3dv_cmd_buffer.c
index 4e866b8dbab..e1edb75266e 100644
--- a/src/broadcom/vulkan/v3dv_cmd_buffer.c
+++ b/src/broadcom/vulkan/v3dv_cmd_buffer.c
@@ -1495,104 +1495,13 @@ cmd_buffer_render_pass_emit_load(struct v3dv_cmd_buffer *cmd_buffer,
 
 static bool
 check_needs_load(const struct v3dv_cmd_buffer_state *state,
-                 VkImageAspectFlags aspect,
                  uint32_t att_first_subpass_idx,
                  VkAttachmentLoadOp load_op)
 {
-   /* We call this with image->aspects & aspect, so 0 means the aspect we are
-    * testing does not exist in the image.
-    */
-   if (!aspect)
-      return false;
-
-   /* Attachment load operations apply on the first subpass that uses the
-    * attachment, otherwise we always need to load.
-    */
-   if (state->job->first_subpass > att_first_subpass_idx)
-      return true;
-
-   /* If the job is continuing a subpass started in another job, we always
-    * need to load.
-    */
-   if (state->job->is_subpass_continue)
-      return true;
-
-   /* If the area is not aligned to tile boundaries, we always need to load */
-   if (!state->tile_aligned_render_area)
-      return true;
-
-   /* The attachment load operations must be LOAD */
-   return load_op == VK_ATTACHMENT_LOAD_OP_LOAD;
-}
-
-static bool
-check_needs_clear(const struct v3dv_cmd_buffer_state *state,
-                  VkImageAspectFlags aspect,
-                  uint32_t att_first_subpass_idx,
-                  VkAttachmentLoadOp load_op,
-                  bool do_clear_with_draw)
-{
-   /* We call this with image->aspects & aspect, so 0 means the aspect we are
-    * testing does not exist in the image.
-    */
-   if (!aspect)
-      return false;
-
-   /* If the aspect needs to be cleared with a draw call then we won't emit
-    * the clear here.
-    */
-   if (do_clear_with_draw)
-      return false;
-
-   /* If this is resuming a subpass started with another job, then attachment
-    * load operations don't apply.
-    */
-   if (state->job->is_subpass_continue)
-      return false;
-
-   /* If the render area is not aligned to tile boudaries we can't use the
-    * TLB for a clear.
-    */
-   if (!state->tile_aligned_render_area)
-      return false;
-
-   /* If this job is running in a subpass other than the first subpass in
-    * which this attachment is used then attachment load operations don't apply.
-    */
-   if (state->job->first_subpass != att_first_subpass_idx)
-      return false;
-
-   /* The attachment load operation must be CLEAR */
-   return load_op == VK_ATTACHMENT_LOAD_OP_CLEAR;
-}
-
-static bool
-check_needs_store(const struct v3dv_cmd_buffer_state *state,
-                  VkImageAspectFlags aspect,
-                  uint32_t att_last_subpass_idx,
-                  VkAttachmentStoreOp store_op)
-{
-   /* We call this with image->aspects & aspect, so 0 means the aspect we are
-    * testing does not exist in the image.
-    */
-   if (!aspect)
-       return false;
-
-   /* Attachment store operations only apply on the last subpass where the
-    * attachment is used, in other subpasses we always need to store.
-    */
-   if (state->subpass_idx < att_last_subpass_idx)
-      return true;
-
-   /* Attachment store operations only apply on the last job we emit on the the
-    * last subpass where the attachment is used, otherwise we always need to
-    * store.
-    */
-   if (!state->job->is_subpass_finish)
-      return true;
-
-   /* The attachment store operation must be STORE */
-   return store_op == VK_ATTACHMENT_STORE_OP_STORE;
+   return state->job->first_subpass > att_first_subpass_idx ||
+          state->job->is_subpass_continue ||
+          load_op == VK_ATTACHMENT_LOAD_OP_LOAD ||
+          !state->tile_aligned_render_area;
 }
 
 static void
@@ -1633,7 +1542,6 @@ cmd_buffer_render_pass_emit_loads(struct v3dv_cmd_buffer *cmd_buffer,
        * render area for any such tiles.
        */
       bool needs_load = check_needs_load(state,
-                                         VK_IMAGE_ASPECT_COLOR_BIT,
                                          attachment->first_subpass,
                                          attachment->desc.loadOp);
       if (needs_load) {
@@ -1648,19 +1556,14 @@ cmd_buffer_render_pass_emit_loads(struct v3dv_cmd_buffer *cmd_buffer,
       const struct v3dv_render_pass_attachment *ds_attachment =
          &state->pass->attachments[ds_attachment_idx];
 
-      const VkImageAspectFlags ds_aspects =
-         vk_format_aspects(ds_attachment->desc.format);
-
       const bool needs_depth_load =
-         check_needs_load(state,
-                          ds_aspects & VK_IMAGE_ASPECT_DEPTH_BIT,
-                          ds_attachment->first_subpass,
+         vk_format_has_depth(ds_attachment->desc.format) &&
+         check_needs_load(state, ds_attachment->first_subpass,
                           ds_attachment->desc.loadOp);
 
       const bool needs_stencil_load =
-         check_needs_load(state,
-                          ds_aspects & VK_IMAGE_ASPECT_STENCIL_BIT,
-                          ds_attachment->first_subpass,
+         vk_format_has_stencil(ds_attachment->desc.format) &&
+         check_needs_load(state, ds_attachment->first_subpass,
                           ds_attachment->desc.stencilLoadOp);
 
       if (needs_depth_load || needs_stencil_load) {
@@ -1739,8 +1642,7 @@ cmd_buffer_render_pass_emit_stores(struct v3dv_cmd_buffer *cmd_buffer,
       &state->pass->subpasses[state->subpass_idx];
 
    bool has_stores = false;
-   bool use_global_zs_clear = false;
-   bool use_global_rt_clear = false;
+   bool use_global_clear = false;
 
    /* FIXME: separate stencil */
    uint32_t ds_attachment_idx = subpass->ds_attachment.attachment;
@@ -1767,56 +1669,48 @@ cmd_buffer_render_pass_emit_stores(struct v3dv_cmd_buffer *cmd_buffer,
 
       /* Only clear once on the first subpass that uses the attachment */
       bool needs_depth_clear =
-         check_needs_clear(state,
-                           aspects & VK_IMAGE_ASPECT_DEPTH_BIT,
-                           ds_attachment->first_subpass,
-                           ds_attachment->desc.loadOp,
-                           subpass->do_depth_clear_with_draw);
+         (aspects & VK_IMAGE_ASPECT_DEPTH_BIT) &&
+         state->tile_aligned_render_area &&
+         state->job->first_subpass == ds_attachment->first_subpass &&
+         ds_attachment->desc.loadOp == VK_ATTACHMENT_LOAD_OP_CLEAR &&
+         !state->job->is_subpass_continue &&
+         !subpass->do_depth_clear_with_draw;
 
       bool needs_stencil_clear =
-         check_needs_clear(state,
-                           aspects & VK_IMAGE_ASPECT_STENCIL_BIT,
-                           ds_attachment->first_subpass,
-                           ds_attachment->desc.stencilLoadOp,
-                           subpass->do_stencil_clear_with_draw);
+         (aspects & VK_IMAGE_ASPECT_STENCIL_BIT) &&
+         state->tile_aligned_render_area &&
+         state->job->first_subpass == ds_attachment->first_subpass &&
+         ds_attachment->desc.stencilLoadOp == VK_ATTACHMENT_LOAD_OP_CLEAR &&
+         !state->job->is_subpass_continue &&
+         !subpass->do_stencil_clear_with_draw;
 
       /* Skip the last store if it is not required */
       bool needs_depth_store =
-         check_needs_store(state,
-                           aspects & VK_IMAGE_ASPECT_DEPTH_BIT,
-                           ds_attachment->last_subpass,
-                           ds_attachment->desc.storeOp);
+         (aspects & VK_IMAGE_ASPECT_DEPTH_BIT) &&
+         (state->subpass_idx < ds_attachment->last_subpass ||
+          ds_attachment->desc.storeOp == VK_ATTACHMENT_STORE_OP_STORE ||
+          !state->job->is_subpass_finish);
 
       bool needs_stencil_store =
-         check_needs_store(state,
-                           aspects & VK_IMAGE_ASPECT_STENCIL_BIT,
-                           ds_attachment->last_subpass,
-                           ds_attachment->desc.stencilStoreOp);
+         (aspects & VK_IMAGE_ASPECT_STENCIL_BIT) &&
+         (state->subpass_idx < ds_attachment->last_subpass ||
+          ds_attachment->desc.stencilStoreOp == VK_ATTACHMENT_STORE_OP_STORE ||
+          !state->job->is_subpass_finish);
 
       /* GFXH-1689: The per-buffer store command's clear buffer bit is broken
-       * for depth/stencil.
-       *
-       * There used to be some confusion regarding the Clear Tile Buffers
-       * Z/S bit also being broken, but we confirmed with Broadcom that this
-       * is not the case, it was just that some other hardware bugs (that we
-       * need to work around, such as GFXH-1461) could cause this bit to behave
-       * incorrectly.
-       *
-       * There used to be another issue where the RTs bit in the Clear Tile
-       * Buffers packet also cleared Z/S, but Broadcom confirmed this is
-       * fixed since V3D 4.1.
+       * for depth/stencil.  In addition, the clear packet's Z/S bit is broken,
+       * but the RTs bit ends up clearing Z/S.
        *
        * So if we have to emit a clear of depth or stencil we don't use
-       * the per-buffer store clear bit, even if we need to store the buffers,
-       * instead we always have to use the Clear Tile Buffers Z/S bit.
-       * If we have configured the job to do early Z/S clearing, then we
-       * don't want to emit any Clear Tile Buffers command at all here.
+       * per-buffer clears, not even for color, since we will have to emit
+       * a clear command for all tile buffers (including color) to handle
+       * the depth/stencil clears.
        *
-       * Note that GFXH-1689 is not reproduced in the simulator, where
-       * using the clear buffer bit in depth/stencil stores works fine.
+       * Note that this bug is not reproduced in the simulator, where
+       * using the clear buffer bit in depth/stencil stores seems to work
+       * correctly.
        */
-      use_global_zs_clear = !state->job->early_zs_clear &&
-                            (needs_depth_clear || needs_stencil_clear);
+      use_global_clear = needs_depth_clear || needs_stencil_clear;
       if (needs_depth_store || needs_stencil_store) {
          const uint32_t zs_buffer =
             v3dv_zs_buffer(needs_depth_store, needs_stencil_store);
@@ -1842,18 +1736,16 @@ cmd_buffer_render_pass_emit_stores(struct v3dv_cmd_buffer *cmd_buffer,
 
       /* Only clear once on the first subpass that uses the attachment */
       bool needs_clear =
-         check_needs_clear(state,
-                           VK_IMAGE_ASPECT_COLOR_BIT,
-                           attachment->first_subpass,
-                           attachment->desc.loadOp,
-                           false);
+         state->tile_aligned_render_area &&
+         state->job->first_subpass == attachment->first_subpass &&
+         attachment->desc.loadOp == VK_ATTACHMENT_LOAD_OP_CLEAR &&
+         !state->job->is_subpass_continue;
 
       /* Skip the last store if it is not required  */
       bool needs_store =
-         check_needs_store(state,
-                           VK_IMAGE_ASPECT_COLOR_BIT,
-                           attachment->last_subpass,
-                           attachment->desc.storeOp);
+         state->subpass_idx < attachment->last_subpass ||
+         attachment->desc.storeOp == VK_ATTACHMENT_STORE_OP_STORE ||
+         !state->job->is_subpass_finish;
 
       /* If we need to resolve this attachment emit that store first. Notice
        * that we must not request a tile buffer clear here in that case, since
@@ -1886,11 +1778,11 @@ cmd_buffer_render_pass_emit_stores(struct v3dv_cmd_buffer *cmd_buffer,
          cmd_buffer_render_pass_emit_store(cmd_buffer, cl,
                                            attachment_idx, layer,
                                            RENDER_TARGET_0 + i,
-                                           needs_clear && !use_global_rt_clear,
+                                           needs_clear && !use_global_clear,
                                            false);
          has_stores = true;
       } else if (needs_clear) {
-         use_global_rt_clear = true;
+         use_global_clear = true;
       }
    }
 
@@ -1904,10 +1796,10 @@ cmd_buffer_render_pass_emit_stores(struct v3dv_cmd_buffer *cmd_buffer,
    /* If we have any depth/stencil clears we can't use the per-buffer clear
     * bit and instead we have to emit a single clear of all tile buffers.
     */
-   if (use_global_zs_clear || use_global_rt_clear) {
+   if (use_global_clear) {
       cl_emit(cl, CLEAR_TILE_BUFFERS, clear) {
-         clear.clear_z_stencil_buffer = use_global_zs_clear;
-         clear.clear_all_render_targets = use_global_rt_clear;
+         clear.clear_z_stencil_buffer = true;
+         clear.clear_all_render_targets = true;
       }
    }
 }
@@ -2019,7 +1911,7 @@ cmd_buffer_emit_render_pass_layer_rcl(struct v3dv_cmd_buffer *cmd_buffer,
       }
       if (i == 0 && cmd_buffer->state.tile_aligned_render_area) {
          cl_emit(rcl, CLEAR_TILE_BUFFERS, clear) {
-            clear.clear_z_stencil_buffer = !job->early_zs_clear;
+            clear.clear_z_stencil_buffer = true;
             clear.clear_all_render_targets = true;
          }
       }
@@ -2060,18 +1952,12 @@ cmd_buffer_emit_render_pass_layer_rcl(struct v3dv_cmd_buffer *cmd_buffer,
 
 static void
 set_rcl_early_z_config(struct v3dv_job *job,
+                       uint32_t fb_width,
+                       uint32_t fb_height,
+                       bool needs_depth_load,
                        bool *early_z_disable,
                        uint32_t *early_z_test_and_update_direction)
 {
-   /* If this is true then we have not emitted any draw calls in this job
-    * and we don't get any benefits form early Z.
-    */
-   if (!job->decided_global_ez_enable) {
-      assert(job->draw_count == 0);
-      *early_z_disable = true;
-      return;
-   }
-
    switch (job->first_ez_state) {
    case VC5_EZ_UNDECIDED:
    case VC5_EZ_LT_LE:
@@ -2086,6 +1972,16 @@ set_rcl_early_z_config(struct v3dv_job *job,
       *early_z_disable = true;
       break;
    }
+
+   /* GFXH-1918: the early-z buffer may load incorrect depth values
+    * if the frame has odd width or height.
+    */
+   if (*early_z_disable == false && needs_depth_load &&
+       ((fb_width % 2) != 0 || (fb_height % 2) != 0)) {
+      perf_debug("Loading depth aspect for framebuffer with odd width "
+                 "or height disables early-Z tests.\n");
+      *early_z_disable = true;
+   }
 }
 
 static void
@@ -2124,7 +2020,6 @@ cmd_buffer_emit_render_pass_rcl(struct v3dv_cmd_buffer *cmd_buffer)
     * Z_STENCIL_CLEAR_VALUES must be last. The ones in between are optional
     * updates to the previous HW state.
     */
-   bool do_early_zs_clear = false;
    const uint32_t ds_attachment_idx = subpass->ds_attachment.attachment;
    cl_emit(rcl, TILE_RENDERING_MODE_CFG_COMMON, config) {
       config.image_width_pixels = framebuffer->width;
@@ -2138,69 +2033,22 @@ cmd_buffer_emit_render_pass_rcl(struct v3dv_cmd_buffer *cmd_buffer)
             framebuffer->attachments[ds_attachment_idx];
          config.internal_depth_type = iview->internal_type;
 
+         bool needs_depth_load =
+            check_needs_load(state,
+                             pass->attachments[ds_attachment_idx].first_subpass,
+                             pass->attachments[ds_attachment_idx].desc.loadOp);
+
          set_rcl_early_z_config(job,
+                                framebuffer->width,
+                                framebuffer->height,
+                                needs_depth_load,
                                 &config.early_z_disable,
                                 &config.early_z_test_and_update_direction);
-
-         /* Early-Z/S clear can be enabled if the job is clearing and not
-          * storing (or loading) depth. If a stencil aspect is also present
-          * we have the same requirements for it, however, in this case we
-          * can accept stencil loadOp DONT_CARE as well, so instead of
-          * checking that stencil is cleared we check that is not loaded.
-          *
-          * Early-Z/S clearing is independent of Early Z/S testing, so it is
-          * possible to enable one but not the other so long as their
-          * respective requirements are met.
-          */
-         struct v3dv_render_pass_attachment *ds_attachment =
-            &pass->attachments[ds_attachment_idx];
-
-         const VkImageAspectFlags ds_aspects =
-            vk_format_aspects(ds_attachment->desc.format);
-
-         bool needs_depth_clear =
-            check_needs_clear(state,
-                              ds_aspects & VK_IMAGE_ASPECT_DEPTH_BIT,
-                              ds_attachment->first_subpass,
-                              ds_attachment->desc.loadOp,
-                              subpass->do_depth_clear_with_draw);
-
-         bool needs_depth_store =
-            check_needs_store(state,
-                              ds_aspects & VK_IMAGE_ASPECT_DEPTH_BIT,
-                              ds_attachment->last_subpass,
-                              ds_attachment->desc.storeOp);
-
-         do_early_zs_clear = needs_depth_clear && !needs_depth_store;
-         if (do_early_zs_clear &&
-             vk_format_has_stencil(ds_attachment->desc.format)) {
-            bool needs_stencil_load =
-               check_needs_load(state,
-                                ds_aspects & VK_IMAGE_ASPECT_STENCIL_BIT,
-                                ds_attachment->first_subpass,
-                                ds_attachment->desc.stencilLoadOp);
-
-            bool needs_stencil_store =
-               check_needs_store(state,
-                                 ds_aspects & VK_IMAGE_ASPECT_STENCIL_BIT,
-                                 ds_attachment->last_subpass,
-                                 ds_attachment->desc.stencilStoreOp);
-
-            do_early_zs_clear = !needs_stencil_load && !needs_stencil_store;
-         }
-
-         config.early_depth_stencil_clear = do_early_zs_clear;
       } else {
          config.early_z_disable = true;
       }
    }
 
-   /* If we enabled early Z/S clear, then we can't emit any "Clear Tile Buffers"
-    * commands with the Z/S bit set, so keep track of whether we enabled this
-    * in the job so we can skip these later.
-    */
-   job->early_zs_clear = do_early_zs_clear;
-
    for (uint32_t i = 0; i < subpass->color_count; i++) {
       uint32_t attachment_idx = subpass->color_attachments[i].attachment;
       if (attachment_idx == VK_ATTACHMENT_UNUSED)
@@ -2995,91 +2843,22 @@ cmd_buffer_bind_pipeline_static_state(struct v3dv_cmd_buffer *cmd_buffer,
 static void
 job_update_ez_state(struct v3dv_job *job,
                     struct v3dv_pipeline *pipeline,
-                    struct v3dv_cmd_buffer *cmd_buffer)
-{
-   /* If first_ez_state is VC5_EZ_DISABLED it means that we have already
-    * determined that we should disable EZ completely for all draw calls in
-    * this job. This will cause us to disable EZ for the entire job in the
-    * Tile Rendering Mode RCL packet and when we do that we need to make sure
-    * we never emit a draw call in the job with EZ enabled in the CFG_BITS
-    * packet, so ez_state must also be VC5_EZ_DISABLED;
-    */
-   if (job->first_ez_state == VC5_EZ_DISABLED) {
-      assert(job->ez_state = VC5_EZ_DISABLED);
+                    struct v3dv_cmd_buffer_state *state)
+{
+   /* If we don't have a depth attachment at all, disable */
+   if (!state->pass) {
+      job->ez_state = VC5_EZ_DISABLED;
       return;
    }
 
-   /* This is part of the pre draw call handling, so we should be inside a
-    * render pass.
-    */
-   assert(cmd_buffer->state.pass);
-
-   /* If this is the first time we update EZ state for this job we first check
-    * if there is anything that requires disabling it completely for the entire
-    * job (based on state that is not related to the current draw call and
-    * pipeline state).
-    */
-   if (!job->decided_global_ez_enable) {
-      job->decided_global_ez_enable = true;
-
-      struct v3dv_cmd_buffer_state *state = &cmd_buffer->state;
-      assert(state->subpass_idx < state->pass->subpass_count);
-      struct v3dv_subpass *subpass = &state->pass->subpasses[state->subpass_idx];
-      if (subpass->ds_attachment.attachment == VK_ATTACHMENT_UNUSED) {
-         job->first_ez_state = VC5_EZ_DISABLED;
-         job->ez_state = VC5_EZ_DISABLED;
-         return;
-      }
-
-      /* GFXH-1918: the early-z buffer may load incorrect depth values
-       * if the frame has odd width or height.
-       *
-       * So we need to disable EZ in this case.
-       */
-      const struct v3dv_render_pass_attachment *ds_attachment =
-         &state->pass->attachments[subpass->ds_attachment.attachment];
-
-      const VkImageAspectFlags ds_aspects =
-         vk_format_aspects(ds_attachment->desc.format);
-
-      bool needs_depth_load =
-         check_needs_load(state,
-                          ds_aspects & VK_IMAGE_ASPECT_DEPTH_BIT,
-                          ds_attachment->first_subpass,
-                          ds_attachment->desc.loadOp);
-
-      if (needs_depth_load) {
-         struct v3dv_framebuffer *fb = state->framebuffer;
-
-         if (!fb) {
-            assert(cmd_buffer->level == VK_COMMAND_BUFFER_LEVEL_SECONDARY);
-            perf_debug("Loading depth aspect in a secondary command buffer "
-                       "without framebuffer info disables early-z tests.\n");
-            job->first_ez_state = VC5_EZ_DISABLED;
-            job->ez_state = VC5_EZ_DISABLED;
-            return;
-         }
-
-         if (((fb->width % 2) != 0 || (fb->height % 2) != 0)) {
-            perf_debug("Loading depth aspect for framebuffer with odd width "
-                       "or height disables early-Z tests.\n");
-            job->first_ez_state = VC5_EZ_DISABLED;
-            job->ez_state = VC5_EZ_DISABLED;
-            return;
-         }
-      }
-   }
-
-   /* Otherwise, we can decide to selectively enable or disable EZ for draw
-    * calls using the CFG_BITS packet based on the bound pipeline state.
-    */
-
-   /* If the FS writes Z, then it may update against the chosen EZ direction */
-   if (pipeline->fs->current_variant->prog_data.fs->writes_z) {
+   assert(state->subpass_idx < state->pass->subpass_count);
+   struct v3dv_subpass *subpass = &state->pass->subpasses[state->subpass_idx];
+   if (subpass->ds_attachment.attachment == VK_ATTACHMENT_UNUSED) {
       job->ez_state = VC5_EZ_DISABLED;
       return;
    }
 
+   /* Otherwise, look at the curently bound pipeline state */
    switch (pipeline->ez_state) {
    case VC5_EZ_UNDECIDED:
       /* If the pipeline didn't pick a direction but didn't disable, then go
@@ -3107,6 +2886,10 @@ job_update_ez_state(struct v3dv_job *job,
       break;
    }
 
+   /* If the FS writes Z, then it may update against the chosen EZ direction */
+   if (pipeline->fs->current_variant->prog_data.fs->writes_z)
+      job->ez_state = VC5_EZ_DISABLED;
+
    if (job->first_ez_state == VC5_EZ_UNDECIDED &&
        job->ez_state != VC5_EZ_DISABLED) {
       job->first_ez_state = job->ez_state;
@@ -3712,56 +3495,31 @@ emit_configuration_bits(struct v3dv_cmd_buffer *cmd_buffer)
    struct v3dv_pipeline *pipeline = cmd_buffer->state.pipeline;
    assert(pipeline);
 
-   job_update_ez_state(job, pipeline, cmd_buffer);
+   job_update_ez_state(job, pipeline, &cmd_buffer->state);
 
    v3dv_cl_ensure_space_with_branch(&job->bcl, cl_packet_length(CFG_BITS));
    v3dv_return_if_oom(cmd_buffer, NULL);
 
    cl_emit_with_prepacked(&job->bcl, CFG_BITS, pipeline->cfg_bits, config) {
-      config.early_z_enable = job->ez_state != VC5_EZ_DISABLED;
-      config.early_z_updates_enable = config.early_z_enable &&
-                                      pipeline->z_updates_enable;
+      config.early_z_updates_enable = job->ez_state != VC5_EZ_DISABLED;
+      config.early_z_enable = config.early_z_updates_enable;
    }
 }
 
 static void
-update_uniform_state(struct v3dv_cmd_buffer *cmd_buffer,
-                     uint32_t dirty_uniform_state)
+update_uniform_state(struct v3dv_cmd_buffer *cmd_buffer)
 {
-   /* We need to update uniform streams if any piece of state that is passed
-    * to the shader as a uniform may have changed.
-    *
-    * If only descriptor sets are dirty then we can safely ignore updates
-    * for shader stages that don't access descriptors.
-    */
-
    struct v3dv_pipeline *pipeline = cmd_buffer->state.pipeline;
    assert(pipeline);
 
-   const bool dirty_descriptors_only =
-      (cmd_buffer->state.dirty & dirty_uniform_state) ==
-      V3DV_CMD_DIRTY_DESCRIPTOR_SETS;
+   cmd_buffer->state.uniforms.fs =
+      v3dv_write_uniforms(cmd_buffer, pipeline->fs);
 
-   const bool needs_fs_update =
-      !dirty_descriptors_only ||
-      (pipeline->layout->shader_stages & VK_SHADER_STAGE_FRAGMENT_BIT);
+   cmd_buffer->state.uniforms.vs =
+      v3dv_write_uniforms(cmd_buffer, pipeline->vs);
 
-   if (needs_fs_update) {
-      cmd_buffer->state.uniforms.fs =
-         v3dv_write_uniforms(cmd_buffer, pipeline->fs);
-   }
-
-   const bool needs_vs_update =
-      !dirty_descriptors_only ||
-      (pipeline->layout->shader_stages & VK_SHADER_STAGE_VERTEX_BIT);
-
-   if (needs_vs_update) {
-      cmd_buffer->state.uniforms.vs =
-         v3dv_write_uniforms(cmd_buffer, pipeline->vs);
-
-      cmd_buffer->state.uniforms.vs_bin =
-         v3dv_write_uniforms(cmd_buffer, pipeline->vs_bin);
-   }
+   cmd_buffer->state.uniforms.vs_bin =
+      v3dv_write_uniforms(cmd_buffer, pipeline->vs_bin);
 }
 
 static void
@@ -4308,16 +4066,14 @@ cmd_buffer_emit_pre_draw(struct v3dv_cmd_buffer *cmd_buffer)
     */
    uint32_t *dirty = &cmd_buffer->state.dirty;
 
-   const uint32_t dirty_uniform_state =
-      *dirty & (V3DV_CMD_DIRTY_PIPELINE |
-                V3DV_CMD_DIRTY_PUSH_CONSTANTS |
-                V3DV_CMD_DIRTY_DESCRIPTOR_SETS |
-                V3DV_CMD_DIRTY_VIEWPORT);
-
-   if (dirty_uniform_state)
-      update_uniform_state(cmd_buffer, dirty_uniform_state);
+   const bool dirty_uniforms = *dirty & (V3DV_CMD_DIRTY_PIPELINE |
+                                         V3DV_CMD_DIRTY_PUSH_CONSTANTS |
+                                         V3DV_CMD_DIRTY_DESCRIPTOR_SETS |
+                                         V3DV_CMD_DIRTY_VIEWPORT);
+   if (dirty_uniforms)
+      update_uniform_state(cmd_buffer);
 
-   if (dirty_uniform_state || (*dirty & V3DV_CMD_DIRTY_VERTEX_BUFFER))
+   if (dirty_uniforms || (*dirty & V3DV_CMD_DIRTY_VERTEX_BUFFER))
       emit_gl_shader_state(cmd_buffer);
 
    if (*dirty & (V3DV_CMD_DIRTY_PIPELINE)) {
@@ -5166,7 +4922,8 @@ v3dv_cmd_buffer_rewrite_indirect_csd_job(
       /* Make sure the GPU is not currently accessing the indirect CL for this
        * job, since we are about to overwrite some of the uniform data.
        */
-      v3dv_bo_wait(job->device, job->indirect.bo, PIPE_TIMEOUT_INFINITE);
+      const uint64_t infinite = 0xffffffffffffffffull;
+      v3dv_bo_wait(job->device, job->indirect.bo, infinite);
 
       for (uint32_t i = 0; i < 3; i++) {
          if (info->wg_uniform_offsets[i]) {
diff --git a/src/broadcom/vulkan/v3dv_descriptor_set.c b/src/broadcom/vulkan/v3dv_descriptor_set.c
index 54787c99abe..1cf09f35248 100644
--- a/src/broadcom/vulkan/v3dv_descriptor_set.c
+++ b/src/broadcom/vulkan/v3dv_descriptor_set.c
@@ -359,8 +359,6 @@ v3dv_CreatePipelineLayout(VkDevice _device,
          dynamic_offset_count += set_layout->binding[b].array_size *
             set_layout->binding[b].dynamic_offset_count;
       }
-
-      layout->shader_stages |= set_layout->shader_stages;
    }
 
    layout->push_constant_size = 0;
@@ -411,7 +409,6 @@ v3dv_CreateDescriptorPool(VkDevice _device,
    uint32_t bo_size = 0;
    uint32_t descriptor_count = 0;
 
-   assert(pCreateInfo->poolSizeCount > 0);
    for (unsigned i = 0; i < pCreateInfo->poolSizeCount; ++i) {
       /* Verify supported descriptor type */
       switch(pCreateInfo->pPoolSizes[i].type) {
@@ -432,7 +429,6 @@ v3dv_CreateDescriptorPool(VkDevice _device,
          break;
       }
 
-      assert(pCreateInfo->pPoolSizes[i].descriptorCount > 0);
       descriptor_count += pCreateInfo->pPoolSizes[i].descriptorCount;
       bo_size += descriptor_bo_size(pCreateInfo->pPoolSizes[i].type) *
          pCreateInfo->pPoolSizes[i].descriptorCount;
@@ -711,6 +707,10 @@ v3dv_CreateDescriptorSetLayout(VkDevice _device,
       dynamic_offset_count += binding->descriptorCount *
          set_layout->binding[binding_number].dynamic_offset_count;
 
+      /* FIXME: right now we don't use shader_stages. We could explore if we
+       * could use it to add another filter to upload or allocate the
+       * descriptor data.
+       */
       set_layout->shader_stages |= binding->stageFlags;
 
       set_layout->binding[binding_number].descriptor_offset = set_layout->bo_size;
diff --git a/src/broadcom/vulkan/v3dv_meta_copy.c b/src/broadcom/vulkan/v3dv_meta_copy.c
index 6adfef7db4d..46035f8f8cb 100644
--- a/src/broadcom/vulkan/v3dv_meta_copy.c
+++ b/src/broadcom/vulkan/v3dv_meta_copy.c
@@ -4987,7 +4987,6 @@ blit_shader(struct v3dv_cmd_buffer *cmd_buffer,
 {
    bool handled = true;
    VkResult result;
-   uint32_t dirty_dynamic_state = 0;
 
    /* We don't support rendering to linear depth/stencil, this should have
     * been rewritten to a compatible color blit by the caller.
@@ -5088,7 +5087,7 @@ blit_shader(struct v3dv_cmd_buffer *cmd_buffer,
 
    uint32_t min_dst_layer;
    uint32_t max_dst_layer;
-   bool dst_mirror_z = false;
+   bool dst_mirror_z;
    if (dst->type != VK_IMAGE_TYPE_3D) {
       min_dst_layer = region.dstSubresource.baseArrayLayer;
       max_dst_layer = min_dst_layer + region.dstSubresource.layerCount;
@@ -5100,7 +5099,7 @@ blit_shader(struct v3dv_cmd_buffer *cmd_buffer,
 
    uint32_t min_src_layer;
    uint32_t max_src_layer;
-   bool src_mirror_z = false;
+   bool src_mirror_z;
    if (src->type != VK_IMAGE_TYPE_3D) {
       min_src_layer = region.srcSubresource.baseArrayLayer;
       max_src_layer = min_src_layer + region.srcSubresource.layerCount;
@@ -5213,13 +5212,14 @@ blit_shader(struct v3dv_cmd_buffer *cmd_buffer,
    };
    v3dv_CmdSetScissor(_cmd_buffer, 0, 1, &scissor);
 
-   bool can_skip_tlb_load = false;
+   bool can_skip_tlb_load;
    const VkRect2D render_area = {
       .offset = { dst_x, dst_y },
       .extent = { dst_w, dst_h },
    };
 
    /* Record per-layer commands */
+   uint32_t dirty_dynamic_state = 0;
    VkImageAspectFlags aspects = region.dstSubresource.aspectMask;
    for (uint32_t i = 0; i < layer_count; i++) {
       /* Setup framebuffer */
diff --git a/src/broadcom/vulkan/v3dv_pipeline.c b/src/broadcom/vulkan/v3dv_pipeline.c
index e9a0ea777f8..0274c4a5bb6 100644
--- a/src/broadcom/vulkan/v3dv_pipeline.c
+++ b/src/broadcom/vulkan/v3dv_pipeline.c
@@ -205,7 +205,6 @@ static const struct spirv_to_nir_options default_spirv_options =  {
 };
 
 const nir_shader_compiler_options v3dv_nir_options = {
-   .lower_add_sat = true,
    .lower_all_io_to_temps = true,
    .lower_extract_byte = true,
    .lower_extract_word = true,
@@ -747,7 +746,7 @@ lower_sampler(nir_builder *b, nir_tex_instr *instr,
               struct v3dv_pipeline *pipeline,
               const struct v3dv_pipeline_layout *layout)
 {
-   uint8_t return_size = 0;
+   uint8_t return_size;
 
    int texture_idx =
       nir_tex_instr_src_index(instr, nir_tex_src_texture_deref);
@@ -2335,8 +2334,6 @@ pack_cfg_bits(struct v3dv_pipeline *pipeline,
 
       config.stencil_enable =
          ds_info ? ds_info->stencilTestEnable && has_ds_attachment: false;
-
-      pipeline->z_updates_enable = config.z_updates_enable;
    };
 }
 
diff --git a/src/broadcom/vulkan/v3dv_private.h b/src/broadcom/vulkan/v3dv_private.h
index 7ffa8639f58..0f0b15c9cc2 100644
--- a/src/broadcom/vulkan/v3dv_private.h
+++ b/src/broadcom/vulkan/v3dv_private.h
@@ -896,14 +896,6 @@ struct v3dv_job {
    enum v3dv_ez_state ez_state;
    enum v3dv_ez_state first_ez_state;
 
-   /* If we have already decided if we need to disable Early Z/S completely
-    * for this job.
-    */
-   bool decided_global_ez_enable;
-
-   /* If this job has been configured to use early Z/S clear */
-   bool early_zs_clear;
-
    /* Number of draw calls recorded into the job */
    uint32_t draw_count;
 
@@ -1556,11 +1548,8 @@ struct v3dv_pipeline_layout {
    } set[MAX_SETS];
 
    uint32_t num_sets;
-
-   /* Shader stages that are declared to use descriptors from this layout */
-   uint32_t shader_stages;
-
    uint32_t dynamic_offset_count;
+
    uint32_t push_constant_size;
 };
 
@@ -1660,9 +1649,6 @@ struct v3dv_pipeline {
 
    struct v3dv_pipeline_layout *layout;
 
-   /* Whether this pipeline enables depth writes */
-   bool z_updates_enable;
-
    enum v3dv_ez_state ez_state;
 
    bool msaa;
diff --git a/src/broadcom/vulkan/v3dv_query.c b/src/broadcom/vulkan/v3dv_query.c
index d3100498cbe..0afd91b4733 100644
--- a/src/broadcom/vulkan/v3dv_query.c
+++ b/src/broadcom/vulkan/v3dv_query.c
@@ -228,7 +228,7 @@ v3dv_get_query_pool_results_cpu(struct v3dv_device *device,
 
    VkResult result = VK_SUCCESS;
    for (uint32_t i = first; i < first + count; i++) {
-      bool available = false;
+      bool available;
       uint64_t value = get_query_result(device, pool, i, do_wait, &available);
 
       /**
diff --git a/src/broadcom/vulkan/v3dv_queue.c b/src/broadcom/vulkan/v3dv_queue.c
index 6ea6d1acff8..cc1b29b0768 100644
--- a/src/broadcom/vulkan/v3dv_queue.c
+++ b/src/broadcom/vulkan/v3dv_queue.c
@@ -153,9 +153,6 @@ v3dv_QueueWaitIdle(VkQueue _queue)
 static VkResult
 handle_reset_query_cpu_job(struct v3dv_job *job)
 {
-   struct v3dv_reset_query_cpu_job_info *info = &job->cpu.query_reset;
-   assert(info->pool);
-
    /* We are about to reset query counters so we need to make sure that
     * The GPU is not using them. The exception is timestamp queries, since
     * we handle those in the CPU.
@@ -163,13 +160,21 @@ handle_reset_query_cpu_job(struct v3dv_job *job)
     * FIXME: we could avoid blocking the main thread for this if we use
     *        submission thread.
     */
+   struct v3dv_reset_query_cpu_job_info *info = &job->cpu.query_reset;
+   assert(info->pool);
+
+   if (info->pool->query_type == VK_QUERY_TYPE_OCCLUSION) {
+      VkResult result = gpu_queue_wait_idle(&job->device->queue);
+      if (result != VK_SUCCESS)
+         return result;
+   }
+
    for (uint32_t i = info->first; i < info->first + info->count; i++) {
       assert(i < info->pool->query_count);
       struct v3dv_query *query = &info->pool->queries[i];
       query->maybe_available = false;
       switch (info->pool->query_type) {
       case VK_QUERY_TYPE_OCCLUSION: {
-         v3dv_bo_wait(job->device, query->bo, PIPE_TIMEOUT_INFINITE);
          uint32_t *counter = (uint32_t *) query->bo->map;
          *counter = 0;
          break;
@@ -465,7 +470,8 @@ handle_csd_indirect_cpu_job(struct v3dv_queue *queue,
 
    /* Make sure the GPU is no longer using the indirect buffer*/
    assert(info->buffer && info->buffer->mem && info->buffer->mem->bo);
-   v3dv_bo_wait(queue->device, info->buffer->mem->bo, PIPE_TIMEOUT_INFINITE);
+   const uint64_t infinite = 0xffffffffffffffffull;
+   v3dv_bo_wait(queue->device, info->buffer->mem->bo, infinite);
 
    /* Map the indirect buffer and read the dispatch parameters */
    assert(info->buffer && info->buffer->mem && info->buffer->mem->bo);
diff --git a/src/compiler/glsl/builtin_functions.cpp b/src/compiler/glsl/builtin_functions.cpp
index 86ad82ba27e..b8e3f2b780b 100644
--- a/src/compiler/glsl/builtin_functions.cpp
+++ b/src/compiler/glsl/builtin_functions.cpp
@@ -764,13 +764,6 @@ buffer_atomics_supported(const _mesa_glsl_parse_state *state)
    return compute_shader(state) || shader_storage_buffer_object(state);
 }
 
-static bool
-buffer_int64_atomics_supported(const _mesa_glsl_parse_state *state)
-{
-   return state->NV_shader_atomic_int64_enable &&
-      buffer_atomics_supported(state);
-}
-
 static bool
 barrier_supported(const _mesa_glsl_parse_state *state)
 {
@@ -1375,7 +1368,7 @@ builtin_builder::create_intrinsics()
                 _atomic_intrinsic2(NV_shader_atomic_float_supported,
                                    glsl_type::float_type,
                                    ir_intrinsic_generic_atomic_add),
-                _atomic_intrinsic2(buffer_int64_atomics_supported,
+                _atomic_intrinsic2(buffer_atomics_supported,
                                    glsl_type::int64_t_type,
                                    ir_intrinsic_generic_atomic_add),
                 _atomic_counter_intrinsic1(shader_atomic_counter_ops_or_v460_desktop,
@@ -1391,10 +1384,10 @@ builtin_builder::create_intrinsics()
                 _atomic_intrinsic2(INTEL_shader_atomic_float_minmax_supported,
                                    glsl_type::float_type,
                                    ir_intrinsic_generic_atomic_min),
-                _atomic_intrinsic2(buffer_int64_atomics_supported,
+                _atomic_intrinsic2(buffer_atomics_supported,
                                    glsl_type::uint64_t_type,
                                    ir_intrinsic_generic_atomic_min),
-                _atomic_intrinsic2(buffer_int64_atomics_supported,
+                _atomic_intrinsic2(buffer_atomics_supported,
                                    glsl_type::int64_t_type,
                                    ir_intrinsic_generic_atomic_min),
                 _atomic_counter_intrinsic1(shader_atomic_counter_ops_or_v460_desktop,
@@ -1410,10 +1403,10 @@ builtin_builder::create_intrinsics()
                 _atomic_intrinsic2(INTEL_shader_atomic_float_minmax_supported,
                                    glsl_type::float_type,
                                    ir_intrinsic_generic_atomic_max),
-                _atomic_intrinsic2(buffer_int64_atomics_supported,
+                _atomic_intrinsic2(buffer_atomics_supported,
                                    glsl_type::uint64_t_type,
                                    ir_intrinsic_generic_atomic_max),
-                _atomic_intrinsic2(buffer_int64_atomics_supported,
+                _atomic_intrinsic2(buffer_atomics_supported,
                                    glsl_type::int64_t_type,
                                    ir_intrinsic_generic_atomic_max),
                 _atomic_counter_intrinsic1(shader_atomic_counter_ops_or_v460_desktop,
@@ -1426,10 +1419,10 @@ builtin_builder::create_intrinsics()
                 _atomic_intrinsic2(buffer_atomics_supported,
                                    glsl_type::int_type,
                                    ir_intrinsic_generic_atomic_and),
-                _atomic_intrinsic2(buffer_int64_atomics_supported,
+                _atomic_intrinsic2(buffer_atomics_supported,
                                    glsl_type::uint64_t_type,
                                    ir_intrinsic_generic_atomic_and),
-                _atomic_intrinsic2(buffer_int64_atomics_supported,
+                _atomic_intrinsic2(buffer_atomics_supported,
                                    glsl_type::int64_t_type,
                                    ir_intrinsic_generic_atomic_and),
                 _atomic_counter_intrinsic1(shader_atomic_counter_ops_or_v460_desktop,
@@ -1442,10 +1435,10 @@ builtin_builder::create_intrinsics()
                 _atomic_intrinsic2(buffer_atomics_supported,
                                    glsl_type::int_type,
                                    ir_intrinsic_generic_atomic_or),
-                _atomic_intrinsic2(buffer_int64_atomics_supported,
+                _atomic_intrinsic2(buffer_atomics_supported,
                                    glsl_type::uint64_t_type,
                                    ir_intrinsic_generic_atomic_or),
-                _atomic_intrinsic2(buffer_int64_atomics_supported,
+                _atomic_intrinsic2(buffer_atomics_supported,
                                    glsl_type::int64_t_type,
                                    ir_intrinsic_generic_atomic_or),
                 _atomic_counter_intrinsic1(shader_atomic_counter_ops_or_v460_desktop,
@@ -1458,10 +1451,10 @@ builtin_builder::create_intrinsics()
                 _atomic_intrinsic2(buffer_atomics_supported,
                                    glsl_type::int_type,
                                    ir_intrinsic_generic_atomic_xor),
-                _atomic_intrinsic2(buffer_int64_atomics_supported,
+                _atomic_intrinsic2(buffer_atomics_supported,
                                    glsl_type::uint64_t_type,
                                    ir_intrinsic_generic_atomic_xor),
-                _atomic_intrinsic2(buffer_int64_atomics_supported,
+                _atomic_intrinsic2(buffer_atomics_supported,
                                    glsl_type::int64_t_type,
                                    ir_intrinsic_generic_atomic_xor),
                 _atomic_counter_intrinsic1(shader_atomic_counter_ops_or_v460_desktop,
@@ -1474,7 +1467,7 @@ builtin_builder::create_intrinsics()
                 _atomic_intrinsic2(buffer_atomics_supported,
                                    glsl_type::int_type,
                                    ir_intrinsic_generic_atomic_exchange),
-                _atomic_intrinsic2(buffer_int64_atomics_supported,
+                _atomic_intrinsic2(buffer_atomics_supported,
                                    glsl_type::int64_t_type,
                                    ir_intrinsic_generic_atomic_exchange),
                 _atomic_intrinsic2(NV_shader_atomic_float_supported,
@@ -1490,7 +1483,7 @@ builtin_builder::create_intrinsics()
                 _atomic_intrinsic3(buffer_atomics_supported,
                                    glsl_type::int_type,
                                    ir_intrinsic_generic_atomic_comp_swap),
-                _atomic_intrinsic3(buffer_int64_atomics_supported,
+                _atomic_intrinsic3(buffer_atomics_supported,
                                    glsl_type::int64_t_type,
                                    ir_intrinsic_generic_atomic_comp_swap),
                 _atomic_intrinsic3(INTEL_shader_atomic_float_minmax_supported,
@@ -4110,7 +4103,7 @@ builtin_builder::create_builtins()
                             shader_atomic_float_add,
                             glsl_type::float_type),
                 _atomic_op2("__intrinsic_atomic_add",
-                            buffer_int64_atomics_supported,
+                            buffer_atomics_supported,
                             glsl_type::int64_t_type),
                 NULL);
    add_function("atomicMin",
@@ -4124,10 +4117,10 @@ builtin_builder::create_builtins()
                             shader_atomic_float_minmax,
                             glsl_type::float_type),
                 _atomic_op2("__intrinsic_atomic_min",
-                            buffer_int64_atomics_supported,
+                            buffer_atomics_supported,
                             glsl_type::uint64_t_type),
                 _atomic_op2("__intrinsic_atomic_min",
-                            buffer_int64_atomics_supported,
+                            buffer_atomics_supported,
                             glsl_type::int64_t_type),
                 NULL);
    add_function("atomicMax",
@@ -4141,10 +4134,10 @@ builtin_builder::create_builtins()
                             shader_atomic_float_minmax,
                             glsl_type::float_type),
                 _atomic_op2("__intrinsic_atomic_max",
-                            buffer_int64_atomics_supported,
+                            buffer_atomics_supported,
                             glsl_type::uint64_t_type),
                 _atomic_op2("__intrinsic_atomic_max",
-                            buffer_int64_atomics_supported,
+                            buffer_atomics_supported,
                             glsl_type::int64_t_type),
                 NULL);
    add_function("atomicAnd",
@@ -4155,10 +4148,10 @@ builtin_builder::create_builtins()
                             buffer_atomics_supported,
                             glsl_type::int_type),
                 _atomic_op2("__intrinsic_atomic_and",
-                            buffer_int64_atomics_supported,
+                            buffer_atomics_supported,
                             glsl_type::uint64_t_type),
                 _atomic_op2("__intrinsic_atomic_and",
-                            buffer_int64_atomics_supported,
+                            buffer_atomics_supported,
                             glsl_type::int64_t_type),
                 NULL);
    add_function("atomicOr",
@@ -4169,10 +4162,10 @@ builtin_builder::create_builtins()
                             buffer_atomics_supported,
                             glsl_type::int_type),
                 _atomic_op2("__intrinsic_atomic_or",
-                            buffer_int64_atomics_supported,
+                            buffer_atomics_supported,
                             glsl_type::uint64_t_type),
                 _atomic_op2("__intrinsic_atomic_or",
-                            buffer_int64_atomics_supported,
+                            buffer_atomics_supported,
                             glsl_type::int64_t_type),
                 NULL);
    add_function("atomicXor",
@@ -4183,10 +4176,10 @@ builtin_builder::create_builtins()
                             buffer_atomics_supported,
                             glsl_type::int_type),
                 _atomic_op2("__intrinsic_atomic_xor",
-                            buffer_int64_atomics_supported,
+                            buffer_atomics_supported,
                             glsl_type::uint64_t_type),
                 _atomic_op2("__intrinsic_atomic_xor",
-                            buffer_int64_atomics_supported,
+                            buffer_atomics_supported,
                             glsl_type::int64_t_type),
                 NULL);
    add_function("atomicExchange",
@@ -4197,7 +4190,7 @@ builtin_builder::create_builtins()
                             buffer_atomics_supported,
                             glsl_type::int_type),
                 _atomic_op2("__intrinsic_atomic_exchange",
-                            buffer_int64_atomics_supported,
+                            buffer_atomics_supported,
                             glsl_type::int64_t_type),
                 _atomic_op2("__intrinsic_atomic_exchange",
                             shader_atomic_float_exchange,
@@ -4211,7 +4204,7 @@ builtin_builder::create_builtins()
                             buffer_atomics_supported,
                             glsl_type::int_type),
                 _atomic_op3("__intrinsic_atomic_comp_swap",
-                            buffer_int64_atomics_supported,
+                            buffer_atomics_supported,
                             glsl_type::int64_t_type),
                 _atomic_op3("__intrinsic_atomic_comp_swap",
                             shader_atomic_float_minmax,
diff --git a/src/compiler/glsl_types.cpp b/src/compiler/glsl_types.cpp
index 98d962481e1..f1c005dbd4b 100644
--- a/src/compiler/glsl_types.cpp
+++ b/src/compiler/glsl_types.cpp
@@ -1830,6 +1830,9 @@ glsl_type::std140_base_alignment(bool row_major) const
       }
    }
 
+   if (this->is_atomic_uint())
+      return this->atomic_size();
+
    /* (4) If the member is an array of scalars or vectors, the base alignment
     *     and array stride are set to match the base alignment of a single
     *     array element, according to rules (1), (2), and (3), and rounded up
@@ -2164,6 +2167,9 @@ glsl_type::std430_base_alignment(bool row_major) const
       }
    }
 
+   if (this->is_atomic_uint())
+      return this->atomic_size();
+
    /* OpenGL 4.30 spec, section 7.6.2.2 "Standard Uniform Block Layout":
     *
     * "When using the std430 storage layout, shader storage blocks will be
diff --git a/src/compiler/nir/nir_inline_uniforms.c b/src/compiler/nir/nir_inline_uniforms.c
index 8fdcd2dfeb2..28d1284e9c8 100644
--- a/src/compiler/nir/nir_inline_uniforms.c
+++ b/src/compiler/nir/nir_inline_uniforms.c
@@ -42,6 +42,35 @@
 /* Maximum value in shader_info::inlinable_uniform_dw_offsets[] */
 #define MAX_OFFSET (UINT16_MAX * 4)
 
+static bool
+src_comes_from_intrinsic(const nir_src *src)
+{
+   if (!src->is_ssa)
+      return false;
+
+   nir_instr *instr = src->ssa->parent_instr;
+
+   switch (instr->type) {
+   case nir_instr_type_alu: {
+      /* Return true if all sources return true. */
+      /* TODO: Swizzles are ignored, so vectors can prevent inlining. */
+      nir_alu_instr *alu = nir_instr_as_alu(instr);
+      for (unsigned i = 0; i < nir_op_infos[alu->op].num_inputs; i++) {
+         if (!src_comes_from_intrinsic(&alu->src[i].src))
+             return false;
+      }
+      return true;
+   }
+
+   case nir_instr_type_intrinsic:
+      /* TODO: maybe only check for intensive intrinsics? */
+      return true;
+
+   default:
+      return false;
+   }
+}
+
 static bool
 src_only_uses_uniforms(const nir_src *src, struct set **uni_offsets)
 {
@@ -97,6 +126,26 @@ src_only_uses_uniforms(const nir_src *src, struct set **uni_offsets)
    }
 }
 
+static struct set *
+generate_offsets(const nir_src *src, struct set *uni_offsets)
+{
+   struct set *found_offsets = NULL;
+
+   if (src_only_uses_uniforms(src, &found_offsets) &&
+       found_offsets) {
+      /* All uniforms are lowerable. Save uniform offsets. */
+      set_foreach(found_offsets, entry) {
+         if (!uni_offsets)
+            uni_offsets = _mesa_set_create_u32_keys(NULL);
+
+         _mesa_set_add(uni_offsets, entry->key);
+      }
+   }
+   if (found_offsets)
+      _mesa_set_destroy(found_offsets, NULL);
+   return uni_offsets;
+}
+
 void
 nir_find_inlinable_uniforms(nir_shader *shader)
 {
@@ -108,20 +157,7 @@ nir_find_inlinable_uniforms(nir_shader *shader)
             switch (node->type) {
             case nir_cf_node_if: {
                const nir_src *cond = &nir_cf_node_as_if(node)->condition;
-               struct set *found_offsets = NULL;
-
-               if (src_only_uses_uniforms(cond, &found_offsets) &&
-                   found_offsets) {
-                  /* All uniforms are lowerable. Save uniform offsets. */
-                  set_foreach(found_offsets, entry) {
-                     if (!uni_offsets)
-                        uni_offsets = _mesa_set_create_u32_keys(NULL);
-
-                     _mesa_set_add(uni_offsets, entry->key);
-                  }
-               }
-               if (found_offsets)
-                  _mesa_set_destroy(found_offsets, NULL);
+               uni_offsets = generate_offsets(cond, uni_offsets);
                break;
             }
 
@@ -129,6 +165,26 @@ nir_find_inlinable_uniforms(nir_shader *shader)
                /* TODO: handle loops if we want to unroll them at draw time */
                break;
 
+            case nir_cf_node_block: {
+               nir_block *block = nir_cf_node_as_block(node);
+               nir_foreach_instr_safe(instr, block) {
+                  switch (instr->type) {
+                  case nir_instr_type_alu: {
+                     nir_alu_instr *alu = nir_instr_as_alu(instr);
+                     if (alu->op != nir_op_bcsel &&
+                         alu->op != nir_op_fcsel)
+                        break;
+                     if (src_comes_from_intrinsic(&alu->src[1].src) ||
+                         src_comes_from_intrinsic(&alu->src[2].src))
+                        uni_offsets = generate_offsets(&alu->src[0].src, uni_offsets);
+                  }
+                  default:
+                     break;
+                  }
+               }
+               break;
+            }
+
             default:
                break;
             }
diff --git a/src/compiler/nir/nir_lower_input_attachments.c b/src/compiler/nir/nir_lower_input_attachments.c
index 2380b0cc88a..f18aac0df0d 100644
--- a/src/compiler/nir/nir_lower_input_attachments.c
+++ b/src/compiler/nir/nir_lower_input_attachments.c
@@ -151,9 +151,8 @@ try_lower_input_load(nir_function_impl *impl, nir_intrinsic_instr *load,
 
    if (tex->is_sparse) {
       unsigned load_result_size = load->dest.ssa.num_components - 1;
-      unsigned load_result_mask = BITFIELD_MASK(load_result_size);
       nir_ssa_def *res = nir_channels(
-         &b, &tex->dest.ssa, load_result_mask | 0x10);
+         &b, &tex->dest.ssa, BITFIELD_MASK(load_result_size) | 0x10);
 
       nir_ssa_def_rewrite_uses(&load->dest.ssa, nir_src_for_ssa(res));
    } else {
diff --git a/src/compiler/nir/nir_lower_io.c b/src/compiler/nir/nir_lower_io.c
index 22d319f74c3..2a0d98d8af4 100644
--- a/src/compiler/nir/nir_lower_io.c
+++ b/src/compiler/nir/nir_lower_io.c
@@ -1986,9 +1986,9 @@ lower_explicit_io_array_length(nir_builder *b, nir_intrinsic_instr *intrin,
    nir_ssa_def *index = addr_to_index(b, addr, addr_format);
    nir_ssa_def *offset = addr_to_offset(b, addr, addr_format);
 
-   nir_ssa_def *arr_size = nir_get_ssbo_size(b, index);
-   arr_size = nir_imax(b, nir_isub(b, arr_size, offset), nir_imm_int(b, 0u));
-   arr_size = nir_idiv(b, arr_size, nir_imm_int(b, stride));
+   nir_ssa_def *arr_size =
+      nir_idiv(b, nir_isub(b, nir_get_ssbo_size(b, index), offset),
+                  nir_imm_int(b, stride));
 
    nir_ssa_def_rewrite_uses(&intrin->dest.ssa, nir_src_for_ssa(arr_size));
    nir_instr_remove(&intrin->instr);
diff --git a/src/compiler/spirv/vtn_alu.c b/src/compiler/spirv/vtn_alu.c
index aaed36bb90e..155682bd0ba 100644
--- a/src/compiler/spirv/vtn_alu.c
+++ b/src/compiler/spirv/vtn_alu.c
@@ -463,7 +463,7 @@ vtn_handle_alu(struct vtn_builder *b, SpvOp opcode,
    struct vtn_value *dest_val = vtn_untyped_value(b, w[2]);
    const struct glsl_type *dest_type = vtn_get_type(b, w[1])->type;
 
-   vtn_foreach_decoration(b, dest_val, handle_no_contraction, NULL);
+   vtn_handle_no_contraction(b, dest_val);
 
    /* Collect the various SSA sources */
    const unsigned num_inputs = count - 3;
@@ -851,3 +851,9 @@ vtn_handle_bitcast(struct vtn_builder *b, const uint32_t *w, unsigned count)
       nir_bitcast_vector(&b->nb, src, glsl_get_bit_size(type->type));
    vtn_push_nir_ssa(b, w[2], val);
 }
+
+void
+vtn_handle_no_contraction(struct vtn_builder *b, struct vtn_value *val)
+{
+   vtn_foreach_decoration(b, val, handle_no_contraction, NULL);
+}
diff --git a/src/compiler/spirv/vtn_glsl450.c b/src/compiler/spirv/vtn_glsl450.c
index af6b1330563..bba9542daf7 100644
--- a/src/compiler/spirv/vtn_glsl450.c
+++ b/src/compiler/spirv/vtn_glsl450.c
@@ -324,6 +324,7 @@ handle_glsl450_alu(struct vtn_builder *b, enum GLSLstd450 entrypoint,
    }
 
    struct vtn_ssa_value *dest = vtn_create_ssa_value(b, dest_type);
+   vtn_handle_no_contraction(b, vtn_untyped_value(b, w[2]));
    switch (entrypoint) {
    case GLSLstd450Radians:
       dest->def = nir_radians(nb, src[0]);
@@ -550,12 +551,13 @@ handle_glsl450_alu(struct vtn_builder *b, enum GLSLstd450 entrypoint,
          b->shader->info.float_controls_execution_mode;
       bool exact;
       nir_op op = vtn_nir_alu_op_for_spirv_glsl_opcode(b, entrypoint, execution_mode, &exact);
-      b->nb.exact = exact;
+      /* don't override explicit decoration */
+      b->nb.exact |= exact;
       dest->def = nir_build_alu(&b->nb, op, src[0], src[1], src[2], NULL);
-      b->nb.exact = false;
       break;
    }
    }
+   b->nb.exact = false;
 
    vtn_push_ssa_value(b, w[2], dest);
 }
diff --git a/src/compiler/spirv/vtn_private.h b/src/compiler/spirv/vtn_private.h
index 11b95b60f4e..f230e6e8152 100644
--- a/src/compiler/spirv/vtn_private.h
+++ b/src/compiler/spirv/vtn_private.h
@@ -916,6 +916,8 @@ void vtn_handle_alu(struct vtn_builder *b, SpvOp opcode,
 void vtn_handle_bitcast(struct vtn_builder *b, const uint32_t *w,
                         unsigned count);
 
+void vtn_handle_no_contraction(struct vtn_builder *b, struct vtn_value *val);
+
 void vtn_handle_subgroup(struct vtn_builder *b, SpvOp opcode,
                          const uint32_t *w, unsigned count);
 
diff --git a/src/freedreno/Android.ir3.mk b/src/freedreno/Android.ir3.mk
index 0fbb8c50ca6..d08a6d789b6 100644
--- a/src/freedreno/Android.ir3.mk
+++ b/src/freedreno/Android.ir3.mk
@@ -46,10 +46,6 @@ LOCAL_C_INCLUDES := \
 	$(MESA_TOP)/src/freedreno/ir3 \
 	$(intermediates)/ir3
 
-LOCAL_WHOLE_STATIC_LIBRARIES := \
-	libir3decode \
-	libir3encode
-
 # We need libmesa_nir to get NIR's generated include directories.
 LOCAL_STATIC_LIBRARIES := \
 	libmesa_nir
diff --git a/src/freedreno/Android.isa.mk b/src/freedreno/Android.isa.mk
deleted file mode 100644
index 97ed0f4a2d3..00000000000
--- a/src/freedreno/Android.isa.mk
+++ /dev/null
@@ -1,123 +0,0 @@
-# Mesa 3-D graphics library
-#
-# Copyright (C) 2021 Mauro Rossi issor.oruam@gmail.com
-#
-# Permission is hereby granted, free of charge, to any person obtaining a
-# copy of this software and associated documentation files (the "Software"),
-# to deal in the Software without restriction, including without limitation
-# the rights to use, copy, modify, merge, publish, distribute, sublicense,
-# and/or sell copies of the Software, and to permit persons to whom the
-# Software is furnished to do so, subject to the following conditions:
-#
-# The above copyright notice and this permission notice shall be included
-# in all copies or substantial portions of the Software.
-#
-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
-# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
-# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
-# DEALINGS IN THE SOFTWARE.
-
-# Android.mk for libir3decode.a and libir3encode.a
-
-# ---------------------------------------
-# Build libir3decode
-# ---------------------------------------
-
-include $(CLEAR_VARS)
-
-LOCAL_SRC_FILES := \
-	$(decode_SOURCES)
-
-LOCAL_MODULE := libir3decode
-
-LOCAL_MODULE_CLASS := STATIC_LIBRARIES
-
-intermediates := $(call local-generated-sources-dir)
-
-LOCAL_C_INCLUDES := \
-	$(MESA_TOP)/src/gallium/include \
-	$(MESA_TOP)/src/gallium/auxiliary \
-	$(MESA_TOP)/src/freedreno/isa \
-	$(intermediates)/isa
-
-LOCAL_GENERATED_SOURCES += $(addprefix $(intermediates)/isa/, ir3-isa.c)
-
-ir3-isa_c_gen := \
-	$(MESA_TOP)/src/freedreno/isa/decode.py
-
-ir3-isa_c_deps := \
-	$(MESA_TOP)/src/freedreno/isa/ir3.xml \
-	$(MESA_TOP)/src/freedreno/isa/ir3-common.xml \
-	$(MESA_TOP)/src/freedreno/isa/ir3-cat0.xml \
-	$(MESA_TOP)/src/freedreno/isa/ir3-cat1.xml \
-	$(MESA_TOP)/src/freedreno/isa/ir3-cat2.xml \
-	$(MESA_TOP)/src/freedreno/isa/ir3-cat3.xml \
-	$(MESA_TOP)/src/freedreno/isa/ir3-cat4.xml \
-	$(MESA_TOP)/src/freedreno/isa/ir3-cat5.xml \
-	$(MESA_TOP)/src/freedreno/isa/ir3-cat6.xml \
-	$(MESA_TOP)/src/freedreno/isa/ir3-cat7.xml \
-	$(MESA_TOP)/src/freedreno/isa/isa.py
-
-$(intermediates)/isa/ir3-isa.c: $(ir3-isa_c_deps)
-	@mkdir -p $(dir $@)
-	$(hide) $(MESA_PYTHON3) $(ir3-isa_c_gen) $< $@
-
-include $(MESA_COMMON_MK)
-include $(BUILD_STATIC_LIBRARY)
-
-# ---------------------------------------
-# Build libir3encode
-# ---------------------------------------
-
-include $(CLEAR_VARS)
-
-LOCAL_SRC_FILES := \
-	$(encode_SOURCES)
-
-LOCAL_MODULE := libir3encode
-
-LOCAL_MODULE_CLASS := STATIC_LIBRARIES
-
-intermediates := $(call local-generated-sources-dir)
-
-LOCAL_C_INCLUDES := \
-	$(MESA_TOP)/src/compiler/nir \
-	$(MESA_TOP)/src/gallium/include \
-	$(MESA_TOP)/src/gallium/auxiliary \
-	$(MESA_TOP)/src/freedreno/isa \
-	$(intermediates)/isa
-
-# We need libmesa_nir to get NIR's generated include directories.
-LOCAL_STATIC_LIBRARIES := \
-	libmesa_nir
-
-LOCAL_GENERATED_SOURCES := \
-	$(MESA_GEN_NIR_H)
-
-LOCAL_GENERATED_SOURCES += $(addprefix $(intermediates)/isa/, encode.h)
-
-encode_h_gen := \
-	$(MESA_TOP)/src/freedreno/isa/encode.py
-
-encode_h_deps := \
-	$(MESA_TOP)/src/freedreno/isa/ir3.xml \
-	$(MESA_TOP)/src/freedreno/isa/ir3-common.xml \
-	$(MESA_TOP)/src/freedreno/isa/ir3-cat0.xml \
-	$(MESA_TOP)/src/freedreno/isa/ir3-cat1.xml \
-	$(MESA_TOP)/src/freedreno/isa/ir3-cat2.xml \
-	$(MESA_TOP)/src/freedreno/isa/ir3-cat3.xml \
-	$(MESA_TOP)/src/freedreno/isa/ir3-cat4.xml \
-	$(MESA_TOP)/src/freedreno/isa/ir3-cat5.xml \
-	$(MESA_TOP)/src/freedreno/isa/ir3-cat6.xml \
-	$(MESA_TOP)/src/freedreno/isa/ir3-cat7.xml \
-	$(MESA_TOP)/src/freedreno/isa/isa.py
-
-$(intermediates)/isa/encode.h: $(encode_h_deps)
-	@mkdir -p $(dir $@)
-	$(hide) $(MESA_PYTHON3) $(encode_h_gen) $< $@
-
-include $(MESA_COMMON_MK)
-include $(BUILD_STATIC_LIBRARY)
diff --git a/src/freedreno/Android.mk b/src/freedreno/Android.mk
index a488803f98c..7c3d30753ba 100644
--- a/src/freedreno/Android.mk
+++ b/src/freedreno/Android.mk
@@ -29,6 +29,5 @@ include $(LOCAL_PATH)/Android.common.mk
 include $(LOCAL_PATH)/Android.drm.mk
 include $(LOCAL_PATH)/Android.ir2.mk
 include $(LOCAL_PATH)/Android.ir3.mk
-include $(LOCAL_PATH)/Android.isa.mk
 include $(LOCAL_PATH)/Android.perfcntrs.mk
 include $(LOCAL_PATH)/Android.registers.mk
diff --git a/src/freedreno/Makefile.sources b/src/freedreno/Makefile.sources
index 2d17e66f7b0..c2e445d042b 100644
--- a/src/freedreno/Makefile.sources
+++ b/src/freedreno/Makefile.sources
@@ -6,11 +6,6 @@ common_SOURCES := \
 	common/freedreno_uuid.c \
 	common/freedreno_uuid.h
 
-decode_SOURCES := \
-	isa/decode.c \
-	isa/decode.h \
-	isa/isa.h
-
 drm_SOURCES := \
 	drm/freedreno_bo.c \
 	drm/freedreno_drmif.h \
@@ -27,10 +22,6 @@ drm_SOURCES := \
 	drm/freedreno_priv.h \
 	drm/msm_ringbuffer.c
 
-encode_SOURCES := \
-	isa/encode.c \
-	isa/isa.h
-
 ir2_SOURCES := \
 	ir2/disasm-a2xx.c \
 	ir2/instr-a2xx.h
diff --git a/src/freedreno/ir3/ir3_compiler.c b/src/freedreno/ir3/ir3_compiler.c
index c27e8bcedfe..09167e9d4a0 100644
--- a/src/freedreno/ir3/ir3_compiler.c
+++ b/src/freedreno/ir3/ir3_compiler.c
@@ -50,10 +50,8 @@ static const struct debug_named_value shader_debug_options[] = {
 };
 
 DEBUG_GET_ONCE_FLAGS_OPTION(ir3_shader_debug, "IR3_SHADER_DEBUG", shader_debug_options, 0)
-DEBUG_GET_ONCE_OPTION(ir3_shader_override_path, "IR3_SHADER_OVERRIDE_PATH", NULL)
 
 enum ir3_shader_debug ir3_shader_debug = 0;
-const char *ir3_shader_override_path = NULL;
 
 void
 ir3_compiler_destroy(struct ir3_compiler *compiler)
@@ -68,12 +66,6 @@ ir3_compiler_create(struct fd_device *dev, uint32_t gpu_id)
 	struct ir3_compiler *compiler = rzalloc(NULL, struct ir3_compiler);
 
 	ir3_shader_debug = debug_get_option_ir3_shader_debug();
-	ir3_shader_override_path =
-		!__check_suid() ? debug_get_option_ir3_shader_override_path() : NULL;
-
-	if (ir3_shader_override_path) {
-		ir3_shader_debug |= IR3_DBG_NOCACHE;
-	}
 
 	compiler->dev = dev;
 	compiler->gpu_id = gpu_id;
diff --git a/src/freedreno/ir3/ir3_compiler.h b/src/freedreno/ir3/ir3_compiler.h
index 54a78f37726..9924140711f 100644
--- a/src/freedreno/ir3/ir3_compiler.h
+++ b/src/freedreno/ir3/ir3_compiler.h
@@ -155,7 +155,6 @@ enum ir3_shader_debug {
 };
 
 extern enum ir3_shader_debug ir3_shader_debug;
-extern const char *ir3_shader_override_path;
 
 static inline bool
 shader_debug_enabled(gl_shader_stage type)
diff --git a/src/freedreno/ir3/ir3_image.c b/src/freedreno/ir3/ir3_image.c
index e2ee4e6dfad..eee4ef09282 100644
--- a/src/freedreno/ir3/ir3_image.c
+++ b/src/freedreno/ir3/ir3_image.c
@@ -119,17 +119,11 @@ ir3_get_type_for_image_intrinsic(const nir_intrinsic_instr *instr)
 	case nir_intrinsic_image_load:
 	case nir_intrinsic_bindless_image_load:
 		type = nir_alu_type_get_base_type(nir_intrinsic_dest_type(instr));
-		/* SpvOpAtomicLoad doesn't have dest type */
-		if (type == nir_type_invalid)
-			type = nir_type_uint;
 		break;
 
 	case nir_intrinsic_image_store:
 	case nir_intrinsic_bindless_image_store:
 		type = nir_alu_type_get_base_type(nir_intrinsic_src_type(instr));
-		/* SpvOpAtomicStore doesn't have src type */
-		if (type == nir_type_invalid)
-			type = nir_type_uint;
 		break;
 
 	case nir_intrinsic_image_atomic_add:
diff --git a/src/freedreno/ir3/ir3_lexer.l b/src/freedreno/ir3/ir3_lexer.l
index 8bad625a549..edabf5c9814 100644
--- a/src/freedreno/ir3/ir3_lexer.l
+++ b/src/freedreno/ir3/ir3_lexer.l
@@ -335,10 +335,6 @@ static int parse_w(const char *str)
 "getspid"                         return TOKEN(T_OP_GETSPID);
 "getwid"                          return TOKEN(T_OP_GETWID);
 
-                                  /* category 7: */
-"bar"                             return TOKEN(T_OP_BAR);
-"fence"                           return TOKEN(T_OP_FENCE);
-
 "f16"                             return TOKEN(T_TYPE_F16);
 "f32"                             return TOKEN(T_TYPE_F32);
 "u16"                             return TOKEN(T_TYPE_U16);
@@ -390,7 +386,6 @@ static int parse_w(const char *str)
 "hc"                              return TOKEN(T_HC);
 "hr"                              return TOKEN(T_HR);
 "g"                               return 'g';
-"w"                               return 'w';
 "l"                               return 'l';
 "<"                               return '<';
 ">"                               return '>';
diff --git a/src/freedreno/ir3/ir3_parser.y b/src/freedreno/ir3/ir3_parser.y
index 1ad4d140e5d..5676cd229b6 100644
--- a/src/freedreno/ir3/ir3_parser.y
+++ b/src/freedreno/ir3/ir3_parser.y
@@ -515,10 +515,6 @@ static void print_token(FILE *file, int type, YYSTYPE value)
 %token <tok> T_OP_GETSPID
 %token <tok> T_OP_GETWID
 
-/* category 7: */
-%token <tok> T_OP_BAR
-%token <tok> T_OP_FENCE
-
 /* type qualifiers: */
 %token <tok> T_TYPE_F16
 %token <tok> T_TYPE_F32
@@ -670,7 +666,6 @@ instr:             iflags cat0_instr
 |                  iflags cat4_instr
 |                  iflags cat5_instr { fixup_cat5_s2en(); }
 |                  iflags cat6_instr
-|                  iflags cat7_instr
 
 cat0_src1:         '!' T_P0        { instr->cat0.inv1 = true; instr->cat0.comp1 = $2 >> 1; }
 |                  T_P0            { instr->cat0.comp1 = $1 >> 1; }
@@ -1042,19 +1037,6 @@ cat6_instr:        cat6_load
 |                  cat6_bindless_ibo
 |                  cat6_todo
 
-cat7_scope:        '.' 'w'  { instr->cat7.w = true; }
-|                  '.' 'r'  { instr->cat7.r = true; }
-|                  '.' 'l'  { instr->cat7.l = true; }
-|                  '.' 'g'  { instr->cat7.g = true; }
-
-cat7_scopes:
-|                  cat7_scope cat7_scopes
-
-cat7_barrier:      T_OP_BAR                { new_instr(OPC_BAR); } cat7_scopes
-|                  T_OP_FENCE              { new_instr(OPC_FENCE); } cat7_scopes
-
-cat7_instr:        cat7_barrier
-
 reg:               T_REGISTER     { $$ = new_reg($1, 0); }
 |                  T_A0           { $$ = new_reg((61 << 3), IR3_REG_HALF); }
 |                  T_A1           { $$ = new_reg((61 << 3) + 1, IR3_REG_HALF); }
diff --git a/src/freedreno/ir3/ir3_shader.c b/src/freedreno/ir3/ir3_shader.c
index 835b67c6465..a4feb9be8f1 100644
--- a/src/freedreno/ir3/ir3_shader.c
+++ b/src/freedreno/ir3/ir3_shader.c
@@ -35,8 +35,6 @@
 #include "ir3_shader.h"
 #include "ir3_compiler.h"
 #include "ir3_nir.h"
-#include "ir3_assembler.h"
-#include "ir3_parser.h"
 
 #include "isa/isa.h"
 
@@ -183,65 +181,17 @@ void * ir3_shader_assemble(struct ir3_shader_variant *v)
 	return bin;
 }
 
-static bool
-try_override_shader_variant(struct ir3_shader_variant *v, const char *identifier)
-{
-	assert(ir3_shader_override_path);
-
-	char *name = ralloc_asprintf(NULL, "%s/%s.asm", ir3_shader_override_path, identifier);
-
-	FILE* f = fopen(name, "r");
-
-	if (!f) {
-		ralloc_free(name);
-		return false;
-	}
-
-	struct ir3_kernel_info info;
-	info.numwg = INVALID_REG;
-	v->ir = ir3_parse(v, &info, f);
-
-	fclose(f);
-
-	if (!v->ir) {
-		fprintf(stderr, "Failed to parse %s\n", name);
-		exit(1);
-	}
-
-	v->bin = ir3_shader_assemble(v);
-	if (!v->bin) {
-		fprintf(stderr, "Failed to assemble %s\n", name);
-		exit(1);
-	}
-
-	ralloc_free(name);
-	return true;
-}
-
 static void
 assemble_variant(struct ir3_shader_variant *v)
 {
 	v->bin = ir3_shader_assemble(v);
 
-	bool dbg_enabled = shader_debug_enabled(v->shader->type);
-	if (dbg_enabled || ir3_shader_override_path) {
-		unsigned char sha1[21];
-		char sha1buf[41];
-
-		_mesa_sha1_compute(v->bin, v->info.size, sha1);
-		_mesa_sha1_format(sha1buf, sha1);
-
-		bool shader_overridden =
-			ir3_shader_override_path && try_override_shader_variant(v, sha1buf);
-
-		if (dbg_enabled || shader_overridden) {
-			fprintf(stdout, "Native code%s for unnamed %s shader %s with sha1 %s:\n",
-				shader_overridden ? " (overridden)" : "",
-				ir3_shader_stage(v), v->shader->nir->info.name, sha1buf);
-			if (v->shader->type == MESA_SHADER_FRAGMENT)
-				fprintf(stdout, "SIMD0\n");
-			ir3_shader_disasm(v, v->bin, stdout);
-		}
+	if (shader_debug_enabled(v->shader->type)) {
+		fprintf(stdout, "Native code for unnamed %s shader %s:\n",
+			ir3_shader_stage(v), v->shader->nir->info.name);
+		if (v->shader->type == MESA_SHADER_FRAGMENT)
+			fprintf(stdout, "SIMD0\n");
+		ir3_shader_disasm(v, v->bin, stdout);
 	}
 
 	/* no need to keep the ir around beyond this point: */
diff --git a/src/freedreno/ir3/tests/disasm.c b/src/freedreno/ir3/tests/disasm.c
index c03484c3522..6deb47e3f7c 100644
--- a/src/freedreno/ir3/tests/disasm.c
+++ b/src/freedreno/ir3/tests/disasm.c
@@ -308,13 +308,6 @@ static const struct test {
 	/* Custom test since we've never seen the blob emit these. */
 	INSTR_6XX(c0260004_00490000, "getspid.u32 r1.x"),
 	INSTR_6XX(c0260005_00494000, "getwid.u32 r1.y"),
-
-	/* cat7 */
-
-	/* dEQP-VK.compute.basic.ssbo_local_barrier_single_invocation */
-	INSTR_6XX(e0fa0000_00000000, "fence.g.l.r.w"),
-	INSTR_6XX(e09a0000_00000000, "fence.r.w"),
-	INSTR_6XX(f0420000_00000000, "(sy)bar.g"),
 };
 
 static void
diff --git a/src/freedreno/isa/encode.py b/src/freedreno/isa/encode.py
index 699d5c9b660..576fef626ac 100644
--- a/src/freedreno/isa/encode.py
+++ b/src/freedreno/isa/encode.py
@@ -345,7 +345,7 @@ struct bitset_params;
 static uint64_t
 pack_field(unsigned low, unsigned high, uint64_t val)
 {
-   val &= ((UINT64_C(1) << (1 + high - low)) - 1);
+   val &= ((1ul << (1 + high - low)) - 1);
    return val << low;
 }
 
diff --git a/src/freedreno/perfcntrs/fd2_perfcntr.c b/src/freedreno/perfcntrs/fd2_perfcntr.c
index af9a514b6e6..a0f1ef8b3c0 100644
--- a/src/freedreno/perfcntrs/fd2_perfcntr.c
+++ b/src/freedreno/perfcntrs/fd2_perfcntr.c
@@ -947,7 +947,6 @@ static const struct fd_perfcntr_counter mh_counters[] = {
 };
 
 static const struct fd_perfcntr_counter rbbm_counters[] = {
-	COUNTER(RBBM_PERFCOUNTER0_SELECT, RBBM_PERFCOUNTER0_LO, RBBM_PERFCOUNTER0_HI),
 	COUNTER(RBBM_PERFCOUNTER1_SELECT, RBBM_PERFCOUNTER1_LO, RBBM_PERFCOUNTER1_HI),
 };
 
@@ -957,9 +956,6 @@ static const struct fd_perfcntr_counter cp_counters[] = {
 
 static const struct fd_perfcntr_counter rb_counters[] = {
 	COUNTER(RB_PERFCOUNTER0_SELECT, RB_PERFCOUNTER0_LOW, RB_PERFCOUNTER0_HI),
-	COUNTER(RB_PERFCOUNTER1_SELECT, RB_PERFCOUNTER1_LOW, RB_PERFCOUNTER1_HI),
-	COUNTER(RB_PERFCOUNTER2_SELECT, RB_PERFCOUNTER2_LOW, RB_PERFCOUNTER2_HI),
-	COUNTER(RB_PERFCOUNTER3_SELECT, RB_PERFCOUNTER3_LOW, RB_PERFCOUNTER3_HI),
 };
 
 const struct fd_perfcntr_group a2xx_perfcntr_groups[] = {
diff --git a/src/freedreno/registers/adreno/a2xx.xml b/src/freedreno/registers/adreno/a2xx.xml
index b95709cf84d..8e7f778f56e 100644
--- a/src/freedreno/registers/adreno/a2xx.xml
+++ b/src/freedreno/registers/adreno/a2xx.xml
@@ -1075,12 +1075,9 @@ xsi:schemaLocation="http://nouveau.freedesktop.org/ rules-ng.xsd">
 	<reg32 offset="0x0047" name="MH_MMU_MPU_END"/>
 
 	<reg32 offset="0x0394" name="NQWAIT_UNTIL"/>
-	<reg32 offset="0x0395" name="RBBM_PERFCOUNTER0_SELECT"/>
-	<reg32 offset="0x0396" name="RBBM_PERFCOUNTER1_SELECT"/>
-	<reg32 offset="0x0397" name="RBBM_PERFCOUNTER0_LO"/>
-	<reg32 offset="0x0398" name="RBBM_PERFCOUNTER0_HI"/>
-	<reg32 offset="0x0399" name="RBBM_PERFCOUNTER1_LO"/>
-	<reg32 offset="0x039a" name="RBBM_PERFCOUNTER1_HI"/>
+	<reg32 offset="0x0395" name="RBBM_PERFCOUNTER1_SELECT"/>
+	<reg32 offset="0x0397" name="RBBM_PERFCOUNTER1_LO"/>
+	<reg32 offset="0x0398" name="RBBM_PERFCOUNTER1_HI"/>
 	<reg32 offset="0x039b" name="RBBM_DEBUG"/>
 	<reg32 offset="0x039c" name="RBBM_PM_OVERRIDE1">
 		<bitfield name="RBBM_AHBCLK_PM_OVERRIDE" pos="0" type="boolean"/>
@@ -1691,17 +1688,8 @@ xsi:schemaLocation="http://nouveau.freedesktop.org/ rules-ng.xsd">
 	<reg32 offset="0x0a49" name="MH_PERFCOUNTER0_HI"/>
 	<reg32 offset="0x0a4d" name="MH_PERFCOUNTER1_HI"/>
 	<reg32 offset="0x0f04" name="RB_PERFCOUNTER0_SELECT"/>
-	<reg32 offset="0x0f05" name="RB_PERFCOUNTER1_SELECT"/>
-	<reg32 offset="0x0f06" name="RB_PERFCOUNTER2_SELECT"/>
-	<reg32 offset="0x0f07" name="RB_PERFCOUNTER3_SELECT"/>
 	<reg32 offset="0x0f08" name="RB_PERFCOUNTER0_LOW"/>
 	<reg32 offset="0x0f09" name="RB_PERFCOUNTER0_HI"/>
-	<reg32 offset="0x0f0a" name="RB_PERFCOUNTER1_LOW"/>
-	<reg32 offset="0x0f0b" name="RB_PERFCOUNTER1_HI"/>
-	<reg32 offset="0x0f0c" name="RB_PERFCOUNTER2_LOW"/>
-	<reg32 offset="0x0f0d" name="RB_PERFCOUNTER2_HI"/>
-	<reg32 offset="0x0f0e" name="RB_PERFCOUNTER3_LOW"/>
-	<reg32 offset="0x0f0f" name="RB_PERFCOUNTER3_HI"/>
 </domain>
 
 <domain name="A2XX_SQ_TEX" width="32">
diff --git a/src/freedreno/vulkan/tu_cmd_buffer.c b/src/freedreno/vulkan/tu_cmd_buffer.c
index 2c3be12104e..8e07f82e0b9 100644
--- a/src/freedreno/vulkan/tu_cmd_buffer.c
+++ b/src/freedreno/vulkan/tu_cmd_buffer.c
@@ -3182,18 +3182,10 @@ tu6_emit_tess_consts(struct tu_cmd_buffer *cmd,
    if (result != VK_SUCCESS)
       return result;
 
-   const struct tu_program_descriptor_linkage *hs_link =
-      &pipeline->program.link[MESA_SHADER_TESS_CTRL];
-   bool hs_uses_bo = pipeline->tess.hs_bo_regid < hs_link->constlen;
-
-   const struct tu_program_descriptor_linkage *ds_link =
-      &pipeline->program.link[MESA_SHADER_TESS_EVAL];
-   bool ds_uses_bo = pipeline->tess.ds_bo_regid < ds_link->constlen;
-
    uint64_t tess_factor_size = get_tess_factor_bo_size(pipeline, draw_count);
    uint64_t tess_param_size = get_tess_param_bo_size(pipeline, draw_count);
    uint64_t tess_bo_size =  tess_factor_size + tess_param_size;
-   if ((hs_uses_bo || ds_uses_bo) && tess_bo_size > 0) {
+   if (tess_bo_size > 0) {
       struct tu_bo *tess_bo;
       result = tu_get_scratch_bo(cmd->device, tess_bo_size, &tess_bo);
       if (result != VK_SUCCESS)
@@ -3202,31 +3194,27 @@ tu6_emit_tess_consts(struct tu_cmd_buffer *cmd,
       uint64_t tess_factor_iova = tess_bo->iova;
       uint64_t tess_param_iova = tess_factor_iova + tess_factor_size;
 
-      if (hs_uses_bo) {
-         tu_cs_emit_pkt7(&cs, CP_LOAD_STATE6_GEOM, 3 + 4);
-         tu_cs_emit(&cs, CP_LOAD_STATE6_0_DST_OFF(pipeline->tess.hs_bo_regid) |
-               CP_LOAD_STATE6_0_STATE_TYPE(ST6_CONSTANTS) |
-               CP_LOAD_STATE6_0_STATE_SRC(SS6_DIRECT) |
-               CP_LOAD_STATE6_0_STATE_BLOCK(SB6_HS_SHADER) |
-               CP_LOAD_STATE6_0_NUM_UNIT(1));
-         tu_cs_emit(&cs, CP_LOAD_STATE6_1_EXT_SRC_ADDR(0));
-         tu_cs_emit(&cs, CP_LOAD_STATE6_2_EXT_SRC_ADDR_HI(0));
-         tu_cs_emit_qw(&cs, tess_param_iova);
-         tu_cs_emit_qw(&cs, tess_factor_iova);
-      }
+      tu_cs_emit_pkt7(&cs, CP_LOAD_STATE6_GEOM, 3 + 4);
+      tu_cs_emit(&cs, CP_LOAD_STATE6_0_DST_OFF(pipeline->tess.hs_bo_regid) |
+            CP_LOAD_STATE6_0_STATE_TYPE(ST6_CONSTANTS) |
+            CP_LOAD_STATE6_0_STATE_SRC(SS6_DIRECT) |
+            CP_LOAD_STATE6_0_STATE_BLOCK(SB6_HS_SHADER) |
+            CP_LOAD_STATE6_0_NUM_UNIT(1));
+      tu_cs_emit(&cs, CP_LOAD_STATE6_1_EXT_SRC_ADDR(0));
+      tu_cs_emit(&cs, CP_LOAD_STATE6_2_EXT_SRC_ADDR_HI(0));
+      tu_cs_emit_qw(&cs, tess_param_iova);
+      tu_cs_emit_qw(&cs, tess_factor_iova);
 
-      if (ds_uses_bo) {
-         tu_cs_emit_pkt7(&cs, CP_LOAD_STATE6_GEOM, 3 + 4);
-         tu_cs_emit(&cs, CP_LOAD_STATE6_0_DST_OFF(pipeline->tess.ds_bo_regid) |
-               CP_LOAD_STATE6_0_STATE_TYPE(ST6_CONSTANTS) |
-               CP_LOAD_STATE6_0_STATE_SRC(SS6_DIRECT) |
-               CP_LOAD_STATE6_0_STATE_BLOCK(SB6_DS_SHADER) |
-               CP_LOAD_STATE6_0_NUM_UNIT(1));
-         tu_cs_emit(&cs, CP_LOAD_STATE6_1_EXT_SRC_ADDR(0));
-         tu_cs_emit(&cs, CP_LOAD_STATE6_2_EXT_SRC_ADDR_HI(0));
-         tu_cs_emit_qw(&cs, tess_param_iova);
-         tu_cs_emit_qw(&cs, tess_factor_iova);
-      }
+      tu_cs_emit_pkt7(&cs, CP_LOAD_STATE6_GEOM, 3 + 4);
+      tu_cs_emit(&cs, CP_LOAD_STATE6_0_DST_OFF(pipeline->tess.ds_bo_regid) |
+            CP_LOAD_STATE6_0_STATE_TYPE(ST6_CONSTANTS) |
+            CP_LOAD_STATE6_0_STATE_SRC(SS6_DIRECT) |
+            CP_LOAD_STATE6_0_STATE_BLOCK(SB6_DS_SHADER) |
+            CP_LOAD_STATE6_0_NUM_UNIT(1));
+      tu_cs_emit(&cs, CP_LOAD_STATE6_1_EXT_SRC_ADDR(0));
+      tu_cs_emit(&cs, CP_LOAD_STATE6_2_EXT_SRC_ADDR_HI(0));
+      tu_cs_emit_qw(&cs, tess_param_iova);
+      tu_cs_emit_qw(&cs, tess_factor_iova);
 
       *factor_iova = tess_factor_iova;
    }
diff --git a/src/freedreno/vulkan/tu_formats.c b/src/freedreno/vulkan/tu_formats.c
index 7f6e5e6b0e8..fe0fd5e18de 100644
--- a/src/freedreno/vulkan/tu_formats.c
+++ b/src/freedreno/vulkan/tu_formats.c
@@ -500,7 +500,7 @@ tu_GetPhysicalDeviceFormatProperties2(
 
       /* note: ubwc_possible() argument values to be ignored except for format */
       if (pFormatProperties->formatProperties.optimalTilingFeatures &&
-          ubwc_possible(format, VK_IMAGE_TYPE_2D, 0, false, VK_SAMPLE_COUNT_1_BIT)) {
+          ubwc_possible(format, VK_IMAGE_TYPE_2D, 0, false)) {
          vk_outarray_append(&out, mod_props) {
             mod_props->drmFormatModifier = DRM_FORMAT_MOD_QCOM_COMPRESSED;
             mod_props->drmFormatModifierPlaneCount = 1;
@@ -547,8 +547,7 @@ tu_get_image_format_properties(
          if (info->flags & VK_IMAGE_CREATE_MUTABLE_FORMAT_BIT)
             return VK_ERROR_FORMAT_NOT_SUPPORTED;
 
-
-         if (!ubwc_possible(info->format, info->type, info->usage, physical_device->info.a6xx.has_z24uint_s8uint, sampleCounts))
+         if (!ubwc_possible(info->format, info->type, info->usage, physical_device->info.a6xx.has_z24uint_s8uint))
             return VK_ERROR_FORMAT_NOT_SUPPORTED;
 
          format_feature_flags = format_props.optimalTilingFeatures;
diff --git a/src/freedreno/vulkan/tu_image.c b/src/freedreno/vulkan/tu_image.c
index c602f4b5878..56e11e180e8 100644
--- a/src/freedreno/vulkan/tu_image.c
+++ b/src/freedreno/vulkan/tu_image.c
@@ -447,8 +447,7 @@ tu_image_view_init(struct tu_image_view *iview,
 }
 
 bool
-ubwc_possible(VkFormat format, VkImageType type, VkImageUsageFlags usage, bool has_z24uint_s8uint,
-              VkSampleCountFlagBits samples)
+ubwc_possible(VkFormat format, VkImageType type, VkImageUsageFlags usage, bool has_z24uint_s8uint)
 {
    /* no UBWC with compressed formats, E5B9G9R9, S8_UINT
     * (S8_UINT because separate stencil doesn't have UBWC-enable bit)
@@ -491,9 +490,6 @@ ubwc_possible(VkFormat format, VkImageType type, VkImageUsageFlags usage, bool h
        (usage & (VK_IMAGE_USAGE_SAMPLED_BIT | VK_IMAGE_USAGE_INPUT_ATTACHMENT_BIT)))
       return false;
 
-   if (!has_z24uint_s8uint && samples > VK_SAMPLE_COUNT_1_BIT)
-      return false;
-
    return true;
 }
 
@@ -599,7 +595,7 @@ tu_CreateImage(VkDevice _device,
    }
 
    if (!ubwc_possible(image->vk_format, pCreateInfo->imageType, pCreateInfo->usage,
-                      device->physical_device->info.a6xx.has_z24uint_s8uint, pCreateInfo->samples))
+                      device->physical_device->info.a6xx.has_z24uint_s8uint))
       ubwc_enabled = false;
 
    /* expect UBWC enabled if we asked for it */
diff --git a/src/freedreno/vulkan/tu_pipeline.c b/src/freedreno/vulkan/tu_pipeline.c
index acc6c12044a..b7e9d2d20aa 100644
--- a/src/freedreno/vulkan/tu_pipeline.c
+++ b/src/freedreno/vulkan/tu_pipeline.c
@@ -1055,8 +1055,6 @@ tu6_emit_vpc(struct tu_cs *cs,
 
    if (gs) {
       uint32_t vertices_out, invocations, output, vec4_size;
-      uint32_t prev_stage_output_size = ds ? ds->output_size : vs->output_size;
-
       /* this detects the tu_clear_blit path, which doesn't set ->nir */
       if (gs->shader->nir) {
          if (hs) {
@@ -1069,7 +1067,7 @@ tu6_emit_vpc(struct tu_cs *cs,
          invocations = gs->shader->nir->info.gs.invocations - 1;
          /* Size of per-primitive alloction in ldlw memory in vec4s. */
          vec4_size = gs->shader->nir->info.gs.vertices_in *
-                     DIV_ROUND_UP(prev_stage_output_size, 4);
+                     DIV_ROUND_UP(vs->output_size, 4);
       } else {
          vertices_out = 3;
          output = TESS_CW_TRIS;
@@ -1093,7 +1091,7 @@ tu6_emit_vpc(struct tu_cs *cs,
       tu_cs_emit(cs, A6XX_PC_PRIMITIVE_CNTL_6_STRIDE_IN_VPC(vec4_size));
 
       tu_cs_emit_pkt4(cs, REG_A6XX_SP_GS_PRIM_SIZE, 1);
-      tu_cs_emit(cs, prev_stage_output_size);
+      tu_cs_emit(cs, vs->output_size);
    }
 }
 
diff --git a/src/freedreno/vulkan/tu_private.h b/src/freedreno/vulkan/tu_private.h
index de672fb53e6..053a68beb87 100644
--- a/src/freedreno/vulkan/tu_private.h
+++ b/src/freedreno/vulkan/tu_private.h
@@ -1396,8 +1396,7 @@ tu_image_view_init(struct tu_image_view *iview,
                    bool limited_z24s8);
 
 bool
-ubwc_possible(VkFormat format, VkImageType type, VkImageUsageFlags usage, bool limited_z24s8,
-              VkSampleCountFlagBits samples);
+ubwc_possible(VkFormat format, VkImageType type, VkImageUsageFlags usage, bool limited_z24s8);
 
 struct tu_buffer_view
 {
diff --git a/src/gallium/auxiliary/cso_cache/cso_context.c b/src/gallium/auxiliary/cso_cache/cso_context.c
index eb683872edf..69077fcf2be 100644
--- a/src/gallium/auxiliary/cso_cache/cso_context.c
+++ b/src/gallium/auxiliary/cso_cache/cso_context.c
@@ -75,6 +75,12 @@ struct cso_context {
 
    unsigned saved_state;  /**< bitmask of CSO_BIT_x flags */
 
+   struct pipe_sampler_view *fragment_views[PIPE_MAX_SHADER_SAMPLER_VIEWS];
+   unsigned nr_fragment_views;
+
+   struct pipe_sampler_view *fragment_views_saved[PIPE_MAX_SHADER_SAMPLER_VIEWS];
+   unsigned nr_fragment_views_saved;
+
    struct sampler_info fragment_samplers_saved;
    struct sampler_info samplers[PIPE_SHADER_TYPES];
 
@@ -83,6 +89,15 @@ struct cso_context {
     */
    int max_sampler_seen;
 
+   struct pipe_vertex_buffer vertex_buffer0_current;
+   struct pipe_vertex_buffer vertex_buffer0_saved;
+
+   struct pipe_constant_buffer aux_constbuf_current[PIPE_SHADER_TYPES];
+   struct pipe_constant_buffer aux_constbuf_saved[PIPE_SHADER_TYPES];
+
+   struct pipe_image_view fragment_image0_current;
+   struct pipe_image_view fragment_image0_saved;
+
    unsigned nr_so_targets;
    struct pipe_stream_output_target *so_targets[PIPE_MAX_SO_BUFFERS];
 
@@ -366,9 +381,27 @@ void cso_destroy_context( struct cso_context *ctx )
          ctx->pipe->set_stream_output_targets(ctx->pipe, 0, NULL, NULL);
    }
 
+   for (i = 0; i < ctx->nr_fragment_views; i++) {
+      pipe_sampler_view_reference(&ctx->fragment_views[i], NULL);
+   }
+   for (i = 0; i < ctx->nr_fragment_views_saved; i++) {
+      pipe_sampler_view_reference(&ctx->fragment_views_saved[i], NULL);
+   }
+
    util_unreference_framebuffer_state(&ctx->fb);
    util_unreference_framebuffer_state(&ctx->fb_saved);
 
+   pipe_vertex_buffer_unreference(&ctx->vertex_buffer0_current);
+   pipe_vertex_buffer_unreference(&ctx->vertex_buffer0_saved);
+
+   for (i = 0; i < PIPE_SHADER_TYPES; i++) {
+      pipe_resource_reference(&ctx->aux_constbuf_current[i].buffer, NULL);
+      pipe_resource_reference(&ctx->aux_constbuf_saved[i].buffer, NULL);
+   }
+
+   pipe_resource_reference(&ctx->fragment_image0_current.resource, NULL);
+   pipe_resource_reference(&ctx->fragment_image0_saved.resource, NULL);
+
    for (i = 0; i < PIPE_MAX_SO_BUFFERS; i++) {
       pipe_so_target_reference(&ctx->so_targets[i], NULL);
       pipe_so_target_reference(&ctx->so_targets_saved[i], NULL);
@@ -1011,6 +1044,27 @@ cso_restore_vertex_elements(struct cso_context *ctx)
 
 /* vertex buffers */
 
+static void
+cso_set_vertex_buffers_direct(struct cso_context *ctx,
+                              unsigned start_slot, unsigned count,
+                              const struct pipe_vertex_buffer *buffers)
+{
+   /* Save what's in the auxiliary slot, so that we can save and restore it
+    * for meta ops.
+    */
+   if (start_slot == 0) {
+      if (buffers) {
+         pipe_vertex_buffer_reference(&ctx->vertex_buffer0_current,
+                                      buffers);
+      } else {
+         pipe_vertex_buffer_unreference(&ctx->vertex_buffer0_current);
+      }
+   }
+
+   ctx->pipe->set_vertex_buffers(ctx->pipe, start_slot, count, buffers);
+}
+
+
 void cso_set_vertex_buffers(struct cso_context *ctx,
                             unsigned start_slot, unsigned count,
                             const struct pipe_vertex_buffer *buffers)
@@ -1025,8 +1079,35 @@ void cso_set_vertex_buffers(struct cso_context *ctx,
       return;
    }
 
-   struct pipe_context *pipe = ctx->pipe;
-   pipe->set_vertex_buffers(pipe, start_slot, count, buffers);
+   cso_set_vertex_buffers_direct(ctx, start_slot, count, buffers);
+}
+
+static void
+cso_save_vertex_buffer0(struct cso_context *ctx)
+{
+   struct u_vbuf *vbuf = ctx->vbuf_current;
+
+   if (vbuf) {
+      u_vbuf_save_vertex_buffer0(vbuf);
+      return;
+   }
+
+   pipe_vertex_buffer_reference(&ctx->vertex_buffer0_saved,
+                                &ctx->vertex_buffer0_current);
+}
+
+static void
+cso_restore_vertex_buffer0(struct cso_context *ctx)
+{
+   struct u_vbuf *vbuf = ctx->vbuf_current;
+
+   if (vbuf) {
+      u_vbuf_restore_vertex_buffer0(vbuf);
+      return;
+   }
+
+   cso_set_vertex_buffers(ctx, 0, 1, &ctx->vertex_buffer0_saved);
+   pipe_vertex_buffer_unreference(&ctx->vertex_buffer0_saved);
 }
 
 /**
@@ -1050,14 +1131,13 @@ cso_set_vertex_buffers_and_elements(struct cso_context *ctx,
                                     bool uses_user_vertex_buffers)
 {
    struct u_vbuf *vbuf = ctx->vbuf;
-   struct pipe_context *pipe = ctx->pipe;
 
    if (vbuf && (ctx->always_use_vbuf || uses_user_vertex_buffers)) {
       if (!ctx->vbuf_current) {
          /* Unbind all buffers in cso_context, because we'll use u_vbuf. */
          unsigned unbind_vb_count = vb_count + unbind_trailing_vb_count;
          if (unbind_vb_count)
-            pipe->set_vertex_buffers(pipe, 0, unbind_vb_count, NULL);
+            cso_set_vertex_buffers_direct(ctx, 0, unbind_vb_count, NULL);
 
          /* Unset this to make sure the CSO is re-bound on the next use. */
          ctx->velements = NULL;
@@ -1083,11 +1163,12 @@ cso_set_vertex_buffers_and_elements(struct cso_context *ctx,
       u_vbuf_unset_vertex_elements(vbuf);
       ctx->vbuf_current = NULL;
    } else if (unbind_trailing_vb_count) {
-      pipe->set_vertex_buffers(pipe, vb_count, unbind_trailing_vb_count, NULL);
+      cso_set_vertex_buffers_direct(ctx, vb_count, unbind_trailing_vb_count,
+                                    NULL);
    }
 
    if (vb_count)
-      pipe->set_vertex_buffers(pipe, 0, vb_count, vbuffers);
+      cso_set_vertex_buffers_direct(ctx, 0, vb_count, vbuffers);
    cso_set_vertex_elements_direct(ctx, velems);
 }
 
@@ -1199,6 +1280,113 @@ cso_restore_fragment_samplers(struct cso_context *ctx)
 }
 
 
+void
+cso_set_sampler_views(struct cso_context *ctx,
+                      enum pipe_shader_type shader_stage,
+                      unsigned count,
+                      struct pipe_sampler_view **views)
+{
+   if (shader_stage == PIPE_SHADER_FRAGMENT) {
+      unsigned i;
+      boolean any_change = FALSE;
+
+      /* reference new views */
+      for (i = 0; i < count; i++) {
+         any_change |= ctx->fragment_views[i] != views[i];
+         pipe_sampler_view_reference(&ctx->fragment_views[i], views[i]);
+      }
+      /* unref extra old views, if any */
+      for (; i < ctx->nr_fragment_views; i++) {
+         any_change |= ctx->fragment_views[i] != NULL;
+         pipe_sampler_view_reference(&ctx->fragment_views[i], NULL);
+      }
+
+      /* bind the new sampler views */
+      if (any_change) {
+         ctx->pipe->set_sampler_views(ctx->pipe, shader_stage, 0,
+                                      MAX2(ctx->nr_fragment_views, count),
+                                      ctx->fragment_views);
+      }
+
+      ctx->nr_fragment_views = count;
+   }
+   else
+      ctx->pipe->set_sampler_views(ctx->pipe, shader_stage, 0, count, views);
+}
+
+
+static void
+cso_save_fragment_sampler_views(struct cso_context *ctx)
+{
+   unsigned i;
+
+   ctx->nr_fragment_views_saved = ctx->nr_fragment_views;
+
+   for (i = 0; i < ctx->nr_fragment_views; i++) {
+      assert(!ctx->fragment_views_saved[i]);
+      pipe_sampler_view_reference(&ctx->fragment_views_saved[i],
+                                  ctx->fragment_views[i]);
+   }
+}
+
+
+static void
+cso_restore_fragment_sampler_views(struct cso_context *ctx)
+{
+   unsigned i, nr_saved = ctx->nr_fragment_views_saved;
+   unsigned num;
+
+   for (i = 0; i < nr_saved; i++) {
+      pipe_sampler_view_reference(&ctx->fragment_views[i], NULL);
+      /* move the reference from one pointer to another */
+      ctx->fragment_views[i] = ctx->fragment_views_saved[i];
+      ctx->fragment_views_saved[i] = NULL;
+   }
+   for (; i < ctx->nr_fragment_views; i++) {
+      pipe_sampler_view_reference(&ctx->fragment_views[i], NULL);
+   }
+
+   num = MAX2(ctx->nr_fragment_views, nr_saved);
+
+   /* bind the old/saved sampler views */
+   ctx->pipe->set_sampler_views(ctx->pipe, PIPE_SHADER_FRAGMENT, 0, num,
+                                ctx->fragment_views);
+
+   ctx->nr_fragment_views = nr_saved;
+   ctx->nr_fragment_views_saved = 0;
+}
+
+
+void
+cso_set_shader_images(struct cso_context *ctx,
+                      enum pipe_shader_type shader_stage,
+                      unsigned start, unsigned count,
+                      struct pipe_image_view *images)
+{
+   if (shader_stage == PIPE_SHADER_FRAGMENT && start == 0 && count >= 1) {
+      util_copy_image_view(&ctx->fragment_image0_current, &images[0]);
+   }
+
+   ctx->pipe->set_shader_images(ctx->pipe, shader_stage, start, count, images);
+}
+
+
+static void
+cso_save_fragment_image0(struct cso_context *ctx)
+{
+   util_copy_image_view(&ctx->fragment_image0_saved,
+                        &ctx->fragment_image0_current);
+}
+
+
+static void
+cso_restore_fragment_image0(struct cso_context *ctx)
+{
+   cso_set_shader_images(ctx, PIPE_SHADER_FRAGMENT, 0, 1,
+                         &ctx->fragment_image0_saved);
+}
+
+
 void
 cso_set_stream_outputs(struct cso_context *ctx,
                        unsigned num_targets,
@@ -1285,6 +1473,75 @@ cso_restore_stream_outputs(struct cso_context *ctx)
    ctx->nr_so_targets_saved = 0;
 }
 
+/* constant buffers */
+
+void
+cso_set_constant_buffer(struct cso_context *cso,
+                        enum pipe_shader_type shader_stage,
+                        unsigned index, struct pipe_constant_buffer *cb)
+{
+   struct pipe_context *pipe = cso->pipe;
+
+   pipe->set_constant_buffer(pipe, shader_stage, index, cb);
+
+   if (index == 0) {
+      util_copy_constant_buffer(&cso->aux_constbuf_current[shader_stage], cb);
+   }
+}
+
+void
+cso_set_constant_buffer_resource(struct cso_context *cso,
+                                 enum pipe_shader_type shader_stage,
+                                 unsigned index,
+                                 struct pipe_resource *buffer)
+{
+   if (buffer) {
+      struct pipe_constant_buffer cb;
+      cb.buffer = buffer;
+      cb.buffer_offset = 0;
+      cb.buffer_size = buffer->width0;
+      cb.user_buffer = NULL;
+      cso_set_constant_buffer(cso, shader_stage, index, &cb);
+   } else {
+      cso_set_constant_buffer(cso, shader_stage, index, NULL);
+   }
+}
+
+void
+cso_set_constant_user_buffer(struct cso_context *cso,
+                             enum pipe_shader_type shader_stage,
+                             unsigned index, void *ptr, unsigned size)
+{
+   if (ptr) {
+      struct pipe_constant_buffer cb;
+      cb.buffer = NULL;
+      cb.buffer_offset = 0;
+      cb.buffer_size = size;
+      cb.user_buffer = ptr;
+      cso_set_constant_buffer(cso, shader_stage, index, &cb);
+   } else {
+      cso_set_constant_buffer(cso, shader_stage, index, NULL);
+   }
+}
+
+void
+cso_save_constant_buffer_slot0(struct cso_context *cso,
+                               enum pipe_shader_type shader_stage)
+{
+   util_copy_constant_buffer(&cso->aux_constbuf_saved[shader_stage],
+                             &cso->aux_constbuf_current[shader_stage]);
+}
+
+void
+cso_restore_constant_buffer_slot0(struct cso_context *cso,
+                                  enum pipe_shader_type shader_stage)
+{
+   cso_set_constant_buffer(cso, shader_stage, 0,
+                           &cso->aux_constbuf_saved[shader_stage]);
+   pipe_resource_reference(&cso->aux_constbuf_saved[shader_stage].buffer,
+                           NULL);
+}
+
 
 /**
  * Save all the CSO state items specified by the state_mask bitmask
@@ -1297,12 +1554,16 @@ cso_save_state(struct cso_context *cso, unsigned state_mask)
 
    cso->saved_state = state_mask;
 
+   if (state_mask & CSO_BIT_AUX_VERTEX_BUFFER_SLOT)
+      cso_save_vertex_buffer0(cso);
    if (state_mask & CSO_BIT_BLEND)
       cso_save_blend(cso);
    if (state_mask & CSO_BIT_DEPTH_STENCIL_ALPHA)
       cso_save_depth_stencil_alpha(cso);
    if (state_mask & CSO_BIT_FRAGMENT_SAMPLERS)
       cso_save_fragment_samplers(cso);
+   if (state_mask & CSO_BIT_FRAGMENT_SAMPLER_VIEWS)
+      cso_save_fragment_sampler_views(cso);
    if (state_mask & CSO_BIT_FRAGMENT_SHADER)
       cso_save_fragment_shader(cso);
    if (state_mask & CSO_BIT_FRAMEBUFFER)
@@ -1333,6 +1594,8 @@ cso_save_state(struct cso_context *cso, unsigned state_mask)
       cso_save_viewport(cso);
    if (state_mask & CSO_BIT_PAUSE_QUERIES)
       cso->pipe->set_active_query_state(cso->pipe, false);
+   if (state_mask & CSO_BIT_FRAGMENT_IMAGE0)
+      cso_save_fragment_image0(cso);
 }
 
 
@@ -1346,12 +1609,16 @@ cso_restore_state(struct cso_context *cso)
 
    assert(state_mask);
 
+   if (state_mask & CSO_BIT_AUX_VERTEX_BUFFER_SLOT)
+      cso_restore_vertex_buffer0(cso);
    if (state_mask & CSO_BIT_BLEND)
       cso_restore_blend(cso);
    if (state_mask & CSO_BIT_DEPTH_STENCIL_ALPHA)
       cso_restore_depth_stencil_alpha(cso);
    if (state_mask & CSO_BIT_FRAGMENT_SAMPLERS)
       cso_restore_fragment_samplers(cso);
+   if (state_mask & CSO_BIT_FRAGMENT_SAMPLER_VIEWS)
+      cso_restore_fragment_sampler_views(cso);
    if (state_mask & CSO_BIT_FRAGMENT_SHADER)
       cso_restore_fragment_shader(cso);
    if (state_mask & CSO_BIT_FRAMEBUFFER)
@@ -1382,6 +1649,8 @@ cso_restore_state(struct cso_context *cso)
       cso_restore_viewport(cso);
    if (state_mask & CSO_BIT_PAUSE_QUERIES)
       cso->pipe->set_active_query_state(cso->pipe, true);
+   if (state_mask & CSO_BIT_FRAGMENT_IMAGE0)
+      cso_restore_fragment_image0(cso);
 
    cso->saved_state = 0;
 }
@@ -1449,7 +1718,6 @@ cso_draw_arrays(struct cso_context *cso, uint mode, uint start, uint count)
    util_draw_init_info(&info);
 
    info.mode = mode;
-   info.index_bounds_valid = true;
    info.min_index = start;
    info.max_index = start + count - 1;
 
@@ -1470,7 +1738,6 @@ cso_draw_arrays_instanced(struct cso_context *cso, uint mode,
    util_draw_init_info(&info);
 
    info.mode = mode;
-   info.index_bounds_valid = true;
    info.min_index = start;
    info.max_index = start + count - 1;
    info.start_instance = start_instance;
diff --git a/src/gallium/auxiliary/cso_cache/cso_context.h b/src/gallium/auxiliary/cso_cache/cso_context.h
index dd7121663ca..1f55097ecf4 100644
--- a/src/gallium/auxiliary/cso_cache/cso_context.h
+++ b/src/gallium/auxiliary/cso_cache/cso_context.h
@@ -132,11 +132,12 @@ void cso_set_render_condition(struct cso_context *cso,
                               boolean condition,
                               enum pipe_render_cond_flag mode);
 
-/* gap */
+
+#define CSO_BIT_AUX_VERTEX_BUFFER_SLOT    0x1
 #define CSO_BIT_BLEND                     0x2
 #define CSO_BIT_DEPTH_STENCIL_ALPHA       0x4
 #define CSO_BIT_FRAGMENT_SAMPLERS         0x8
-/* gap */
+#define CSO_BIT_FRAGMENT_SAMPLER_VIEWS   0x10
 #define CSO_BIT_FRAGMENT_SHADER          0x20
 #define CSO_BIT_FRAMEBUFFER              0x40
 #define CSO_BIT_GEOMETRY_SHADER          0x80
@@ -152,6 +153,7 @@ void cso_set_render_condition(struct cso_context *cso,
 #define CSO_BIT_VERTEX_SHADER         0x20000
 #define CSO_BIT_VIEWPORT              0x40000
 #define CSO_BIT_PAUSE_QUERIES         0x80000
+#define CSO_BIT_FRAGMENT_IMAGE0      0x100000
 
 #define CSO_BITS_ALL_SHADERS (CSO_BIT_VERTEX_SHADER | \
                               CSO_BIT_FRAGMENT_SHADER | \
@@ -163,6 +165,41 @@ void cso_save_state(struct cso_context *cso, unsigned state_mask);
 void cso_restore_state(struct cso_context *cso);
 
 
+/* sampler view state */
+
+void
+cso_set_sampler_views(struct cso_context *cso,
+                      enum pipe_shader_type shader_stage,
+                      unsigned count,
+                      struct pipe_sampler_view **views);
+
+
+/* shader images */
+
+void
+cso_set_shader_images(struct cso_context *cso,
+                      enum pipe_shader_type shader_stage,
+                      unsigned start, unsigned count,
+                      struct pipe_image_view *views);
+
+
+/* constant buffers */
+
+void cso_set_constant_buffer(struct cso_context *cso,
+                             enum pipe_shader_type shader_stage,
+                             unsigned index, struct pipe_constant_buffer *cb);
+void cso_set_constant_buffer_resource(struct cso_context *cso,
+                                      enum pipe_shader_type shader_stage,
+                                      unsigned index,
+                                      struct pipe_resource *buffer);
+void cso_set_constant_user_buffer(struct cso_context *cso,
+                                  enum pipe_shader_type shader_stage,
+                                  unsigned index, void *ptr, unsigned size);
+void cso_save_constant_buffer_slot0(struct cso_context *cso,
+                                    enum pipe_shader_type shader_stage);
+void cso_restore_constant_buffer_slot0(struct cso_context *cso,
+                                       enum pipe_shader_type shader_stage);
+
 /* Optimized version. */
 void
 cso_set_vertex_buffers_and_elements(struct cso_context *ctx,
diff --git a/src/gallium/auxiliary/gallivm/lp_bld_tgsi.c b/src/gallium/auxiliary/gallivm/lp_bld_tgsi.c
index f39397727d5..3b111138b79 100644
--- a/src/gallium/auxiliary/gallivm/lp_bld_tgsi.c
+++ b/src/gallium/auxiliary/gallivm/lp_bld_tgsi.c
@@ -377,11 +377,11 @@ lp_build_emit_fetch_src(
    if (reg->Register.Absolute) {
       switch (stype) {
       case TGSI_TYPE_FLOAT:
+      case TGSI_TYPE_DOUBLE:
       case TGSI_TYPE_UNTYPED:
           /* modifiers on movs assume data is float */
          res = lp_build_abs(&bld_base->base, res);
          break;
-      case TGSI_TYPE_DOUBLE:
       case TGSI_TYPE_UNSIGNED:
       case TGSI_TYPE_SIGNED:
       case TGSI_TYPE_UNSIGNED64:
diff --git a/src/gallium/auxiliary/hud/hud_context.c b/src/gallium/auxiliary/hud/hud_context.c
index cf98c59f087..bb614f76a95 100644
--- a/src/gallium/auxiliary/hud/hud_context.c
+++ b/src/gallium/auxiliary/hud/hud_context.c
@@ -40,7 +40,6 @@
 #include "hud/hud_context.h"
 #include "hud/hud_private.h"
 
-#include "frontend/api.h"
 #include "cso_cache/cso_context.h"
 #include "util/u_draw_quad.h"
 #include "util/format/u_format.h"
@@ -74,7 +73,6 @@ hud_draw_colored_prims(struct hud_context *hud, unsigned prim,
                        int xoffset, int yoffset, float yscale)
 {
    struct cso_context *cso = hud->cso;
-   struct pipe_context *pipe = hud->pipe;
    struct pipe_vertex_buffer vbuffer = {0};
 
    hud->constants.color[0] = r;
@@ -85,7 +83,7 @@ hud_draw_colored_prims(struct hud_context *hud, unsigned prim,
    hud->constants.translate[1] = (float) (yoffset * hud_scale);
    hud->constants.scale[0] = hud_scale;
    hud->constants.scale[1] = yscale * hud_scale;
-   pipe->set_constant_buffer(pipe, PIPE_SHADER_VERTEX, 0, &hud->constbuf);
+   cso_set_constant_buffer(cso, PIPE_SHADER_VERTEX, 0, &hud->constbuf);
 
    u_upload_data(hud->pipe->stream_uploader, 0,
                  num_vertices * 2 * sizeof(float), 16, buffer,
@@ -481,6 +479,7 @@ hud_draw_results(struct hud_context *hud, struct pipe_resource *tex)
                         CSO_BIT_BLEND |
                         CSO_BIT_DEPTH_STENCIL_ALPHA |
                         CSO_BIT_FRAGMENT_SHADER |
+                        CSO_BIT_FRAGMENT_SAMPLER_VIEWS |
                         CSO_BIT_FRAGMENT_SAMPLERS |
                         CSO_BIT_RASTERIZER |
                         CSO_BIT_VIEWPORT |
@@ -490,8 +489,10 @@ hud_draw_results(struct hud_context *hud, struct pipe_resource *tex)
                         CSO_BIT_TESSEVAL_SHADER |
                         CSO_BIT_VERTEX_SHADER |
                         CSO_BIT_VERTEX_ELEMENTS |
+                        CSO_BIT_AUX_VERTEX_BUFFER_SLOT |
                         CSO_BIT_PAUSE_QUERIES |
                         CSO_BIT_RENDER_CONDITION));
+   cso_save_constant_buffer_slot0(cso, PIPE_SHADER_VERTEX);
 
    /* set states */
    memset(&surf_templ, 0, sizeof(surf_templ));
@@ -537,10 +538,10 @@ hud_draw_results(struct hud_context *hud, struct pipe_resource *tex)
    cso_set_vertex_shader_handle(cso, hud->vs);
    cso_set_vertex_elements(cso, &hud->velems);
    cso_set_render_condition(cso, NULL, FALSE, 0);
-   pipe->set_sampler_views(pipe, PIPE_SHADER_FRAGMENT, 0, 1,
-                           &hud->font_sampler_view);
+   cso_set_sampler_views(cso, PIPE_SHADER_FRAGMENT, 1,
+                         &hud->font_sampler_view);
    cso_set_samplers(cso, PIPE_SHADER_FRAGMENT, 1, sampler_states);
-   pipe->set_constant_buffer(pipe, PIPE_SHADER_VERTEX, 0, &hud->constbuf);
+   cso_set_constant_buffer(cso, PIPE_SHADER_VERTEX, 0, &hud->constbuf);
 
    /* draw accumulated vertices for background quads */
    cso_set_blend(cso, &hud->alpha_blend);
@@ -556,7 +557,7 @@ hud_draw_results(struct hud_context *hud, struct pipe_resource *tex)
       hud->constants.scale[0] = hud_scale;
       hud->constants.scale[1] = hud_scale;
 
-      pipe->set_constant_buffer(pipe, PIPE_SHADER_VERTEX, 0, &hud->constbuf);
+      cso_set_constant_buffer(cso, PIPE_SHADER_VERTEX, 0, &hud->constbuf);
 
       cso_set_vertex_buffers(cso, 0, 1, &hud->bg.vbuf);
       cso_draw_arrays(cso, PIPE_PRIM_QUADS, 0, hud->bg.num_vertices);
@@ -571,8 +572,13 @@ hud_draw_results(struct hud_context *hud, struct pipe_resource *tex)
    }
    pipe_resource_reference(&hud->text.vbuf.buffer.resource, NULL);
 
-   if (hud->simple)
-      goto done;
+   if (hud->simple) {
+      cso_restore_state(cso);
+      cso_restore_constant_buffer_slot0(cso, PIPE_SHADER_VERTEX);
+
+      pipe_surface_reference(&surf, NULL);
+      return;
+   }
 
    /* draw accumulated vertices for white lines */
    cso_set_blend(cso, &hud->no_blend);
@@ -585,7 +591,7 @@ hud_draw_results(struct hud_context *hud, struct pipe_resource *tex)
    hud->constants.translate[1] = 0;
    hud->constants.scale[0] = hud_scale;
    hud->constants.scale[1] = hud_scale;
-   pipe->set_constant_buffer(pipe, PIPE_SHADER_VERTEX, 0, &hud->constbuf);
+   cso_set_constant_buffer(cso, PIPE_SHADER_VERTEX, 0, &hud->constbuf);
 
    if (hud->whitelines.num_vertices) {
       cso_set_vertex_buffers(cso, 0, 1, &hud->whitelines.vbuf);
@@ -602,21 +608,8 @@ hud_draw_results(struct hud_context *hud, struct pipe_resource *tex)
          hud_pane_draw_colored_objects(hud, pane);
    }
 
-done:
    cso_restore_state(cso);
-
-   /* Unbind resources that we have bound. */
-   pipe->set_constant_buffer(pipe, PIPE_SHADER_VERTEX, 0, NULL);
-   pipe->set_vertex_buffers(pipe, 0, 1, NULL);
-   pipe->set_sampler_views(pipe, PIPE_SHADER_FRAGMENT, 0, 1, NULL);
-
-   /* restore states not restored by cso */
-   if (hud->st) {
-      hud->st->invalidate_state(hud->st,
-                                ST_INVALIDATE_FS_SAMPLER_VIEWS |
-                                ST_INVALIDATE_VS_CONSTBUF0 |
-                                ST_INVALIDATE_VERTEX_BUFFERS);
-   }
+   cso_restore_constant_buffer_slot0(cso, PIPE_SHADER_VERTEX);
 
    pipe_surface_reference(&surf, NULL);
 }
@@ -1645,15 +1638,13 @@ hud_unset_draw_context(struct hud_context *hud)
 }
 
 static bool
-hud_set_draw_context(struct hud_context *hud, struct cso_context *cso,
-                     struct st_context_iface *st)
+hud_set_draw_context(struct hud_context *hud, struct cso_context *cso)
 {
    struct pipe_context *pipe = cso_get_pipe_context(cso);
 
    assert(!hud->pipe);
    hud->pipe = pipe;
    hud->cso = cso;
-   hud->st = st;
 
    struct pipe_sampler_view view_templ;
    u_sampler_view_default_template(
@@ -1779,8 +1770,7 @@ hud_set_record_context(struct hud_context *hud, struct pipe_context *pipe)
  * record queries in one context and draw them in another.
  */
 struct hud_context *
-hud_create(struct cso_context *cso, struct st_context_iface *st,
-           struct hud_context *share)
+hud_create(struct cso_context *cso, struct hud_context *share)
 {
    const char *share_env = debug_get_option("GALLIUM_HUD_SHARE", NULL);
    unsigned record_ctx = 0, draw_ctx = 0;
@@ -1804,7 +1794,7 @@ hud_create(struct cso_context *cso, struct st_context_iface *st,
 
       if (context_id == draw_ctx) {
          assert(!share->pipe);
-         hud_set_draw_context(share, cso, st);
+         hud_set_draw_context(share, cso);
       }
 
       return share;
@@ -1911,7 +1901,7 @@ hud_create(struct cso_context *cso, struct st_context_iface *st,
    if (record_ctx == 0)
       hud_set_record_context(hud, cso_get_pipe_context(cso));
    if (draw_ctx == 0)
-      hud_set_draw_context(hud, cso, st);
+      hud_set_draw_context(hud, cso);
 
    hud_parse_env_var(hud, screen, env);
    return hud;
diff --git a/src/gallium/auxiliary/hud/hud_context.h b/src/gallium/auxiliary/hud/hud_context.h
index ed5dd5dbff2..99e6f8d7e67 100644
--- a/src/gallium/auxiliary/hud/hud_context.h
+++ b/src/gallium/auxiliary/hud/hud_context.h
@@ -33,11 +33,9 @@ struct cso_context;
 struct pipe_context;
 struct pipe_resource;
 struct util_queue_monitoring;
-struct st_context_iface;
 
 struct hud_context *
-hud_create(struct cso_context *cso, struct st_context_iface *st,
-           struct hud_context *share);
+hud_create(struct cso_context *cso, struct hud_context *share);
 
 void
 hud_destroy(struct hud_context *hud, struct cso_context *cso);
diff --git a/src/gallium/auxiliary/hud/hud_private.h b/src/gallium/auxiliary/hud/hud_private.h
index 4caa99f3c72..c95f4c42b44 100644
--- a/src/gallium/auxiliary/hud/hud_private.h
+++ b/src/gallium/auxiliary/hud/hud_private.h
@@ -50,7 +50,6 @@ struct hud_context {
    /* Context where the HUD is drawn: */
    struct pipe_context *pipe;
    struct cso_context *cso;
-   struct st_context_iface *st;
 
    struct hud_batch_query_context *batch_query;
    struct list_head pane_list;
diff --git a/src/gallium/auxiliary/nir/nir_to_tgsi.c b/src/gallium/auxiliary/nir/nir_to_tgsi.c
index fbc96b35160..9384406d7ca 100644
--- a/src/gallium/auxiliary/nir/nir_to_tgsi.c
+++ b/src/gallium/auxiliary/nir/nir_to_tgsi.c
@@ -162,124 +162,6 @@ ntt_tgsi_var_usage_mask(const struct nir_variable *var)
                               glsl_type_is_64bit(type_without_array));
 }
 
-static struct ureg_dst
-ntt_store_output_decl(struct ntt_compile *c, nir_intrinsic_instr *instr, uint32_t *frac)
-{
-   nir_io_semantics semantics = nir_intrinsic_io_semantics(instr);
-   int base = nir_intrinsic_base(instr);
-   *frac = nir_intrinsic_component(instr);
-   bool is_64 = nir_src_bit_size(instr->src[0]) == 64;
-
-   struct ureg_dst out;
-   if (c->s->info.stage == MESA_SHADER_FRAGMENT) {
-      if (semantics.location == FRAG_RESULT_COLOR)
-         ureg_property(c->ureg, TGSI_PROPERTY_FS_COLOR0_WRITES_ALL_CBUFS, 1);
-
-      unsigned semantic_name, semantic_index;
-      tgsi_get_gl_frag_result_semantic(semantics.location,
-                                       &semantic_name, &semantic_index);
-      semantic_index += semantics.dual_source_blend_index;
-
-      switch (semantics.location) {
-      case FRAG_RESULT_DEPTH:
-         *frac = 2; /* z write is the to the .z channel in TGSI */
-         break;
-      case FRAG_RESULT_STENCIL:
-         *frac = 1;
-         break;
-      default:
-         break;
-      }
-
-      out = ureg_DECL_output(c->ureg, semantic_name, semantic_index);
-   } else {
-      unsigned semantic_name, semantic_index;
-
-      ntt_get_gl_varying_semantic(c, semantics.location,
-                                  &semantic_name, &semantic_index);
-
-      uint32_t usage_mask = ntt_tgsi_usage_mask(*frac,
-                                                instr->num_components,
-                                                is_64);
-      uint32_t gs_streams = semantics.gs_streams;
-      for (int i = 0; i < 4; i++) {
-         if (!(usage_mask & (1 << i)))
-            gs_streams &= ~(0x3 << 2 * i);
-      }
-
-      /* No driver appears to use array_id of outputs. */
-      unsigned array_id = 0;
-
-      /* This bit is lost in the i/o semantics, but it's unused in in-tree
-       * drivers.
-       */
-      bool invariant = false;
-
-      out = ureg_DECL_output_layout(c->ureg,
-                                    semantic_name, semantic_index,
-                                    gs_streams,
-                                    base,
-                                    usage_mask,
-                                    array_id,
-                                    semantics.num_slots,
-                                    invariant);
-   }
-
-   unsigned write_mask = nir_intrinsic_write_mask(instr);
-
-   if (is_64) {
-      write_mask = ntt_64bit_write_mask(write_mask);
-      if (*frac >= 2)
-         write_mask = write_mask << 2;
-   } else {
-      write_mask = write_mask << *frac;
-   }
-   return ureg_writemask(out, write_mask);
-}
-
-/* If this reg or SSA def is used only for storing an output, then in the simple
- * cases we can write directly to the TGSI output instead of having store_output
- * emit its own MOV.
- */
-static bool
-ntt_try_store_in_tgsi_output(struct ntt_compile *c, struct ureg_dst *dst,
-                             struct list_head *uses, struct list_head *if_uses)
-{
-   *dst = ureg_dst_undef();
-
-   switch (c->s->info.stage) {
-   case MESA_SHADER_FRAGMENT:
-   case MESA_SHADER_VERTEX:
-      break;
-   default:
-      /* tgsi_exec (at least) requires that output stores happen per vertex
-       * emitted, you don't get to reuse a previous output value for the next
-       * vertex.
-       */
-      return false;
-   }
-
-   if (!list_is_empty(if_uses) || !list_is_singular(uses))
-      return false;
-
-   nir_src *src = list_first_entry(uses, nir_src, use_link);
-
-   if (src->parent_instr->type != nir_instr_type_intrinsic)
-      return false;
-
-   nir_intrinsic_instr *intr = nir_instr_as_intrinsic(src->parent_instr);
-   if (intr->intrinsic != nir_intrinsic_store_output ||
-       !nir_src_is_const(intr->src[1])) {
-      return false;
-   }
-
-   uint32_t frac;
-   *dst = ntt_store_output_decl(c, intr, &frac);
-   dst->Index += nir_src_as_uint(intr->src[1]);
-
-   return frac == 0;
-}
-
 static void
 ntt_setup_inputs(struct ntt_compile *c)
 {
@@ -415,18 +297,16 @@ ntt_setup_registers(struct ntt_compile *c, struct exec_list *list)
       struct ureg_dst decl;
       if (nir_reg->num_array_elems == 0) {
          uint32_t write_mask = BITFIELD_MASK(nir_reg->num_components);
-         if (!ntt_try_store_in_tgsi_output(c, &decl, &nir_reg->uses, &nir_reg->if_uses)) {
-            if (nir_reg->bit_size == 64) {
-               if (nir_reg->num_components > 2) {
-                  fprintf(stderr, "NIR-to-TGSI: error: %d-component NIR r%d\n",
-                        nir_reg->num_components, nir_reg->index);
-               }
-
-               write_mask = ntt_64bit_write_mask(write_mask);
+         if (nir_reg->bit_size == 64) {
+            if (nir_reg->num_components > 2) {
+               fprintf(stderr, "NIR-to-TGSI: error: %d-component NIR r%d\n",
+                       nir_reg->num_components, nir_reg->index);
             }
 
-            decl = ureg_writemask(ureg_DECL_temporary(c->ureg), write_mask);
+            write_mask = ntt_64bit_write_mask(write_mask);
          }
+
+         decl = ureg_writemask(ureg_DECL_temporary(c->ureg), write_mask);
       } else {
          decl = ureg_DECL_array_temporary(c->ureg, nir_reg->num_array_elems,
                                           true);
@@ -557,33 +437,16 @@ ntt_get_alu_src(struct ntt_compile *c, nir_alu_instr *instr, int i)
    return usrc;
 }
 
-/* Reswizzles a source so that the unset channels in the write mask still refer
- * to one of the channels present in the write mask.
- */
-static struct ureg_src
-ntt_swizzle_for_write_mask(struct ureg_src src, uint32_t write_mask)
-{
-   assert(write_mask);
-   int first_chan = ffs(write_mask) - 1;
-   return ureg_swizzle(src,
-                       (write_mask & TGSI_WRITEMASK_X) ? TGSI_SWIZZLE_X : first_chan,
-                       (write_mask & TGSI_WRITEMASK_Y) ? TGSI_SWIZZLE_Y : first_chan,
-                       (write_mask & TGSI_WRITEMASK_Z) ? TGSI_SWIZZLE_Z : first_chan,
-                       (write_mask & TGSI_WRITEMASK_W) ? TGSI_SWIZZLE_W : first_chan);
-}
-
 static struct ureg_dst *
 ntt_get_ssa_def_decl(struct ntt_compile *c, nir_ssa_def *ssa)
 {
+   struct ureg_dst temp = ureg_DECL_temporary(c->ureg);
+
    uint32_t writemask = BITSET_MASK(ssa->num_components);
    if (ssa->bit_size == 64)
       writemask = ntt_64bit_write_mask(writemask);
 
-   struct ureg_dst dst;
-   if (!ntt_try_store_in_tgsi_output(c, &dst, &ssa->uses, &ssa->if_uses))
-      dst = ureg_DECL_temporary(c->ureg);
-
-   c->ssa_temp[ssa->index] = ureg_writemask(dst, writemask);
+   c->ssa_temp[ssa->index] = ureg_writemask(temp, writemask);
 
    return &c->ssa_temp[ssa->index];
 }
@@ -1119,27 +982,13 @@ ntt_ureg_src_dimension_indirect(struct ntt_compile *c, struct ureg_src usrc,
 {
    if (nir_src_is_const(src)) {
       return ureg_src_dimension(usrc, nir_src_as_uint(src));
-   }
-   else
-   {
+   } else {
       return ureg_src_dimension_indirect(usrc,
                                          ntt_reladdr(c, ntt_get_src(c, src)),
                                          0);
    }
 }
 
-static struct ureg_dst
-ntt_ureg_dst_dimension_indirect(struct ntt_compile *c, struct ureg_dst udst,
-                                nir_src src)
-{
-   if (nir_src_is_const(src)) {
-      return ureg_dst_dimension(udst, nir_src_as_uint(src));
-   } else {
-      return ureg_dst_dimension_indirect(udst,
-                                         ntt_reladdr(c, ntt_get_src(c, src)),
-                                         0);
-   }
-}
 /* Some load operations in NIR will have a fractional offset that we need to
  * swizzle down before storing to the result register.
  */
@@ -1569,33 +1418,91 @@ ntt_emit_load_input(struct ntt_compile *c, nir_intrinsic_instr *instr)
 static void
 ntt_emit_store_output(struct ntt_compile *c, nir_intrinsic_instr *instr)
 {
+   /* TODO: When making an SSA def's storage, we should check if it's only
+    * used as the source of a store_output and point it at our
+    * TGSI_FILE_OUTPUT instead of generating the extra MOV here.
+    */
+   uint32_t base = nir_intrinsic_base(instr);
    struct ureg_src src = ntt_get_src(c, instr->src[0]);
+   bool is_64 = nir_src_bit_size(instr->src[0]) == 64;
+   struct ureg_dst out;
+   nir_io_semantics semantics = nir_intrinsic_io_semantics(instr);
+   uint32_t frac = nir_intrinsic_component(instr);
+
+   if (c->s->info.stage == MESA_SHADER_FRAGMENT) {
+      if (semantics.location == FRAG_RESULT_COLOR)
+         ureg_property(c->ureg, TGSI_PROPERTY_FS_COLOR0_WRITES_ALL_CBUFS, 1);
+
+      unsigned semantic_name, semantic_index;
+      tgsi_get_gl_frag_result_semantic(semantics.location,
+                                       &semantic_name, &semantic_index);
+      semantic_index += semantics.dual_source_blend_index;
+
+      out = ureg_DECL_output(c->ureg, semantic_name, semantic_index);
+
+      switch (semantics.location) {
+      case FRAG_RESULT_DEPTH:
+         frac = 2; /* z write is the to the .z channel in TGSI */
+         break;
+      case FRAG_RESULT_STENCIL:
+         frac = 1;
+         break;
+      default:
+         break;
+      }
+   } else {
+      unsigned semantic_name, semantic_index;
+
+      ntt_get_gl_varying_semantic(c, semantics.location,
+                                  &semantic_name, &semantic_index);
 
-   if (src.File == TGSI_FILE_OUTPUT) {
-      /* If our src is the output file, that's an indication that we were able
-       * to emit the output stores in the generating instructions and we have
-       * nothing to do here.
+      uint32_t usage_mask = ntt_tgsi_usage_mask(frac,
+                                                instr->num_components,
+                                                is_64);
+      uint32_t gs_streams = semantics.gs_streams;
+      for (int i = 0; i < 4; i++) {
+         if (!(usage_mask & (1 << i)))
+            gs_streams &= ~(0x3 << 2 * i);
+      }
+
+      /* No driver appears to use array_id of outputs. */
+      unsigned array_id = 0;
+
+      /* This bit is lost in the i/o semantics, but it's unused in in-tree
+       * drivers.
        */
-      return;
+      bool invariant = false;
+
+      out = ureg_DECL_output_layout(c->ureg,
+                                    semantic_name, semantic_index,
+                                    gs_streams,
+                                    base,
+                                    usage_mask,
+                                    array_id,
+                                    semantics.num_slots,
+                                    invariant);
    }
 
-   uint32_t frac;
-   struct ureg_dst out = ntt_store_output_decl(c, instr, &frac);
+   out = ntt_ureg_dst_indirect(c, out, instr->src[1]);
 
-   if (instr->intrinsic == nir_intrinsic_store_per_vertex_output) {
-      out = ntt_ureg_dst_indirect(c, out, instr->src[2]);
-      out = ntt_ureg_dst_dimension_indirect(c, out, instr->src[1]);
+   unsigned write_mask = nir_intrinsic_write_mask(instr);
+
+   if (is_64) {
+      write_mask = ntt_64bit_write_mask(write_mask);
+      if (frac >= 2)
+         write_mask = write_mask << 2;
    } else {
-      out = ntt_ureg_dst_indirect(c, out, instr->src[1]);
+      write_mask = write_mask << frac;
    }
 
    uint8_t swizzle[4] = { 0, 0, 0, 0 };
    for (int i = frac; i <= 4; i++) {
-      if (out.WriteMask & (1 << i))
+      if (write_mask & (1 << i))
          swizzle[i] = i - frac;
    }
 
    src = ureg_swizzle(src, swizzle[0], swizzle[1], swizzle[2], swizzle[3]);
+   out = ureg_writemask(out, write_mask);
 
    ureg_MOV(c->ureg, out, src);
    ntt_reladdr_dst_put(c, out);
@@ -1606,16 +1513,7 @@ ntt_emit_load_sysval(struct ntt_compile *c, nir_intrinsic_instr *instr)
 {
    gl_system_value sysval = nir_system_value_from_intrinsic(instr->intrinsic);
    enum tgsi_semantic semantic = tgsi_get_sysval_semantic(sysval);
-   struct ureg_src sv = ureg_DECL_system_value(c->ureg, semantic, 0);
-
-   /* virglrenderer doesn't like references to channels of the sysval that
-    * aren't defined, even if they aren't really read.  (GLSL compile fails on
-    * gl_NumWorkGroups.w, for example).
-    */
-   uint32_t write_mask = BITSET_MASK(nir_dest_num_components(instr->dest));
-   sv = ntt_swizzle_for_write_mask(sv, write_mask);
-
-   ntt_store(c, &instr->dest, sv);
+   ntt_store(c, &instr->dest, ureg_DECL_system_value(c->ureg, semantic, 0));
 }
 
 static void
@@ -1666,7 +1564,6 @@ ntt_emit_intrinsic(struct ntt_compile *c, nir_intrinsic_instr *instr)
       break;
 
    case nir_intrinsic_store_output:
-   case nir_intrinsic_store_per_vertex_output:
       ntt_emit_store_output(c, instr);
       break;
 
@@ -2318,22 +2215,6 @@ ntt_should_vectorize_instr(const nir_instr *instr, void *data)
 
    nir_alu_instr *alu = nir_instr_as_alu(instr);
 
-   switch (alu->op) {
-   case nir_op_ibitfield_extract:
-   case nir_op_ubitfield_extract:
-   case nir_op_bitfield_insert:
-      /* virglrenderer only looks at the .x channel of the offset/bits operands
-       * when translating to GLSL.  tgsi.rst doesn't seem to require scalar
-       * offset/bits operands.
-       *
-       * https://gitlab.freedesktop.org/virgl/virglrenderer/-/issues/195
-       */
-      return false;
-
-   default:
-      break;
-   }
-
    unsigned num_components = alu->dest.dest.ssa.num_components;
 
    int src_bit_size = nir_src_bit_size(alu->src[0].src);
@@ -2747,11 +2628,9 @@ nir_to_tgsi(struct nir_shader *s,
       NIR_PASS_V(s, nir_lower_bool_to_float);
    }
 
-   /* Only lower 32-bit floats.  The only other modifier type officially
-    * supported by TGSI is 32-bit integer negates, but even those are broken on
-    * virglrenderer, so skip lowering all integer and f64 float mods.
-    */
-   NIR_PASS_V(s, nir_lower_to_source_mods, nir_lower_float_source_mods);
+   NIR_PASS_V(s, nir_lower_to_source_mods,
+              nir_lower_float_source_mods |
+              nir_lower_int_source_mods); /* no doubles */
    NIR_PASS_V(s, nir_convert_from_ssa, true);
    NIR_PASS_V(s, nir_lower_vec_to_movs, NULL, NULL);
 
diff --git a/src/gallium/auxiliary/nir/tgsi_to_nir.c b/src/gallium/auxiliary/nir/tgsi_to_nir.c
index f0d5e282787..4eac81856f9 100644
--- a/src/gallium/auxiliary/nir/tgsi_to_nir.c
+++ b/src/gallium/auxiliary/nir/tgsi_to_nir.c
@@ -897,8 +897,10 @@ ttn_get_src(struct ttn_compile *c, struct tgsi_full_src_register *tgsi_fsrc,
       def = nir_bitcast_vector(b, def, 64);
 
    if (tgsi_src->Absolute) {
-      assert(src_is_float);
-      def = nir_fabs(b, def);
+      if (src_is_float)
+         def = nir_fabs(b, def);
+      else
+         def = nir_iabs(b, def);
    }
 
    if (tgsi_src->Negate) {
@@ -1953,8 +1955,6 @@ static const nir_op op_trans[TGSI_OPCODE_LAST] = {
    [TGSI_OPCODE_U64MUL] = nir_op_imul,
    [TGSI_OPCODE_U64DIV] = nir_op_udiv,
    [TGSI_OPCODE_U64SNE] = nir_op_ine,
-   [TGSI_OPCODE_I64NEG] = nir_op_ineg,
-   [TGSI_OPCODE_I64ABS] = nir_op_iabs,
 };
 
 static void
diff --git a/src/gallium/auxiliary/postprocess/postprocess.h b/src/gallium/auxiliary/postprocess/postprocess.h
index 293bd5a12d0..9b9f981a5ff 100644
--- a/src/gallium/auxiliary/postprocess/postprocess.h
+++ b/src/gallium/auxiliary/postprocess/postprocess.h
@@ -35,7 +35,6 @@ extern "C" {
 #endif
 
 struct cso_context;
-struct st_context_iface;
 
 struct pp_queue_t;              /* Forward definition */
 struct pp_program;
@@ -53,8 +52,7 @@ typedef void (*pp_func) (struct pp_queue_t *, struct pipe_resource *,
  */
 struct pp_queue_t *pp_init(struct pipe_context *pipe,
                            const unsigned int *enabled,
-                           struct cso_context *,
-                           struct st_context_iface *st);
+                           struct cso_context *);
 
 void pp_run(struct pp_queue_t *, struct pipe_resource *,
             struct pipe_resource *, struct pipe_resource *);
diff --git a/src/gallium/auxiliary/postprocess/pp_colors.c b/src/gallium/auxiliary/postprocess/pp_colors.c
index 42403642982..e6ea0102eac 100644
--- a/src/gallium/auxiliary/postprocess/pp_colors.c
+++ b/src/gallium/auxiliary/postprocess/pp_colors.c
@@ -37,7 +37,6 @@ pp_nocolor(struct pp_queue_t *ppq, struct pipe_resource *in,
 {
 
    struct pp_program *p = ppq->p;
-   struct pipe_context *pipe = p->pipe;
    const struct pipe_sampler_state *samplers[] = {&p->sampler_point};
 
    pp_filter_setup_in(p, in);
@@ -47,7 +46,7 @@ pp_nocolor(struct pp_queue_t *ppq, struct pipe_resource *in,
    pp_filter_misc_state(p);
 
    cso_set_samplers(p->cso, PIPE_SHADER_FRAGMENT, 1, samplers);
-   pipe->set_sampler_views(pipe, PIPE_SHADER_FRAGMENT, 0, 1, &p->view);
+   cso_set_sampler_views(p->cso, PIPE_SHADER_FRAGMENT, 1, &p->view);
 
    cso_set_vertex_shader_handle(p->cso, ppq->shaders[n][0]);
    cso_set_fragment_shader_handle(p->cso, ppq->shaders[n][1]);
diff --git a/src/gallium/auxiliary/postprocess/pp_init.c b/src/gallium/auxiliary/postprocess/pp_init.c
index 97660ee609a..2c830e81bd7 100644
--- a/src/gallium/auxiliary/postprocess/pp_init.c
+++ b/src/gallium/auxiliary/postprocess/pp_init.c
@@ -40,7 +40,7 @@
 /** Initialize the post-processing queue. */
 struct pp_queue_t *
 pp_init(struct pipe_context *pipe, const unsigned int *enabled,
-        struct cso_context *cso, struct st_context_iface *st)
+        struct cso_context *cso)
 {
    unsigned int num_filters = 0;
    unsigned int curpos = 0, i, tmp_req = 0;
@@ -78,7 +78,7 @@ pp_init(struct pipe_context *pipe, const unsigned int *enabled,
       goto error;
    }
 
-   ppq->p = pp_init_prog(ppq, pipe, cso, st);
+   ppq->p = pp_init_prog(ppq, pipe, cso);
    if (ppq->p == NULL) {
       pp_debug("pp_init_prog returned NULL.\n");
       goto error;
diff --git a/src/gallium/auxiliary/postprocess/pp_mlaa.c b/src/gallium/auxiliary/postprocess/pp_mlaa.c
index 6ec4269c241..537fd242b9a 100644
--- a/src/gallium/auxiliary/postprocess/pp_mlaa.c
+++ b/src/gallium/auxiliary/postprocess/pp_mlaa.c
@@ -97,15 +97,10 @@ pp_jimenezmlaa_run(struct pp_queue_t *ppq, struct pipe_resource *in,
       dimensions[1] = p->framebuffer.height;
    }
 
-   struct pipe_constant_buffer cb;
-   cb.buffer = NULL;
-   cb.buffer_offset = 0;
-   cb.buffer_size = sizeof(constants);
-   cb.user_buffer = constants;
-
-   struct pipe_context *pipe = ppq->p->pipe;
-   pipe->set_constant_buffer(pipe, PIPE_SHADER_VERTEX, 0, &cb);
-   pipe->set_constant_buffer(pipe, PIPE_SHADER_FRAGMENT, 0, &cb);
+   cso_set_constant_user_buffer(p->cso, PIPE_SHADER_VERTEX,
+                                0, constants, sizeof(constants));
+   cso_set_constant_user_buffer(p->cso, PIPE_SHADER_FRAGMENT,
+                                0, constants, sizeof(constants));
 
    mstencil.stencil[0].enabled = 1;
    mstencil.stencil[0].valuemask = mstencil.stencil[0].writemask = ~0;
@@ -134,7 +129,7 @@ pp_jimenezmlaa_run(struct pp_queue_t *ppq, struct pipe_resource *in,
       const struct pipe_sampler_state *samplers[] = {&p->sampler_point};
       cso_set_samplers(p->cso, PIPE_SHADER_FRAGMENT, 1, samplers);
    }
-   pipe->set_sampler_views(pipe, PIPE_SHADER_FRAGMENT, 0, 1, &p->view);
+   cso_set_sampler_views(p->cso, PIPE_SHADER_FRAGMENT, 1, &p->view);
 
    cso_set_vertex_shader_handle(p->cso, ppq->shaders[n][1]);    /* offsetvs */
    cso_set_fragment_shader_handle(p->cso, ppq->shaders[n][2]);
@@ -166,7 +161,7 @@ pp_jimenezmlaa_run(struct pp_queue_t *ppq, struct pipe_resource *in,
    }
 
    arr[0] = p->view;
-   pipe->set_sampler_views(pipe, PIPE_SHADER_FRAGMENT, 0, 3, arr);
+   cso_set_sampler_views(p->cso, PIPE_SHADER_FRAGMENT, 3, arr);
 
    cso_set_vertex_shader_handle(p->cso, ppq->shaders[n][0]);    /* passvs */
    cso_set_fragment_shader_handle(p->cso, ppq->shaders[n][3]);
@@ -198,7 +193,7 @@ pp_jimenezmlaa_run(struct pp_queue_t *ppq, struct pipe_resource *in,
    }
 
    arr[1] = p->view;
-   pipe->set_sampler_views(pipe, PIPE_SHADER_FRAGMENT, 0, 2, arr);
+   cso_set_sampler_views(p->cso, PIPE_SHADER_FRAGMENT, 2, arr);
 
    cso_set_vertex_shader_handle(p->cso, ppq->shaders[n][1]);    /* offsetvs */
    cso_set_fragment_shader_handle(p->cso, ppq->shaders[n][4]);
diff --git a/src/gallium/auxiliary/postprocess/pp_private.h b/src/gallium/auxiliary/postprocess/pp_private.h
index 922b93c7ec7..7e63b5b2e4c 100644
--- a/src/gallium/auxiliary/postprocess/pp_private.h
+++ b/src/gallium/auxiliary/postprocess/pp_private.h
@@ -41,7 +41,6 @@ struct pp_program
    struct pipe_screen *screen;
    struct pipe_context *pipe;
    struct cso_context *cso;
-   struct st_context_iface *st;
 
    struct pipe_blend_state blend;
    struct pipe_depth_stencil_alpha_state depthstencil;
@@ -95,7 +94,7 @@ void pp_free_fbos(struct pp_queue_t *);
 void pp_debug(const char *, ...);
 
 struct pp_program *pp_init_prog(struct pp_queue_t *, struct pipe_context *pipe,
-                                struct cso_context *, struct st_context_iface *st);
+                                struct cso_context *);
 
 void pp_blit(struct pipe_context *pipe,
              struct pipe_resource *src_tex,
diff --git a/src/gallium/auxiliary/postprocess/pp_program.c b/src/gallium/auxiliary/postprocess/pp_program.c
index 258133bb3c5..65d7f957ac5 100644
--- a/src/gallium/auxiliary/postprocess/pp_program.c
+++ b/src/gallium/auxiliary/postprocess/pp_program.c
@@ -41,7 +41,7 @@
 /** Initialize the internal details */
 struct pp_program *
 pp_init_prog(struct pp_queue_t *ppq, struct pipe_context *pipe,
-             struct cso_context *cso, struct st_context_iface *st)
+             struct cso_context *cso)
 {
    struct pp_program *p;
 
@@ -56,7 +56,6 @@ pp_init_prog(struct pp_queue_t *ppq, struct pipe_context *pipe,
    p->screen = pipe->screen;
    p->pipe = pipe;
    p->cso = cso;
-   p->st = st;
 
    {
       static const float verts[4][2][4] = {
diff --git a/src/gallium/auxiliary/postprocess/pp_run.c b/src/gallium/auxiliary/postprocess/pp_run.c
index dd931b949ad..c6987153f73 100644
--- a/src/gallium/auxiliary/postprocess/pp_run.c
+++ b/src/gallium/auxiliary/postprocess/pp_run.c
@@ -29,7 +29,6 @@
 #include "postprocess/pp_filters.h"
 #include "postprocess/pp_private.h"
 
-#include "frontend/api.h"
 #include "util/u_inlines.h"
 #include "util/u_sampler.h"
 
@@ -127,13 +126,17 @@ pp_run(struct pp_queue_t *ppq, struct pipe_resource *in,
                         CSO_BIT_SAMPLE_MASK |
                         CSO_BIT_MIN_SAMPLES |
                         CSO_BIT_FRAGMENT_SAMPLERS |
+                        CSO_BIT_FRAGMENT_SAMPLER_VIEWS |
                         CSO_BIT_STENCIL_REF |
                         CSO_BIT_STREAM_OUTPUTS |
                         CSO_BIT_VERTEX_ELEMENTS |
                         CSO_BIT_VERTEX_SHADER |
                         CSO_BIT_VIEWPORT |
+                        CSO_BIT_AUX_VERTEX_BUFFER_SLOT |
                         CSO_BIT_PAUSE_QUERIES |
                         CSO_BIT_RENDER_CONDITION));
+   cso_save_constant_buffer_slot0(cso, PIPE_SHADER_VERTEX);
+   cso_save_constant_buffer_slot0(cso, PIPE_SHADER_FRAGMENT);
 
    /* set default state */
    cso_set_sample_mask(cso, ~0);
@@ -185,22 +188,8 @@ pp_run(struct pp_queue_t *ppq, struct pipe_resource *in,
 
    /* restore state we changed */
    cso_restore_state(cso);
-
-   /* Unbind resources that we have bound. */
-   struct pipe_context *pipe = ppq->p->pipe;
-   pipe->set_constant_buffer(pipe, PIPE_SHADER_VERTEX, 0, NULL);
-   pipe->set_constant_buffer(pipe, PIPE_SHADER_FRAGMENT, 0, NULL);
-   pipe->set_vertex_buffers(pipe, 0, 1, NULL);
-   pipe->set_sampler_views(pipe, PIPE_SHADER_FRAGMENT, 0, 3, NULL);
-
-   /* restore states not restored by cso */
-   if (ppq->p->st) {
-      ppq->p->st->invalidate_state(ppq->p->st,
-                                   ST_INVALIDATE_FS_SAMPLER_VIEWS |
-                                   ST_INVALIDATE_FS_CONSTBUF0 |
-                                   ST_INVALIDATE_VS_CONSTBUF0 |
-                                   ST_INVALIDATE_VERTEX_BUFFERS);
-   }
+   cso_restore_constant_buffer_slot0(cso, PIPE_SHADER_VERTEX);
+   cso_restore_constant_buffer_slot0(cso, PIPE_SHADER_FRAGMENT);
 
    pipe_resource_reference(&ppq->depth, NULL);
    pipe_resource_reference(&refin, NULL);
diff --git a/src/gallium/auxiliary/tgsi/tgsi_exec.c b/src/gallium/auxiliary/tgsi/tgsi_exec.c
index f7ed64121f0..8eaae4e46e3 100644
--- a/src/gallium/auxiliary/tgsi/tgsi_exec.c
+++ b/src/gallium/auxiliary/tgsi/tgsi_exec.c
@@ -1713,8 +1713,11 @@ fetch_source(const struct tgsi_exec_machine *mach,
    fetch_source_d(mach, chan, reg, chan_index);
 
    if (reg->Register.Absolute) {
-      assert(src_datatype == TGSI_EXEC_DATA_FLOAT);
-      micro_abs(chan, chan);
+      if (src_datatype == TGSI_EXEC_DATA_FLOAT) {
+         micro_abs(chan, chan);
+      } else {
+         micro_iabs(chan, chan);
+      }
    }
 
    if (reg->Register.Negate) {
@@ -3594,8 +3597,12 @@ fetch_double_channel(struct tgsi_exec_machine *mach,
       chan->u[i][0] = src[0].u[i];
       chan->u[i][1] = src[1].u[i];
    }
-   assert(!reg->Register.Absolute);
-   assert(!reg->Register.Negate);
+   if (reg->Register.Absolute) {
+      micro_dabs(chan, chan);
+   }
+   if (reg->Register.Negate) {
+      micro_dneg(chan, chan);
+   }
 }
 
 static void
diff --git a/src/gallium/auxiliary/tgsi/tgsi_util.c b/src/gallium/auxiliary/tgsi/tgsi_util.c
index d6ef81d281d..e1b604cff0e 100644
--- a/src/gallium/auxiliary/tgsi/tgsi_util.c
+++ b/src/gallium/auxiliary/tgsi/tgsi_util.c
@@ -103,6 +103,67 @@ tgsi_util_set_src_register_swizzle(struct tgsi_src_register *reg,
 }
 
 
+unsigned
+tgsi_util_get_full_src_register_sign_mode(
+   const struct tgsi_full_src_register *reg,
+   UNUSED unsigned component)
+{
+   unsigned sign_mode;
+
+   if (reg->Register.Absolute) {
+      /* Consider only the post-abs negation. */
+
+      if (reg->Register.Negate) {
+         sign_mode = TGSI_UTIL_SIGN_SET;
+      }
+      else {
+         sign_mode = TGSI_UTIL_SIGN_CLEAR;
+      }
+   }
+   else {
+      if (reg->Register.Negate) {
+         sign_mode = TGSI_UTIL_SIGN_TOGGLE;
+      }
+      else {
+         sign_mode = TGSI_UTIL_SIGN_KEEP;
+      }
+   }
+
+   return sign_mode;
+}
+
+
+void
+tgsi_util_set_full_src_register_sign_mode(struct tgsi_full_src_register *reg,
+                                          unsigned sign_mode)
+{
+   switch (sign_mode) {
+   case TGSI_UTIL_SIGN_CLEAR:
+      reg->Register.Negate = 0;
+      reg->Register.Absolute = 1;
+      break;
+
+   case TGSI_UTIL_SIGN_SET:
+      reg->Register.Absolute = 1;
+      reg->Register.Negate = 1;
+      break;
+
+   case TGSI_UTIL_SIGN_TOGGLE:
+      reg->Register.Negate = 1;
+      reg->Register.Absolute = 0;
+      break;
+
+   case TGSI_UTIL_SIGN_KEEP:
+      reg->Register.Negate = 0;
+      reg->Register.Absolute = 0;
+      break;
+
+   default:
+      assert(0);
+   }
+}
+
+
 /**
  * Determine which channels of the specificed src register are effectively
  * used by this instruction.
diff --git a/src/gallium/auxiliary/tgsi/tgsi_util.h b/src/gallium/auxiliary/tgsi/tgsi_util.h
index e1f913d74b9..6dc576b1a00 100644
--- a/src/gallium/auxiliary/tgsi/tgsi_util.h
+++ b/src/gallium/auxiliary/tgsi/tgsi_util.h
@@ -57,6 +57,21 @@ tgsi_util_set_src_register_swizzle(struct tgsi_src_register *reg,
                                    unsigned swizzle,
                                    unsigned component);
 
+
+#define TGSI_UTIL_SIGN_CLEAR    0   /* Force positive */
+#define TGSI_UTIL_SIGN_SET      1   /* Force negative */
+#define TGSI_UTIL_SIGN_TOGGLE   2   /* Negate */
+#define TGSI_UTIL_SIGN_KEEP     3   /* No change */
+
+unsigned
+tgsi_util_get_full_src_register_sign_mode(
+   const struct tgsi_full_src_register *reg,
+   unsigned component);
+
+void
+tgsi_util_set_full_src_register_sign_mode(struct tgsi_full_src_register *reg,
+                                          unsigned sign_mode);
+
 unsigned
 tgsi_util_get_inst_usage_mask(const struct tgsi_full_instruction *inst,
                               unsigned src_idx);
diff --git a/src/gallium/auxiliary/util/u_blitter.c b/src/gallium/auxiliary/util/u_blitter.c
index eefe69b8fe3..cf045030f48 100644
--- a/src/gallium/auxiliary/util/u_blitter.c
+++ b/src/gallium/auxiliary/util/u_blitter.c
@@ -151,6 +151,7 @@ struct blitter_context_priv
    bool has_txf;
    bool cube_as_2darray;
    bool cached_all_shaders;
+   bool emulate_argb;
 
    /* The Draw module overrides these functions.
     * Always create the blitter before Draw. */
@@ -207,6 +208,10 @@ struct blitter_context *util_blitter_create(struct pipe_context *pipe)
          pipe->screen->get_param(pipe->screen,
                                  PIPE_CAP_SHADER_STENCIL_EXPORT);
 
+   ctx->emulate_argb =
+         pipe->screen->get_param(pipe->screen,
+                                 PIPE_CAP_EMULATE_ARGB);
+
    ctx->has_texture_multisample =
       pipe->screen->get_param(pipe->screen, PIPE_CAP_TEXTURE_MULTISAMPLE);
 
@@ -1720,6 +1725,8 @@ void util_blitter_copy_texture(struct blitter_context *blitter,
 
    /* Initialize the sampler view. */
    util_blitter_default_src_texture(blitter, &src_templ, src, src_level);
+   if (ctx->emulate_argb)
+      u_sampler_view_swizzle_argb(&src_templ, dst->format);
    src_view = pipe->create_sampler_view(pipe, src, &src_templ);
 
    /* Copy. */
@@ -2148,6 +2155,8 @@ util_blitter_blit(struct blitter_context *blitter,
    /* Initialize the sampler view. */
    util_blitter_default_src_texture(blitter, &src_templ, src, info->src.level);
    src_templ.format = info->src.format;
+   if (ctx->emulate_argb)
+      u_sampler_view_swizzle_argb(&src_templ, info->dst.format);
    src_view = pipe->create_sampler_view(pipe, src, &src_templ);
 
    /* Copy. */
diff --git a/src/gallium/auxiliary/util/u_inlines.h b/src/gallium/auxiliary/util/u_inlines.h
index c5aadc7ca18..965c663e9c9 100644
--- a/src/gallium/auxiliary/util/u_inlines.h
+++ b/src/gallium/auxiliary/util/u_inlines.h
@@ -427,6 +427,20 @@ pipe_buffer_write_nooverlap(struct pipe_context *pipe,
                         offset, size, data);
 }
 
+/**
+ * Utility for simplifying pipe_context::resource_copy_region calls
+ */
+static inline void
+pipe_buffer_copy(struct pipe_context *pipe,
+                 struct pipe_resource *dst,
+                 struct pipe_resource *src,
+                 unsigned dst_offset,
+                 unsigned src_offset,
+                 unsigned size)
+{
+   struct pipe_box box = { .x = (int)src_offset, .width = (int)size };
+   pipe->resource_copy_region(pipe, dst, 0, dst_offset, 0, 0, src, 0, &box);
+}
 
 /**
  * Create a new resource and immediately put data into it
@@ -763,26 +777,6 @@ util_logicop_reads_dest(enum pipe_logicop op)
    unreachable("bad logicop");
 }
 
-static inline bool
-util_writes_stencil(const struct pipe_stencil_state *s)
-{
-   return s->enabled && s->writemask &&
-        ((s->fail_op != PIPE_STENCIL_OP_KEEP) ||
-         (s->zpass_op != PIPE_STENCIL_OP_KEEP) ||
-         (s->zfail_op != PIPE_STENCIL_OP_KEEP));
-}
-
-static inline bool
-util_writes_depth_stencil(const struct pipe_depth_stencil_alpha_state *zsa)
-{
-   if (zsa->depth_enabled && zsa->depth_writemask &&
-       (zsa->depth_func != PIPE_FUNC_NEVER))
-      return true;
-
-   return util_writes_stencil(&zsa->stencil[0]) ||
-          util_writes_stencil(&zsa->stencil[1]);
-}
-
 static inline struct pipe_context *
 pipe_create_multimedia_context(struct pipe_screen *screen)
 {
diff --git a/src/gallium/auxiliary/util/u_sampler.c b/src/gallium/auxiliary/util/u_sampler.c
index 2c823d45609..4bef3e55632 100644
--- a/src/gallium/auxiliary/util/u_sampler.c
+++ b/src/gallium/auxiliary/util/u_sampler.c
@@ -108,3 +108,92 @@ u_sampler_view_default_dx9_template(struct pipe_sampler_view *view,
                     format,
                     PIPE_SWIZZLE_1);
 }
+
+static inline void
+swizzle_src(bool src_is_argb, bool src_is_abgr, unsigned char *swizz)
+{
+   if (src_is_argb) {
+      /* compose swizzle with alpha at the end */
+      swizz[0] = PIPE_SWIZZLE_Y;
+      swizz[1] = PIPE_SWIZZLE_Z;
+      swizz[2] = PIPE_SWIZZLE_W;
+      swizz[3] = PIPE_SWIZZLE_X;
+   } else if (src_is_abgr) {
+      /* this is weird because we're already using a codepath with
+       * a swizzle
+       */
+      swizz[0] = PIPE_SWIZZLE_W;
+      swizz[1] = PIPE_SWIZZLE_X;
+      swizz[2] = PIPE_SWIZZLE_Y;
+      swizz[3] = PIPE_SWIZZLE_Z;
+   }
+}
+
+void
+u_sampler_view_swizzle_argb(struct pipe_sampler_view *view,
+                            enum pipe_format dst_format)
+{
+   bool src_is_argb = util_format_is_argb(view->format);
+   bool src_is_abgr = util_format_is_abgr(view->format);
+   bool dst_is_argb = util_format_is_argb(dst_format);
+   bool dst_is_abgr = util_format_is_abgr(dst_format);
+
+   if (src_is_argb == dst_is_argb && src_is_abgr == dst_is_abgr)
+      return;
+
+   unsigned char view_swiz[4] = {
+      view->swizzle_r,
+      view->swizzle_g,
+      view->swizzle_b,
+      view->swizzle_a,
+   };
+   unsigned char dst_swiz[4];
+
+   if (src_is_argb || src_is_abgr) {
+      unsigned char reverse_alpha[4];
+      swizzle_src(src_is_argb, src_is_abgr, reverse_alpha);
+      util_format_compose_swizzles(view_swiz, reverse_alpha, dst_swiz);
+   } else if (dst_is_argb) {
+      unsigned char reverse_alpha[] = {
+         PIPE_SWIZZLE_W,
+         PIPE_SWIZZLE_X,
+         PIPE_SWIZZLE_Y,
+         PIPE_SWIZZLE_Z,
+      };
+      /* compose swizzle with alpha at the start */
+      util_format_compose_swizzles(view_swiz, reverse_alpha, dst_swiz);
+   } else if (dst_is_abgr) {
+      /* I don't understand why this swizzle is correct when doing abgr -> rgba? */
+      unsigned char reverse_alpha[] = {
+         PIPE_SWIZZLE_Y,
+         PIPE_SWIZZLE_Z,
+         PIPE_SWIZZLE_W,
+         PIPE_SWIZZLE_X,
+      };
+      util_format_compose_swizzles(view_swiz, reverse_alpha, dst_swiz);
+   }
+   view->swizzle_r = dst_swiz[PIPE_SWIZZLE_X];
+   view->swizzle_g = dst_swiz[PIPE_SWIZZLE_Y];
+   view->swizzle_b = dst_swiz[PIPE_SWIZZLE_Z];
+   view->swizzle_a = dst_swiz[PIPE_SWIZZLE_W];
+}
+
+void
+u_sampler_format_swizzle_color_argb(union pipe_color_union *color, bool is_integer)
+{
+   unsigned char reverse_alpha[4];
+   swizzle_src(true, false, reverse_alpha);
+   union pipe_color_union src_color = *color;
+   util_format_apply_color_swizzle(color, &src_color, reverse_alpha,
+                                   is_integer);
+}
+
+void
+u_sampler_format_swizzle_color_abgr(union pipe_color_union *color, bool is_integer)
+{
+   unsigned char reverse_alpha[4];
+   swizzle_src(false, true, reverse_alpha);
+   union pipe_color_union src_color = *color;
+   util_format_apply_color_swizzle(color, &src_color, reverse_alpha,
+                                   is_integer);
+}
diff --git a/src/gallium/auxiliary/util/u_sampler.h b/src/gallium/auxiliary/util/u_sampler.h
index f3dad7417e0..2ab1155d9e2 100644
--- a/src/gallium/auxiliary/util/u_sampler.h
+++ b/src/gallium/auxiliary/util/u_sampler.h
@@ -49,6 +49,14 @@ u_sampler_view_default_dx9_template(struct pipe_sampler_view *view,
                                     const struct pipe_resource *texture,
                                     enum pipe_format format);
 
+void
+u_sampler_view_swizzle_argb(struct pipe_sampler_view *view,
+                            enum pipe_format dst_format);
+
+void
+u_sampler_format_swizzle_color_argb(union pipe_color_union *color, bool is_integer);
+void
+u_sampler_format_swizzle_color_abgr(union pipe_color_union *color, bool is_integer);
 
 #ifdef __cplusplus
 } /* extern "C" { */
diff --git a/src/gallium/auxiliary/util/u_screen.c b/src/gallium/auxiliary/util/u_screen.c
index 048749e3099..dc9d0af129e 100644
--- a/src/gallium/auxiliary/util/u_screen.c
+++ b/src/gallium/auxiliary/util/u_screen.c
@@ -67,6 +67,8 @@ u_pipe_screen_get_param_defaults(struct pipe_screen *pscreen,
    case PIPE_CAP_PRIMITIVE_RESTART_FIXED_INDEX:
    case PIPE_CAP_INDEP_BLEND_ENABLE:
    case PIPE_CAP_INDEP_BLEND_FUNC:
+   case PIPE_CAP_NO_DITHERING:
+   case PIPE_CAP_EMULATE_ARGB:
    case PIPE_CAP_MAX_TEXTURE_ARRAY_LAYERS: /* Enables GL_EXT_texture_array */
    case PIPE_CAP_TGSI_FS_COORD_ORIGIN_UPPER_LEFT:
    case PIPE_CAP_TGSI_FS_COORD_ORIGIN_LOWER_LEFT:
diff --git a/src/gallium/auxiliary/util/u_vbuf.c b/src/gallium/auxiliary/util/u_vbuf.c
index add103df0bf..4120948f401 100644
--- a/src/gallium/auxiliary/util/u_vbuf.c
+++ b/src/gallium/auxiliary/util/u_vbuf.c
@@ -157,6 +157,9 @@ struct u_vbuf {
    struct pipe_vertex_buffer vertex_buffer[PIPE_MAX_ATTRIBS];
    uint32_t enabled_vb_mask;
 
+   /* Saved vertex buffer. */
+   struct pipe_vertex_buffer vertex_buffer0_saved;
+
    /* Vertex buffers for the driver.
     * There are usually no user buffers. */
    struct pipe_vertex_buffer real_vertex_buffer[PIPE_MAX_ATTRIBS];
@@ -398,6 +401,8 @@ void u_vbuf_destroy(struct u_vbuf *mgr)
    for (i = 0; i < PIPE_MAX_ATTRIBS; i++)
       pipe_vertex_buffer_unreference(&mgr->real_vertex_buffer[i]);
 
+   pipe_vertex_buffer_unreference(&mgr->vertex_buffer0_saved);
+
    translate_cache_destroy(mgr->translate_cache);
    cso_cache_delete(&mgr->cso_cache);
    FREE(mgr);
@@ -1599,3 +1604,15 @@ void u_vbuf_restore_vertex_elements(struct u_vbuf *mgr)
    }
    mgr->ve_saved = NULL;
 }
+
+void u_vbuf_save_vertex_buffer0(struct u_vbuf *mgr)
+{
+   pipe_vertex_buffer_reference(&mgr->vertex_buffer0_saved,
+                                &mgr->vertex_buffer[0]);
+}
+
+void u_vbuf_restore_vertex_buffer0(struct u_vbuf *mgr)
+{
+   u_vbuf_set_vertex_buffers(mgr, 0, 1, &mgr->vertex_buffer0_saved);
+   pipe_vertex_buffer_unreference(&mgr->vertex_buffer0_saved);
+}
diff --git a/src/gallium/auxiliary/util/u_vbuf.h b/src/gallium/auxiliary/util/u_vbuf.h
index 3df24030872..30d831d2656 100644
--- a/src/gallium/auxiliary/util/u_vbuf.h
+++ b/src/gallium/auxiliary/util/u_vbuf.h
@@ -88,5 +88,7 @@ void u_vbuf_get_minmax_index(struct pipe_context *pipe,
 /* Save/restore functionality. */
 void u_vbuf_save_vertex_elements(struct u_vbuf *mgr);
 void u_vbuf_restore_vertex_elements(struct u_vbuf *mgr);
+void u_vbuf_save_vertex_buffer0(struct u_vbuf *mgr);
+void u_vbuf_restore_vertex_buffer0(struct u_vbuf *mgr);
 
 #endif
diff --git a/src/gallium/drivers/freedreno/a6xx/fd6_blitter.c b/src/gallium/drivers/freedreno/a6xx/fd6_blitter.c
index 6ea48b04470..f7e66ef05ee 100644
--- a/src/gallium/drivers/freedreno/a6xx/fd6_blitter.c
+++ b/src/gallium/drivers/freedreno/a6xx/fd6_blitter.c
@@ -256,6 +256,9 @@ emit_blit_setup(struct fd_ringbuffer *ring,
 	bool is_srgb = util_format_is_srgb(pfmt);
 	enum a6xx_2d_ifmt ifmt = fd6_ifmt(fmt);
 
+	OUT_PKT7(ring, CP_SET_MARKER, 1);
+	OUT_RING(ring, A6XX_CP_SET_MARKER_0_MODE(RM6_BLIT2DSCALE));
+
 	if (is_srgb) {
 		assert(ifmt == R2D_UNORM8);
 		ifmt = R2D_UNORM8_SRGB;
@@ -786,77 +789,6 @@ fd6_clear_surface(struct fd_context *ctx,
 	}
 }
 
-void
-fd6_resolve_tile(struct fd_batch *batch, struct fd_ringbuffer *ring,
-		uint32_t base, struct pipe_surface *psurf)
-{
-	const struct fd_gmem_stateobj *gmem = batch->gmem_state;
-	uint64_t gmem_base = batch->ctx->screen->gmem_base + base;
-	uint32_t gmem_pitch = gmem->bin_w * batch->framebuffer.samples *
-			util_format_get_blocksize(psurf->format);
-
-	OUT_PKT4(ring, REG_A6XX_GRAS_2D_DST_TL, 2);
-	OUT_RING(ring, A6XX_GRAS_2D_DST_TL_X(0) | A6XX_GRAS_2D_DST_TL_Y(0));
-	OUT_RING(ring, A6XX_GRAS_2D_DST_BR_X(psurf->width - 1) |
-			A6XX_GRAS_2D_DST_BR_Y(psurf->height - 1));
-
-	OUT_PKT4(ring, REG_A6XX_GRAS_2D_SRC_TL_X, 4);
-	OUT_RING(ring, A6XX_GRAS_2D_SRC_TL_X(0));
-	OUT_RING(ring, A6XX_GRAS_2D_SRC_BR_X(psurf->width - 1));
-	OUT_RING(ring, A6XX_GRAS_2D_SRC_TL_Y(0));
-	OUT_RING(ring, A6XX_GRAS_2D_SRC_BR_Y(psurf->height - 1));
-
-	/* Enable scissor bit, which will take into account the window scissor
-	 * which is set per-tile
-	 */
-	emit_blit_setup(ring, psurf->format, true, NULL);
-
-	/* We shouldn't be using GMEM in the layered rendering case: */
-	assert(psurf->u.tex.first_layer == psurf->u.tex.last_layer);
-
-	emit_blit_dst(ring, psurf->texture, psurf->format, psurf->u.tex.level,
-			psurf->u.tex.first_layer);
-
-	enum a6xx_format sfmt = fd6_pipe2color(psurf->format);
-	enum a3xx_msaa_samples samples = fd_msaa_samples(batch->framebuffer.samples);
-
-	OUT_PKT4(ring, REG_A6XX_SP_PS_2D_SRC_INFO, 10);
-	OUT_RING(ring, A6XX_SP_PS_2D_SRC_INFO_COLOR_FORMAT(sfmt) |
-			A6XX_SP_PS_2D_SRC_INFO_TILE_MODE(TILE6_2) |
-			A6XX_SP_PS_2D_SRC_INFO_SAMPLES(samples) |
-			COND(samples > MSAA_ONE, A6XX_SP_PS_2D_SRC_INFO_SAMPLES_AVERAGE) |
-			COND(util_format_is_srgb(psurf->format), A6XX_SP_PS_2D_SRC_INFO_SRGB) |
-			A6XX_SP_PS_2D_SRC_INFO_UNK20 |
-			A6XX_SP_PS_2D_SRC_INFO_UNK22);
-	OUT_RING(ring, A6XX_SP_PS_2D_SRC_SIZE_WIDTH(psurf->width) |
-			A6XX_SP_PS_2D_SRC_SIZE_HEIGHT(psurf->height));
-	OUT_RING(ring, gmem_base);                   /* SP_PS_2D_SRC_LO */
-	OUT_RING(ring, gmem_base >> 32);             /* SP_PS_2D_SRC_HI */
-	OUT_RING(ring, A6XX_SP_PS_2D_SRC_PITCH_PITCH(gmem_pitch));
-	OUT_RING(ring, 0x00000000);
-	OUT_RING(ring, 0x00000000);
-	OUT_RING(ring, 0x00000000);
-	OUT_RING(ring, 0x00000000);
-	OUT_RING(ring, 0x00000000);
-
-	/* sync GMEM writes with CACHE. */
-	fd6_cache_inv(batch, ring);
-
-	/* Wait for CACHE_INVALIDATE to land */
-	fd_wfi(batch, ring);
-
-	OUT_PKT7(ring, CP_BLIT, 1);
-	OUT_RING(ring, CP_BLIT_0_OP(BLIT_OP_SCALE));
-
-	OUT_WFI5(ring);
-
-	/* CP_BLIT writes to the CCU, unlike CP_EVENT_WRITE::BLIT which writes to
-	 * sysmem, and we generally assume that GMEM renderpasses leave their
-	 * results in sysmem, so we need to flush manually here.
-	 */
-	fd6_event_write(batch, ring, PC_CCU_FLUSH_COLOR_TS, true);
-}
-
 static bool
 handle_rgba_blit(struct fd_context *ctx, const struct pipe_blit_info *info)
 {
diff --git a/src/gallium/drivers/freedreno/a6xx/fd6_blitter.h b/src/gallium/drivers/freedreno/a6xx/fd6_blitter.h
index 6494e96c4dd..b75013c6d04 100644
--- a/src/gallium/drivers/freedreno/a6xx/fd6_blitter.h
+++ b/src/gallium/drivers/freedreno/a6xx/fd6_blitter.h
@@ -35,15 +35,9 @@
 void fd6_blitter_init(struct pipe_context *pctx);
 unsigned fd6_tile_mode(const struct pipe_resource *tmpl);
 
-/*
- * Blitter APIs used by gmem for cases that need CP_BLIT's (r2d)
- * instead of CP_EVENT_WRITE::BLITs
- */
-
-void fd6_clear_surface(struct fd_context *ctx,
+void
+fd6_clear_surface(struct fd_context *ctx,
 		struct fd_ringbuffer *ring, struct pipe_surface *psurf,
 		uint32_t width, uint32_t height, union pipe_color_union *color);
-void fd6_resolve_tile(struct fd_batch *batch, struct fd_ringbuffer *ring,
-		uint32_t base, struct pipe_surface *psurf);
 
 #endif /* FD6_BLIT_H_ */
diff --git a/src/gallium/drivers/freedreno/a6xx/fd6_emit.c b/src/gallium/drivers/freedreno/a6xx/fd6_emit.c
index bea8b0619b3..1b773295bae 100644
--- a/src/gallium/drivers/freedreno/a6xx/fd6_emit.c
+++ b/src/gallium/drivers/freedreno/a6xx/fd6_emit.c
@@ -593,7 +593,7 @@ compute_ztest_mode(struct fd6_emit *emit, bool lrz_valid)
 			fs->writes_stencilref) {
 		return A6XX_LATE_Z;
 	} else if ((fs->has_kill || zsa->alpha_test) &&
-			(zsa->writes_zs || !pfb->zsbuf)) {
+			(zsa->base.depth_writemask || !pfb->zsbuf)) {
 		/* Slightly odd, but seems like the hw wants us to select
 		 * LATE_Z mode if there is no depth buffer + discard.  Either
 		 * that, or when occlusion query is enabled.  See:
diff --git a/src/gallium/drivers/freedreno/a6xx/fd6_gmem.c b/src/gallium/drivers/freedreno/a6xx/fd6_gmem.c
index 0bb01eb7fa7..affba171fc1 100644
--- a/src/gallium/drivers/freedreno/a6xx/fd6_gmem.c
+++ b/src/gallium/drivers/freedreno/a6xx/fd6_gmem.c
@@ -1129,47 +1129,6 @@ fd6_emit_tile_renderprep(struct fd_batch *batch, const struct fd_tile *tile)
 	trace_end_clear_restore(&batch->trace);
 }
 
-static bool
-blit_can_resolve(enum pipe_format format)
-{
-	const struct util_format_description *desc = util_format_description(format);
-
-	/* blit event can only do resolve for simple cases:
-	 * averaging samples as unsigned integers or choosing only one sample
-	 */
-	if (util_format_is_snorm(format) || util_format_is_srgb(format))
-		return false;
-
-	/* can't do formats with larger channel sizes
-	 * note: this includes all float formats
-	 * note2: single channel integer formats seem OK
-	 */
-	if (desc->channel[0].size > 10)
-		return false;
-
-	switch (format) {
-	/* for unknown reasons blit event can't msaa resolve these formats when tiled
-	 * likely related to these formats having different layout from other cpp=2 formats
-	 */
-	case PIPE_FORMAT_R8G8_UNORM:
-	case PIPE_FORMAT_R8G8_UINT:
-	case PIPE_FORMAT_R8G8_SINT:
-	/* TODO: this one should be able to work? */
-	case PIPE_FORMAT_Z24_UNORM_S8_UINT:
-		return false;
-	default:
-		break;
-	}
-
-	return true;
-}
-
-static bool
-needs_resolve(struct pipe_surface *psurf)
-{
-	return psurf->nr_samples && (psurf->nr_samples != psurf->texture->nr_samples);
-}
-
 static void
 emit_resolve_blit(struct fd_batch *batch,
 				  struct fd_ringbuffer *ring,
@@ -1183,18 +1142,6 @@ emit_resolve_blit(struct fd_batch *batch,
 	if (!fd_resource(psurf->texture)->valid)
 		return;
 
-	/* if we need to resolve, but cannot with BLIT event, we instead need
-	 * to generate per-tile CP_BLIT (r2d) commands:
-	 *
-	 * The separate-stencil is a special case, we might need to use CP_BLIT
-	 * for depth, but we can still resolve stencil with a BLIT event
-	 */
-	if (needs_resolve(psurf) && !blit_can_resolve(psurf->format) &&
-			(buffer != FD_BUFFER_STENCIL)) {
-		fd6_resolve_tile(batch, ring, base, psurf);
-		return;
-	}
-
 	switch (buffer) {
 	case FD_BUFFER_COLOR:
 		break;
diff --git a/src/gallium/drivers/freedreno/a6xx/fd6_zsa.c b/src/gallium/drivers/freedreno/a6xx/fd6_zsa.c
index d262e2049a0..60347d0dc85 100644
--- a/src/gallium/drivers/freedreno/a6xx/fd6_zsa.c
+++ b/src/gallium/drivers/freedreno/a6xx/fd6_zsa.c
@@ -102,8 +102,6 @@ fd6_zsa_state_create(struct pipe_context *pctx,
 
 	so->base = *cso;
 
-	so->writes_zs = util_writes_depth_stencil(cso);
-
 	so->rb_depth_cntl |=
 		A6XX_RB_DEPTH_CNTL_ZFUNC(cso->depth_func); /* maps 1:1 */
 
diff --git a/src/gallium/drivers/freedreno/a6xx/fd6_zsa.h b/src/gallium/drivers/freedreno/a6xx/fd6_zsa.h
index 1ba5bc166b0..5cd636727b6 100644
--- a/src/gallium/drivers/freedreno/a6xx/fd6_zsa.h
+++ b/src/gallium/drivers/freedreno/a6xx/fd6_zsa.h
@@ -49,7 +49,6 @@ struct fd6_zsa_stateobj {
 	uint32_t rb_stencilwrmask;
 
 	struct fd6_lrz_state lrz;
-	bool writes_zs;     /* writes depth and/or stencil */
 	bool invalidate_lrz;
 	bool alpha_test;
 
diff --git a/src/gallium/drivers/iris/Android.mk b/src/gallium/drivers/iris/Android.mk
index 12a3afab7bd..507e68d0bf6 100644
--- a/src/gallium/drivers/iris/Android.mk
+++ b/src/gallium/drivers/iris/Android.mk
@@ -170,8 +170,7 @@ LOCAL_WHOLE_STATIC_LIBRARIES := \
 	libmesa_iris_gen8 \
 	libmesa_iris_gen9 \
 	libmesa_iris_gen11 \
-	libmesa_iris_gen12 \
-	libmesa_iris_gen125
+	libmesa_iris_gen12
 
 include $(GALLIUM_COMMON_MK)
 include $(BUILD_STATIC_LIBRARY)
diff --git a/src/gallium/drivers/iris/iris_state.c b/src/gallium/drivers/iris/iris_state.c
index f240cdac0bf..3cef43bd3bf 100644
--- a/src/gallium/drivers/iris/iris_state.c
+++ b/src/gallium/drivers/iris/iris_state.c
@@ -1669,8 +1669,6 @@ struct iris_rasterizer_state {
    bool multisample;
    bool force_persample_interp;
    bool conservative_rasterization;
-   bool fill_mode_point;
-   bool fill_mode_line;
    bool fill_mode_point_or_line;
    enum pipe_sprite_coord_mode sprite_coord_mode; /* PIPE_SPRITE_* */
    uint16_t sprite_coord_enable;
@@ -1734,15 +1732,11 @@ iris_create_rasterizer_state(struct pipe_context *ctx,
    cso->conservative_rasterization =
       state->conservative_raster_mode == PIPE_CONSERVATIVE_RASTER_POST_SNAP;
 
-   cso->fill_mode_point =
+   cso->fill_mode_point_or_line =
+      state->fill_front == PIPE_POLYGON_MODE_LINE ||
       state->fill_front == PIPE_POLYGON_MODE_POINT ||
+      state->fill_back == PIPE_POLYGON_MODE_LINE ||
       state->fill_back == PIPE_POLYGON_MODE_POINT;
-   cso->fill_mode_line =
-      state->fill_front == PIPE_POLYGON_MODE_LINE ||
-      state->fill_back == PIPE_POLYGON_MODE_LINE;
-   cso->fill_mode_point_or_line =
-      cso->fill_mode_point ||
-      cso->fill_mode_line;
 
    if (state->clip_plane_enable != 0)
       cso->num_clip_plane_consts = util_logbase2(state->clip_plane_enable) + 1;
@@ -4068,28 +4062,6 @@ iris_emit_sbe_swiz(struct iris_batch *batch,
    }
 }
 
-static bool
-iris_is_drawing_points(const struct iris_context *ice)
-{
-   const struct iris_rasterizer_state *cso_rast = ice->state.cso_rast;
-
-   if (cso_rast->fill_mode_point) {
-      return true;
-   }
-
-   if (ice->shaders.prog[MESA_SHADER_GEOMETRY]) {
-      const struct brw_gs_prog_data *gs_prog_data =
-         (void *) ice->shaders.prog[MESA_SHADER_GEOMETRY]->prog_data;
-      return gs_prog_data->output_topology == _3DPRIM_POINTLIST;
-   } else if (ice->shaders.prog[MESA_SHADER_TESS_EVAL]) {
-      const struct brw_tes_prog_data *tes_data =
-         (void *) ice->shaders.prog[MESA_SHADER_TESS_EVAL]->prog_data;
-      return tes_data->output_topology == BRW_TESS_OUTPUT_TOPOLOGY_POINT;
-   } else {
-      return ice->state.prim_mode == PIPE_PRIM_POINTS;
-   }
-}
-
 static unsigned
 iris_calculate_point_sprite_overrides(const struct brw_wm_prog_data *prog_data,
                                       const struct iris_rasterizer_state *cso)
@@ -4124,8 +4096,7 @@ iris_emit_sbe(struct iris_batch *batch, const struct iris_context *ice)
                                       &urb_read_offset, &urb_read_length);
 
    unsigned sprite_coord_overrides =
-      iris_is_drawing_points(ice) ?
-      iris_calculate_point_sprite_overrides(wm_prog_data, cso_rast) : 0;
+      iris_calculate_point_sprite_overrides(wm_prog_data, cso_rast);
 
    iris_emit_cmd(batch, GENX(3DSTATE_SBE), sbe) {
       sbe.AttributeSwizzleEnable = true;
diff --git a/src/gallium/drivers/lima/ir/gp/nir.c b/src/gallium/drivers/lima/ir/gp/nir.c
index afb43e51435..5143f2ba90b 100644
--- a/src/gallium/drivers/lima/ir/gp/nir.c
+++ b/src/gallium/drivers/lima/ir/gp/nir.c
@@ -487,12 +487,6 @@ bool gpir_compile_nir(struct lima_vs_shader_state *prog, struct nir_shader *nir,
    if (!gpir_codegen_prog(comp))
       goto err_out0;
 
-   /* initialize to support accumulating below */
-   nir_foreach_shader_out_variable(var, nir) {
-      struct lima_varying_info *v = prog->varying + var->data.driver_location;
-      v->components = 0;
-   }
-
    nir_foreach_shader_out_variable(var, nir) {
       bool varying = true;
       switch (var->data.location) {
diff --git a/src/gallium/drivers/lima/lima_context.c b/src/gallium/drivers/lima/lima_context.c
index e14f5ae0fb1..9538f9a24e7 100644
--- a/src/gallium/drivers/lima/lima_context.c
+++ b/src/gallium/drivers/lima/lima_context.c
@@ -143,7 +143,6 @@ lima_context_destroy(struct pipe_context *pctx)
    for (int i = 0; i < lima_ctx_buff_num; i++)
       pipe_resource_reference(&ctx->buffer_state[i].res, NULL);
 
-   lima_program_fini(ctx);
    lima_state_fini(ctx);
 
    if (ctx->blitter)
diff --git a/src/gallium/drivers/lima/lima_context.h b/src/gallium/drivers/lima/lima_context.h
index 400fa0e979c..8ed9db9fadc 100644
--- a/src/gallium/drivers/lima/lima_context.h
+++ b/src/gallium/drivers/lima/lima_context.h
@@ -44,22 +44,15 @@ struct lima_depth_stencil_alpha_state {
 };
 
 struct lima_fs_shader_state {
+   struct pipe_shader_state base;
    void *shader;
    int shader_size;
    int stack_size;
+   uint8_t swizzles[PIPE_MAX_SAMPLERS][4];
    bool uses_discard;
    struct lima_bo *bo;
 };
 
-struct lima_fs_bind_state {
-   struct pipe_shader_state base;
-};
-
-struct lima_fs_key {
-   struct lima_fs_bind_state *shader_state;
-   uint8_t swizzles[PIPE_MAX_SAMPLERS][4];
-};
-
 #define LIMA_MAX_VARYING_NUM 13
 
 struct lima_varying_info {
@@ -87,14 +80,6 @@ struct lima_vs_shader_state {
    struct lima_bo *bo;
 };
 
-struct lima_vs_bind_state {
-   struct pipe_shader_state base;
-};
-
-struct lima_vs_key {
-   struct lima_vs_bind_state *shader_state;
-};
-
 struct lima_rasterizer_state {
    struct pipe_rasterizer_state base;
 };
@@ -179,8 +164,8 @@ struct lima_context {
    enum {
       LIMA_CONTEXT_DIRTY_FRAMEBUFFER  = (1 << 0),
       LIMA_CONTEXT_DIRTY_CLEAR        = (1 << 1),
-      LIMA_CONTEXT_DIRTY_COMPILED_VS  = (1 << 2),
-      LIMA_CONTEXT_DIRTY_COMPILED_FS  = (1 << 3),
+      LIMA_CONTEXT_DIRTY_SHADER_VERT  = (1 << 2),
+      LIMA_CONTEXT_DIRTY_SHADER_FRAG  = (1 << 3),
       LIMA_CONTEXT_DIRTY_VERTEX_ELEM  = (1 << 4),
       LIMA_CONTEXT_DIRTY_VERTEX_BUFF  = (1 << 5),
       LIMA_CONTEXT_DIRTY_VIEWPORT     = (1 << 6),
@@ -193,8 +178,6 @@ struct lima_context {
       LIMA_CONTEXT_DIRTY_CONST_BUFF   = (1 << 13),
       LIMA_CONTEXT_DIRTY_TEXTURES     = (1 << 14),
       LIMA_CONTEXT_DIRTY_CLIP         = (1 << 15),
-      LIMA_CONTEXT_DIRTY_UNCOMPILED_VS = (1 << 16),
-      LIMA_CONTEXT_DIRTY_UNCOMPILED_FS = (1 << 17),
    } dirty;
 
    struct u_upload_mgr *uploader;
@@ -208,8 +191,6 @@ struct lima_context {
    struct pipe_scissor_state clipped_scissor;
    struct lima_vs_shader_state *vs;
    struct lima_fs_shader_state *fs;
-   struct lima_vs_bind_state *bind_vs;
-   struct lima_fs_bind_state *bind_fs;
    struct lima_vertex_element_state *vertex_elements;
    struct lima_context_vertex_buffer vertex_buffers;
    struct lima_rasterizer_state *rasterizer;
@@ -245,9 +226,6 @@ struct lima_context {
    uint32_t plb_index;
    size_t plb_stream_cache_size;
 
-   struct hash_table *fs_cache;
-   struct hash_table *vs_cache;
-
    struct lima_ctx_buff_state buffer_state[lima_ctx_buff_num];
 
    /* current job */
@@ -306,7 +284,6 @@ void lima_state_init(struct lima_context *ctx);
 void lima_state_fini(struct lima_context *ctx);
 void lima_draw_init(struct lima_context *ctx);
 void lima_program_init(struct lima_context *ctx);
-void lima_program_fini(struct lima_context *ctx);
 void lima_query_init(struct lima_context *ctx);
 
 struct pipe_context *
diff --git a/src/gallium/drivers/lima/lima_draw.c b/src/gallium/drivers/lima/lima_draw.c
index 69ef3e3b1b8..a88b1732883 100644
--- a/src/gallium/drivers/lima/lima_draw.c
+++ b/src/gallium/drivers/lima/lima_draw.c
@@ -1027,7 +1027,7 @@ lima_draw_vbo_update(struct pipe_context *pctx,
    if ((ctx->dirty & LIMA_CONTEXT_DIRTY_CONST_BUFF &&
         ctx->const_buffer[PIPE_SHADER_VERTEX].dirty) ||
        ctx->dirty & LIMA_CONTEXT_DIRTY_VIEWPORT ||
-       ctx->dirty & LIMA_CONTEXT_DIRTY_COMPILED_VS) {
+       ctx->dirty & LIMA_CONTEXT_DIRTY_SHADER_VERT) {
       lima_update_gp_uniform(ctx);
       ctx->const_buffer[PIPE_SHADER_VERTEX].dirty = false;
    }
@@ -1153,7 +1153,7 @@ lima_draw_vbo(struct pipe_context *pctx,
 
    struct lima_context *ctx = lima_context(pctx);
 
-   if (!ctx->bind_fs || !ctx->bind_vs) {
+   if (!ctx->vs || !ctx->fs) {
       debug_warn_once("no shader, skip draw\n");
       return;
    }
@@ -1162,11 +1162,10 @@ lima_draw_vbo(struct pipe_context *pctx,
    if (lima_is_scissor_zero(ctx))
       return;
 
-   if (!lima_update_fs_state(ctx) || !lima_update_vs_state(ctx))
+   if (!lima_update_vs_state(ctx) || !lima_update_fs_state(ctx))
       return;
 
    struct lima_job *job = lima_job_get(ctx);
-   job->pp_max_stack_size = MAX2(job->pp_max_stack_size, ctx->fs->stack_size);
 
    lima_dump_command_stream_print(
       job->dump, ctx->vs->bo->map, ctx->vs->shader_size, false,
diff --git a/src/gallium/drivers/lima/lima_program.c b/src/gallium/drivers/lima/lima_program.c
index 5a04274a9cf..78b6fadc905 100644
--- a/src/gallium/drivers/lima/lima_program.c
+++ b/src/gallium/drivers/lima/lima_program.c
@@ -272,29 +272,13 @@ lima_program_optimize_fs_nir(struct nir_shader *s,
 
 static bool
 lima_fs_compile_shader(struct lima_context *ctx,
-                       struct lima_fs_key *key,
-                       struct lima_fs_shader_state *fs)
+                       struct lima_fs_shader_state *fs,
+                       struct nir_lower_tex_options *tex_options)
 {
    struct lima_screen *screen = lima_screen(ctx->base.screen);
-   nir_shader *nir = nir_shader_clone(fs, key->shader_state->base.ir.nir);
+   nir_shader *nir = nir_shader_clone(fs, fs->base.ir.nir);
 
-   struct nir_lower_tex_options tex_options = {
-      .lower_txp = ~0u,
-      .swizzle_result = 0,
-   };
-
-   uint8_t identity[4] = { PIPE_SWIZZLE_X, PIPE_SWIZZLE_Y,
-                           PIPE_SWIZZLE_Z, PIPE_SWIZZLE_W };
-
-   for (int i = 0; i < PIPE_MAX_SAMPLERS; i++) {
-      for (int j = 0; j < 4; j++)
-         tex_options.swizzles[i][j] = key->swizzles[i][j];
-
-      if (memcmp(tex_options.swizzles[i], identity, 4) != 0)
-         tex_options.swizzle_result |= (1 << i);
-   }
-
-   lima_program_optimize_fs_nir(nir, &tex_options);
+   lima_program_optimize_fs_nir(nir, tex_options);
 
    if (lima_debug & LIMA_DEBUG_PP)
       nir_print_shader(nir, stdout);
@@ -315,7 +299,7 @@ lima_create_fs_state(struct pipe_context *pctx,
                      const struct pipe_shader_state *cso)
 {
    struct lima_context *ctx = lima_context(pctx);
-   struct lima_fs_bind_state *so = rzalloc(NULL, struct lima_fs_bind_state);
+   struct lima_fs_shader_state *so = rzalloc(NULL, struct lima_fs_shader_state);
 
    if (!so)
       return NULL;
@@ -323,7 +307,8 @@ lima_create_fs_state(struct pipe_context *pctx,
    nir_shader *nir;
    if (cso->type == PIPE_SHADER_IR_NIR)
       /* The backend takes ownership of the NIR shader on state
-       * creation. */
+       * creation.
+       */
       nir = cso->ir.nir;
    else {
       assert(cso->type == PIPE_SHADER_IR_TGSI);
@@ -334,10 +319,24 @@ lima_create_fs_state(struct pipe_context *pctx,
    so->base.type = PIPE_SHADER_IR_NIR;
    so->base.ir.nir = nir;
 
-   /* Trigger initial compilation with default settings */
-   ctx->bind_fs = so;
-   ctx->dirty |= LIMA_CONTEXT_DIRTY_UNCOMPILED_FS;
-   lima_update_fs_state(ctx);
+   uint8_t identity[4] = { PIPE_SWIZZLE_X,
+                           PIPE_SWIZZLE_Y,
+                           PIPE_SWIZZLE_Z,
+                           PIPE_SWIZZLE_W };
+
+   struct nir_lower_tex_options tex_options = {
+      .lower_txp = ~0u,
+      .swizzle_result = 0,
+   };
+
+   /* Initialize with identity swizzles. That should suffice for most shaders  */
+   for (int i = 0; i < PIPE_MAX_SAMPLERS; i++)
+      memcpy(so->swizzles[i], identity, 4);
+
+   if (!lima_fs_compile_shader(ctx, so, &tex_options)) {
+      ralloc_free(so);
+      return NULL;
+   }
 
    return so;
 }
@@ -347,106 +346,26 @@ lima_bind_fs_state(struct pipe_context *pctx, void *hwcso)
 {
    struct lima_context *ctx = lima_context(pctx);
 
-   ctx->bind_fs = hwcso;
-   ctx->dirty |= LIMA_CONTEXT_DIRTY_UNCOMPILED_FS;
+   ctx->fs = hwcso;
+   ctx->dirty |= LIMA_CONTEXT_DIRTY_SHADER_FRAG;
 }
 
 static void
 lima_delete_fs_state(struct pipe_context *pctx, void *hwcso)
 {
-   struct lima_context *ctx = lima_context(pctx);
-   struct lima_fs_bind_state *so = hwcso;
-
-   hash_table_foreach(ctx->fs_cache, entry) {
-      const struct lima_fs_key *key = entry->key;
-      if (key->shader_state == so) {
-         struct lima_fs_shader_state *fs = entry->data;
-         _mesa_hash_table_remove(ctx->fs_cache, entry);
-         if (fs->bo)
-            lima_bo_unreference(fs->bo);
+   struct lima_fs_shader_state *so = hwcso;
 
-         if (fs == ctx->fs)
-            ctx->fs = NULL;
-
-         ralloc_free(fs);
-      }
-   }
+   if (so->bo)
+      lima_bo_unreference(so->bo);
 
    ralloc_free(so->base.ir.nir);
    ralloc_free(so);
 }
 
-static bool
-lima_vs_compile_shader(struct lima_context *ctx,
-                       struct lima_vs_key *key,
-                       struct lima_vs_shader_state *vs)
-{
-   nir_shader *nir = nir_shader_clone(vs, key->shader_state->base.ir.nir);
-
-   lima_program_optimize_vs_nir(nir);
-
-   if (lima_debug & LIMA_DEBUG_GP)
-      nir_print_shader(nir, stdout);
-
-   if (!gpir_compile_nir(vs, nir, &ctx->debug)) {
-      ralloc_free(nir);
-      return false;
-   }
-
-   ralloc_free(nir);
-   return true;
-}
-
-static struct lima_vs_shader_state *
-lima_get_compiled_vs(struct lima_context *ctx,
-                     struct lima_vs_key *key)
-{
-   struct hash_table *ht;
-   uint32_t key_size;
-
-   ht = ctx->vs_cache;
-   key_size = sizeof(struct lima_vs_key);
-
-   struct hash_entry *entry = _mesa_hash_table_search(ht, key);
-   if (entry)
-      return entry->data;
-
-   /* not on cache, compile and insert into the cache */
-   struct lima_vs_shader_state *vs = rzalloc(NULL, struct lima_vs_shader_state);
-   if (!vs)
-      return NULL;
-
-   if (!lima_vs_compile_shader(ctx, key, vs))
-      return NULL;
-
-   struct lima_key *dup_key;
-   dup_key = rzalloc_size(vs, key_size);
-   memcpy(dup_key, key, key_size);
-   _mesa_hash_table_insert(ht, dup_key, vs);
-
-   return vs;
-}
-
 bool
 lima_update_vs_state(struct lima_context *ctx)
 {
-   if (!(ctx->dirty & LIMA_CONTEXT_DIRTY_UNCOMPILED_VS)) {
-      return true;
-   }
-
-   struct lima_vs_key local_key;
-   struct lima_vs_key *key = &local_key;
-   memset(key, 0, sizeof(*key));
-   key->shader_state = ctx->bind_vs;
-
-   struct lima_vs_shader_state *old_vs = ctx->vs;
-
-   struct lima_vs_shader_state *vs = lima_get_compiled_vs(ctx, key);
-   if (!vs)
-      return false;
-
-   ctx->vs = vs;
-
+   struct lima_vs_shader_state *vs = ctx->vs;
    if (!vs->bo) {
       struct lima_screen *screen = lima_screen(ctx->base.screen);
       vs->bo = lima_bo_create(screen, vs->shader_size, 0);
@@ -460,80 +379,60 @@ lima_update_vs_state(struct lima_context *ctx)
       vs->shader = NULL;
    }
 
-   if (ctx->vs != old_vs)
-      ctx->dirty |= LIMA_CONTEXT_DIRTY_COMPILED_VS;
-
    return true;
 }
 
-static struct lima_fs_shader_state *
-lima_get_compiled_fs(struct lima_context *ctx,
-                     struct lima_fs_key *key)
-{
-   struct hash_table *ht;
-   uint32_t key_size;
-
-   ht = ctx->fs_cache;
-   key_size = sizeof(struct lima_fs_key);
-
-   struct hash_entry *entry = _mesa_hash_table_search(ht, key);
-   if (entry)
-      return entry->data;
-
-   /* not on cache, compile and insert into the cache */
-   struct lima_fs_shader_state *fs = rzalloc(NULL, struct lima_fs_shader_state);
-   if (!fs)
-      return NULL;
-
-   if (!lima_fs_compile_shader(ctx, key, fs))
-      return NULL;
-
-   struct lima_key *dup_key;
-   dup_key = rzalloc_size(fs, key_size);
-   memcpy(dup_key, key, key_size);
-   _mesa_hash_table_insert(ht, dup_key, fs);
-
-   return fs;
-}
-
 bool
 lima_update_fs_state(struct lima_context *ctx)
 {
-   if (!(ctx->dirty & (LIMA_CONTEXT_DIRTY_UNCOMPILED_FS |
-                       LIMA_CONTEXT_DIRTY_TEXTURES))) {
-      return true;
-   }
-
+   struct lima_fs_shader_state *fs = ctx->fs;
    struct lima_texture_stateobj *lima_tex = &ctx->tex_stateobj;
-   struct lima_fs_key local_key;
-   struct lima_fs_key *key = &local_key;
-   memset(key, 0, sizeof(*key));
-   key->shader_state = ctx->bind_fs;
+   struct nir_lower_tex_options tex_options = {
+      .lower_txp = ~0u,
+      .swizzle_result = 0,
+   };
+   bool needs_recompile = false;
 
+   /* Check if texture formats has changed since last compilation.
+    * If it has we need to recompile shader.
+    */
    if (((ctx->dirty & LIMA_CONTEXT_DIRTY_TEXTURES) &&
        lima_tex->num_samplers &&
        lima_tex->num_textures)) {
+      uint8_t identity[4] = { PIPE_SWIZZLE_X,
+                              PIPE_SWIZZLE_Y,
+                              PIPE_SWIZZLE_Z,
+                              PIPE_SWIZZLE_W };
       for (int i = 0; i < lima_tex->num_samplers; i++) {
          struct lima_sampler_view *texture = lima_sampler_view(lima_tex->textures[i]);
          struct pipe_resource *prsc = texture->base.texture;
          const uint8_t *swizzle = lima_format_get_texel_swizzle(prsc->format);
-         memcpy(key->swizzles[i], swizzle, 4);
-      }
-   }
+         if (memcmp(fs->swizzles[i], swizzle, 4)) {
+            needs_recompile = true;
+            memcpy(fs->swizzles[i], swizzle, 4);
+         }
 
-   /* Fill rest with identity swizzle */
-   uint8_t identity[4] = { PIPE_SWIZZLE_X, PIPE_SWIZZLE_Y,
-                           PIPE_SWIZZLE_Z, PIPE_SWIZZLE_W };
-   for (int i = lima_tex->num_samplers; i < PIPE_MAX_SAMPLERS; i++)
-      memcpy(key->swizzles[i], identity, 4);
+         for (int j = 0; j < 4; j++)
+            tex_options.swizzles[i][j] = swizzle[j];
 
-   struct lima_fs_shader_state *old_fs = ctx->fs;
+         if (memcmp(swizzle, identity, 4))
+            tex_options.swizzle_result |= (1 << i);
+      }
 
-   struct lima_fs_shader_state *fs = lima_get_compiled_fs(ctx, key);
-   if (!fs)
-      return false;
+      /* Fill rest with identity swizzle */
+      for (int i = lima_tex->num_samplers; i < PIPE_MAX_SAMPLERS; i++)
+         memcpy(fs->swizzles[i], identity, 4);
+   }
+
+   if (needs_recompile) {
+      if (fs->bo) {
+         lima_bo_unreference(fs->bo);
+         fs->bo = NULL;
+      }
 
-   ctx->fs = fs;
+      if (!lima_fs_compile_shader(ctx, fs, &tex_options))
+         return false;
+   }
 
    if (!fs->bo) {
       struct lima_screen *screen = lima_screen(ctx->base.screen);
@@ -548,8 +447,8 @@ lima_update_fs_state(struct lima_context *ctx)
       fs->shader = NULL;
    }
 
-   if (ctx->fs != old_fs)
-      ctx->dirty |= LIMA_CONTEXT_DIRTY_COMPILED_FS;
+   struct lima_job *job = lima_job_get(ctx);
+   job->pp_max_stack_size = MAX2(job->pp_max_stack_size, ctx->fs->stack_size);
 
    return true;
 }
@@ -559,15 +458,13 @@ lima_create_vs_state(struct pipe_context *pctx,
                      const struct pipe_shader_state *cso)
 {
    struct lima_context *ctx = lima_context(pctx);
-   struct lima_vs_bind_state *so = rzalloc(NULL, struct lima_vs_bind_state);
+   struct lima_vs_shader_state *so = rzalloc(NULL, struct lima_vs_shader_state);
 
    if (!so)
       return NULL;
 
    nir_shader *nir;
    if (cso->type == PIPE_SHADER_IR_NIR)
-      /* The backend takes ownership of the NIR shader on state
-       * creation. */
       nir = cso->ir.nir;
    else {
       assert(cso->type == PIPE_SHADER_IR_TGSI);
@@ -575,13 +472,17 @@ lima_create_vs_state(struct pipe_context *pctx,
       nir = tgsi_to_nir(cso->tokens, pctx->screen, false);
    }
 
-   so->base.type = PIPE_SHADER_IR_NIR;
-   so->base.ir.nir = nir;
+   lima_program_optimize_vs_nir(nir);
+
+   if (lima_debug & LIMA_DEBUG_GP)
+      nir_print_shader(nir, stdout);
+
+   if (!gpir_compile_nir(so, nir, &ctx->debug)) {
+      ralloc_free(so);
+      return NULL;
+   }
 
-   /* Trigger initial compilation with default settings */
-   ctx->bind_vs = so;
-   ctx->dirty |= LIMA_CONTEXT_DIRTY_UNCOMPILED_VS;
-   lima_update_vs_state(ctx);
+   ralloc_free(nir);
 
    return so;
 }
@@ -591,59 +492,21 @@ lima_bind_vs_state(struct pipe_context *pctx, void *hwcso)
 {
    struct lima_context *ctx = lima_context(pctx);
 
-   ctx->bind_vs = hwcso;
-   ctx->dirty |= LIMA_CONTEXT_DIRTY_UNCOMPILED_VS;
+   ctx->vs = hwcso;
+   ctx->dirty |= LIMA_CONTEXT_DIRTY_SHADER_VERT;
 }
 
 static void
 lima_delete_vs_state(struct pipe_context *pctx, void *hwcso)
 {
-   struct lima_context *ctx = lima_context(pctx);
-   struct lima_vs_bind_state *so = hwcso;
+   struct lima_vs_shader_state *so = hwcso;
 
-   hash_table_foreach(ctx->vs_cache, entry) {
-      const struct lima_vs_key *key = entry->key;
-      if (key->shader_state == so) {
-         struct lima_vs_shader_state *vs = entry->data;
-         _mesa_hash_table_remove(ctx->vs_cache, entry);
-         if (vs->bo)
-            lima_bo_unreference(vs->bo);
-
-         if (vs == ctx->vs)
-            ctx->vs = NULL;
-
-         ralloc_free(vs);
-      }
-   }
+   if (so->bo)
+      lima_bo_unreference(so->bo);
 
-   ralloc_free(so->base.ir.nir);
    ralloc_free(so);
 }
 
-static uint32_t
-lima_fs_cache_hash(const void *key)
-{
-   return _mesa_hash_data(key, sizeof(struct lima_fs_key));
-}
-
-static uint32_t
-lima_vs_cache_hash(const void *key)
-{
-   return _mesa_hash_data(key, sizeof(struct lima_vs_key));
-}
-
-static bool
-lima_fs_cache_compare(const void *key1, const void *key2)
-{
-   return memcmp(key1, key2, sizeof(struct lima_fs_key)) == 0;
-}
-
-static bool
-lima_vs_cache_compare(const void *key1, const void *key2)
-{
-   return memcmp(key1, key2, sizeof(struct lima_vs_key)) == 0;
-}
-
 void
 lima_program_init(struct lima_context *ctx)
 {
@@ -654,29 +517,4 @@ lima_program_init(struct lima_context *ctx)
    ctx->base.create_vs_state = lima_create_vs_state;
    ctx->base.bind_vs_state = lima_bind_vs_state;
    ctx->base.delete_vs_state = lima_delete_vs_state;
-
-   ctx->fs_cache = _mesa_hash_table_create(ctx, lima_fs_cache_hash,
-                                           lima_fs_cache_compare);
-   ctx->vs_cache = _mesa_hash_table_create(ctx, lima_vs_cache_hash,
-                                           lima_vs_cache_compare);
-}
-
-void
-lima_program_fini(struct lima_context *ctx)
-{
-   hash_table_foreach(ctx->vs_cache, entry) {
-      struct lima_vs_shader_state *vs = entry->data;
-      if (vs->bo)
-         lima_bo_unreference(vs->bo);
-      ralloc_free(vs);
-      _mesa_hash_table_remove(ctx->vs_cache, entry);
-   }
-
-   hash_table_foreach(ctx->fs_cache, entry) {
-      struct lima_fs_shader_state *fs = entry->data;
-      if (fs->bo)
-         lima_bo_unreference(fs->bo);
-      ralloc_free(fs);
-      _mesa_hash_table_remove(ctx->fs_cache, entry);
-   }
 }
diff --git a/src/gallium/drivers/lima/lima_resource.c b/src/gallium/drivers/lima/lima_resource.c
index 0a122ced287..3c417f7e018 100644
--- a/src/gallium/drivers/lima/lima_resource.c
+++ b/src/gallium/drivers/lima/lima_resource.c
@@ -706,8 +706,8 @@ lima_util_blitter_save_states(struct lima_context *ctx)
    util_blitter_save_depth_stencil_alpha(ctx->blitter, (void *)ctx->zsa);
    util_blitter_save_stencil_ref(ctx->blitter, &ctx->stencil_ref);
    util_blitter_save_rasterizer(ctx->blitter, (void *)ctx->rasterizer);
-   util_blitter_save_fragment_shader(ctx->blitter, ctx->bind_fs);
-   util_blitter_save_vertex_shader(ctx->blitter, ctx->bind_vs);
+   util_blitter_save_fragment_shader(ctx->blitter, ctx->fs);
+   util_blitter_save_vertex_shader(ctx->blitter, ctx->vs);
    util_blitter_save_viewport(ctx->blitter,
                               &ctx->viewport.transform);
    util_blitter_save_scissor(ctx->blitter, &ctx->scissor);
diff --git a/src/gallium/drivers/nouveau/codegen/lib/gk104.asm b/src/gallium/drivers/nouveau/codegen/lib/gk104.asm
index 21a6b4de662..576da1bab60 100644
--- a/src/gallium/drivers/nouveau/codegen/lib/gk104.asm
+++ b/src/gallium/drivers/nouveau/codegen/lib/gk104.asm
@@ -563,7 +563,7 @@ gk104_rcp_f64:
    add b32 $r3 $r2 0xffffffff
    joinat #rcp_rejoin
    // We want to check whether the exponent is 0 or 0x7ff (i.e. NaN, inf,
-   // denorm, or 0). Do this by subtracting 1 from the exponent, which will
+   // denorm, or 0). Do this by substracting 1 from the exponent, which will
    // mean that it's > 0x7fd in those cases when doing unsigned comparison
    set $p0 0x1 gt u32 $r3 0x7fd
    // $r3: 0 for norms, 0x36 for denorms, -1 for others
@@ -770,7 +770,7 @@ rsq_norm:
 //
 // Trap handler.
 // Requires at least 4 GPRs and 32 bytes of l[] memory to temporarily save GPRs.
-// Low 32 bytes of l[] memory shouldn't be used if resumability is required.
+// Low 32 bytes of l[] memory shouldn't be used if resumeability is required.
 //
 // Trap info:
 // 0x000: mutex
diff --git a/src/gallium/drivers/nouveau/codegen/lib/gk110.asm b/src/gallium/drivers/nouveau/codegen/lib/gk110.asm
index 66626b471b3..4047a565a9f 100644
--- a/src/gallium/drivers/nouveau/codegen/lib/gk110.asm
+++ b/src/gallium/drivers/nouveau/codegen/lib/gk110.asm
@@ -105,7 +105,7 @@ gk110_rcp_f64:
    add b32 $r3 $r2 0xffffffff
    joinat #rcp_rejoin
    // We want to check whether the exponent is 0 or 0x7ff (i.e. NaN, inf,
-   // denorm, or 0). Do this by subtracting 1 from the exponent, which will
+   // denorm, or 0). Do this by substracting 1 from the exponent, which will
    // mean that it's > 0x7fd in those cases when doing unsigned comparison
    set b32 $p0 0x1 gt u32 $r3 0x7fd
    // $r3: 0 for norms, 0x36 for denorms, -1 for others
diff --git a/src/gallium/drivers/nouveau/codegen/lib/gm107.asm b/src/gallium/drivers/nouveau/codegen/lib/gm107.asm
index b0f670e0d4d..faee0218d18 100644
--- a/src/gallium/drivers/nouveau/codegen/lib/gm107.asm
+++ b/src/gallium/drivers/nouveau/codegen/lib/gm107.asm
@@ -119,7 +119,7 @@ gm107_rcp_f64:
    iadd32i $r3 $r2 -1
    ssy #rcp_rejoin
    // We want to check whether the exponent is 0 or 0x7ff (i.e. NaN, inf,
-   // denorm, or 0). Do this by subtracting 1 from the exponent, which will
+   // denorm, or 0). Do this by substracting 1 from the exponent, which will
    // mean that it's > 0x7fd in those cases when doing unsigned comparison
    sched (st 0x0) (st 0x0) (st 0x0)
    isetp gt u32 and $p0 1 $r3 0x7fd 1
diff --git a/src/gallium/drivers/nouveau/codegen/nv50_ir.h b/src/gallium/drivers/nouveau/codegen/nv50_ir.h
index 645f81a23c2..c523dccde75 100644
--- a/src/gallium/drivers/nouveau/codegen/nv50_ir.h
+++ b/src/gallium/drivers/nouveau/codegen/nv50_ir.h
@@ -121,7 +121,7 @@ enum operation
    OP_FINAL, // finish emitting primitives
    OP_TEX,
    OP_TXB, // texture bias
-   OP_TXL, // texture lod
+   OP_TXL, // texure lod
    OP_TXF, // texel fetch
    OP_TXQ, // texture size query
    OP_TXD, // texture derivatives
diff --git a/src/gallium/drivers/nouveau/codegen/nv50_ir_build_util.h b/src/gallium/drivers/nouveau/codegen/nv50_ir_build_util.h
index c2bc0d686cf..d171f64d9a1 100644
--- a/src/gallium/drivers/nouveau/codegen/nv50_ir_build_util.h
+++ b/src/gallium/drivers/nouveau/codegen/nv50_ir_build_util.h
@@ -137,9 +137,7 @@ public:
    class DataArray
    {
    public:
-      DataArray(BuildUtil *bld) : up(bld), array(0), arrayIdx(0), baseAddr(0),
-         arrayLen(0), baseSym(NULL), vecDim(0), eltSize(0), file(FILE_NULL),
-         regOnly(false) { }
+      DataArray(BuildUtil *bld) : up(bld) { }
 
       void setup(unsigned array, unsigned arrayIdx,
                  uint32_t base, int len, int vecDim, int eltSize,
diff --git a/src/gallium/drivers/nouveau/codegen/nv50_ir_from_tgsi.cpp b/src/gallium/drivers/nouveau/codegen/nv50_ir_from_tgsi.cpp
index d4881af6281..eb4ef7e4f3d 100644
--- a/src/gallium/drivers/nouveau/codegen/nv50_ir_from_tgsi.cpp
+++ b/src/gallium/drivers/nouveau/codegen/nv50_ir_from_tgsi.cpp
@@ -1180,7 +1180,6 @@ void Source::scanProperty(const struct tgsi_full_property *prop)
    case TGSI_PROPERTY_FS_COORD_PIXEL_CENTER:
    case TGSI_PROPERTY_FS_DEPTH_LAYOUT:
    case TGSI_PROPERTY_GS_INPUT_PRIM:
-   case TGSI_PROPERTY_FS_BLEND_EQUATION_ADVANCED:
       // we don't care
       break;
    case TGSI_PROPERTY_VS_PROHIBIT_UCPS:
diff --git a/src/gallium/drivers/nouveau/codegen/nv50_ir_lowering_nvc0.cpp b/src/gallium/drivers/nouveau/codegen/nv50_ir_lowering_nvc0.cpp
index adc93a05244..0e69f25ca28 100644
--- a/src/gallium/drivers/nouveau/codegen/nv50_ir_lowering_nvc0.cpp
+++ b/src/gallium/drivers/nouveau/codegen/nv50_ir_lowering_nvc0.cpp
@@ -1184,7 +1184,7 @@ bool
 NVC0LoweringPass::handleManualTXD(TexInstruction *i)
 {
    // Always done from the l0 perspective. This is the way that NVIDIA's
-   // driver does it, and doing it from the "current" lane's perspective
+   // driver does it, and doing it from the "current" lane's perpsective
    // doesn't seem to always work for reasons that aren't altogether clear,
    // even in frag shaders.
    //
diff --git a/src/gallium/drivers/nouveau/codegen/nv50_ir_peephole.cpp b/src/gallium/drivers/nouveau/codegen/nv50_ir_peephole.cpp
index f5514c6f5fd..2d32067729d 100644
--- a/src/gallium/drivers/nouveau/codegen/nv50_ir_peephole.cpp
+++ b/src/gallium/drivers/nouveau/codegen/nv50_ir_peephole.cpp
@@ -1108,7 +1108,6 @@ ConstantFolding::opnd(Instruction *i, ImmediateValue &imm0, int s)
          }
       } else
       if (imm0.isInteger(0)) {
-         i->dnz = 0;
          i->op = OP_MOV;
          i->setSrc(0, new_ImmediateValue(prog, 0u));
          i->src(0).mod = Modifier(0);
@@ -1118,7 +1117,6 @@ ConstantFolding::opnd(Instruction *i, ImmediateValue &imm0, int s)
       if (!i->postFactor && (imm0.isInteger(1) || imm0.isInteger(-1))) {
          if (imm0.isNegative())
             i->src(t).mod = i->src(t).mod ^ Modifier(NV50_IR_MOD_NEG);
-         i->dnz = 0;
          i->op = i->src(t).mod.getOp();
          if (s == 0) {
             i->setSrc(0, i->getSrc(1));
@@ -1159,7 +1157,6 @@ ConstantFolding::opnd(Instruction *i, ImmediateValue &imm0, int s)
          i->src(0).mod = i->src(2).mod;
          i->setSrc(1, NULL);
          i->setSrc(2, NULL);
-         i->dnz = 0;
          i->op = i->src(0).mod.getOp();
          if (i->op != OP_CVT)
             i->src(0).mod = 0;
diff --git a/src/gallium/drivers/nouveau/codegen/nv50_ir_ra.cpp b/src/gallium/drivers/nouveau/codegen/nv50_ir_ra.cpp
index 228ae3d9ba1..8977c5f65b3 100644
--- a/src/gallium/drivers/nouveau/codegen/nv50_ir_ra.cpp
+++ b/src/gallium/drivers/nouveau/codegen/nv50_ir_ra.cpp
@@ -251,7 +251,6 @@ private:
 
    class InsertConstraintsPass : public Pass {
    public:
-      InsertConstraintsPass() : targ(NULL) { }
       bool exec(Function *func);
    private:
       virtual bool visit(BasicBlock *);
diff --git a/src/gallium/drivers/nouveau/codegen/nv50_ir_target.cpp b/src/gallium/drivers/nouveau/codegen/nv50_ir_target.cpp
index 5d03f8f6055..b827819075a 100644
--- a/src/gallium/drivers/nouveau/codegen/nv50_ir_target.cpp
+++ b/src/gallium/drivers/nouveau/codegen/nv50_ir_target.cpp
@@ -222,7 +222,7 @@ CodeEmitter::prepareEmission(Program *prog)
       func->binPos = prog->binSize;
       prepareEmission(func);
 
-      // adjust sizes & positions for scheduling info:
+      // adjust sizes & positions for schedulding info:
       if (prog->getTarget()->hasSWSched) {
          uint32_t adjPos = func->binPos;
          BasicBlock *bb = NULL;
@@ -390,7 +390,7 @@ Program::emitBinary(struct nv50_ir_prog_info_out *info)
    info->bin.relocData = emit->getRelocInfo();
    info->bin.fixupData = emit->getFixupInfo();
 
-   // the nvc0 driver will print the binary itself together with the header
+   // the nvc0 driver will print the binary iself together with the header
    if ((dbgFlags & NV50_IR_DEBUG_BASIC) && getTarget()->getChipset() < 0xc0)
       emit->printBinary();
 
diff --git a/src/gallium/drivers/nouveau/nv30/nv30_state_validate.c b/src/gallium/drivers/nouveau/nv30/nv30_state_validate.c
index 26c77595c2a..1276561895f 100644
--- a/src/gallium/drivers/nouveau/nv30/nv30_state_validate.c
+++ b/src/gallium/drivers/nouveau/nv30/nv30_state_validate.c
@@ -80,7 +80,7 @@ nv30_validate_fb(struct nv30_context *nv30)
 
    /* hardware rounds down render target offset to 64 bytes, but surfaces
     * with a size of 2x2 pixel (16bpp) or 1x1 pixel (32bpp) have an
-    * unaligned start address.  For these two important square formats
+    * unaligned start aaddress.  For these two important square formats
     * we can hack around this limitation by adjusting the viewport origin
     */
    if (nv30->state.rt_enable) {
diff --git a/src/gallium/drivers/nouveau/nv50/nv50_screen.c b/src/gallium/drivers/nouveau/nv50/nv50_screen.c
index 6be9dde3039..a54cb652b8d 100644
--- a/src/gallium/drivers/nouveau/nv50/nv50_screen.c
+++ b/src/gallium/drivers/nouveau/nv50/nv50_screen.c
@@ -164,8 +164,6 @@ nv50_screen_get_param(struct pipe_screen *pscreen, enum pipe_cap param)
       return 16;
    case PIPE_CAP_GL_BEGIN_END_BUFFER_SIZE:
       return 512 * 1024; /* TODO: Investigate tuning this */
-   case PIPE_CAP_MAX_TEXTURE_MB:
-      return 0; /* TODO: use 1/2 of VRAM for this? */
 
    /* supported caps */
    case PIPE_CAP_TEXTURE_MIRROR_CLAMP:
@@ -359,16 +357,6 @@ nv50_screen_get_param(struct pipe_screen *pscreen, enum pipe_cap param)
    case PIPE_CAP_PSIZ_CLAMPED:
    case PIPE_CAP_VIEWPORT_SWIZZLE:
    case PIPE_CAP_VIEWPORT_MASK:
-   case PIPE_CAP_TEXTURE_BUFFER_SAMPLER:
-   case PIPE_CAP_PREFER_REAL_BUFFER_IN_CONSTBUF0:
-   case PIPE_CAP_MAP_UNSYNCHRONIZED_THREAD_SAFE: /* when we fix MT stuff */
-   case PIPE_CAP_ALPHA_TO_COVERAGE_DITHER_CONTROL:
-   case PIPE_CAP_SHADER_ATOMIC_INT64:
-   case PIPE_CAP_GLSL_ZERO_INIT:
-   case PIPE_CAP_BLEND_EQUATION_ADVANCED:
-   case PIPE_CAP_NO_CLIP_ON_COPY_TEX:
-   case PIPE_CAP_CLEAR_SCISSORED: /* TODO */
-   case PIPE_CAP_DEVICE_PROTECTED_CONTENT:
       return 0;
 
    case PIPE_CAP_VENDOR_ID:
diff --git a/src/gallium/drivers/nouveau/nv50/nv84_video.c b/src/gallium/drivers/nouveau/nv50/nv84_video.c
index 3d276f91149..0b5ebd48cc9 100644
--- a/src/gallium/drivers/nouveau/nv50/nv84_video.c
+++ b/src/gallium/drivers/nouveau/nv50/nv84_video.c
@@ -44,7 +44,7 @@ nv84_copy_firmware(const char *path, void *dest, ssize_t len)
    close(fd);
 
    if (r != len) {
-      fprintf(stderr, "reading firmware file %s failed: %m\n", path);
+      fprintf(stderr, "reading firwmare file %s failed: %m\n", path);
       return 1;
    }
 
diff --git a/src/gallium/drivers/nouveau/nvc0/mme/com9097.mme b/src/gallium/drivers/nouveau/nvc0/mme/com9097.mme
index 27a7c324be6..d6af8221b65 100644
--- a/src/gallium/drivers/nouveau/nvc0/mme/com9097.mme
+++ b/src/gallium/drivers/nouveau/nvc0/mme/com9097.mme
@@ -592,7 +592,7 @@ crs_loop:
  * SCRATCH[4] = current counter [low]
  * SCRATCH[5] = current counter [high]
  *
- * arg     = number of parameters to multiply together, ideally 6
+ * arg     = number of parameters to muliply together, ideally 6
  * parm[0] = num_groups_x
  * parm[1] = num_groups_y
  * parm[2] = num_groups_z
diff --git a/src/gallium/drivers/nouveau/nvc0/nvc0_program.c b/src/gallium/drivers/nouveau/nvc0/nvc0_program.c
index 98dae6c703e..3b97f4781a4 100644
--- a/src/gallium/drivers/nouveau/nvc0/nvc0_program.c
+++ b/src/gallium/drivers/nouveau/nvc0/nvc0_program.c
@@ -361,7 +361,7 @@ nvc0_tcp_gen_header(struct nvc0_program *tcp, struct nv50_ir_prog_info_out *info
    if (info->target >= NVISA_GM107_CHIPSET) {
       /* On GM107+, the number of output patch components has moved in the TCP
        * header, but it seems like blob still also uses the old position.
-       * Also, the high 8-bits are located in between the min/max parallel
+       * Also, the high 8-bits are located inbetween the min/max parallel
        * field and has to be set after updating the outputs. */
       tcp->hdr[3] = (opcs & 0x0f) << 28;
       tcp->hdr[4] |= (opcs & 0xf0) << 16;
diff --git a/src/gallium/drivers/nouveau/nvc0/nvc0_query_hw.c b/src/gallium/drivers/nouveau/nvc0/nvc0_query_hw.c
index 389ccb7ddf0..af23798c178 100644
--- a/src/gallium/drivers/nouveau/nvc0/nvc0_query_hw.c
+++ b/src/gallium/drivers/nouveau/nvc0/nvc0_query_hw.c
@@ -149,7 +149,7 @@ nvc0_hw_begin_query(struct nvc0_context *nvc0, struct nvc0_query *q)
       return hq->funcs->begin_query(nvc0, hq);
 
    /* For occlusion queries we have to change the storage, because a previous
-    * query might set the initial render condition to false even *after* we re-
+    * query might set the initial render conition to false even *after* we re-
     * initialized it to true.
     */
    if (hq->rotate) {
diff --git a/src/gallium/drivers/nouveau/nvc0/nvc0_query_hw_sm.c b/src/gallium/drivers/nouveau/nvc0/nvc0_query_hw_sm.c
index 9050f99dc6e..8d75995ab74 100644
--- a/src/gallium/drivers/nouveau/nvc0/nvc0_query_hw_sm.c
+++ b/src/gallium/drivers/nouveau/nvc0/nvc0_query_hw_sm.c
@@ -2436,7 +2436,7 @@ nvc0_hw_sm_begin_query(struct nvc0_context *nvc0, struct nvc0_hw_query *hq)
       }
 
       /* Oddly-enough, the signal id depends on the slot selected on Fermi but
-       * not on Kepler. Fortunately, the signal ids are just offsetted by the
+       * not on Kepler. Fortunately, the signal ids are just offseted by the
        * slot id! */
       mask_sel |= c;
       mask_sel |= (c << 8);
diff --git a/src/gallium/drivers/nouveau/nvc0/nvc0_screen.c b/src/gallium/drivers/nouveau/nvc0/nvc0_screen.c
index 265cc7ecee3..bdaa2289a85 100644
--- a/src/gallium/drivers/nouveau/nvc0/nvc0_screen.c
+++ b/src/gallium/drivers/nouveau/nvc0/nvc0_screen.c
@@ -196,8 +196,6 @@ nvc0_screen_get_param(struct pipe_screen *pscreen, enum pipe_cap param)
       return 16;
    case PIPE_CAP_GL_BEGIN_END_BUFFER_SIZE:
       return 512 * 1024; /* TODO: Investigate tuning this */
-   case PIPE_CAP_MAX_TEXTURE_MB:
-      return 0; /* TODO: use 1/2 of VRAM for this? */
 
    /* supported caps */
    case PIPE_CAP_TEXTURE_MIRROR_CLAMP:
@@ -401,16 +399,6 @@ nvc0_screen_get_param(struct pipe_screen *pscreen, enum pipe_cap param)
    case PIPE_CAP_SHADER_SAMPLES_IDENTICAL:
    case PIPE_CAP_VIEWPORT_TRANSFORM_LOWERED:
    case PIPE_CAP_PSIZ_CLAMPED:
-   case PIPE_CAP_TEXTURE_BUFFER_SAMPLER:
-   case PIPE_CAP_PREFER_REAL_BUFFER_IN_CONSTBUF0:
-   case PIPE_CAP_MAP_UNSYNCHRONIZED_THREAD_SAFE: /* when we fix MT stuff */
-   case PIPE_CAP_ALPHA_TO_COVERAGE_DITHER_CONTROL: /* TODO */
-   case PIPE_CAP_SHADER_ATOMIC_INT64: /* TODO */
-   case PIPE_CAP_GLSL_ZERO_INIT:
-   case PIPE_CAP_BLEND_EQUATION_ADVANCED:
-   case PIPE_CAP_NO_CLIP_ON_COPY_TEX:
-   case PIPE_CAP_CLEAR_SCISSORED: /* TODO */
-   case PIPE_CAP_DEVICE_PROTECTED_CONTENT:
       return 0;
 
    case PIPE_CAP_VENDOR_ID:
diff --git a/src/gallium/drivers/nouveau/nvc0/nvc0_vbo_translate.c b/src/gallium/drivers/nouveau/nvc0/nvc0_vbo_translate.c
index 2be35e99b01..5b9e003b267 100644
--- a/src/gallium/drivers/nouveau/nvc0/nvc0_vbo_translate.c
+++ b/src/gallium/drivers/nouveau/nvc0/nvc0_vbo_translate.c
@@ -561,14 +561,13 @@ nvc0_push_vbo(struct nvc0_context *nvc0, const struct pipe_draw_info *info,
 {
    struct push_context ctx;
    unsigned i, index_size;
-   unsigned index_bias = info->index_size ? info->index_bias : 0;
    unsigned inst_count = info->instance_count;
    unsigned vert_count = draw->count;
    unsigned prim;
 
    nvc0_push_context_init(nvc0, &ctx);
 
-   nvc0_vertex_configure_translate(nvc0, index_bias);
+   nvc0_vertex_configure_translate(nvc0, info->index_bias);
 
    if (nvc0->state.index_bias) {
       /* this is already taken care of by translate */
@@ -577,7 +576,7 @@ nvc0_push_vbo(struct nvc0_context *nvc0, const struct pipe_draw_info *info,
    }
 
    if (unlikely(ctx.edgeflag.enabled))
-      nvc0_push_map_edgeflag(&ctx, nvc0, index_bias);
+      nvc0_push_map_edgeflag(&ctx, nvc0, info->index_bias);
 
    ctx.prim_restart = info->primitive_restart;
    ctx.restart_index = info->restart_index;
@@ -586,7 +585,7 @@ nvc0_push_vbo(struct nvc0_context *nvc0, const struct pipe_draw_info *info,
       /* NOTE: I hope we won't ever need that last index (~0).
        * If we do, we have to disable primitive restart here always and
        * use END,BEGIN to restart. (XXX: would that affect PrimitiveID ?)
-       * We could also deactivate PRIM_RESTART_WITH_DRAW_ARRAYS temporarily,
+       * We could also deactive PRIM_RESTART_WITH_DRAW_ARRAYS temporarily,
        * and add manual restart to disp_vertices_seq.
        */
       BEGIN_NVC0(ctx.push, NVC0_3D(PRIM_RESTART_ENABLE), 2);
diff --git a/src/gallium/drivers/panfrost/pan_assemble.c b/src/gallium/drivers/panfrost/pan_assemble.c
index ed1d3279f84..8e45025f1ca 100644
--- a/src/gallium/drivers/panfrost/pan_assemble.c
+++ b/src/gallium/drivers/panfrost/pan_assemble.c
@@ -98,24 +98,6 @@ pan_prepare_bifrost_props(struct panfrost_shader_state *state,
                 state->preload.fragment.coverage = true;
                 state->preload.fragment.primitive_flags = state->reads_face;
                 break;
-        case MESA_SHADER_COMPUTE:
-                pan_prepare(&state->properties, RENDERER_PROPERTIES);
-                state->properties.uniform_buffer_count = state->ubo_count;
-
-                pan_prepare(&state->preload, PRELOAD);
-                state->preload.uniform_count = state->uniform_count;
-
-                state->preload.compute.local_invocation_xy = true;
-                state->preload.compute.local_invocation_z = true;
-
-                state->preload.compute.work_group_x = true;
-                state->preload.compute.work_group_y = true;
-                state->preload.compute.work_group_z = true;
-
-                state->preload.compute.global_invocation_x = true;
-                state->preload.compute.global_invocation_y = true;
-                state->preload.compute.global_invocation_z = true;
-                break;
         default:
                 unreachable("TODO");
         }
diff --git a/src/gallium/drivers/panfrost/pan_blend_cso.c b/src/gallium/drivers/panfrost/pan_blend_cso.c
index d55eef0a409..dace098213a 100644
--- a/src/gallium/drivers/panfrost/pan_blend_cso.c
+++ b/src/gallium/drivers/panfrost/pan_blend_cso.c
@@ -233,7 +233,7 @@ panfrost_get_blend_for_context(struct panfrost_context *ctx, unsigned rti, struc
         struct panfrost_blend_state *blend = ctx->blend;
         struct panfrost_blend_rt *rt = &blend->rt[rti];
 
-        /* First, we'll try fixed function, matching equation and constant */
+        /* First, we'll try fixed function, matching equationn and constant */
         if (rt->has_fixed_function && panfrost_can_fixed_blend(fmt)) {
                 float constant = 0.0;
 
diff --git a/src/gallium/drivers/panfrost/pan_blending.c b/src/gallium/drivers/panfrost/pan_blending.c
index bc76fac605b..ac01a5f0ce0 100644
--- a/src/gallium/drivers/panfrost/pan_blending.c
+++ b/src/gallium/drivers/panfrost/pan_blending.c
@@ -267,7 +267,7 @@ panfrost_blend_constant_mask(const struct pipe_rt_blend_state *blend)
 /* Create the descriptor for a fixed blend mode given the corresponding Gallium
  * state, if possible. Return true and write out the blend descriptor into
  * blend_equation. If it is not possible with the fixed function
- * representation, return false to handle degenerate cases with a blend shader
+ * representating, return false to handle degenerate cases with a blend shader
  */
 
 bool
diff --git a/src/gallium/drivers/panfrost/pan_cmdstream.c b/src/gallium/drivers/panfrost/pan_cmdstream.c
index 7707696dbbb..9922c76e29d 100644
--- a/src/gallium/drivers/panfrost/pan_cmdstream.c
+++ b/src/gallium/drivers/panfrost/pan_cmdstream.c
@@ -844,26 +844,6 @@ panfrost_upload_num_work_groups_sysval(struct panfrost_batch *batch,
         uniform->u[2] = ctx->compute_grid->grid[2];
 }
 
-static void
-panfrost_upload_local_group_size_sysval(struct panfrost_batch *batch,
-                                        struct sysval_uniform *uniform)
-{
-        struct panfrost_context *ctx = batch->ctx;
-
-        uniform->u[0] = ctx->compute_grid->block[0];
-        uniform->u[1] = ctx->compute_grid->block[1];
-        uniform->u[2] = ctx->compute_grid->block[2];
-}
-
-static void
-panfrost_upload_work_dim_sysval(struct panfrost_batch *batch,
-                                struct sysval_uniform *uniform)
-{
-        struct panfrost_context *ctx = batch->ctx;
-
-        uniform->u[0] = ctx->compute_grid->work_dim;
-}
-
 static void
 panfrost_upload_sysvals(struct panfrost_batch *batch, void *buf,
                         struct panfrost_shader_state *ss,
@@ -897,14 +877,6 @@ panfrost_upload_sysvals(struct panfrost_batch *batch, void *buf,
                         panfrost_upload_num_work_groups_sysval(batch,
                                                                &uniforms[i]);
                         break;
-                case PAN_SYSVAL_LOCAL_GROUP_SIZE:
-                        panfrost_upload_local_group_size_sysval(batch,
-                                                                &uniforms[i]);
-                        break;
-                case PAN_SYSVAL_WORK_DIM:
-                        panfrost_upload_work_dim_sysval(batch,
-                                                        &uniforms[i]);
-                        break;
                 case PAN_SYSVAL_SAMPLER:
                         panfrost_upload_sampler_sysval(batch, st,
                                                        PAN_SYSVAL_ID(sysval),
@@ -1048,19 +1020,6 @@ panfrost_emit_shared_memory(struct panfrost_batch *batch,
                 ls.wls_base_pointer = bo->ptr.gpu;
                 ls.wls_instances = instances;
                 ls.wls_size_scale = util_logbase2(single_size) + 1;
-
-                if (ss->stack_size) {
-                        unsigned shift =
-                                panfrost_get_stack_shift(ss->stack_size);
-                        struct panfrost_bo *bo =
-                                panfrost_batch_get_scratchpad(batch,
-                                                              ss->stack_size,
-                                                              dev->thread_tls_alloc,
-                                                              dev->core_count);
-
-                        ls.tls_size = shift;
-                        ls.tls_base_pointer = bo->ptr.gpu;
-                }
         };
 
         return t.gpu;
@@ -1490,7 +1449,7 @@ enum pan_special_varying {
         PAN_VARY_MAX,
 };
 
-/* Given a varying, figure out which index it corresponds to */
+/* Given a varying, figure out which index it correpsonds to */
 
 static inline unsigned
 pan_varying_index(unsigned present, enum pan_special_varying v)
diff --git a/src/gallium/drivers/panfrost/pan_compute.c b/src/gallium/drivers/panfrost/pan_compute.c
index 324b2fe0fbf..103327eab46 100644
--- a/src/gallium/drivers/panfrost/pan_compute.c
+++ b/src/gallium/drivers/panfrost/pan_compute.c
@@ -32,8 +32,6 @@
 #include "pan_bo.h"
 #include "util/u_memory.h"
 #include "nir_serialize.h"
-#include "midgard/midgard_compile.h"
-#include "bifrost/bifrost_compile.h"
 
 /* Compute CSOs are tracked like graphics shader CSOs, but are
  * considerably simpler. We do not implement multiple
@@ -46,7 +44,6 @@ panfrost_create_compute_state(
         const struct pipe_compute_state *cso)
 {
         struct panfrost_context *ctx = pan_context(pctx);
-        struct panfrost_device *dev = pan_device(pctx->screen);
 
         struct panfrost_shader_variants *so = CALLOC_STRUCT(panfrost_shader_variants);
         so->cbase = *cso;
@@ -63,12 +60,7 @@ panfrost_create_compute_state(
                 const struct pipe_binary_program_header *hdr = cso->prog;
 
                 blob_reader_init(&reader, hdr->blob, hdr->num_bytes);
-
-                const struct nir_shader_compiler_options *options =
-                        (dev->quirks & IS_BIFROST) ? &bifrost_nir_options
-                                                   : &midgard_nir_options;
-
-                so->cbase.prog = nir_deserialize(NULL, options, &reader);
+                so->cbase.prog = nir_deserialize(NULL, &midgard_nir_options, &reader);
                 so->cbase.ir_type = PIPE_SHADER_IR_NIR;
         }
 
diff --git a/src/gallium/drivers/panfrost/pan_mfbd.c b/src/gallium/drivers/panfrost/pan_mfbd.c
index 17514d1bfc8..cbdb75a8015 100644
--- a/src/gallium/drivers/panfrost/pan_mfbd.c
+++ b/src/gallium/drivers/panfrost/pan_mfbd.c
@@ -257,8 +257,7 @@ get_z_internal_format(struct panfrost_batch *batch)
 
 static void
 panfrost_mfbd_zs_crc_ext_set_bufs(struct panfrost_batch *batch,
-                                  struct MALI_ZS_CRC_EXTENSION *ext,
-                                  struct panfrost_slice **checksum_slice)
+                                  struct MALI_ZS_CRC_EXTENSION *ext)
 {
         struct panfrost_device *dev = pan_device(batch->ctx->base.screen);
         bool is_bifrost = dev->quirks & IS_BIFROST;
@@ -273,8 +272,6 @@ panfrost_mfbd_zs_crc_ext_set_bufs(struct panfrost_batch *batch,
                         unsigned level = c_surf->u.tex.level;
                         struct panfrost_slice *slice = &rsrc->layout.slices[level];
 
-                        *checksum_slice = slice;
-
                         ext->crc_row_stride = slice->crc.stride;
                         if (rsrc->checksum_bo)
                                 ext->crc_base = rsrc->checksum_bo->ptr.gpu;
@@ -400,12 +397,11 @@ panfrost_mfbd_zs_crc_ext_set_bufs(struct panfrost_batch *batch,
 }
 
 static void
-panfrost_mfbd_emit_zs_crc_ext(struct panfrost_batch *batch, void *extp,
-                              struct panfrost_slice **checksum_slice)
+panfrost_mfbd_emit_zs_crc_ext(struct panfrost_batch *batch, void *extp)
 {
         pan_pack(extp, ZS_CRC_EXTENSION, ext) {
                 ext.zs_clean_pixel_write_enable = true;
-                panfrost_mfbd_zs_crc_ext_set_bufs(batch, &ext, checksum_slice);
+                panfrost_mfbd_zs_crc_ext_set_bufs(batch, &ext);
         }
 }
 
@@ -456,7 +452,7 @@ pan_internal_cbuf_size(struct panfrost_batch *batch, unsigned *tile_size)
         total_size = ALIGN_POT(total_size, 1024);
 
         /* Minimum tile size is 4x4. */
-        assert(*tile_size >= 4 * 4);
+        assert(*tile_size > 4 * 4);
         return total_size;
 }
 
@@ -568,14 +564,12 @@ panfrost_mfbd_fragment(struct panfrost_batch *batch, bool has_draws)
         if (panfrost_batch_is_scanout(batch))
                 batch->requirements &= ~PAN_REQ_DEPTH_WRITE;
 
-        struct panfrost_slice *checksum_slice = NULL;
-
         if (zs_crc_ext) {
                 if (batch->key.zsbuf &&
                     MAX2(batch->key.zsbuf->nr_samples, batch->key.zsbuf->nr_samples) > 1)
                         batch->requirements |= PAN_REQ_MSAA;
 
-                panfrost_mfbd_emit_zs_crc_ext(batch, zs_crc_ext, &checksum_slice);
+                panfrost_mfbd_emit_zs_crc_ext(batch, zs_crc_ext);
         }
 
         /* We always upload at least one dummy GL_NONE render target */
@@ -604,10 +598,6 @@ panfrost_mfbd_fragment(struct panfrost_batch *batch, bool has_draws)
 
                         rt_offset += pan_bytes_per_pixel_tib(surf->format) * tib_size *
                                 MAX2(samples, 1);
-
-                        struct panfrost_resource *prsrc = pan_resource(surf->texture);
-                        if (!checksum_slice)
-                                prsrc->layout.slices[surf->u.tex.level].checksum_valid = false;
                 }
         }
 
@@ -647,22 +637,6 @@ panfrost_mfbd_fragment(struct panfrost_batch *batch, bool has_draws)
                 }
 
                 params.has_zs_crc_extension = !!zs_crc_ext;
-
-                if (checksum_slice) {
-                        bool valid = checksum_slice->checksum_valid;
-                        bool full = !batch->minx && !batch->miny &&
-                                batch->maxx == batch->key.width &&
-                                batch->maxy == batch->key.height;
-
-                        params.crc_read_enable = valid;
-
-                        /* If the data is currently invalid, still write CRC
-                         * data if we are doing a full write, so that it is
-                         * valid for next time. */
-                        params.crc_write_enable = valid || full;
-
-                        checksum_slice->checksum_valid |= full;
-                }
         }
 
         if (dev->quirks & IS_BIFROST)
diff --git a/src/gallium/drivers/panfrost/pan_resource.c b/src/gallium/drivers/panfrost/pan_resource.c
index f93a3eaa600..24b7c85e9a0 100644
--- a/src/gallium/drivers/panfrost/pan_resource.c
+++ b/src/gallium/drivers/panfrost/pan_resource.c
@@ -52,9 +52,6 @@
 #include "decode.h"
 #include "panfrost-quirks.h"
 
-static bool
-panfrost_should_checksum(const struct panfrost_device *dev, const struct panfrost_resource *pres);
-
 bool
 pan_render_condition_check(struct pipe_context *pctx)
 {
@@ -120,7 +117,8 @@ panfrost_resource_from_handle(struct pipe_screen *pscreen,
         rsc->layout.slices[0].initialized = true;
         panfrost_resource_set_damage_region(NULL, &rsc->base, 0, NULL);
 
-        if (panfrost_should_checksum(dev, rsc)) {
+        if (dev->quirks & IS_BIFROST &&
+            templat->bind & PIPE_BIND_RENDER_TARGET) {
                 unsigned size =
                         panfrost_compute_checksum_size(&rsc->layout.slices[0],
                                                        templat->width0,
@@ -493,13 +491,6 @@ panfrost_setup_layout(struct panfrost_device *dev,
                 *bo_size = ALIGN_POT(pres->layout.array_stride * res->array_size, 4096);
 }
 
-static inline bool
-panfrost_is_2d(const struct panfrost_resource *pres)
-{
-        return (pres->base.target == PIPE_TEXTURE_2D)
-                || (pres->base.target == PIPE_TEXTURE_RECT);
-}
-
 /* Based on the usage, determine if it makes sense to use u-inteleaved tiling.
  * We only have routines to tile 2D textures of sane bpps. On the hardware
  * level, not all usages are valid for tiling. Finally, if the app is hinting
@@ -594,9 +585,10 @@ panfrost_should_tile(struct panfrost_device *dev, const struct panfrost_resource
                 bpp == 8 || bpp == 16 || bpp == 24 || bpp == 32 ||
                 bpp == 64 || bpp == 128;
 
-        bool can_tile = panfrost_is_2d(pres)
-                && is_sane_bpp
-                && ((pres->base.bind & ~valid_binding) == 0);
+        bool is_2d = (pres->base.target == PIPE_TEXTURE_2D)
+                || (pres->base.target == PIPE_TEXTURE_RECT);
+
+        bool can_tile = is_2d && is_sane_bpp && ((pres->base.bind & ~valid_binding) == 0);
 
         return can_tile && (pres->base.usage != PIPE_USAGE_STREAM);
 }
@@ -620,31 +612,13 @@ panfrost_best_modifier(struct panfrost_device *dev,
                 return DRM_FORMAT_MOD_LINEAR;
 }
 
-static bool
-panfrost_should_checksum(const struct panfrost_device *dev, const struct panfrost_resource *pres)
-{
-        /* When checksumming is enabled, the tile data must fit in the
-         * size of the writeback buffer, so don't checksum formats
-         * that use too much space. */
-
-        unsigned bytes_per_pixel_max = (dev->arch == 6) ? 6 : 4;
-
-        unsigned bytes_per_pixel = MAX2(pres->base.nr_samples, 1) *
-                util_format_get_blocksize(pres->base.format);
-
-        return pres->base.bind & PIPE_BIND_RENDER_TARGET &&
-                panfrost_is_2d(pres) &&
-                bytes_per_pixel <= bytes_per_pixel_max &&
-                !(dev->debug & PAN_DBG_NO_CRC);
-}
-
 static void
 panfrost_resource_setup(struct panfrost_device *dev, struct panfrost_resource *pres,
                         size_t *bo_size, uint64_t modifier)
 {
         pres->layout.modifier = (modifier != DRM_FORMAT_MOD_INVALID) ? modifier :
                 panfrost_best_modifier(dev, pres);
-        pres->checksummed = panfrost_should_checksum(dev, pres);
+        pres->checksummed = (pres->base.bind & PIPE_BIND_RENDER_TARGET);
 
         /* We can only switch tiled->linear if the resource isn't already
          * linear and if we control the modifier */
@@ -1156,9 +1130,6 @@ panfrost_ptr_unmap(struct pipe_context *pctx,
         struct panfrost_resource *prsrc = (struct panfrost_resource *) transfer->resource;
         struct panfrost_device *dev = pan_device(pctx->screen);
 
-        if (transfer->usage & PIPE_MAP_WRITE)
-                prsrc->layout.slices[transfer->level].checksum_valid = false;
-
         /* AFBC will use a staging resource. `initialized` will be set when the
          * fragment job is created; this is deferred to prevent useless surface
          * reloads that can cascade into DATA_INVALID_FAULTs due to reading
diff --git a/src/gallium/drivers/panfrost/pan_resource.h b/src/gallium/drivers/panfrost/pan_resource.h
index a7d8265d560..fa2d89039c7 100644
--- a/src/gallium/drivers/panfrost/pan_resource.h
+++ b/src/gallium/drivers/panfrost/pan_resource.h
@@ -58,7 +58,7 @@ struct panfrost_resource {
         /* Whether the modifier can be changed */
         bool modifier_constant;
 
-        /* Is transaction elimination enabled? */
+        /* Is transaciton elimination enabled? */
         bool checksummed;
 
         /* The CRC BO can be allocated separately */
diff --git a/src/gallium/drivers/panfrost/pan_screen.c b/src/gallium/drivers/panfrost/pan_screen.c
index 83c23bfe026..e94534abb64 100644
--- a/src/gallium/drivers/panfrost/pan_screen.c
+++ b/src/gallium/drivers/panfrost/pan_screen.c
@@ -66,7 +66,6 @@ static const struct debug_named_value panfrost_debug_options[] = {
         {"nofp16",     PAN_DBG_NOFP16,     "Disable 16-bit support"},
         {"gl3",       PAN_DBG_GL3,      "Enable experimental GL 3.x implementation, up to 3.3"},
         {"noafbc",    PAN_DBG_NO_AFBC,  "Disable AFBC support"},
-        {"nocrc",     PAN_DBG_NO_CRC,   "Disable transaction elimination"},
         DEBUG_NAMED_VALUE_END
 };
 
diff --git a/src/gallium/drivers/r300/r300_hyperz.c b/src/gallium/drivers/r300/r300_hyperz.c
index 7e146dfe57a..3c2902d2c73 100644
--- a/src/gallium/drivers/r300/r300_hyperz.c
+++ b/src/gallium/drivers/r300/r300_hyperz.c
@@ -218,6 +218,32 @@ static void r300_update_hyperz(struct r300_context* r300)
 /* The ZTOP state                                                            */
 /*****************************************************************************/
 
+static boolean r300_dsa_writes_stencil(
+        struct pipe_stencil_state *s)
+{
+    return s->enabled && s->writemask &&
+           (s->fail_op  != PIPE_STENCIL_OP_KEEP ||
+            s->zfail_op != PIPE_STENCIL_OP_KEEP ||
+            s->zpass_op != PIPE_STENCIL_OP_KEEP);
+}
+
+static boolean r300_dsa_writes_depth_stencil(
+        struct pipe_depth_stencil_alpha_state *dsa)
+{
+    /* We are interested only in the cases when a depth or stencil value
+     * can be changed. */
+
+    if (dsa->depth_enabled && dsa->depth_writemask &&
+        dsa->depth_func != PIPE_FUNC_NEVER)
+        return TRUE;
+
+    if (r300_dsa_writes_stencil(&dsa->stencil[0]) ||
+        r300_dsa_writes_stencil(&dsa->stencil[1]))
+        return TRUE;
+
+    return FALSE;
+}
+
 static boolean r300_dsa_alpha_test_enabled(
         struct pipe_depth_stencil_alpha_state *dsa)
 {
@@ -261,7 +287,7 @@ static void r300_update_ztop(struct r300_context* r300)
      */
 
     /* ZS writes */
-    if (util_writes_depth_stencil(r300->dsa_state.state) &&
+    if (r300_dsa_writes_depth_stencil(r300->dsa_state.state) &&
            (r300_dsa_alpha_test_enabled(r300->dsa_state.state) ||  /* (1) */
             r300_fs(r300)->shader->info.uses_kill)) {              /* (2) */
         ztop_state->z_buffer_top = R300_ZTOP_DISABLE;
diff --git a/src/gallium/drivers/r600/Makefile.sources b/src/gallium/drivers/r600/Makefile.sources
index 63079602188..28f2319241a 100644
--- a/src/gallium/drivers/r600/Makefile.sources
+++ b/src/gallium/drivers/r600/Makefile.sources
@@ -132,7 +132,6 @@ CXX_SOURCES = \
 	sfn/sfn_liverange.h \
 	sfn/sfn_nir.cpp \
 	sfn/sfn_nir.h \
-	sfn/sfn_nir_lower_64bit.cpp \
 	sfn/sfn_nir_lower_fs_out_to_vector.cpp \
 	sfn/sfn_nir_lower_fs_out_to_vector.h \
 	sfn/sfn_nir_lower_tess_io.cpp \
diff --git a/src/gallium/drivers/r600/r600_isa.c b/src/gallium/drivers/r600/r600_isa.c
index 0a5c4dac101..57b0e044f9d 100644
--- a/src/gallium/drivers/r600/r600_isa.c
+++ b/src/gallium/drivers/r600/r600_isa.c
@@ -194,8 +194,8 @@ const struct alu_op_info r600_alu_op_table[] = {
 		{"MULADD_IEEE_PREV",          2, {   -1, 0xD5 },{      0,     0,  AF_V,  AF_V},  AF_PREV_INTERLEAVE | AF_IEEE },
 		{"INTERP_XY",                 2, {   -1, 0xD6 },{      0,     0, AF_4V, AF_4V},  AF_INTERP },
 		{"INTERP_ZW",                 2, {   -1, 0xD7 },{      0,     0, AF_4V, AF_4V},  AF_INTERP },
-		{"INTERP_X",                  2, {   -1, 0xD8 },{      0,     0, AF_2V, AF_2V},  AF_INTERP },
-		{"INTERP_Z",                  2, {   -1, 0xD9 },{      0,     0, AF_2V, AF_2V},  AF_INTERP },
+		{"INTERP_X",                  2, {   -1, 0xD8 },{      0,     0,  AF_V,  AF_V},  AF_INTERP },
+		{"INTERP_Z",                  2, {   -1, 0xD9 },{      0,     0,  AF_V,  AF_V},  AF_INTERP },
 		{"STORE_FLAGS",               1, {   -1, 0xDA },{      0,     0,  AF_V,  AF_V},  0 },
 		{"LOAD_STORE_FLAGS",          1, {   -1, 0xDB },{      0,     0,  AF_V,  AF_V},  0 },
 		{"LDS_1A",                    2, {   -1, 0xDC },{      0,     0,  AF_V,  AF_V},  0 },
diff --git a/src/gallium/drivers/r600/r600_isa.h b/src/gallium/drivers/r600/r600_isa.h
index 1c098fbb187..73694f7b434 100644
--- a/src/gallium/drivers/r600/r600_isa.h
+++ b/src/gallium/drivers/r600/r600_isa.h
@@ -47,9 +47,6 @@ enum alu_op_flags
 	AF_4V		= (AF_V | AF_4SLOT),
 	AF_VS		= (AF_V | AF_S),     /* allowed in any slot */
 
-	AF_2SLOT = (1 << 3),
-	AF_2V    = AF_V | AF_2SLOT, /* XY or ZW */
-
 	AF_KILL		= (1<<4),
 	AF_PRED		= (1<<5),
 	AF_SET		= (1<<6),
@@ -58,7 +55,6 @@ enum alu_op_flags
 	AF_PREV_INTERLEAVE	= (1<<7),
 
 	AF_MOVA		= (1<<8),    /* all MOVA instructions */
-
 	AF_IEEE		= (1<<10),
 
 	AF_DST_TYPE_MASK = (3<<11),
@@ -111,7 +107,6 @@ enum alu_op_flags
 
 	/* condition codes - 3 bits */
 	AF_CC_SHIFT = 29,
-
 	AF_CC_MASK	= (7U << AF_CC_SHIFT),
 	AF_CC_E		= (0U << AF_CC_SHIFT),
 	AF_CC_GT	= (1U << AF_CC_SHIFT),
diff --git a/src/gallium/drivers/r600/r600_shader.c b/src/gallium/drivers/r600/r600_shader.c
index 54235937a4a..cfada0a8f11 100644
--- a/src/gallium/drivers/r600/r600_shader.c
+++ b/src/gallium/drivers/r600/r600_shader.c
@@ -179,8 +179,8 @@ int r600_pipe_shader_create(struct pipe_context *ctx,
 		pipe_shader_type_from_mesa(sel->nir->info.stage);
 	
 	bool dump = r600_can_dump_shader(&rctx->screen->b, processor);
-	unsigned use_sb = !(rctx->screen->b.debug_flags & DBG_NO_SB) /*&&
-		!(rscreen->b.debug_flags & DBG_NIR)*/;
+	unsigned use_sb = !(rctx->screen->b.debug_flags & DBG_NO_SB) &&
+		!(rscreen->b.debug_flags & DBG_NIR);
 	unsigned sb_disasm;
 	unsigned export_shader;
 	
diff --git a/src/gallium/drivers/r600/sb/sb_bc_decoder.cpp b/src/gallium/drivers/r600/sb/sb_bc_decoder.cpp
index b04cb73e22a..4a7f82ba753 100644
--- a/src/gallium/drivers/r600/sb/sb_bc_decoder.cpp
+++ b/src/gallium/drivers/r600/sb/sb_bc_decoder.cpp
@@ -540,8 +540,7 @@ int bc_decoder::decode_fetch_mem(unsigned & i, bc_fetch& bc) {
 	uint32_t dw2 = dw[i+2];
 
 	i += 4; // MEM instructions align to 4 words boundaries
-
-	assert(i <= ndw);
+	assert(i < ndw);
 
 	MEM_RD_WORD0_R7EGCM w0(dw0);
 	bc.elem_size = w0.get_ELEM_SIZE();
diff --git a/src/gallium/drivers/r600/sb/sb_bc_parser.cpp b/src/gallium/drivers/r600/sb/sb_bc_parser.cpp
index 446486c36ea..abbb26c13c1 100644
--- a/src/gallium/drivers/r600/sb/sb_bc_parser.cpp
+++ b/src/gallium/drivers/r600/sb/sb_bc_parser.cpp
@@ -385,9 +385,6 @@ int bc_parser::prepare_alu_group(cf_node* cf, alu_group_node *g) {
 		if (ctx.alu_slots(n->bc.op) & AF_4SLOT)
 			n->flags |= NF_ALU_4SLOT;
 
-		if (ctx.alu_slots(n->bc.op) & AF_2SLOT)
-			n->flags |= NF_ALU_2SLOT;
-
 		n->src.resize(src_count);
 
 		unsigned flags = n->bc.op_ptr->flags;
@@ -589,16 +586,12 @@ int bc_parser::prepare_alu_group(cf_node* cf, alu_group_node *g) {
 		alu_node *a = static_cast<alu_node*>(*I);
 		unsigned sflags = a->bc.slot_flags;
 
-		if (sflags == AF_4V || sflags == AF_2V  || (ctx.is_cayman() && sflags == AF_S)) {
+		if (sflags == AF_4V || (ctx.is_cayman() && sflags == AF_S)) {
 			if (!p)
 				p = sh->create_alu_packed();
 
 			a->remove();
 			p->push_back(a);
-                        if (sflags == AF_2V && p->count() == 2) {
-                           g->push_front(p);
-                           p = NULL;
-                        }
 		}
 	}
 
diff --git a/src/gallium/drivers/r600/sb/sb_dump.cpp b/src/gallium/drivers/r600/sb/sb_dump.cpp
index 402ba357fb1..57dded5ef00 100644
--- a/src/gallium/drivers/r600/sb/sb_dump.cpp
+++ b/src/gallium/drivers/r600/sb/sb_dump.cpp
@@ -396,8 +396,6 @@ void dump::dump_flags(node &n) {
 		sblog << "CH_CONS  ";
 	if (n.flags & NF_ALU_4SLOT)
 		sblog << "4S  ";
-	if (n.flags & NF_ALU_2SLOT)
-		sblog << "2S  ";
 }
 
 void dump::dump_val(value* v) {
diff --git a/src/gallium/drivers/r600/sb/sb_ir.h b/src/gallium/drivers/r600/sb/sb_ir.h
index 7a6742d1fc0..2d629443641 100644
--- a/src/gallium/drivers/r600/sb/sb_ir.h
+++ b/src/gallium/drivers/r600/sb/sb_ir.h
@@ -713,8 +713,7 @@ enum node_flags {
 	NF_SCHEDULE_EARLY = (1 << 9),
 
 	// for ALU_PUSH_BEFORE - when set, replace with PUSH + ALU
-	NF_ALU_STACK_WORKAROUND = (1 << 10),
-	NF_ALU_2SLOT = (1 << 11),
+	NF_ALU_STACK_WORKAROUND = (1 << 10)
 };
 
 inline node_flags operator |(node_flags l, node_flags r) {
@@ -1022,9 +1021,8 @@ public:
 	virtual bool fold_dispatch(expr_handler *ex);
 
 	unsigned forced_bank_swizzle() {
-		return ((bc.op_ptr->flags & AF_INTERP) &&
-			((bc.slot_flags == AF_4V) ||
-			 (bc.slot_flags == AF_2V))) ? VEC_210 : 0;
+		return ((bc.op_ptr->flags & AF_INTERP) && (bc.slot_flags == AF_4V)) ?
+				VEC_210 : 0;
 	}
 
 	// return param index + 1 if instruction references interpolation param,
diff --git a/src/gallium/drivers/r600/sb/sb_pass.h b/src/gallium/drivers/r600/sb/sb_pass.h
index 179eab4789d..a21b0bf9971 100644
--- a/src/gallium/drivers/r600/sb/sb_pass.h
+++ b/src/gallium/drivers/r600/sb/sb_pass.h
@@ -546,10 +546,10 @@ private:
 	void add_prev_chan(unsigned chan);
 	unsigned get_preferable_chan_mask();
 
-	bool ra_node(container_node *c);
-	bool process_op(node *n);
+	void ra_node(container_node *c);
+	void process_op(node *n);
 
-	bool color(value *v);
+	void color(value *v);
 
 	void color_bs_constraint(ra_constraint *c);
 
diff --git a/src/gallium/drivers/r600/sb/sb_ra_init.cpp b/src/gallium/drivers/r600/sb/sb_ra_init.cpp
index e14b187de61..c557b86871d 100644
--- a/src/gallium/drivers/r600/sb/sb_ra_init.cpp
+++ b/src/gallium/drivers/r600/sb/sb_ra_init.cpp
@@ -313,26 +313,24 @@ int ra_init::run() {
 
 	alloc_arrays();
 
-	return ra_node(sh.root) ? 0 : 1;
+	ra_node(sh.root);
+	return 0;
 }
 
-bool ra_init::ra_node(container_node* c) {
+void ra_init::ra_node(container_node* c) {
 
 	for (node_iterator I = c->begin(), E = c->end(); I != E; ++I) {
 		node *n = *I;
 		if (n->type == NT_OP) {
-			if (!process_op(n))
-                           return false;
+			process_op(n);
 		}
 		if (n->is_container() && !n->is_alu_packed()) {
-			if (!ra_node(static_cast<container_node*>(n)))
-                           return false;
+			ra_node(static_cast<container_node*>(n));
 		}
 	}
-        return true;
 }
 
-bool ra_init::process_op(node* n) {
+void ra_init::process_op(node* n) {
 
 	bool copy = n->is_copy_mov();
 
@@ -357,8 +355,7 @@ bool ra_init::process_op(node* n) {
 		for (vvec::iterator I = n->src.begin(), E = n->src.end(); I != E; ++I) {
 			value *v = *I;
 			if (v && v->is_sgpr())
-				if (!color(v))
-                                       return false;
+				color(v);
 		}
 	}
 
@@ -375,12 +372,10 @@ bool ra_init::process_op(node* n) {
 						assign_color(v, s->gpr);
 					}
 				} else
-                                   if (!color(v))
-                                          return false;
+					color(v);
 			}
 		}
 	}
-        return true;
 }
 
 void ra_init::color_bs_constraint(ra_constraint* c) {
@@ -481,15 +476,15 @@ void ra_init::color_bs_constraint(ra_constraint* c) {
 	}
 }
 
-bool ra_init::color(value* v) {
+void ra_init::color(value* v) {
 
 	if (v->constraint && v->constraint->kind == CK_PACKED_BS) {
 		color_bs_constraint(v->constraint);
-		return true;
+		return;
 	}
 
 	if (v->chunk && v->chunk->is_fixed())
-		return true;
+		return;
 
 	RA_DUMP(
 		sblog << "coloring ";
@@ -502,24 +497,24 @@ bool ra_init::color(value* v) {
 	if (v->is_reg_pinned()) {
 		assert(v->is_chan_pinned());
 		assign_color(v, v->pin_gpr);
-		return true;
+		return;
 	}
 
 	regbits rb(sh, v->interferences);
 	sel_chan c;
 
 	if (v->is_chan_pinned()) {
+		RA_DUMP( sblog << "chan_pinned = " << v->pin_gpr.chan() << "  ";	);
 		unsigned mask = 1 << v->pin_gpr.chan();
 		c = rb.find_free_chans(mask) + v->pin_gpr.chan();
 	} else {
 		unsigned cm = get_preferable_chan_mask();
+		RA_DUMP( sblog << "pref chan mask: " << cm << "\n"; );
 		c = rb.find_free_chan_by_mask(cm);
-	}    
+	}
 
-        if (!c || c.sel() >= 128 - ctx.alu_temp_gprs)
-           return false;
+	assert(c && c.sel() < 128 - ctx.alu_temp_gprs && "color failed");
 	assign_color(v, c);
-        return true;
 }
 
 void ra_init::assign_color(value* v, sel_chan c) {
diff --git a/src/gallium/drivers/r600/sfn/sfn_emitaluinstruction.cpp b/src/gallium/drivers/r600/sfn/sfn_emitaluinstruction.cpp
index 413b2218dd5..1a29ea57b8c 100644
--- a/src/gallium/drivers/r600/sfn/sfn_emitaluinstruction.cpp
+++ b/src/gallium/drivers/r600/sfn/sfn_emitaluinstruction.cpp
@@ -165,21 +165,6 @@ bool EmitAluInstruction::do_emit(nir_instr* ir)
    case nir_op_ball_fequal3: return emit_any_all_fcomp(instr, op2_sete, 3, true);
    case nir_op_ball_fequal4: return emit_any_all_fcomp(instr, op2_sete, 4, true);
 
-   case nir_op_b32any_inequal2: return emit_any_all_icomp(instr, op2_setne_int, 2, false);
-   case nir_op_b32any_inequal3: return emit_any_all_icomp(instr, op2_setne_int, 3, false);
-   case nir_op_b32any_inequal4: return emit_any_all_icomp(instr, op2_setne_int, 4, false);
-
-   case nir_op_b32all_iequal2: return emit_any_all_icomp(instr, op2_sete_int, 2, true);
-   case nir_op_b32all_iequal3: return emit_any_all_icomp(instr, op2_sete_int, 3, true);
-   case nir_op_b32all_iequal4: return emit_any_all_icomp(instr, op2_sete_int, 4, true);
-
-   case nir_op_b32any_fnequal2: return emit_any_all_fcomp2(instr, op2_setne_dx10, false);
-   case nir_op_b32any_fnequal3: return emit_any_all_fcomp(instr, op2_setne, 3, false);
-   case nir_op_b32any_fnequal4: return emit_any_all_fcomp(instr, op2_setne, 4, false);
-
-   case nir_op_b32all_fequal2: return emit_any_all_fcomp2(instr, op2_sete_dx10, true);
-   case nir_op_b32all_fequal3: return emit_any_all_fcomp(instr, op2_sete, 3, true);
-   case nir_op_b32all_fequal4: return emit_any_all_fcomp(instr, op2_sete, 4, true);
 
    case nir_op_ffma: return emit_alu_op3(instr, op3_muladd_ieee);
    case nir_op_b32csel: return emit_alu_op3(instr, op3_cnde,  {0, 2, 1});
@@ -247,10 +232,6 @@ unsigned EmitAluInstruction::num_src_comp(const nir_alu_instr& instr)
    case nir_op_ball_iequal2:
    case nir_op_bany_fnequal2:
    case nir_op_ball_fequal2:
-   case nir_op_b32any_inequal2:
-   case nir_op_b32all_iequal2:
-   case nir_op_b32any_fnequal2:
-   case nir_op_b32all_fequal2:
    case nir_op_unpack_64_2x32_split_y:
       return 2;
 
@@ -259,10 +240,6 @@ unsigned EmitAluInstruction::num_src_comp(const nir_alu_instr& instr)
    case nir_op_ball_iequal3:
    case nir_op_bany_fnequal3:
    case nir_op_ball_fequal3:
-   case nir_op_b32any_inequal3:
-   case nir_op_b32all_iequal3:
-   case nir_op_b32any_fnequal3:
-   case nir_op_b32all_fequal3:
       return 3;
 
    case nir_op_fdot4:
@@ -271,10 +248,6 @@ unsigned EmitAluInstruction::num_src_comp(const nir_alu_instr& instr)
    case nir_op_ball_iequal4:
    case nir_op_bany_fnequal4:
    case nir_op_ball_fequal4:
-   case nir_op_b32any_inequal4:
-   case nir_op_b32all_iequal4:
-   case nir_op_b32any_fnequal4:
-   case nir_op_b32all_fequal4:
       return 4;
 
    case nir_op_vec2:
diff --git a/src/gallium/drivers/r600/sfn/sfn_nir.cpp b/src/gallium/drivers/r600/sfn/sfn_nir.cpp
index 5ce326ec29e..8379dc8a72d 100644
--- a/src/gallium/drivers/r600/sfn/sfn_nir.cpp
+++ b/src/gallium/drivers/r600/sfn/sfn_nir.cpp
@@ -236,13 +236,10 @@ bool ShaderFromNir::lower(const nir_shader *shader, r600_pipe_shader *pipe_shade
    sfn_log << SfnLog::trans << "Finalize\n";
    impl->finalize();
 
-   impl->get_array_info(pipe_shader->shader);
-
    if (!sfn_log.has_debug_flag(SfnLog::nomerge)) {
       sfn_log << SfnLog::trans << "Merge registers\n";
       impl->remap_registers();
    }
-
    sfn_log << SfnLog::trans << "Finished translating to R600 IR\n";
    return true;
 }
@@ -996,50 +993,49 @@ int r600_shader_from_nir(struct r600_context *rctx,
       NIR_PASS_V(sel->nir, r600_lower_fs_out_to_vector);
    }
 
-	auto sh = nir_shader_clone(sel->nir, sel->nir);
-
-   if (sh->info.stage == MESA_SHADER_TESS_CTRL ||
-       sh->info.stage == MESA_SHADER_TESS_EVAL ||
-       (sh->info.stage == MESA_SHADER_VERTEX && key->vs.as_ls)) {
-      auto prim_type = sh->info.stage == MESA_SHADER_TESS_EVAL ?
-                          sh->info.tess.primitive_mode: key->tcs.prim_mode;
-      NIR_PASS_V(sh, r600_lower_tess_io, static_cast<pipe_prim_type>(prim_type));
+   if (sel->nir->info.stage == MESA_SHADER_TESS_CTRL ||
+       sel->nir->info.stage == MESA_SHADER_TESS_EVAL ||
+       (sel->nir->info.stage == MESA_SHADER_VERTEX && key->vs.as_ls)) {
+      auto prim_type = sel->nir->info.stage == MESA_SHADER_TESS_EVAL ?
+                          sel->nir->info.tess.primitive_mode: key->tcs.prim_mode;
+      NIR_PASS_V(sel->nir, r600_lower_tess_io, static_cast<pipe_prim_type>(prim_type));
    }
 
-   if (sh->info.stage == MESA_SHADER_TESS_CTRL)
-      NIR_PASS_V(sh, r600_append_tcs_TF_emission,
+   if (sel->nir->info.stage == MESA_SHADER_TESS_CTRL)
+      NIR_PASS_V(sel->nir, r600_append_tcs_TF_emission,
                  (pipe_prim_type)key->tcs.prim_mode);
 
-   NIR_PASS_V(sh, nir_lower_ubo_vec4);
+   NIR_PASS_V(sel->nir, nir_lower_ubo_vec4);
    if (lower_64bit)
-      NIR_PASS_V(sh, r600::r600_nir_64_to_vec2);
+      NIR_PASS_V(sel->nir, r600::r600_nir_64_to_vec2);
 
    /* Lower to scalar to let some optimization work out better */
-   while(optimize_once(sh, false));
+   while(optimize_once(sel->nir, false));
 
-   NIR_PASS_V(sh, r600::r600_merge_vec2_stores);
+   NIR_PASS_V(sel->nir, r600::r600_merge_vec2_stores);
 
-   NIR_PASS_V(sh, nir_remove_dead_variables, nir_var_shader_in, NULL);
-   NIR_PASS_V(sh, nir_remove_dead_variables,  nir_var_shader_out, NULL);
+   NIR_PASS_V(sel->nir, nir_remove_dead_variables, nir_var_shader_in, NULL);
+   NIR_PASS_V(sel->nir, nir_remove_dead_variables,  nir_var_shader_out, NULL);
 
 
-   NIR_PASS_V(sh, nir_lower_vars_to_scratch,
+   NIR_PASS_V(sel->nir, nir_lower_vars_to_scratch,
               nir_var_function_temp,
               40,
               r600_get_natural_size_align_bytes);
 
-   while (optimize_once(sh, true));
+   while (optimize_once(sel->nir, true));
 
-   NIR_PASS_V(sh, nir_lower_bool_to_int32);
+   auto sh = nir_shader_clone(sel->nir, sel->nir);
+   NIR_PASS_V(sel->nir, nir_lower_bool_to_int32);
    NIR_PASS_V(sh, nir_opt_algebraic_late);
 
-   if (sh->info.stage == MESA_SHADER_FRAGMENT)
+   if (sel->nir->info.stage == MESA_SHADER_FRAGMENT)
       r600::sort_fsoutput(sh);
 
    NIR_PASS_V(sh, nir_lower_locals_to_regs);
 
-   //NIR_PASS_V(sh, nir_opt_algebraic);
-   //NIR_PASS_V(sh, nir_copy_prop);
+   //NIR_PASS_V(sel->nir, nir_opt_algebraic);
+   //NIR_PASS_V(sel->nir, nir_copy_prop);
    NIR_PASS_V(sh, nir_lower_to_source_mods,
 	      (nir_lower_to_source_mods_flags)(nir_lower_float_source_mods |
 					       nir_lower_64bit_source_mods));
@@ -1056,16 +1052,16 @@ int r600_shader_from_nir(struct r600_context *rctx,
    }
 
    memset(&pipeshader->shader, 0, sizeof(r600_shader));
-   pipeshader->scratch_space_needed = sh->scratch_size;
-
-   if (sh->info.stage == MESA_SHADER_TESS_EVAL ||
-       sh->info.stage == MESA_SHADER_VERTEX ||
-       sh->info.stage == MESA_SHADER_GEOMETRY) {
-      pipeshader->shader.clip_dist_write |= ((1 << sh->info.clip_distance_array_size) - 1);
-      pipeshader->shader.cull_dist_write = ((1 << sh->info.cull_distance_array_size) - 1)
-                                           << sh->info.clip_distance_array_size;
-      pipeshader->shader.cc_dist_mask = (1 <<  (sh->info.cull_distance_array_size +
-                                                sh->info.clip_distance_array_size)) - 1;
+   pipeshader->scratch_space_needed = sel->nir->scratch_size;
+
+   if (sel->nir->info.stage == MESA_SHADER_TESS_EVAL ||
+       sel->nir->info.stage == MESA_SHADER_VERTEX ||
+       sel->nir->info.stage == MESA_SHADER_GEOMETRY) {
+      pipeshader->shader.clip_dist_write |= ((1 << sel->nir->info.clip_distance_array_size) - 1);
+      pipeshader->shader.cull_dist_write = ((1 << sel->nir->info.cull_distance_array_size) - 1)
+                                           << sel->nir->info.clip_distance_array_size;
+      pipeshader->shader.cc_dist_mask = (1 <<  (sel->nir->info.cull_distance_array_size +
+                                                sel->nir->info.clip_distance_array_size)) - 1;
    }
 
    struct r600_shader* gs_shader = nullptr;
@@ -1077,13 +1073,13 @@ int r600_shader_from_nir(struct r600_context *rctx,
    if (!r || rctx->screen->b.debug_flags & DBG_ALL_SHADERS) {
       static int shnr = 0;
 
-      snprintf(filename, 4000, "nir-%s_%d.inc", sh->info.name, shnr++);
+      snprintf(filename, 4000, "nir-%s_%d.inc", sel->nir->info.name, shnr++);
 
       if (access(filename, F_OK) == -1) {
          FILE *f = fopen(filename, "w");
 
          if (f) {
-            fprintf(f, "const char *shader_blob_%s = {\nR\"(", sh->info.name);
+            fprintf(f, "const char *shader_blob_%s = {\nR\"(", sel->nir->info.name);
             nir_print_shader(sh, f);
             fprintf(f, ")\";\n");
             fclose(f);
@@ -1111,7 +1107,7 @@ int r600_shader_from_nir(struct r600_context *rctx,
       return -1;
    }
 
-   if (sh->info.stage == MESA_SHADER_GEOMETRY) {
+   if (sel->nir->info.stage == MESA_SHADER_GEOMETRY) {
       r600::sfn_log << r600::SfnLog::shader_info << "Geometry shader, create copy shader\n";
       generate_gs_copy_shader(rctx, pipeshader, &sel->so);
       assert(pipeshader->gs_copy_shader);
diff --git a/src/gallium/drivers/r600/sfn/sfn_nir_lower_64bit.cpp b/src/gallium/drivers/r600/sfn/sfn_nir_lower_64bit.cpp
index fcff6af61fa..904c10461d1 100644
--- a/src/gallium/drivers/r600/sfn/sfn_nir_lower_64bit.cpp
+++ b/src/gallium/drivers/r600/sfn/sfn_nir_lower_64bit.cpp
@@ -967,6 +967,7 @@ public:
 
    StoreCombos m_stores;
    nir_shader *sh;
+   nir_builder b;
 };
 
 StoreMerger::StoreMerger(nir_shader *shader):
diff --git a/src/gallium/drivers/r600/sfn/sfn_shader_base.cpp b/src/gallium/drivers/r600/sfn/sfn_shader_base.cpp
index fdd548d305e..e33884aebc6 100644
--- a/src/gallium/drivers/r600/sfn/sfn_shader_base.cpp
+++ b/src/gallium/drivers/r600/sfn/sfn_shader_base.cpp
@@ -168,13 +168,6 @@ static void remap_shader_info(r600_shader& sh_info,
                               std::vector<rename_reg_pair>& map,
                               UNUSED ValueMap& values)
 {
-   for (unsigned i = 0; i < sh_info.num_arrays; ++i) {
-      auto new_index = map[sh_info.arrays[i].gpr_start];
-      if (new_index.valid)
-         sh_info.arrays[i].gpr_start = new_index.new_reg;
-      map[sh_info.arrays[i].gpr_start].used = true;
-   }
-
    for (unsigned i = 0; i < sh_info.ninput; ++i) {
       sfn_log << SfnLog::merge << "Input " << i << " gpr:" << sh_info.input[i].gpr
               << " of map.size()\n";
@@ -1171,20 +1164,6 @@ void ShaderFromNirProcessor::append_block(int nesting_change)
    m_output.push_back(InstructionBlock(m_nesting_depth, m_block_number++));
 }
 
-void ShaderFromNirProcessor::get_array_info(r600_shader& shader) const
-{
-   shader.num_arrays = m_reg_arrays.size();
-   if (shader.num_arrays) {
-      shader.arrays = (r600_shader_array *)calloc(shader.num_arrays, sizeof(r600_shader_array));
-      for (unsigned i = 0; i < shader.num_arrays; ++i) {
-         shader.arrays[i].comp_mask = m_reg_arrays[i]->mask();
-         shader.arrays[i].gpr_start = m_reg_arrays[i]->sel();
-         shader.arrays[i].gpr_count = m_reg_arrays[i]->size();
-      }
-      shader.indirect_files |= (1 << TGSI_FILE_TEMPORARY);
-   }
-}
-
 void ShaderFromNirProcessor::finalize()
 {
    do_finalize();
diff --git a/src/gallium/drivers/r600/sfn/sfn_shader_base.h b/src/gallium/drivers/r600/sfn/sfn_shader_base.h
index 54b2a14ebf4..9f21baed8c1 100644
--- a/src/gallium/drivers/r600/sfn/sfn_shader_base.h
+++ b/src/gallium/drivers/r600/sfn/sfn_shader_base.h
@@ -90,8 +90,6 @@ public:
       return m_atomic_base_map[base];
    }
 
-   void get_array_info(r600_shader& shader) const;
-
 protected:
 
    void set_var_address(nir_deref_instr *instr);
diff --git a/src/gallium/drivers/r600/sfn/sfn_value_gpr.cpp b/src/gallium/drivers/r600/sfn/sfn_value_gpr.cpp
index c53b3252788..b818cd3d498 100644
--- a/src/gallium/drivers/r600/sfn/sfn_value_gpr.cpp
+++ b/src/gallium/drivers/r600/sfn/sfn_value_gpr.cpp
@@ -279,16 +279,8 @@ GPRArray::GPRArray(int base, int size, int mask, int frac):
    m_values.resize(size);
    for (int i = 0; i < size; ++i) {
       for (int j = 0; j < 4; ++j) {
-         if (mask & (1 << j)) {
-            auto gpr = new GPRValue(base + i, j);
-            /* If we want to use sb, we have to keep arrays
-             * alife for the whole shader range, otherwise the sb scheduler
-             * thinks is not capable to rename non-array uses of these registers */
-            gpr->set_as_input();
-            gpr->set_keep_alive();
-            m_values[i].set_reg_i(j, PValue(gpr));
-
-         }
+         if (mask & (1 << j))
+            m_values[i].set_reg_i(j, PValue(new GPRValue(base + i, j)));
       }
    }
 }
diff --git a/src/gallium/drivers/r600/sfn/sfn_value_gpr.h b/src/gallium/drivers/r600/sfn/sfn_value_gpr.h
index 789348875b9..671ca8dbbe4 100644
--- a/src/gallium/drivers/r600/sfn/sfn_value_gpr.h
+++ b/src/gallium/drivers/r600/sfn/sfn_value_gpr.h
@@ -127,8 +127,6 @@ public:
 
    uint32_t sel() const override;
 
-   uint32_t mask() const { return m_component_mask; };
-
    size_t size() const {return m_values.size();}
 
    PValue get_indirect(unsigned index, PValue indirect, unsigned component);
diff --git a/src/gallium/drivers/r600/sfn/sfn_valuepool.cpp b/src/gallium/drivers/r600/sfn/sfn_valuepool.cpp
index a645bb20d5e..a790f18820e 100644
--- a/src/gallium/drivers/r600/sfn/sfn_valuepool.cpp
+++ b/src/gallium/drivers/r600/sfn/sfn_valuepool.cpp
@@ -425,9 +425,7 @@ void ValuePool::allocate_arrays(array_list& arrays)
 
       uint32_t mask = ((1 << a.ncomponents) - 1) << ncomponents;
 
-      PGPRArray array = PGPRArray(new GPRArray(current_index, a.length, mask, ncomponents));
-
-      m_reg_arrays.push_back(array);
+      PValue  array = PValue(new GPRArray(current_index, a.length, mask, ncomponents));
 
       sfn_log << SfnLog::reg << "Add array at "<< current_index
               << " of size " << a.length << " with " << a.ncomponents
diff --git a/src/gallium/drivers/r600/sfn/sfn_valuepool.h b/src/gallium/drivers/r600/sfn/sfn_valuepool.h
index 4fbf5857144..f61f6974bf4 100644
--- a/src/gallium/drivers/r600/sfn/sfn_valuepool.h
+++ b/src/gallium/drivers/r600/sfn/sfn_valuepool.h
@@ -180,9 +180,6 @@ public:
 
    GPRVector get_temp_vec4();
 
-protected:
-   std::vector<PGPRArray> m_reg_arrays;
-
 private:
 
    unsigned get_ssa_register_index(const nir_ssa_def& ssa) const;
@@ -236,6 +233,7 @@ private:
 
    unsigned m_next_register_index;
 
+   std::map<unsigned, PGPRArray> m_arrays_map;
 
    std::map<uint32_t, PValue> m_literals;
 
diff --git a/src/gallium/drivers/radeonsi/si_build_pm4.h b/src/gallium/drivers/radeonsi/si_build_pm4.h
index 3ccf3529d56..0294c455fb2 100644
--- a/src/gallium/drivers/radeonsi/si_build_pm4.h
+++ b/src/gallium/drivers/radeonsi/si_build_pm4.h
@@ -283,71 +283,4 @@ static inline void radeon_opt_set_context_regn(struct si_context *sctx, unsigned
    }
 }
 
-/* This should be evaluated at compile time if all parameters are constants. */
-static ALWAYS_INLINE unsigned
-si_get_user_data_base(enum chip_class chip_class, enum si_has_tess has_tess,
-                      enum si_has_gs has_gs, enum si_has_ngg ngg,
-                      enum pipe_shader_type shader)
-{
-   switch (shader) {
-   case PIPE_SHADER_VERTEX:
-      /* VS can be bound as VS, ES, or LS. */
-      if (has_tess) {
-         if (chip_class >= GFX10) {
-            return R_00B430_SPI_SHADER_USER_DATA_HS_0;
-         } else if (chip_class == GFX9) {
-            return R_00B430_SPI_SHADER_USER_DATA_LS_0;
-         } else {
-            return R_00B530_SPI_SHADER_USER_DATA_LS_0;
-         }
-      } else if (chip_class >= GFX10) {
-         if (ngg || has_gs) {
-            return R_00B230_SPI_SHADER_USER_DATA_GS_0;
-         } else {
-            return R_00B130_SPI_SHADER_USER_DATA_VS_0;
-         }
-      } else if (has_gs) {
-         return R_00B330_SPI_SHADER_USER_DATA_ES_0;
-      } else {
-         return R_00B130_SPI_SHADER_USER_DATA_VS_0;
-      }
-
-   case PIPE_SHADER_TESS_CTRL:
-      if (chip_class == GFX9) {
-         return R_00B430_SPI_SHADER_USER_DATA_LS_0;
-      } else {
-         return R_00B430_SPI_SHADER_USER_DATA_HS_0;
-      }
-
-   case PIPE_SHADER_TESS_EVAL:
-      /* TES can be bound as ES, VS, or not bound. */
-      if (has_tess) {
-         if (chip_class >= GFX10) {
-            if (ngg || has_gs) {
-               return R_00B230_SPI_SHADER_USER_DATA_GS_0;
-            } else {
-               return R_00B130_SPI_SHADER_USER_DATA_VS_0;
-            }
-         } else if (has_gs) {
-            return R_00B330_SPI_SHADER_USER_DATA_ES_0;
-         } else {
-            return R_00B130_SPI_SHADER_USER_DATA_VS_0;
-         }
-      } else {
-         return 0;
-      }
-
-   case PIPE_SHADER_GEOMETRY:
-      if (chip_class == GFX9) {
-         return R_00B330_SPI_SHADER_USER_DATA_ES_0;
-      } else {
-         return R_00B230_SPI_SHADER_USER_DATA_GS_0;
-      }
-
-   default:
-      assert(0);
-      return 0;
-   }
-}
-
 #endif
diff --git a/src/gallium/drivers/radeonsi/si_compute.c b/src/gallium/drivers/radeonsi/si_compute.c
index c2b0c24887f..4289e83083b 100644
--- a/src/gallium/drivers/radeonsi/si_compute.c
+++ b/src/gallium/drivers/radeonsi/si_compute.c
@@ -900,7 +900,7 @@ static void si_launch_grid(struct pipe_context *ctx, const struct pipe_grid_info
 
    /* Prefetch the compute shader to L2. */
    if (sctx->chip_class >= GFX7 && prefetch)
-      si_cp_dma_prefetch(sctx, &program->shader.bo->b.b, 0, program->shader.bo->b.b.width0);
+      cik_prefetch_TC_L2_async(sctx, &program->shader.bo->b.b, 0, program->shader.bo->b.b.width0);
 
    if (program->ir_type != PIPE_SHADER_IR_NATIVE)
       si_setup_nir_user_data(sctx, info);
@@ -914,6 +914,8 @@ static void si_launch_grid(struct pipe_context *ctx, const struct pipe_grid_info
 
    sctx->compute_is_busy = true;
    sctx->num_compute_calls++;
+   if (sctx->cs_shader_state.uses_scratch)
+      sctx->num_spill_compute_calls++;
 
    if (cs_regalloc_hang)
       sctx->flags |= SI_CONTEXT_CS_PARTIAL_FLUSH;
diff --git a/src/gallium/drivers/radeonsi/si_cp_dma.c b/src/gallium/drivers/radeonsi/si_cp_dma.c
index 7945143f916..b911d3e839c 100644
--- a/src/gallium/drivers/radeonsi/si_cp_dma.c
+++ b/src/gallium/drivers/radeonsi/si_cp_dma.c
@@ -399,42 +399,132 @@ void si_cp_dma_copy_buffer(struct si_context *sctx, struct pipe_resource *dst,
    }
 }
 
-void si_cp_dma_prefetch(struct si_context *sctx, struct pipe_resource *buf,
-                        unsigned offset, unsigned size)
+void cik_prefetch_TC_L2_async(struct si_context *sctx, struct pipe_resource *buf, uint64_t offset,
+                              unsigned size)
 {
-   uint64_t address = si_resource(buf)->gpu_address + offset;
-
    assert(sctx->chip_class >= GFX7);
 
-   /* The prefetch address and size must be aligned, so that we don't have to apply
-    * the complicated hw bug workaround.
-    *
-    * The size should also be less than 2 MB, so that we don't have to use a loop.
-    * Callers shouldn't need to prefetch more than 2 MB.
-    */
-   assert(size % SI_CPDMA_ALIGNMENT == 0);
-   assert(address % SI_CPDMA_ALIGNMENT == 0);
-   assert(size < S_414_BYTE_COUNT_GFX6(~0u));
+   si_cp_dma_copy_buffer(sctx, buf, buf, offset, offset, size, SI_CPDMA_SKIP_ALL,
+                         SI_COHERENCY_SHADER, L2_LRU);
+}
 
-   uint32_t header = S_411_SRC_SEL(V_411_SRC_ADDR_TC_L2);
-   uint32_t command = S_414_BYTE_COUNT_GFX6(size);
+static void cik_prefetch_shader_async(struct si_context *sctx, struct si_pm4_state *state)
+{
+   struct pipe_resource *bo = &state->shader->bo->b.b;
 
+   cik_prefetch_TC_L2_async(sctx, bo, 0, bo->width0);
+}
+
+static void cik_prefetch_VBO_descriptors(struct si_context *sctx)
+{
+   if (!sctx->vertex_elements || !sctx->vertex_elements->vb_desc_list_alloc_size)
+      return;
+
+   cik_prefetch_TC_L2_async(sctx, &sctx->vb_descriptors_buffer->b.b, sctx->vb_descriptors_offset,
+                            sctx->vertex_elements->vb_desc_list_alloc_size);
+}
+
+/**
+ * Prefetch shaders and VBO descriptors.
+ *
+ * \param vertex_stage_only  Whether only the the API VS and VBO descriptors
+ *                           should be prefetched.
+ */
+void cik_emit_prefetch_L2(struct si_context *sctx, bool vertex_stage_only)
+{
+   unsigned mask = sctx->prefetch_L2_mask;
+   assert(mask);
+
+   /* Prefetch shaders and VBO descriptors to TC L2. */
    if (sctx->chip_class >= GFX9) {
-      command |= S_414_DISABLE_WR_CONFIRM_GFX9(1);
-      header |= S_411_DST_SEL(V_411_NOWHERE);
+      /* Choose the right spot for the VBO prefetch. */
+      if (sctx->queued.named.hs) {
+         if (mask & SI_PREFETCH_HS)
+            cik_prefetch_shader_async(sctx, sctx->queued.named.hs);
+         if (mask & SI_PREFETCH_VBO_DESCRIPTORS)
+            cik_prefetch_VBO_descriptors(sctx);
+         if (vertex_stage_only) {
+            sctx->prefetch_L2_mask &= ~(SI_PREFETCH_HS | SI_PREFETCH_VBO_DESCRIPTORS);
+            return;
+         }
+
+         if (mask & SI_PREFETCH_GS)
+            cik_prefetch_shader_async(sctx, sctx->queued.named.gs);
+         if (mask & SI_PREFETCH_VS)
+            cik_prefetch_shader_async(sctx, sctx->queued.named.vs);
+      } else if (sctx->queued.named.gs) {
+         if (mask & SI_PREFETCH_GS)
+            cik_prefetch_shader_async(sctx, sctx->queued.named.gs);
+         if (mask & SI_PREFETCH_VBO_DESCRIPTORS)
+            cik_prefetch_VBO_descriptors(sctx);
+         if (vertex_stage_only) {
+            sctx->prefetch_L2_mask &= ~(SI_PREFETCH_GS | SI_PREFETCH_VBO_DESCRIPTORS);
+            return;
+         }
+
+         if (mask & SI_PREFETCH_VS)
+            cik_prefetch_shader_async(sctx, sctx->queued.named.vs);
+      } else {
+         if (mask & SI_PREFETCH_VS)
+            cik_prefetch_shader_async(sctx, sctx->queued.named.vs);
+         if (mask & SI_PREFETCH_VBO_DESCRIPTORS)
+            cik_prefetch_VBO_descriptors(sctx);
+         if (vertex_stage_only) {
+            sctx->prefetch_L2_mask &= ~(SI_PREFETCH_VS | SI_PREFETCH_VBO_DESCRIPTORS);
+            return;
+         }
+      }
    } else {
-      command |= S_414_DISABLE_WR_CONFIRM_GFX6(1);
-      header |= S_411_DST_SEL(V_411_DST_ADDR_TC_L2);
+      /* GFX6-GFX8 */
+      /* Choose the right spot for the VBO prefetch. */
+      if (sctx->tes_shader.cso) {
+         if (mask & SI_PREFETCH_LS)
+            cik_prefetch_shader_async(sctx, sctx->queued.named.ls);
+         if (mask & SI_PREFETCH_VBO_DESCRIPTORS)
+            cik_prefetch_VBO_descriptors(sctx);
+         if (vertex_stage_only) {
+            sctx->prefetch_L2_mask &= ~(SI_PREFETCH_LS | SI_PREFETCH_VBO_DESCRIPTORS);
+            return;
+         }
+
+         if (mask & SI_PREFETCH_HS)
+            cik_prefetch_shader_async(sctx, sctx->queued.named.hs);
+         if (mask & SI_PREFETCH_ES)
+            cik_prefetch_shader_async(sctx, sctx->queued.named.es);
+         if (mask & SI_PREFETCH_GS)
+            cik_prefetch_shader_async(sctx, sctx->queued.named.gs);
+         if (mask & SI_PREFETCH_VS)
+            cik_prefetch_shader_async(sctx, sctx->queued.named.vs);
+      } else if (sctx->gs_shader.cso) {
+         if (mask & SI_PREFETCH_ES)
+            cik_prefetch_shader_async(sctx, sctx->queued.named.es);
+         if (mask & SI_PREFETCH_VBO_DESCRIPTORS)
+            cik_prefetch_VBO_descriptors(sctx);
+         if (vertex_stage_only) {
+            sctx->prefetch_L2_mask &= ~(SI_PREFETCH_ES | SI_PREFETCH_VBO_DESCRIPTORS);
+            return;
+         }
+
+         if (mask & SI_PREFETCH_GS)
+            cik_prefetch_shader_async(sctx, sctx->queued.named.gs);
+         if (mask & SI_PREFETCH_VS)
+            cik_prefetch_shader_async(sctx, sctx->queued.named.vs);
+      } else {
+         if (mask & SI_PREFETCH_VS)
+            cik_prefetch_shader_async(sctx, sctx->queued.named.vs);
+         if (mask & SI_PREFETCH_VBO_DESCRIPTORS)
+            cik_prefetch_VBO_descriptors(sctx);
+         if (vertex_stage_only) {
+            sctx->prefetch_L2_mask &= ~(SI_PREFETCH_VS | SI_PREFETCH_VBO_DESCRIPTORS);
+            return;
+         }
+      }
    }
 
-   struct radeon_cmdbuf *cs = &sctx->gfx_cs;
-   radeon_emit(cs, PKT3(PKT3_DMA_DATA, 5, 0));
-   radeon_emit(cs, header);
-   radeon_emit(cs, address);       /* SRC_ADDR_LO [31:0] */
-   radeon_emit(cs, address >> 32); /* SRC_ADDR_HI [31:0] */
-   radeon_emit(cs, address);       /* DST_ADDR_LO [31:0] */
-   radeon_emit(cs, address >> 32); /* DST_ADDR_HI [31:0] */
-   radeon_emit(cs, command);
+   if (mask & SI_PREFETCH_PS)
+      cik_prefetch_shader_async(sctx, sctx->queued.named.ps);
+
+   sctx->prefetch_L2_mask = 0;
 }
 
 void si_test_gds(struct si_context *sctx)
diff --git a/src/gallium/drivers/radeonsi/si_debug.c b/src/gallium/drivers/radeonsi/si_debug.c
index 4c1dd5491a2..71db7f81271 100644
--- a/src/gallium/drivers/radeonsi/si_debug.c
+++ b/src/gallium/drivers/radeonsi/si_debug.c
@@ -807,7 +807,7 @@ static void si_dump_descriptors(struct si_context *sctx, gl_shader_stage stage,
    }
 
    if (stage == MESA_SHADER_VERTEX && sctx->vb_descriptors_buffer &&
-       sctx->vb_descriptors_gpu_list) {
+       sctx->vb_descriptors_gpu_list && sctx->vertex_elements) {
       assert(info); /* only CS may not have an info struct */
       struct si_descriptors desc = {};
 
diff --git a/src/gallium/drivers/radeonsi/si_descriptors.c b/src/gallium/drivers/radeonsi/si_descriptors.c
index 75cfc1c8662..4b9aeaff4d5 100644
--- a/src/gallium/drivers/radeonsi/si_descriptors.c
+++ b/src/gallium/drivers/radeonsi/si_descriptors.c
@@ -1081,8 +1081,8 @@ static struct si_descriptors *si_const_and_shader_buffer_descriptors(struct si_c
    return &sctx->descriptors[si_const_and_shader_buffer_descriptors_idx(shader)];
 }
 
-static void si_upload_const_buffer(struct si_context *sctx, struct si_resource **buf,
-                                   const uint8_t *ptr, unsigned size, uint32_t *const_offset)
+void si_upload_const_buffer(struct si_context *sctx, struct si_resource **buf, const uint8_t *ptr,
+                            unsigned size, uint32_t *const_offset)
 {
    void *tmp;
 
@@ -1192,8 +1192,8 @@ static void si_set_inlinable_constants(struct pipe_context *ctx,
    struct si_context *sctx = (struct si_context *)ctx;
 
    memcpy(sctx->inlinable_uniforms[shader], values, num_values * 4);
+   sctx->inlinable_uniforms_dirty_mask |= 1 << shader;
    sctx->inlinable_uniforms_valid_mask |= 1 << shader;
-   sctx->do_update_shaders = true;
 }
 
 void si_get_pipe_constant_buffer(struct si_context *sctx, uint shader, uint slot,
@@ -1482,12 +1482,11 @@ void si_update_needs_color_decompress_masks(struct si_context *sctx)
 /* Reset descriptors of buffer resources after \p buf has been invalidated.
  * If buf == NULL, reset all descriptors.
  */
-static bool si_reset_buffer_resources(struct si_context *sctx, struct si_buffer_resources *buffers,
+static void si_reset_buffer_resources(struct si_context *sctx, struct si_buffer_resources *buffers,
                                       unsigned descriptors_idx, uint64_t slot_mask,
                                       struct pipe_resource *buf, enum radeon_bo_priority priority)
 {
    struct si_descriptors *descs = &sctx->descriptors[descriptors_idx];
-   bool noop = true;
    uint64_t mask = buffers->enabled_mask & slot_mask;
 
    while (mask) {
@@ -1502,10 +1501,8 @@ static bool si_reset_buffer_resources(struct si_context *sctx, struct si_buffer_
             sctx, si_resource(buffer),
             buffers->writable_mask & (1llu << i) ? RADEON_USAGE_READWRITE : RADEON_USAGE_READ,
             priority, true);
-         noop = false;
       }
    }
-   return !noop;
 }
 
 /* Update all buffer bindings where the buffer is bound, including
@@ -1580,15 +1577,11 @@ void si_rebind_buffer(struct si_context *sctx, struct pipe_resource *buf)
    }
 
    if (!buffer || buffer->bind_history & PIPE_BIND_SHADER_BUFFER) {
-      for (shader = 0; shader < SI_NUM_SHADERS; shader++) {
-         if (si_reset_buffer_resources(sctx, &sctx->const_and_shader_buffers[shader],
-                                       si_const_and_shader_buffer_descriptors_idx(shader),
-                                       u_bit_consecutive64(0, SI_NUM_SHADER_BUFFERS), buf,
-                                       sctx->const_and_shader_buffers[shader].priority) &&
-             shader == PIPE_SHADER_COMPUTE) {
-            sctx->compute_shaderbuf_sgprs_dirty = true;
-         }
-      }
+      for (shader = 0; shader < SI_NUM_SHADERS; shader++)
+         si_reset_buffer_resources(sctx, &sctx->const_and_shader_buffers[shader],
+                                   si_const_and_shader_buffer_descriptors_idx(shader),
+                                   u_bit_consecutive64(0, SI_NUM_SHADER_BUFFERS), buf,
+                                   sctx->const_and_shader_buffers[shader].priority);
    }
 
    if (!buffer || buffer->bind_history & PIPE_BIND_SAMPLER_VIEW) {
@@ -1640,9 +1633,6 @@ void si_rebind_buffer(struct si_context *sctx, struct pipe_resource *buf)
                radeon_add_to_gfx_buffer_list_check_mem(sctx, si_resource(buffer),
                                                        RADEON_USAGE_READWRITE,
                                                        RADEON_PRIO_SAMPLER_BUFFER, true);
-
-               if (shader == PIPE_SHADER_COMPUTE)
-                  sctx->compute_image_sgprs_dirty = true;
             }
          }
       }
@@ -1915,19 +1905,43 @@ static void si_set_user_data_base(struct si_context *sctx, unsigned shader, uint
  */
 void si_shader_change_notify(struct si_context *sctx)
 {
-   si_set_user_data_base(sctx, PIPE_SHADER_VERTEX,
-                         si_get_user_data_base(sctx->chip_class,
-                                               sctx->tes_shader.cso ? TESS_ON : TESS_OFF,
-                                               sctx->gs_shader.cso ? GS_ON : GS_OFF,
-                                               sctx->ngg ? NGG_ON : NGG_OFF,
-                                               PIPE_SHADER_VERTEX));
+   /* VS can be bound as VS, ES, or LS. */
+   if (sctx->tes_shader.cso) {
+      if (sctx->chip_class >= GFX10) {
+         si_set_user_data_base(sctx, PIPE_SHADER_VERTEX, R_00B430_SPI_SHADER_USER_DATA_HS_0);
+      } else if (sctx->chip_class == GFX9) {
+         si_set_user_data_base(sctx, PIPE_SHADER_VERTEX, R_00B430_SPI_SHADER_USER_DATA_LS_0);
+      } else {
+         si_set_user_data_base(sctx, PIPE_SHADER_VERTEX, R_00B530_SPI_SHADER_USER_DATA_LS_0);
+      }
+   } else if (sctx->chip_class >= GFX10) {
+      if (sctx->ngg || sctx->gs_shader.cso) {
+         si_set_user_data_base(sctx, PIPE_SHADER_VERTEX, R_00B230_SPI_SHADER_USER_DATA_GS_0);
+      } else {
+         si_set_user_data_base(sctx, PIPE_SHADER_VERTEX, R_00B130_SPI_SHADER_USER_DATA_VS_0);
+      }
+   } else if (sctx->gs_shader.cso) {
+      si_set_user_data_base(sctx, PIPE_SHADER_VERTEX, R_00B330_SPI_SHADER_USER_DATA_ES_0);
+   } else {
+      si_set_user_data_base(sctx, PIPE_SHADER_VERTEX, R_00B130_SPI_SHADER_USER_DATA_VS_0);
+   }
 
-   si_set_user_data_base(sctx, PIPE_SHADER_TESS_EVAL,
-                         si_get_user_data_base(sctx->chip_class,
-                                               sctx->tes_shader.cso ? TESS_ON : TESS_OFF,
-                                               sctx->gs_shader.cso ? GS_ON : GS_OFF,
-                                               sctx->ngg ? NGG_ON : NGG_OFF,
-                                               PIPE_SHADER_TESS_EVAL));
+   /* TES can be bound as ES, VS, or not bound. */
+   if (sctx->tes_shader.cso) {
+      if (sctx->chip_class >= GFX10) {
+         if (sctx->ngg || sctx->gs_shader.cso) {
+            si_set_user_data_base(sctx, PIPE_SHADER_TESS_EVAL, R_00B230_SPI_SHADER_USER_DATA_GS_0);
+         } else {
+            si_set_user_data_base(sctx, PIPE_SHADER_TESS_EVAL, R_00B130_SPI_SHADER_USER_DATA_VS_0);
+         }
+      } else if (sctx->gs_shader.cso) {
+         si_set_user_data_base(sctx, PIPE_SHADER_TESS_EVAL, R_00B330_SPI_SHADER_USER_DATA_ES_0);
+      } else {
+         si_set_user_data_base(sctx, PIPE_SHADER_TESS_EVAL, R_00B130_SPI_SHADER_USER_DATA_VS_0);
+      }
+   } else {
+      si_set_user_data_base(sctx, PIPE_SHADER_TESS_EVAL, 0);
+   }
 }
 
 static void si_emit_shader_pointer_head(struct radeon_cmdbuf *cs, unsigned sh_offset,
@@ -2607,15 +2621,20 @@ void si_init_all_descriptors(struct si_context *sctx)
    sctx->atoms.s.shader_pointers.emit = si_emit_graphics_shader_pointers;
 
    /* Set default and immutable mappings. */
-   si_set_user_data_base(sctx, PIPE_SHADER_VERTEX,
-                         si_get_user_data_base(sctx->chip_class, TESS_OFF, GS_OFF,
-                                               sctx->ngg, PIPE_SHADER_VERTEX));
-   si_set_user_data_base(sctx, PIPE_SHADER_TESS_CTRL,
-                         si_get_user_data_base(sctx->chip_class, TESS_OFF, GS_OFF,
-                                               NGG_OFF, PIPE_SHADER_TESS_CTRL));
-   si_set_user_data_base(sctx, PIPE_SHADER_GEOMETRY,
-                         si_get_user_data_base(sctx->chip_class, TESS_OFF, GS_OFF,
-                                               NGG_OFF, PIPE_SHADER_GEOMETRY));
+   if (sctx->ngg) {
+      assert(sctx->chip_class >= GFX10);
+      si_set_user_data_base(sctx, PIPE_SHADER_VERTEX, R_00B230_SPI_SHADER_USER_DATA_GS_0);
+   } else {
+      si_set_user_data_base(sctx, PIPE_SHADER_VERTEX, R_00B130_SPI_SHADER_USER_DATA_VS_0);
+   }
+
+   if (sctx->chip_class == GFX9) {
+      si_set_user_data_base(sctx, PIPE_SHADER_TESS_CTRL, R_00B430_SPI_SHADER_USER_DATA_LS_0);
+      si_set_user_data_base(sctx, PIPE_SHADER_GEOMETRY, R_00B330_SPI_SHADER_USER_DATA_ES_0);
+   } else {
+      si_set_user_data_base(sctx, PIPE_SHADER_TESS_CTRL, R_00B430_SPI_SHADER_USER_DATA_HS_0);
+      si_set_user_data_base(sctx, PIPE_SHADER_GEOMETRY, R_00B230_SPI_SHADER_USER_DATA_GS_0);
+   }
    si_set_user_data_base(sctx, PIPE_SHADER_FRAGMENT, R_00B030_SPI_SHADER_USER_DATA_PS_0);
 }
 
diff --git a/src/gallium/drivers/radeonsi/si_gfx_cs.c b/src/gallium/drivers/radeonsi/si_gfx_cs.c
index 16b6a10986c..42d895de91e 100644
--- a/src/gallium/drivers/radeonsi/si_gfx_cs.c
+++ b/src/gallium/drivers/radeonsi/si_gfx_cs.c
@@ -443,7 +443,7 @@ void si_begin_new_gfx_cs(struct si_context *ctx, bool first_cs)
       ctx->prefetch_L2_mask |= SI_PREFETCH_VS;
    if (ctx->queued.named.ps)
       ctx->prefetch_L2_mask |= SI_PREFETCH_PS;
-   if (ctx->vb_descriptors_buffer)
+   if (ctx->vb_descriptors_buffer && ctx->vertex_elements)
       ctx->prefetch_L2_mask |= SI_PREFETCH_VBO_DESCRIPTORS;
 
    /* CLEAR_STATE disables all colorbuffers, so only enable bound ones. */
@@ -552,448 +552,3 @@ void si_begin_new_gfx_cs(struct si_context *ctx, bool first_cs)
 
    ctx->index_ring_offset = 0;
 }
-
-void si_emit_surface_sync(struct si_context *sctx, struct radeon_cmdbuf *cs, unsigned cp_coher_cntl)
-{
-   bool compute_ib = !sctx->has_graphics || cs == &sctx->prim_discard_compute_cs;
-
-   assert(sctx->chip_class <= GFX9);
-
-   if (sctx->chip_class == GFX9 || compute_ib) {
-      /* Flush caches and wait for the caches to assert idle. */
-      radeon_emit(cs, PKT3(PKT3_ACQUIRE_MEM, 5, 0));
-      radeon_emit(cs, cp_coher_cntl); /* CP_COHER_CNTL */
-      radeon_emit(cs, 0xffffffff);    /* CP_COHER_SIZE */
-      radeon_emit(cs, 0xffffff);      /* CP_COHER_SIZE_HI */
-      radeon_emit(cs, 0);             /* CP_COHER_BASE */
-      radeon_emit(cs, 0);             /* CP_COHER_BASE_HI */
-      radeon_emit(cs, 0x0000000A);    /* POLL_INTERVAL */
-   } else {
-      /* ACQUIRE_MEM is only required on a compute ring. */
-      radeon_emit(cs, PKT3(PKT3_SURFACE_SYNC, 3, 0));
-      radeon_emit(cs, cp_coher_cntl); /* CP_COHER_CNTL */
-      radeon_emit(cs, 0xffffffff);    /* CP_COHER_SIZE */
-      radeon_emit(cs, 0);             /* CP_COHER_BASE */
-      radeon_emit(cs, 0x0000000A);    /* POLL_INTERVAL */
-   }
-
-   /* ACQUIRE_MEM has an implicit context roll if the current context
-    * is busy. */
-   if (!compute_ib)
-      sctx->context_roll = true;
-}
-
-void gfx10_emit_cache_flush(struct si_context *ctx, struct radeon_cmdbuf *cs)
-{
-   uint32_t gcr_cntl = 0;
-   unsigned cb_db_event = 0;
-   unsigned flags = ctx->flags;
-
-   if (!ctx->has_graphics) {
-      /* Only process compute flags. */
-      flags &= SI_CONTEXT_INV_ICACHE | SI_CONTEXT_INV_SCACHE | SI_CONTEXT_INV_VCACHE |
-               SI_CONTEXT_INV_L2 | SI_CONTEXT_WB_L2 | SI_CONTEXT_INV_L2_METADATA |
-               SI_CONTEXT_CS_PARTIAL_FLUSH;
-   }
-
-   /* We don't need these. */
-   assert(!(flags & (SI_CONTEXT_VGT_STREAMOUT_SYNC | SI_CONTEXT_FLUSH_AND_INV_DB_META)));
-
-   if (flags & SI_CONTEXT_VGT_FLUSH) {
-      radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
-      radeon_emit(cs, EVENT_TYPE(V_028A90_VGT_FLUSH) | EVENT_INDEX(0));
-   }
-
-   if (flags & SI_CONTEXT_FLUSH_AND_INV_CB)
-      ctx->num_cb_cache_flushes++;
-   if (flags & SI_CONTEXT_FLUSH_AND_INV_DB)
-      ctx->num_db_cache_flushes++;
-
-   if (flags & SI_CONTEXT_INV_ICACHE)
-      gcr_cntl |= S_586_GLI_INV(V_586_GLI_ALL);
-   if (flags & SI_CONTEXT_INV_SCACHE) {
-      /* TODO: When writing to the SMEM L1 cache, we need to set SEQ
-       * to FORWARD when both L1 and L2 are written out (WB or INV).
-       */
-      gcr_cntl |= S_586_GL1_INV(1) | S_586_GLK_INV(1);
-   }
-   if (flags & SI_CONTEXT_INV_VCACHE)
-      gcr_cntl |= S_586_GL1_INV(1) | S_586_GLV_INV(1);
-
-   /* The L2 cache ops are:
-    * - INV: - invalidate lines that reflect memory (were loaded from memory)
-    *        - don't touch lines that were overwritten (were stored by gfx clients)
-    * - WB: - don't touch lines that reflect memory
-    *       - write back lines that were overwritten
-    * - WB | INV: - invalidate lines that reflect memory
-    *             - write back lines that were overwritten
-    *
-    * GLM doesn't support WB alone. If WB is set, INV must be set too.
-    */
-   if (flags & SI_CONTEXT_INV_L2) {
-      /* Writeback and invalidate everything in L2. */
-      gcr_cntl |= S_586_GL2_INV(1) | S_586_GL2_WB(1) | S_586_GLM_INV(1) | S_586_GLM_WB(1);
-      ctx->num_L2_invalidates++;
-   } else if (flags & SI_CONTEXT_WB_L2) {
-      gcr_cntl |= S_586_GL2_WB(1) | S_586_GLM_WB(1) | S_586_GLM_INV(1);
-   } else if (flags & SI_CONTEXT_INV_L2_METADATA) {
-      gcr_cntl |= S_586_GLM_INV(1) | S_586_GLM_WB(1);
-   }
-
-   if (flags & (SI_CONTEXT_FLUSH_AND_INV_CB | SI_CONTEXT_FLUSH_AND_INV_DB)) {
-      if (flags & SI_CONTEXT_FLUSH_AND_INV_CB) {
-         /* Flush CMASK/FMASK/DCC. Will wait for idle later. */
-         radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
-         radeon_emit(cs, EVENT_TYPE(V_028A90_FLUSH_AND_INV_CB_META) | EVENT_INDEX(0));
-      }
-      if (flags & SI_CONTEXT_FLUSH_AND_INV_DB) {
-         /* Flush HTILE. Will wait for idle later. */
-         radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
-         radeon_emit(cs, EVENT_TYPE(V_028A90_FLUSH_AND_INV_DB_META) | EVENT_INDEX(0));
-      }
-
-      /* First flush CB/DB, then L1/L2. */
-      gcr_cntl |= S_586_SEQ(V_586_SEQ_FORWARD);
-
-      if ((flags & (SI_CONTEXT_FLUSH_AND_INV_CB | SI_CONTEXT_FLUSH_AND_INV_DB)) ==
-          (SI_CONTEXT_FLUSH_AND_INV_CB | SI_CONTEXT_FLUSH_AND_INV_DB)) {
-         cb_db_event = V_028A90_CACHE_FLUSH_AND_INV_TS_EVENT;
-      } else if (flags & SI_CONTEXT_FLUSH_AND_INV_CB) {
-         cb_db_event = V_028A90_FLUSH_AND_INV_CB_DATA_TS;
-      } else if (flags & SI_CONTEXT_FLUSH_AND_INV_DB) {
-         cb_db_event = V_028A90_FLUSH_AND_INV_DB_DATA_TS;
-      } else {
-         assert(0);
-      }
-   } else {
-      /* Wait for graphics shaders to go idle if requested. */
-      if (flags & SI_CONTEXT_PS_PARTIAL_FLUSH) {
-         radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
-         radeon_emit(cs, EVENT_TYPE(V_028A90_PS_PARTIAL_FLUSH) | EVENT_INDEX(4));
-         /* Only count explicit shader flushes, not implicit ones. */
-         ctx->num_vs_flushes++;
-         ctx->num_ps_flushes++;
-      } else if (flags & SI_CONTEXT_VS_PARTIAL_FLUSH) {
-         radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
-         radeon_emit(cs, EVENT_TYPE(V_028A90_VS_PARTIAL_FLUSH) | EVENT_INDEX(4));
-         ctx->num_vs_flushes++;
-      }
-   }
-
-   if (flags & SI_CONTEXT_CS_PARTIAL_FLUSH && ctx->compute_is_busy) {
-      radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
-      radeon_emit(cs, EVENT_TYPE(V_028A90_CS_PARTIAL_FLUSH | EVENT_INDEX(4)));
-      ctx->num_cs_flushes++;
-      ctx->compute_is_busy = false;
-   }
-
-   if (cb_db_event) {
-      struct si_resource* wait_mem_scratch = unlikely(ctx->ws->cs_is_secure(cs)) ?
-        ctx->wait_mem_scratch_tmz : ctx->wait_mem_scratch;
-      /* CB/DB flush and invalidate (or possibly just a wait for a
-       * meta flush) via RELEASE_MEM.
-       *
-       * Combine this with other cache flushes when possible; this
-       * requires affected shaders to be idle, so do it after the
-       * CS_PARTIAL_FLUSH before (VS/PS partial flushes are always
-       * implied).
-       */
-      uint64_t va;
-
-      /* Do the flush (enqueue the event and wait for it). */
-      va = wait_mem_scratch->gpu_address;
-      ctx->wait_mem_number++;
-
-      /* Get GCR_CNTL fields, because the encoding is different in RELEASE_MEM. */
-      unsigned glm_wb = G_586_GLM_WB(gcr_cntl);
-      unsigned glm_inv = G_586_GLM_INV(gcr_cntl);
-      unsigned glv_inv = G_586_GLV_INV(gcr_cntl);
-      unsigned gl1_inv = G_586_GL1_INV(gcr_cntl);
-      assert(G_586_GL2_US(gcr_cntl) == 0);
-      assert(G_586_GL2_RANGE(gcr_cntl) == 0);
-      assert(G_586_GL2_DISCARD(gcr_cntl) == 0);
-      unsigned gl2_inv = G_586_GL2_INV(gcr_cntl);
-      unsigned gl2_wb = G_586_GL2_WB(gcr_cntl);
-      unsigned gcr_seq = G_586_SEQ(gcr_cntl);
-
-      gcr_cntl &= C_586_GLM_WB & C_586_GLM_INV & C_586_GLV_INV & C_586_GL1_INV & C_586_GL2_INV &
-                  C_586_GL2_WB; /* keep SEQ */
-
-      si_cp_release_mem(ctx, cs, cb_db_event,
-                        S_490_GLM_WB(glm_wb) | S_490_GLM_INV(glm_inv) | S_490_GLV_INV(glv_inv) |
-                           S_490_GL1_INV(gl1_inv) | S_490_GL2_INV(gl2_inv) | S_490_GL2_WB(gl2_wb) |
-                           S_490_SEQ(gcr_seq),
-                        EOP_DST_SEL_MEM, EOP_INT_SEL_SEND_DATA_AFTER_WR_CONFIRM,
-                        EOP_DATA_SEL_VALUE_32BIT, wait_mem_scratch, va, ctx->wait_mem_number,
-                        SI_NOT_QUERY);
-      si_cp_wait_mem(ctx, cs, va, ctx->wait_mem_number, 0xffffffff, WAIT_REG_MEM_EQUAL);
-   }
-
-   /* Ignore fields that only modify the behavior of other fields. */
-   if (gcr_cntl & C_586_GL1_RANGE & C_586_GL2_RANGE & C_586_SEQ) {
-      /* Flush caches and wait for the caches to assert idle.
-       * The cache flush is executed in the ME, but the PFP waits
-       * for completion.
-       */
-      radeon_emit(cs, PKT3(PKT3_ACQUIRE_MEM, 6, 0));
-      radeon_emit(cs, 0);          /* CP_COHER_CNTL */
-      radeon_emit(cs, 0xffffffff); /* CP_COHER_SIZE */
-      radeon_emit(cs, 0xffffff);   /* CP_COHER_SIZE_HI */
-      radeon_emit(cs, 0);          /* CP_COHER_BASE */
-      radeon_emit(cs, 0);          /* CP_COHER_BASE_HI */
-      radeon_emit(cs, 0x0000000A); /* POLL_INTERVAL */
-      radeon_emit(cs, gcr_cntl);   /* GCR_CNTL */
-   } else if (cb_db_event || (flags & (SI_CONTEXT_VS_PARTIAL_FLUSH | SI_CONTEXT_PS_PARTIAL_FLUSH |
-                                       SI_CONTEXT_CS_PARTIAL_FLUSH))) {
-      /* We need to ensure that PFP waits as well. */
-      radeon_emit(cs, PKT3(PKT3_PFP_SYNC_ME, 0, 0));
-      radeon_emit(cs, 0);
-   }
-
-   if (flags & SI_CONTEXT_START_PIPELINE_STATS) {
-      radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
-      radeon_emit(cs, EVENT_TYPE(V_028A90_PIPELINESTAT_START) | EVENT_INDEX(0));
-   } else if (flags & SI_CONTEXT_STOP_PIPELINE_STATS) {
-      radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
-      radeon_emit(cs, EVENT_TYPE(V_028A90_PIPELINESTAT_STOP) | EVENT_INDEX(0));
-   }
-
-   ctx->flags = 0;
-}
-
-void si_emit_cache_flush(struct si_context *sctx, struct radeon_cmdbuf *cs)
-{
-   uint32_t flags = sctx->flags;
-
-   if (!sctx->has_graphics) {
-      /* Only process compute flags. */
-      flags &= SI_CONTEXT_INV_ICACHE | SI_CONTEXT_INV_SCACHE | SI_CONTEXT_INV_VCACHE |
-               SI_CONTEXT_INV_L2 | SI_CONTEXT_WB_L2 | SI_CONTEXT_INV_L2_METADATA |
-               SI_CONTEXT_CS_PARTIAL_FLUSH;
-   }
-
-   uint32_t cp_coher_cntl = 0;
-   const uint32_t flush_cb_db = flags & (SI_CONTEXT_FLUSH_AND_INV_CB | SI_CONTEXT_FLUSH_AND_INV_DB);
-   const bool is_barrier =
-      flush_cb_db ||
-      /* INV_ICACHE == beginning of gfx IB. Checking
-       * INV_ICACHE fixes corruption for DeusExMD with
-       * compute-based culling, but I don't know why.
-       */
-      flags & (SI_CONTEXT_INV_ICACHE | SI_CONTEXT_PS_PARTIAL_FLUSH | SI_CONTEXT_VS_PARTIAL_FLUSH) ||
-      (flags & SI_CONTEXT_CS_PARTIAL_FLUSH && sctx->compute_is_busy);
-
-   assert(sctx->chip_class <= GFX9);
-
-   if (flags & SI_CONTEXT_FLUSH_AND_INV_CB)
-      sctx->num_cb_cache_flushes++;
-   if (flags & SI_CONTEXT_FLUSH_AND_INV_DB)
-      sctx->num_db_cache_flushes++;
-
-   /* GFX6 has a bug that it always flushes ICACHE and KCACHE if either
-    * bit is set. An alternative way is to write SQC_CACHES, but that
-    * doesn't seem to work reliably. Since the bug doesn't affect
-    * correctness (it only does more work than necessary) and
-    * the performance impact is likely negligible, there is no plan
-    * to add a workaround for it.
-    */
-
-   if (flags & SI_CONTEXT_INV_ICACHE)
-      cp_coher_cntl |= S_0085F0_SH_ICACHE_ACTION_ENA(1);
-   if (flags & SI_CONTEXT_INV_SCACHE)
-      cp_coher_cntl |= S_0085F0_SH_KCACHE_ACTION_ENA(1);
-
-   if (sctx->chip_class <= GFX8) {
-      if (flags & SI_CONTEXT_FLUSH_AND_INV_CB) {
-         cp_coher_cntl |= S_0085F0_CB_ACTION_ENA(1) | S_0085F0_CB0_DEST_BASE_ENA(1) |
-                          S_0085F0_CB1_DEST_BASE_ENA(1) | S_0085F0_CB2_DEST_BASE_ENA(1) |
-                          S_0085F0_CB3_DEST_BASE_ENA(1) | S_0085F0_CB4_DEST_BASE_ENA(1) |
-                          S_0085F0_CB5_DEST_BASE_ENA(1) | S_0085F0_CB6_DEST_BASE_ENA(1) |
-                          S_0085F0_CB7_DEST_BASE_ENA(1);
-
-         /* Necessary for DCC */
-         if (sctx->chip_class == GFX8)
-            si_cp_release_mem(sctx, cs, V_028A90_FLUSH_AND_INV_CB_DATA_TS, 0, EOP_DST_SEL_MEM,
-                              EOP_INT_SEL_NONE, EOP_DATA_SEL_DISCARD, NULL, 0, 0, SI_NOT_QUERY);
-      }
-      if (flags & SI_CONTEXT_FLUSH_AND_INV_DB)
-         cp_coher_cntl |= S_0085F0_DB_ACTION_ENA(1) | S_0085F0_DB_DEST_BASE_ENA(1);
-   }
-
-   if (flags & SI_CONTEXT_FLUSH_AND_INV_CB) {
-      /* Flush CMASK/FMASK/DCC. SURFACE_SYNC will wait for idle. */
-      radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
-      radeon_emit(cs, EVENT_TYPE(V_028A90_FLUSH_AND_INV_CB_META) | EVENT_INDEX(0));
-   }
-   if (flags & (SI_CONTEXT_FLUSH_AND_INV_DB | SI_CONTEXT_FLUSH_AND_INV_DB_META)) {
-      /* Flush HTILE. SURFACE_SYNC will wait for idle. */
-      radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
-      radeon_emit(cs, EVENT_TYPE(V_028A90_FLUSH_AND_INV_DB_META) | EVENT_INDEX(0));
-   }
-
-   /* Wait for shader engines to go idle.
-    * VS and PS waits are unnecessary if SURFACE_SYNC is going to wait
-    * for everything including CB/DB cache flushes.
-    */
-   if (!flush_cb_db) {
-      if (flags & SI_CONTEXT_PS_PARTIAL_FLUSH) {
-         radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
-         radeon_emit(cs, EVENT_TYPE(V_028A90_PS_PARTIAL_FLUSH) | EVENT_INDEX(4));
-         /* Only count explicit shader flushes, not implicit ones
-          * done by SURFACE_SYNC.
-          */
-         sctx->num_vs_flushes++;
-         sctx->num_ps_flushes++;
-      } else if (flags & SI_CONTEXT_VS_PARTIAL_FLUSH) {
-         radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
-         radeon_emit(cs, EVENT_TYPE(V_028A90_VS_PARTIAL_FLUSH) | EVENT_INDEX(4));
-         sctx->num_vs_flushes++;
-      }
-   }
-
-   if (flags & SI_CONTEXT_CS_PARTIAL_FLUSH && sctx->compute_is_busy) {
-      radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
-      radeon_emit(cs, EVENT_TYPE(V_028A90_CS_PARTIAL_FLUSH) | EVENT_INDEX(4));
-      sctx->num_cs_flushes++;
-      sctx->compute_is_busy = false;
-   }
-
-   /* VGT state synchronization. */
-   if (flags & SI_CONTEXT_VGT_FLUSH) {
-      radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
-      radeon_emit(cs, EVENT_TYPE(V_028A90_VGT_FLUSH) | EVENT_INDEX(0));
-   }
-   if (flags & SI_CONTEXT_VGT_STREAMOUT_SYNC) {
-      radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
-      radeon_emit(cs, EVENT_TYPE(V_028A90_VGT_STREAMOUT_SYNC) | EVENT_INDEX(0));
-   }
-
-   /* GFX9: Wait for idle if we're flushing CB or DB. ACQUIRE_MEM doesn't
-    * wait for idle on GFX9. We have to use a TS event.
-    */
-   if (sctx->chip_class == GFX9 && flush_cb_db) {
-      uint64_t va;
-      unsigned tc_flags, cb_db_event;
-
-      /* Set the CB/DB flush event. */
-      switch (flush_cb_db) {
-      case SI_CONTEXT_FLUSH_AND_INV_CB:
-         cb_db_event = V_028A90_FLUSH_AND_INV_CB_DATA_TS;
-         break;
-      case SI_CONTEXT_FLUSH_AND_INV_DB:
-         cb_db_event = V_028A90_FLUSH_AND_INV_DB_DATA_TS;
-         break;
-      default:
-         /* both CB & DB */
-         cb_db_event = V_028A90_CACHE_FLUSH_AND_INV_TS_EVENT;
-      }
-
-      /* These are the only allowed combinations. If you need to
-       * do multiple operations at once, do them separately.
-       * All operations that invalidate L2 also seem to invalidate
-       * metadata. Volatile (VOL) and WC flushes are not listed here.
-       *
-       * TC    | TC_WB         = writeback & invalidate L2 & L1
-       * TC    | TC_WB | TC_NC = writeback & invalidate L2 for MTYPE == NC
-       *         TC_WB | TC_NC = writeback L2 for MTYPE == NC
-       * TC            | TC_NC = invalidate L2 for MTYPE == NC
-       * TC    | TC_MD         = writeback & invalidate L2 metadata (DCC, etc.)
-       * TCL1                  = invalidate L1
-       */
-      tc_flags = 0;
-
-      if (flags & SI_CONTEXT_INV_L2_METADATA) {
-         tc_flags = EVENT_TC_ACTION_ENA | EVENT_TC_MD_ACTION_ENA;
-      }
-
-      /* Ideally flush TC together with CB/DB. */
-      if (flags & SI_CONTEXT_INV_L2) {
-         /* Writeback and invalidate everything in L2 & L1. */
-         tc_flags = EVENT_TC_ACTION_ENA | EVENT_TC_WB_ACTION_ENA;
-
-         /* Clear the flags. */
-         flags &= ~(SI_CONTEXT_INV_L2 | SI_CONTEXT_WB_L2 | SI_CONTEXT_INV_VCACHE);
-         sctx->num_L2_invalidates++;
-      }
-
-      /* Do the flush (enqueue the event and wait for it). */
-      struct si_resource* wait_mem_scratch = unlikely(sctx->ws->cs_is_secure(cs)) ?
-        sctx->wait_mem_scratch_tmz : sctx->wait_mem_scratch;
-      va = wait_mem_scratch->gpu_address;
-      sctx->wait_mem_number++;
-
-      si_cp_release_mem(sctx, cs, cb_db_event, tc_flags, EOP_DST_SEL_MEM,
-                        EOP_INT_SEL_SEND_DATA_AFTER_WR_CONFIRM, EOP_DATA_SEL_VALUE_32BIT,
-                        wait_mem_scratch, va, sctx->wait_mem_number, SI_NOT_QUERY);
-      si_cp_wait_mem(sctx, cs, va, sctx->wait_mem_number, 0xffffffff, WAIT_REG_MEM_EQUAL);
-   }
-
-   /* Make sure ME is idle (it executes most packets) before continuing.
-    * This prevents read-after-write hazards between PFP and ME.
-    */
-   if (sctx->has_graphics &&
-       (cp_coher_cntl || (flags & (SI_CONTEXT_CS_PARTIAL_FLUSH | SI_CONTEXT_INV_VCACHE |
-                                   SI_CONTEXT_INV_L2 | SI_CONTEXT_WB_L2)))) {
-      radeon_emit(cs, PKT3(PKT3_PFP_SYNC_ME, 0, 0));
-      radeon_emit(cs, 0);
-   }
-
-   /* GFX6-GFX8 only:
-    *   When one of the CP_COHER_CNTL.DEST_BASE flags is set, SURFACE_SYNC
-    *   waits for idle, so it should be last. SURFACE_SYNC is done in PFP.
-    *
-    * cp_coher_cntl should contain all necessary flags except TC flags
-    * at this point.
-    *
-    * GFX6-GFX7 don't support L2 write-back.
-    */
-   if (flags & SI_CONTEXT_INV_L2 || (sctx->chip_class <= GFX7 && (flags & SI_CONTEXT_WB_L2))) {
-      /* Invalidate L1 & L2. (L1 is always invalidated on GFX6)
-       * WB must be set on GFX8+ when TC_ACTION is set.
-       */
-      si_emit_surface_sync(sctx, cs,
-                           cp_coher_cntl | S_0085F0_TC_ACTION_ENA(1) | S_0085F0_TCL1_ACTION_ENA(1) |
-                              S_0301F0_TC_WB_ACTION_ENA(sctx->chip_class >= GFX8));
-      cp_coher_cntl = 0;
-      sctx->num_L2_invalidates++;
-   } else {
-      /* L1 invalidation and L2 writeback must be done separately,
-       * because both operations can't be done together.
-       */
-      if (flags & SI_CONTEXT_WB_L2) {
-         /* WB = write-back
-          * NC = apply to non-coherent MTYPEs
-          *      (i.e. MTYPE <= 1, which is what we use everywhere)
-          *
-          * WB doesn't work without NC.
-          */
-         si_emit_surface_sync(
-            sctx, cs,
-            cp_coher_cntl | S_0301F0_TC_WB_ACTION_ENA(1) | S_0301F0_TC_NC_ACTION_ENA(1));
-         cp_coher_cntl = 0;
-         sctx->num_L2_writebacks++;
-      }
-      if (flags & SI_CONTEXT_INV_VCACHE) {
-         /* Invalidate per-CU VMEM L1. */
-         si_emit_surface_sync(sctx, cs, cp_coher_cntl | S_0085F0_TCL1_ACTION_ENA(1));
-         cp_coher_cntl = 0;
-      }
-   }
-
-   /* If TC flushes haven't cleared this... */
-   if (cp_coher_cntl)
-      si_emit_surface_sync(sctx, cs, cp_coher_cntl);
-
-   if (is_barrier)
-      si_prim_discard_signal_next_compute_ib_start(sctx);
-
-   if (flags & SI_CONTEXT_START_PIPELINE_STATS) {
-      radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
-      radeon_emit(cs, EVENT_TYPE(V_028A90_PIPELINESTAT_START) | EVENT_INDEX(0));
-   } else if (flags & SI_CONTEXT_STOP_PIPELINE_STATS) {
-      radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
-      radeon_emit(cs, EVENT_TYPE(V_028A90_PIPELINESTAT_STOP) | EVENT_INDEX(0));
-   }
-
-   sctx->flags = 0;
-}
diff --git a/src/gallium/drivers/radeonsi/si_perfcounter.c b/src/gallium/drivers/radeonsi/si_perfcounter.c
index 6363368c5a3..f28a51ffd52 100644
--- a/src/gallium/drivers/radeonsi/si_perfcounter.c
+++ b/src/gallium/drivers/radeonsi/si_perfcounter.c
@@ -919,14 +919,14 @@ static void si_pc_query_destroy(struct si_context *sctx, struct si_query *squery
    FREE(query);
 }
 
-void si_inhibit_clockgating(struct si_context *sctx, struct radeon_cmdbuf *cs, bool inhibit)
+static void si_inhibit_clockgating(struct si_context *sctx, bool inhibit)
 {
    if (sctx->chip_class >= GFX10) {
-      radeon_set_uconfig_reg(cs, R_037390_RLC_PERFMON_CLK_CNTL,
-                             S_037390_PERFMON_CLOCK_STATE(inhibit));
+      radeon_set_uconfig_reg(&sctx->gfx_cs, R_037390_RLC_PERFMON_CLK_CNTL,
+                            S_037390_PERFMON_CLOCK_STATE(inhibit));
    } else if (sctx->chip_class >= GFX8) {
-      radeon_set_uconfig_reg(cs, R_0372FC_RLC_PERFMON_CLK_CNTL,
-                             S_0372FC_PERFMON_CLOCK_STATE(inhibit));
+      radeon_set_uconfig_reg(&sctx->gfx_cs, R_0372FC_RLC_PERFMON_CLK_CNTL,
+                            S_0372FC_PERFMON_CLOCK_STATE(inhibit));
    }
 }
 
@@ -946,7 +946,7 @@ static void si_pc_query_resume(struct si_context *sctx, struct si_query *squery)
    if (query->shaders)
       si_pc_emit_shaders(sctx, query->shaders);
 
-   si_inhibit_clockgating(sctx, &sctx->gfx_cs, true);
+   si_inhibit_clockgating(sctx, true);
 
    for (struct si_query_group *group = query->groups; group; group = group->next) {
       struct si_pc_block *block = group->block;
@@ -1000,7 +1000,7 @@ static void si_pc_query_suspend(struct si_context *sctx, struct si_query *squery
 
    si_pc_emit_instance(sctx, -1, -1);
 
-   si_inhibit_clockgating(sctx, &sctx->gfx_cs, false);
+   si_inhibit_clockgating(sctx, false);
 }
 
 static bool si_pc_query_begin(struct si_context *ctx, struct si_query *squery)
diff --git a/src/gallium/drivers/radeonsi/si_pipe.c b/src/gallium/drivers/radeonsi/si_pipe.c
index 9c904b077b6..fdee2c509a2 100644
--- a/src/gallium/drivers/radeonsi/si_pipe.c
+++ b/src/gallium/drivers/radeonsi/si_pipe.c
@@ -255,8 +255,6 @@ static void si_destroy_context(struct pipe_context *context)
       sctx->b.delete_compute_state(&sctx->b, sctx->cs_dcc_decompress);
    if (sctx->cs_dcc_retile)
       sctx->b.delete_compute_state(&sctx->b, sctx->cs_dcc_retile);
-   if (sctx->no_velems_state)
-      sctx->b.delete_vertex_elements_state(&sctx->b, sctx->no_velems_state);
 
    for (unsigned i = 0; i < ARRAY_SIZE(sctx->cs_fmask_expand); i++) {
       for (unsigned j = 0; j < ARRAY_SIZE(sctx->cs_fmask_expand[i]); j++) {
@@ -499,21 +497,15 @@ static struct pipe_context *si_create_context(struct pipe_screen *screen, unsign
       goto fail;
 
    /* Initialize public allocators. */
-   /* Unify uploaders as follows:
-    * - dGPUs with Smart Access Memory: there is only one uploader instance writing to VRAM.
-    * - APUs: There is only one uploader instance writing to RAM. VRAM has the same perf on APUs.
-    * - Other chips: The const uploader writes to VRAM and the stream uploader writes to RAM.
-    */
    bool smart_access_memory = sscreen->info.smart_access_memory;
-   bool is_apu = !sscreen->info.has_dedicated_vram;
    sctx->b.stream_uploader =
       u_upload_create(&sctx->b, 1024 * 1024, 0,
-                      smart_access_memory && !is_apu ? PIPE_USAGE_DEFAULT : PIPE_USAGE_STREAM,
+                      smart_access_memory ? PIPE_USAGE_DEFAULT : PIPE_USAGE_STREAM,
                       SI_RESOURCE_FLAG_32BIT); /* same flags as const_uploader */
    if (!sctx->b.stream_uploader)
       goto fail;
 
-   if (smart_access_memory || is_apu) {
+   if (smart_access_memory) {
       sctx->b.const_uploader = sctx->b.stream_uploader;
    } else {
       sctx->b.const_uploader =
@@ -588,9 +580,6 @@ static struct pipe_context *si_create_context(struct pipe_screen *screen, unsign
       sctx->noop_dsa = util_blitter_get_noop_dsa_state(sctx->blitter);
       sctx->queued.named.dsa = sctx->noop_dsa;
 
-      sctx->no_velems_state = sctx->b.create_vertex_elements_state(&sctx->b, 0, NULL);
-      sctx->vertex_elements = sctx->no_velems_state;
-
       sctx->discard_rasterizer_state = util_blitter_get_discard_rasterizer_state(sctx->blitter);
       sctx->queued.named.rasterizer = sctx->discard_rasterizer_state;
 
@@ -1345,7 +1334,7 @@ static struct pipe_screen *radeonsi_screen_create_impl(struct radeon_winsys *ws,
 
    ac_print_shadowed_regs(&sscreen->info);
 
-   STATIC_ASSERT(sizeof(union si_vgt_stages_key) == 1);
+   STATIC_ASSERT(sizeof(union si_vgt_stages_key) == 4);
    return &sscreen->b;
 }
 
diff --git a/src/gallium/drivers/radeonsi/si_pipe.h b/src/gallium/drivers/radeonsi/si_pipe.h
index ba77a1ebf5e..339021a792d 100644
--- a/src/gallium/drivers/radeonsi/si_pipe.h
+++ b/src/gallium/drivers/radeonsi/si_pipe.h
@@ -820,30 +820,30 @@ struct si_shader_ctx_state {
 union si_vgt_param_key {
    struct {
 #if UTIL_ARCH_LITTLE_ENDIAN
-      uint16_t prim : 4;
-      uint16_t uses_instancing : 1;
-      uint16_t multi_instances_smaller_than_primgroup : 1;
-      uint16_t primitive_restart : 1;
-      uint16_t count_from_stream_output : 1;
-      uint16_t line_stipple_enabled : 1;
-      uint16_t uses_tess : 1;
-      uint16_t tess_uses_prim_id : 1;
-      uint16_t uses_gs : 1;
-      uint16_t _pad : 16 - SI_NUM_VGT_PARAM_KEY_BITS;
+      unsigned prim : 4;
+      unsigned uses_instancing : 1;
+      unsigned multi_instances_smaller_than_primgroup : 1;
+      unsigned primitive_restart : 1;
+      unsigned count_from_stream_output : 1;
+      unsigned line_stipple_enabled : 1;
+      unsigned uses_tess : 1;
+      unsigned tess_uses_prim_id : 1;
+      unsigned uses_gs : 1;
+      unsigned _pad : 32 - SI_NUM_VGT_PARAM_KEY_BITS;
 #else /* UTIL_ARCH_BIG_ENDIAN */
-      uint16_t _pad : 16 - SI_NUM_VGT_PARAM_KEY_BITS;
-      uint16_t uses_gs : 1;
-      uint16_t tess_uses_prim_id : 1;
-      uint16_t uses_tess : 1;
-      uint16_t line_stipple_enabled : 1;
-      uint16_t count_from_stream_output : 1;
-      uint16_t primitive_restart : 1;
-      uint16_t multi_instances_smaller_than_primgroup : 1;
-      uint16_t uses_instancing : 1;
-      uint16_t prim : 4;
+      unsigned _pad : 32 - SI_NUM_VGT_PARAM_KEY_BITS;
+      unsigned uses_gs : 1;
+      unsigned tess_uses_prim_id : 1;
+      unsigned uses_tess : 1;
+      unsigned line_stipple_enabled : 1;
+      unsigned count_from_stream_output : 1;
+      unsigned primitive_restart : 1;
+      unsigned multi_instances_smaller_than_primgroup : 1;
+      unsigned uses_instancing : 1;
+      unsigned prim : 4;
 #endif
    } u;
-   uint16_t index;
+   uint32_t index;
 };
 
 #define SI_NUM_VGT_STAGES_KEY_BITS 6
@@ -855,24 +855,24 @@ union si_vgt_param_key {
 union si_vgt_stages_key {
    struct {
 #if UTIL_ARCH_LITTLE_ENDIAN
-      uint8_t tess : 1;
-      uint8_t gs : 1;
-      uint8_t ngg_gs_fast_launch : 1;
-      uint8_t ngg_passthrough : 1;
-      uint8_t ngg : 1;       /* gfx10+ */
-      uint8_t streamout : 1; /* only used with NGG */
-      uint8_t _pad : 8 - SI_NUM_VGT_STAGES_KEY_BITS;
+      unsigned tess : 1;
+      unsigned gs : 1;
+      unsigned ngg_gs_fast_launch : 1;
+      unsigned ngg_passthrough : 1;
+      unsigned ngg : 1;       /* gfx10+ */
+      unsigned streamout : 1; /* only used with NGG */
+      unsigned _pad : 32 - SI_NUM_VGT_STAGES_KEY_BITS;
 #else /* UTIL_ARCH_BIG_ENDIAN */
-      uint8_t _pad : 8 - SI_NUM_VGT_STAGES_KEY_BITS;
-      uint8_t streamout : 1;
-      uint8_t ngg : 1;
-      uint8_t ngg_passthrough : 1;
-      uint8_t ngg_gs_fast_launch : 1;
-      uint8_t gs : 1;
-      uint8_t tess : 1;
+      unsigned _pad : 32 - SI_NUM_VGT_STAGES_KEY_BITS;
+      unsigned streamout : 1;
+      unsigned ngg : 1;
+      unsigned ngg_passthrough : 1;
+      unsigned ngg_gs_fast_launch : 1;
+      unsigned gs : 1;
+      unsigned tess : 1;
 #endif
    } u;
-   uint8_t index;
+   uint32_t index;
 };
 
 struct si_texture_handle {
@@ -941,7 +941,6 @@ struct si_context {
    struct blitter_context *blitter;
    void *noop_blend;
    void *noop_dsa;
-   void *no_velems_state;
    void *discard_rasterizer_state;
    void *custom_dsa_flush;
    void *custom_blend_resolve;
@@ -1073,6 +1072,8 @@ struct si_context {
    unsigned descriptors_dirty;
    unsigned shader_pointers_dirty;
    unsigned shader_needs_decompress_mask;
+   unsigned shader_has_inlinable_uniforms_mask;
+   unsigned inlinable_uniforms_dirty_mask;
    unsigned inlinable_uniforms_valid_mask;
    uint32_t inlinable_uniforms[SI_NUM_SHADERS][MAX_INLINABLE_UNIFORMS];
    struct si_buffer_resources rw_buffers;
@@ -1231,8 +1232,11 @@ struct si_context {
    /* Misc stats. */
    unsigned num_draw_calls;
    unsigned num_decompress_calls;
+   unsigned num_mrt_draw_calls;
    unsigned num_prim_restart_calls;
+   unsigned num_spill_draw_calls;
    unsigned num_compute_calls;
+   unsigned num_spill_compute_calls;
    unsigned num_cp_dma_calls;
    unsigned num_vs_flushes;
    unsigned num_ps_flushes;
@@ -1397,8 +1401,9 @@ void si_cp_dma_copy_buffer(struct si_context *sctx, struct pipe_resource *dst,
                            struct pipe_resource *src, uint64_t dst_offset, uint64_t src_offset,
                            unsigned size, unsigned user_flags, enum si_coherency coher,
                            enum si_cache_policy cache_policy);
-void si_cp_dma_prefetch(struct si_context *sctx, struct pipe_resource *buf,
-                        unsigned offset, unsigned size);
+void cik_prefetch_TC_L2_async(struct si_context *sctx, struct pipe_resource *buf, uint64_t offset,
+                              unsigned size);
+void cik_emit_prefetch_L2(struct si_context *sctx, bool vertex_stage_only);
 void si_test_gds(struct si_context *sctx);
 void si_cp_write_data(struct si_context *sctx, struct si_resource *buf, unsigned offset,
                       unsigned size, unsigned dst_sel, unsigned engine, const void *data);
@@ -1445,10 +1450,6 @@ void si_allocate_gds(struct si_context *ctx);
 void si_set_tracked_regs_to_clear_state(struct si_context *ctx);
 void si_begin_new_gfx_cs(struct si_context *ctx, bool first_cs);
 void si_need_gfx_cs_space(struct si_context *ctx, unsigned num_draws);
-void si_emit_surface_sync(struct si_context *sctx, struct radeon_cmdbuf *cs,
-                          unsigned cp_coher_cntl);
-void gfx10_emit_cache_flush(struct si_context *sctx, struct radeon_cmdbuf *cs);
-void si_emit_cache_flush(struct si_context *sctx, struct radeon_cmdbuf *cs);
 
 /* si_gpu_load.c */
 void si_gpu_load_kill_thread(struct si_screen *sscreen);
@@ -1490,7 +1491,6 @@ void si_init_compiler(struct si_screen *sscreen, struct ac_llvm_compiler *compil
 /* si_perfcounters.c */
 void si_init_perfcounters(struct si_screen *screen);
 void si_destroy_perfcounters(struct si_screen *screen);
-void si_inhibit_clockgating(struct si_context *sctx, struct radeon_cmdbuf *cs, bool inhibit);
 
 /* si_query.c */
 void si_init_screen_query_functions(struct si_screen *sscreen);
@@ -1676,24 +1676,16 @@ static inline void si_mark_atom_dirty(struct si_context *sctx, struct si_atom *a
    si_set_atom_dirty(sctx, atom, true);
 }
 
-/* This should be evaluated at compile time if all parameters except sctx are constants. */
-static ALWAYS_INLINE struct si_shader_ctx_state *
-si_get_vs_inline(struct si_context *sctx, enum si_has_tess has_tess, enum si_has_gs has_gs)
+static inline struct si_shader_ctx_state *si_get_vs(struct si_context *sctx)
 {
-   if (has_gs)
+   if (sctx->gs_shader.cso)
       return &sctx->gs_shader;
-   if (has_tess)
+   if (sctx->tes_shader.cso)
       return &sctx->tes_shader;
 
    return &sctx->vs_shader;
 }
 
-static inline struct si_shader_ctx_state *si_get_vs(struct si_context *sctx)
-{
-   return si_get_vs_inline(sctx, sctx->tes_shader.cso ? TESS_ON : TESS_OFF,
-                           sctx->gs_shader.cso ? GS_ON : GS_OFF);
-}
-
 static inline struct si_shader_info *si_get_vs_info(struct si_context *sctx)
 {
    struct si_shader_ctx_state *vs = si_get_vs(sctx);
@@ -1701,6 +1693,15 @@ static inline struct si_shader_info *si_get_vs_info(struct si_context *sctx)
    return vs->cso ? &vs->cso->info : NULL;
 }
 
+static inline struct si_shader *si_get_vs_state(struct si_context *sctx)
+{
+   if (sctx->gs_shader.cso && sctx->gs_shader.current && !sctx->gs_shader.current->key.as_ngg)
+      return sctx->gs_shader.cso->gs_copy_shader;
+
+   struct si_shader_ctx_state *vs = si_get_vs(sctx);
+   return vs->current ? vs->current : NULL;
+}
+
 static inline bool si_can_dump_shader(struct si_screen *sscreen, gl_shader_stage stage)
 {
    return sscreen->debug_flags & (1 << stage);
diff --git a/src/gallium/drivers/radeonsi/si_query.c b/src/gallium/drivers/radeonsi/si_query.c
index a109501e179..3437681eb9e 100644
--- a/src/gallium/drivers/radeonsi/si_query.c
+++ b/src/gallium/drivers/radeonsi/si_query.c
@@ -126,12 +126,21 @@ static bool si_query_sw_begin(struct si_context *sctx, struct si_query *squery)
    case SI_QUERY_DECOMPRESS_CALLS:
       query->begin_result = sctx->num_decompress_calls;
       break;
+   case SI_QUERY_MRT_DRAW_CALLS:
+      query->begin_result = sctx->num_mrt_draw_calls;
+      break;
    case SI_QUERY_PRIM_RESTART_CALLS:
       query->begin_result = sctx->num_prim_restart_calls;
       break;
+   case SI_QUERY_SPILL_DRAW_CALLS:
+      query->begin_result = sctx->num_spill_draw_calls;
+      break;
    case SI_QUERY_COMPUTE_CALLS:
       query->begin_result = sctx->num_compute_calls;
       break;
+   case SI_QUERY_SPILL_COMPUTE_CALLS:
+      query->begin_result = sctx->num_spill_compute_calls;
+      break;
    case SI_QUERY_CP_DMA_CALLS:
       query->begin_result = sctx->num_cp_dma_calls;
       break;
@@ -292,12 +301,21 @@ static bool si_query_sw_end(struct si_context *sctx, struct si_query *squery)
    case SI_QUERY_DECOMPRESS_CALLS:
       query->end_result = sctx->num_decompress_calls;
       break;
+   case SI_QUERY_MRT_DRAW_CALLS:
+      query->end_result = sctx->num_mrt_draw_calls;
+      break;
    case SI_QUERY_PRIM_RESTART_CALLS:
       query->end_result = sctx->num_prim_restart_calls;
       break;
+   case SI_QUERY_SPILL_DRAW_CALLS:
+      query->end_result = sctx->num_spill_draw_calls;
+      break;
    case SI_QUERY_COMPUTE_CALLS:
       query->end_result = sctx->num_compute_calls;
       break;
+   case SI_QUERY_SPILL_COMPUTE_CALLS:
+      query->end_result = sctx->num_spill_compute_calls;
+      break;
    case SI_QUERY_CP_DMA_CALLS:
       query->end_result = sctx->num_cp_dma_calls;
       break;
@@ -1652,8 +1670,11 @@ static struct pipe_driver_query_info si_driver_query_list[] = {
    X("num-shaders-created", NUM_SHADERS_CREATED, UINT64, CUMULATIVE),
    X("draw-calls", DRAW_CALLS, UINT64, AVERAGE),
    X("decompress-calls", DECOMPRESS_CALLS, UINT64, AVERAGE),
+   X("MRT-draw-calls", MRT_DRAW_CALLS, UINT64, AVERAGE),
    X("prim-restart-calls", PRIM_RESTART_CALLS, UINT64, AVERAGE),
+   X("spill-draw-calls", SPILL_DRAW_CALLS, UINT64, AVERAGE),
    X("compute-calls", COMPUTE_CALLS, UINT64, AVERAGE),
+   X("spill-compute-calls", SPILL_COMPUTE_CALLS, UINT64, AVERAGE),
    X("cp-dma-calls", CP_DMA_CALLS, UINT64, AVERAGE),
    X("num-vs-flushes", NUM_VS_FLUSHES, UINT64, AVERAGE),
    X("num-ps-flushes", NUM_PS_FLUSHES, UINT64, AVERAGE),
diff --git a/src/gallium/drivers/radeonsi/si_query.h b/src/gallium/drivers/radeonsi/si_query.h
index 48485c245f6..9bdac5f83fc 100644
--- a/src/gallium/drivers/radeonsi/si_query.h
+++ b/src/gallium/drivers/radeonsi/si_query.h
@@ -44,8 +44,11 @@ enum
 {
    SI_QUERY_DRAW_CALLS = PIPE_QUERY_DRIVER_SPECIFIC,
    SI_QUERY_DECOMPRESS_CALLS,
+   SI_QUERY_MRT_DRAW_CALLS,
    SI_QUERY_PRIM_RESTART_CALLS,
+   SI_QUERY_SPILL_DRAW_CALLS,
    SI_QUERY_COMPUTE_CALLS,
+   SI_QUERY_SPILL_COMPUTE_CALLS,
    SI_QUERY_CP_DMA_CALLS,
    SI_QUERY_NUM_VS_FLUSHES,
    SI_QUERY_NUM_PS_FLUSHES,
diff --git a/src/gallium/drivers/radeonsi/si_sqtt.c b/src/gallium/drivers/radeonsi/si_sqtt.c
index 1366430cff8..9bd1483ae31 100644
--- a/src/gallium/drivers/radeonsi/si_sqtt.c
+++ b/src/gallium/drivers/radeonsi/si_sqtt.c
@@ -378,8 +378,6 @@ si_thread_trace_start(struct si_context *sctx, int family, struct radeon_cmdbuf
       SI_CONTEXT_INV_L2;
    sctx->emit_cache_flush(sctx, cs);
 
-   si_inhibit_clockgating(sctx, cs, true);
-
    /* Enable SQG events that collects thread trace data. */
    si_emit_spi_config_cntl(sctx, cs, true);
 
@@ -420,8 +418,6 @@ si_thread_trace_stop(struct si_context *sctx, int family, struct radeon_cmdbuf *
 
    /* Restore previous state by disabling SQG events. */
    si_emit_spi_config_cntl(sctx, cs, false);
-
-   si_inhibit_clockgating(sctx, cs, false);
 }
 
 
diff --git a/src/gallium/drivers/radeonsi/si_state.c b/src/gallium/drivers/radeonsi/si_state.c
index 278c9a733f0..9e3132e8c70 100644
--- a/src/gallium/drivers/radeonsi/si_state.c
+++ b/src/gallium/drivers/radeonsi/si_state.c
@@ -715,6 +715,7 @@ static void si_set_clip_state(struct pipe_context *ctx, const struct pipe_clip_s
    cb.buffer_offset = 0;
    cb.buffer_size = 4 * 4 * 8;
    si_set_rw_buffer(sctx, SI_VS_CONST_CLIP_PLANES, &cb);
+   pipe_resource_reference(&cb.buffer, NULL);
 }
 
 static void si_emit_clip_state(struct si_context *sctx)
@@ -727,7 +728,7 @@ static void si_emit_clip_state(struct si_context *sctx)
 
 static void si_emit_clip_regs(struct si_context *sctx)
 {
-   struct si_shader *vs = si_get_vs(sctx)->current;
+   struct si_shader *vs = si_get_vs_state(sctx);
    struct si_shader_selector *vs_sel = vs->selector;
    struct si_shader_info *info = &vs_sel->info;
    struct si_state_rasterizer *rs = sctx->queued.named.rasterizer;
@@ -1100,6 +1101,13 @@ static uint32_t si_translate_stencil_op(int s_op)
    return 0;
 }
 
+static bool si_dsa_writes_stencil(const struct pipe_stencil_state *s)
+{
+   return s->enabled && s->writemask &&
+          (s->fail_op != PIPE_STENCIL_OP_KEEP || s->zfail_op != PIPE_STENCIL_OP_KEEP ||
+           s->zpass_op != PIPE_STENCIL_OP_KEEP);
+}
+
 static bool si_order_invariant_stencil_op(enum pipe_stencil_op op)
 {
    /* REPLACE is normally order invariant, except when the stencil
@@ -1187,7 +1195,8 @@ static void *si_create_dsa_state(struct pipe_context *ctx,
    dsa->depth_write_enabled = state->depth_enabled && state->depth_writemask;
    dsa->stencil_enabled = state->stencil[0].enabled;
    dsa->stencil_write_enabled =
-      (util_writes_stencil(&state->stencil[0]) || util_writes_stencil(&state->stencil[1]));
+      state->stencil[0].enabled &&
+      (si_dsa_writes_stencil(&state->stencil[0]) || si_dsa_writes_stencil(&state->stencil[1]));
    dsa->db_can_write = dsa->depth_write_enabled || dsa->stencil_write_enabled;
 
    bool zfunc_is_ordered =
@@ -4762,11 +4771,8 @@ static void si_bind_vertex_elements(struct pipe_context *ctx, void *state)
    struct si_vertex_elements *old = sctx->vertex_elements;
    struct si_vertex_elements *v = (struct si_vertex_elements *)state;
 
-   if (!v)
-      v = sctx->no_velems_state;
-
    sctx->vertex_elements = v;
-   sctx->num_vertex_elements = v->count;
+   sctx->num_vertex_elements = v ? v->count : 0;
 
    if (sctx->num_vertex_elements) {
       sctx->vertex_buffers_dirty = true;
@@ -4775,24 +4781,24 @@ static void si_bind_vertex_elements(struct pipe_context *ctx, void *state)
       sctx->vertex_buffer_user_sgprs_dirty = false;
    }
 
-   if (old->count != v->count ||
-       old->uses_instance_divisors != v->uses_instance_divisors ||
-       /* we don't check which divisors changed */
-       v->uses_instance_divisors ||
-       (old->vb_alignment_check_mask ^ v->vb_alignment_check_mask) &
-       sctx->vertex_buffer_unaligned ||
-       ((v->vb_alignment_check_mask & sctx->vertex_buffer_unaligned) &&
-        memcmp(old->vertex_buffer_index, v->vertex_buffer_index,
-               sizeof(v->vertex_buffer_index[0]) * v->count)) ||
-       /* fix_fetch_{always,opencode,unaligned} and hw_load_is_dword are
-        * functions of fix_fetch and the src_offset alignment.
-        * If they change and fix_fetch doesn't, it must be due to different
-        * src_offset alignment, which is reflected in fix_fetch_opencode. */
-       old->fix_fetch_opencode != v->fix_fetch_opencode ||
-       memcmp(old->fix_fetch, v->fix_fetch, sizeof(v->fix_fetch[0]) * v->count))
+   if (v && (!old || old->count != v->count ||
+             old->uses_instance_divisors != v->uses_instance_divisors ||
+             /* we don't check which divisors changed */
+             v->uses_instance_divisors ||
+             (old->vb_alignment_check_mask ^ v->vb_alignment_check_mask) &
+                sctx->vertex_buffer_unaligned ||
+             ((v->vb_alignment_check_mask & sctx->vertex_buffer_unaligned) &&
+              memcmp(old->vertex_buffer_index, v->vertex_buffer_index,
+                     sizeof(v->vertex_buffer_index[0]) * v->count)) ||
+             /* fix_fetch_{always,opencode,unaligned} and hw_load_is_dword are
+              * functions of fix_fetch and the src_offset alignment.
+              * If they change and fix_fetch doesn't, it must be due to different
+              * src_offset alignment, which is reflected in fix_fetch_opencode. */
+             old->fix_fetch_opencode != v->fix_fetch_opencode ||
+             memcmp(old->fix_fetch, v->fix_fetch, sizeof(v->fix_fetch[0]) * v->count)))
       sctx->do_update_shaders = true;
 
-   if (v->instance_divisor_is_fetched) {
+   if (v && v->instance_divisor_is_fetched) {
       struct pipe_constant_buffer cb;
 
       cb.buffer = &v->instance_divisor_factor_buffer->b.b;
@@ -4808,9 +4814,10 @@ static void si_delete_vertex_element(struct pipe_context *ctx, void *state)
    struct si_context *sctx = (struct si_context *)ctx;
    struct si_vertex_elements *v = (struct si_vertex_elements *)state;
 
-   if (sctx->vertex_elements == state)
-      si_bind_vertex_elements(ctx, sctx->no_velems_state);
-
+   if (sctx->vertex_elements == state) {
+      sctx->vertex_elements = NULL;
+      sctx->num_vertex_elements = 0;
+   }
    si_resource_reference(&v->instance_divisor_factor_buffer, NULL);
    FREE(state);
 }
@@ -4846,8 +4853,10 @@ static void si_set_vertex_buffers(struct pipe_context *ctx, unsigned start_slot,
             si_resource(buf)->bind_history |= PIPE_BIND_VERTEX_BUFFER;
       }
    } else {
-      for (i = 0; i < count; i++)
+      for (i = 0; i < count; i++) {
          pipe_resource_reference(&dst[i].buffer.resource, NULL);
+      }
+      unaligned &= ~updated_mask;
    }
    sctx->vertex_buffers_dirty = true;
    sctx->vertex_buffer_unaligned = (orig_unaligned & ~updated_mask) | unaligned;
@@ -4859,8 +4868,8 @@ static void si_set_vertex_buffers(struct pipe_context *ctx, unsigned start_slot,
     * whether buffers are at least dword-aligned, since that should always
     * be the case in well-behaved applications anyway.
     */
-   if ((sctx->vertex_elements->vb_alignment_check_mask &
-        (unaligned | orig_unaligned) & updated_mask))
+   if (sctx->vertex_elements && (sctx->vertex_elements->vb_alignment_check_mask &
+                                 (unaligned | orig_unaligned) & updated_mask))
       sctx->do_update_shaders = true;
 }
 
@@ -4879,11 +4888,14 @@ static void si_set_tess_state(struct pipe_context *ctx, const float default_oute
    memcpy(array + 4, default_inner_level, sizeof(float) * 2);
 
    cb.buffer = NULL;
-   cb.user_buffer = array;
-   cb.buffer_offset = 0;
+   cb.user_buffer = NULL;
    cb.buffer_size = sizeof(array);
 
+   si_upload_const_buffer(sctx, (struct si_resource **)&cb.buffer, (void *)array, sizeof(array),
+                          &cb.buffer_offset);
+
    si_set_rw_buffer(sctx, SI_HS_CONST_DEFAULT_TESS_LEVELS, &cb);
+   pipe_resource_reference(&cb.buffer, NULL);
 }
 
 static void si_texture_barrier(struct pipe_context *ctx, unsigned flags)
diff --git a/src/gallium/drivers/radeonsi/si_state.h b/src/gallium/drivers/radeonsi/si_state.h
index a293787487a..9eb798e0fd7 100644
--- a/src/gallium/drivers/radeonsi/si_state.h
+++ b/src/gallium/drivers/radeonsi/si_state.h
@@ -512,6 +512,8 @@ bool si_gfx_resources_check_encrypted(struct si_context *sctx);
 bool si_compute_resources_check_encrypted(struct si_context *sctx);
 void si_shader_pointers_mark_dirty(struct si_context *sctx);
 void si_add_all_descriptors_to_bo_list(struct si_context *sctx);
+void si_upload_const_buffer(struct si_context *sctx, struct si_resource **buf, const uint8_t *ptr,
+                            unsigned size, uint32_t *const_offset);
 void si_update_all_texture_descriptors(struct si_context *sctx);
 void si_shader_change_notify(struct si_context *sctx);
 void si_update_needs_color_decompress_masks(struct si_context *sctx);
@@ -587,7 +589,11 @@ unsigned si_get_input_prim(const struct si_shader_selector *gs);
 bool si_update_ngg(struct si_context *sctx);
 
 /* si_state_draw.c */
+void si_emit_surface_sync(struct si_context *sctx, struct radeon_cmdbuf *cs,
+                          unsigned cp_coher_cntl);
 void si_prim_discard_signal_next_compute_ib_start(struct si_context *sctx);
+void gfx10_emit_cache_flush(struct si_context *sctx, struct radeon_cmdbuf *cs);
+void si_emit_cache_flush(struct si_context *sctx, struct radeon_cmdbuf *cs);
 void si_trace_emit(struct si_context *sctx);
 void si_init_draw_functions(struct si_context *sctx);
 
diff --git a/src/gallium/drivers/radeonsi/si_state_draw.cpp b/src/gallium/drivers/radeonsi/si_state_draw.cpp
index e2beac6f7fa..c4042b53533 100644
--- a/src/gallium/drivers/radeonsi/si_state_draw.cpp
+++ b/src/gallium/drivers/radeonsi/si_state_draw.cpp
@@ -59,134 +59,6 @@ static unsigned si_conv_pipe_prim(unsigned mode)
    return prim_conv[mode];
 }
 
-static void si_prefetch_shader_async(struct si_context *sctx, struct si_pm4_state *state)
-{
-   struct pipe_resource *bo = &state->shader->bo->b.b;
-
-   si_cp_dma_prefetch(sctx, bo, 0, bo->width0);
-}
-
-static void si_prefetch_VBO_descriptors(struct si_context *sctx)
-{
-   if (!sctx->vertex_elements->vb_desc_list_alloc_size)
-      return;
-
-   si_cp_dma_prefetch(sctx, &sctx->vb_descriptors_buffer->b.b, sctx->vb_descriptors_offset,
-                            sctx->vertex_elements->vb_desc_list_alloc_size);
-}
-
-/**
- * Prefetch shaders and VBO descriptors.
- *
- * \param VS_ONLY   Whether only the the API VS and VBO descriptors should be prefetched.
- */
-template<chip_class GFX_VERSION, si_has_tess HAS_TESS, si_has_gs HAS_GS, si_has_ngg NGG, bool VS_ONLY>
-static void si_emit_prefetch_L2(struct si_context *sctx)
-{
-   unsigned mask = sctx->prefetch_L2_mask;
-
-   /* GFX6 doesn't support the L2 prefetch. */
-   if (GFX_VERSION < GFX7 || !mask)
-      return;
-
-   /* Prefetch shaders and VBO descriptors to TC L2. */
-   if (GFX_VERSION >= GFX9) {
-      /* Choose the right spot for the VBO prefetch. */
-      if (HAS_TESS) {
-         if (mask & SI_PREFETCH_HS)
-            si_prefetch_shader_async(sctx, sctx->queued.named.hs);
-         if (mask & SI_PREFETCH_VBO_DESCRIPTORS)
-            si_prefetch_VBO_descriptors(sctx);
-
-         if (VS_ONLY) {
-            sctx->prefetch_L2_mask &= ~(SI_PREFETCH_HS | SI_PREFETCH_VBO_DESCRIPTORS);
-            return;
-         }
-
-         if ((HAS_GS || NGG) && mask & SI_PREFETCH_GS)
-            si_prefetch_shader_async(sctx, sctx->queued.named.gs);
-         if (!NGG && mask & SI_PREFETCH_VS)
-            si_prefetch_shader_async(sctx, sctx->queued.named.vs);
-      } else if (HAS_GS || NGG) {
-         if (mask & SI_PREFETCH_GS)
-            si_prefetch_shader_async(sctx, sctx->queued.named.gs);
-         if (mask & SI_PREFETCH_VBO_DESCRIPTORS)
-            si_prefetch_VBO_descriptors(sctx);
-
-         if (VS_ONLY) {
-            sctx->prefetch_L2_mask &= ~(SI_PREFETCH_GS | SI_PREFETCH_VBO_DESCRIPTORS);
-            return;
-         }
-
-         if (!NGG && mask & SI_PREFETCH_VS)
-            si_prefetch_shader_async(sctx, sctx->queued.named.vs);
-      } else {
-         if (mask & SI_PREFETCH_VS)
-            si_prefetch_shader_async(sctx, sctx->queued.named.vs);
-         if (mask & SI_PREFETCH_VBO_DESCRIPTORS)
-            si_prefetch_VBO_descriptors(sctx);
-
-         if (VS_ONLY) {
-            sctx->prefetch_L2_mask &= ~(SI_PREFETCH_VS | SI_PREFETCH_VBO_DESCRIPTORS);
-            return;
-         }
-      }
-   } else {
-      /* GFX6-GFX8 */
-      /* Choose the right spot for the VBO prefetch. */
-      if (HAS_TESS) {
-         if (mask & SI_PREFETCH_LS)
-            si_prefetch_shader_async(sctx, sctx->queued.named.ls);
-         if (mask & SI_PREFETCH_VBO_DESCRIPTORS)
-            si_prefetch_VBO_descriptors(sctx);
-
-         if (VS_ONLY) {
-            sctx->prefetch_L2_mask &= ~(SI_PREFETCH_LS | SI_PREFETCH_VBO_DESCRIPTORS);
-            return;
-         }
-
-         if (mask & SI_PREFETCH_HS)
-            si_prefetch_shader_async(sctx, sctx->queued.named.hs);
-         if (mask & SI_PREFETCH_ES)
-            si_prefetch_shader_async(sctx, sctx->queued.named.es);
-         if (mask & SI_PREFETCH_GS)
-            si_prefetch_shader_async(sctx, sctx->queued.named.gs);
-         if (mask & SI_PREFETCH_VS)
-            si_prefetch_shader_async(sctx, sctx->queued.named.vs);
-      } else if (HAS_GS) {
-         if (mask & SI_PREFETCH_ES)
-            si_prefetch_shader_async(sctx, sctx->queued.named.es);
-         if (mask & SI_PREFETCH_VBO_DESCRIPTORS)
-            si_prefetch_VBO_descriptors(sctx);
-
-         if (VS_ONLY) {
-            sctx->prefetch_L2_mask &= ~(SI_PREFETCH_ES | SI_PREFETCH_VBO_DESCRIPTORS);
-            return;
-         }
-
-         if (mask & SI_PREFETCH_GS)
-            si_prefetch_shader_async(sctx, sctx->queued.named.gs);
-         if (mask & SI_PREFETCH_VS)
-            si_prefetch_shader_async(sctx, sctx->queued.named.vs);
-      } else {
-         if (mask & SI_PREFETCH_VS)
-            si_prefetch_shader_async(sctx, sctx->queued.named.vs);
-         if (mask & SI_PREFETCH_VBO_DESCRIPTORS)
-            si_prefetch_VBO_descriptors(sctx);
-
-         if (VS_ONLY) {
-            sctx->prefetch_L2_mask &= ~(SI_PREFETCH_VS | SI_PREFETCH_VBO_DESCRIPTORS);
-            return;
-         }
-      }
-   }
-
-   if (mask & SI_PREFETCH_PS)
-      si_prefetch_shader_async(sctx, sctx->queued.named.ps);
-
-   sctx->prefetch_L2_mask = 0;
-}
-
 /**
  * This calculates the LDS size for tessellation shaders (VS, TCS, TES).
  * LS.LDS_SIZE is shared by all 3 shader stages.
@@ -194,10 +66,10 @@ static void si_emit_prefetch_L2(struct si_context *sctx)
  * The information about LDS and other non-compile-time parameters is then
  * written to userdata SGPRs.
  */
-static void si_emit_derived_tess_state(struct si_context *sctx,
-                                       unsigned num_tcs_input_cp,
+static void si_emit_derived_tess_state(struct si_context *sctx, const struct pipe_draw_info *info,
                                        unsigned *num_patches)
 {
+   struct radeon_cmdbuf *cs = &sctx->gfx_cs;
    struct si_shader *ls_current;
    struct si_shader_selector *ls;
    /* The TES pointer will only be used for sctx->last_tcs.
@@ -207,6 +79,14 @@ static void si_emit_derived_tess_state(struct si_context *sctx,
    unsigned tess_uses_primid = sctx->ia_multi_vgt_param_key.u.tess_uses_prim_id;
    bool has_primid_instancing_bug = sctx->chip_class == GFX6 && sctx->screen->info.max_se == 1;
    unsigned tes_sh_base = sctx->shader_pointers.sh_base[PIPE_SHADER_TESS_EVAL];
+   unsigned num_tcs_input_cp = info->vertices_per_patch;
+   unsigned num_tcs_output_cp, num_tcs_inputs, num_tcs_outputs;
+   unsigned num_tcs_patch_outputs;
+   unsigned input_vertex_size, output_vertex_size, pervertex_output_patch_size;
+   unsigned input_patch_size, output_patch_size, output_patch0_offset;
+   unsigned perpatch_output_offset, lds_per_patch, lds_size;
+   unsigned tcs_in_layout, tcs_out_layout, tcs_out_offsets;
+   unsigned offchip_layout, target_lds_size, ls_hs_config;
 
    /* Since GFX9 has merged LS-HS in the TCS state, set LS = TCS. */
    if (sctx->chip_class >= GFX9) {
@@ -236,8 +116,7 @@ static void si_emit_derived_tess_state(struct si_context *sctx,
 
    /* This calculates how shader inputs and outputs among VS, TCS, and TES
     * are laid out in LDS. */
-   unsigned num_tcs_inputs = util_last_bit64(ls->outputs_written);
-   unsigned num_tcs_output_cp, num_tcs_outputs, num_tcs_patch_outputs;
+   num_tcs_inputs = util_last_bit64(ls->outputs_written);
 
    if (sctx->tcs_shader.cso) {
       num_tcs_outputs = util_last_bit64(tcs->outputs_written);
@@ -250,9 +129,8 @@ static void si_emit_derived_tess_state(struct si_context *sctx,
       num_tcs_patch_outputs = 2; /* TESSINNER + TESSOUTER */
    }
 
-   unsigned input_vertex_size = ls->lshs_vertex_stride;
-   unsigned output_vertex_size = num_tcs_outputs * 16;
-   unsigned input_patch_size;
+   input_vertex_size = ls->lshs_vertex_stride;
+   output_vertex_size = num_tcs_outputs * 16;
 
    /* Allocate LDS for TCS inputs only if it's used. */
    if (!ls_current->key.opt.same_patch_vertices ||
@@ -261,9 +139,8 @@ static void si_emit_derived_tess_state(struct si_context *sctx,
    else
       input_patch_size = 0;
 
-   unsigned pervertex_output_patch_size = num_tcs_output_cp * output_vertex_size;
-   unsigned output_patch_size = pervertex_output_patch_size + num_tcs_patch_outputs * 16;
-   unsigned lds_per_patch;
+   pervertex_output_patch_size = num_tcs_output_cp * output_vertex_size;
+   output_patch_size = pervertex_output_patch_size + num_tcs_patch_outputs * 16;
 
    /* Compute the LDS size per patch.
     *
@@ -296,7 +173,7 @@ static void si_emit_derived_tess_state(struct si_context *sctx,
     * Use 16K so that we can fit 2 workgroups on the same CU.
     */
    ASSERTED unsigned max_lds_size = 32 * 1024; /* hw limit */
-   unsigned target_lds_size = 16 * 1024; /* target at least 2 workgroups per CU, 16K each */
+   target_lds_size = 16 * 1024; /* target at least 2 workgroups per CU, 16K each */
    *num_patches = MIN2(*num_patches, target_lds_size / lds_per_patch);
    *num_patches = MAX2(*num_patches, 1);
    assert(*num_patches * lds_per_patch <= max_lds_size);
@@ -351,8 +228,8 @@ static void si_emit_derived_tess_state(struct si_context *sctx,
 
    sctx->last_num_patches = *num_patches;
 
-   unsigned output_patch0_offset = input_patch_size * *num_patches;
-   unsigned perpatch_output_offset = output_patch0_offset + pervertex_output_patch_size;
+   output_patch0_offset = input_patch_size * *num_patches;
+   perpatch_output_offset = output_patch0_offset + pervertex_output_patch_size;
 
    /* Compute userdata SGPRs. */
    assert(((input_vertex_size / 4) & ~0xff) == 0);
@@ -370,16 +247,16 @@ static void si_emit_derived_tess_state(struct si_context *sctx,
       si_resource(sctx->tess_rings_tmz) : si_resource(sctx->tess_rings))->gpu_address;
    assert((ring_va & u_bit_consecutive(0, 19)) == 0);
 
-   unsigned tcs_in_layout = S_VS_STATE_LS_OUT_PATCH_SIZE(input_patch_size / 4) |
-                            S_VS_STATE_LS_OUT_VERTEX_SIZE(input_vertex_size / 4);
-   unsigned tcs_out_layout = (output_patch_size / 4) | (num_tcs_input_cp << 13) | ring_va;
-   unsigned tcs_out_offsets = (output_patch0_offset / 16) | ((perpatch_output_offset / 16) << 16);
-   unsigned offchip_layout =
+   tcs_in_layout = S_VS_STATE_LS_OUT_PATCH_SIZE(input_patch_size / 4) |
+                   S_VS_STATE_LS_OUT_VERTEX_SIZE(input_vertex_size / 4);
+   tcs_out_layout = (output_patch_size / 4) | (num_tcs_input_cp << 13) | ring_va;
+   tcs_out_offsets = (output_patch0_offset / 16) | ((perpatch_output_offset / 16) << 16);
+   offchip_layout =
       (*num_patches - 1) | ((num_tcs_output_cp - 1) << 6) |
       ((pervertex_output_patch_size * *num_patches) << 11);
 
    /* Compute the LDS size. */
-   unsigned lds_size = lds_per_patch * *num_patches;
+   lds_size = lds_per_patch * *num_patches;
 
    if (sctx->chip_class >= GFX7) {
       assert(lds_size <= 65536);
@@ -398,8 +275,6 @@ static void si_emit_derived_tess_state(struct si_context *sctx,
     * been tested. */
    assert(ls_current->config.lds_size == 0);
 
-   struct radeon_cmdbuf *cs = &sctx->gfx_cs;
-
    if (sctx->chip_class >= GFX9) {
       unsigned hs_rsrc2 = ls_current->config.rsrc2;
 
@@ -444,10 +319,8 @@ static void si_emit_derived_tess_state(struct si_context *sctx,
    radeon_emit(cs, offchip_layout);
    radeon_emit(cs, ring_va);
 
-   unsigned ls_hs_config =
-         S_028B58_NUM_PATCHES(*num_patches) |
-         S_028B58_HS_NUM_INPUT_CP(num_tcs_input_cp) |
-         S_028B58_HS_NUM_OUTPUT_CP(num_tcs_output_cp);
+   ls_hs_config = S_028B58_NUM_PATCHES(*num_patches) | S_028B58_HS_NUM_INPUT_CP(num_tcs_input_cp) |
+                  S_028B58_HS_NUM_OUTPUT_CP(num_tcs_output_cp);
 
    if (sctx->last_ls_hs_config != ls_hs_config) {
       if (sctx->chip_class >= GFX7) {
@@ -467,8 +340,7 @@ static unsigned si_num_prims_for_vertices(enum pipe_prim_type prim,
    case PIPE_PRIM_PATCHES:
       return count / vertices_per_patch;
    case PIPE_PRIM_POLYGON:
-      /* It's a triangle fan with different edge flags. */
-      return count >= 3 ? count - 2 : 0;
+      return count >= 3;
    case SI_PRIM_RECTANGLE_LIST:
       return count / 3;
    default:
@@ -478,7 +350,7 @@ static unsigned si_num_prims_for_vertices(enum pipe_prim_type prim,
 
 static unsigned si_get_init_multi_vgt_param(struct si_screen *sscreen, union si_vgt_param_key *key)
 {
-   STATIC_ASSERT(sizeof(union si_vgt_param_key) == 2);
+   STATIC_ASSERT(sizeof(union si_vgt_param_key) == 4);
    unsigned max_primgroup_in_wave = 2;
 
    /* SWITCH_ON_EOP(0) is always preferable. */
@@ -634,28 +506,29 @@ static bool si_is_line_stipple_enabled(struct si_context *sctx)
           (rs->polygon_mode_is_lines || util_prim_is_lines(sctx->current_rast_prim));
 }
 
-static bool num_instanced_prims_less_than(const struct pipe_draw_indirect_info *indirect,
+static bool num_instanced_prims_less_than(const struct pipe_draw_info *info,
+                                          const struct pipe_draw_indirect_info *indirect,
                                           enum pipe_prim_type prim,
                                           unsigned min_vertex_count,
                                           unsigned instance_count,
-                                          unsigned num_prims,
-                                          ubyte vertices_per_patch)
+                                          unsigned num_prims)
 {
    if (indirect) {
       return indirect->buffer ||
              (instance_count > 1 && indirect->count_from_stream_output);
    } else {
       return instance_count > 1 &&
-             si_num_prims_for_vertices(prim, min_vertex_count, vertices_per_patch) < num_prims;
+             si_num_prims_for_vertices(prim, min_vertex_count, info->vertices_per_patch) < num_prims;
    }
 }
 
 template <chip_class GFX_VERSION, si_has_tess HAS_TESS, si_has_gs HAS_GS> ALWAYS_INLINE
 static unsigned si_get_ia_multi_vgt_param(struct si_context *sctx,
+                                          const struct pipe_draw_info *info,
                                           const struct pipe_draw_indirect_info *indirect,
                                           enum pipe_prim_type prim, unsigned num_patches,
                                           unsigned instance_count, bool primitive_restart,
-                                          unsigned min_vertex_count, ubyte vertices_per_patch)
+                                          unsigned min_vertex_count)
 {
    union si_vgt_param_key key = sctx->ia_multi_vgt_param_key;
    unsigned primgroup_size;
@@ -672,8 +545,7 @@ static unsigned si_get_ia_multi_vgt_param(struct si_context *sctx,
    key.u.prim = prim;
    key.u.uses_instancing = (indirect && indirect->buffer) || instance_count > 1;
    key.u.multi_instances_smaller_than_primgroup =
-      num_instanced_prims_less_than(indirect, prim, min_vertex_count, instance_count,
-                                    primgroup_size, vertices_per_patch);
+      num_instanced_prims_less_than(info, indirect, prim, min_vertex_count, instance_count, primgroup_size);
    key.u.primitive_restart = primitive_restart;
    key.u.count_from_stream_output = indirect && indirect->count_from_stream_output;
    key.u.line_stipple_enabled = si_is_line_stipple_enabled(sctx);
@@ -693,8 +565,7 @@ static unsigned si_get_ia_multi_vgt_param(struct si_context *sctx,
        */
       if (GFX_VERSION == GFX7 &&
           sctx->family == CHIP_HAWAII && G_028AA8_SWITCH_ON_EOI(ia_multi_vgt_param) &&
-          num_instanced_prims_less_than(indirect, prim, min_vertex_count, instance_count, 2,
-                                        vertices_per_patch))
+          num_instanced_prims_less_than(info, indirect, prim, min_vertex_count, instance_count, 2))
          sctx->flags |= SI_CONTEXT_VGT_FLUSH;
    }
 
@@ -728,7 +599,7 @@ static unsigned si_conv_prim_to_gs_out(unsigned mode)
 }
 
 /* rast_prim is the primitive type after GS. */
-template<chip_class GFX_VERSION, si_has_tess HAS_TESS, si_has_gs HAS_GS, si_has_ngg NGG> ALWAYS_INLINE
+template<si_has_gs HAS_GS, si_has_ngg NGG> ALWAYS_INLINE
 static void si_emit_rasterizer_prim_state(struct si_context *sctx)
 {
    struct radeon_cmdbuf *cs = &sctx->gfx_cs;
@@ -756,11 +627,11 @@ static void si_emit_rasterizer_prim_state(struct si_context *sctx)
       sctx->last_gs_out_prim = gs_out_prim;
    }
 
-   if (GFX_VERSION == GFX9 && initial_cdw != cs->current.cdw)
+   if (initial_cdw != cs->current.cdw)
       sctx->context_roll = true;
 
    if (NGG) {
-      struct si_shader *hw_vs = si_get_vs_inline(sctx, HAS_TESS, HAS_GS)->current;
+      struct si_shader *hw_vs = si_get_vs_state(sctx);
 
       if (hw_vs->uses_vs_state_provoking_vertex) {
          unsigned vtx_index = rs->flatshade_first ? 0 : gs_out_prim;
@@ -776,42 +647,41 @@ static void si_emit_rasterizer_prim_state(struct si_context *sctx)
    }
 }
 
-template <chip_class GFX_VERSION, si_has_tess HAS_TESS, si_has_gs HAS_GS, si_has_ngg NGG>
 ALWAYS_INLINE
-static void si_emit_vs_state(struct si_context *sctx, unsigned index_size)
+static void si_emit_vs_state(struct si_context *sctx, const struct pipe_draw_info *info)
 {
+   if (sctx->vs_shader.cso->info.uses_base_vertex) {
+      sctx->current_vs_state &= C_VS_STATE_INDEXED;
+      sctx->current_vs_state |= S_VS_STATE_INDEXED(!!info->index_size);
+   }
+
    if (sctx->num_vs_blit_sgprs) {
       /* Re-emit the state after we leave u_blitter. */
       sctx->last_vs_state = ~0;
       return;
    }
 
-   if (sctx->vs_shader.cso->info.uses_base_vertex) {
-      sctx->current_vs_state &= C_VS_STATE_INDEXED;
-      sctx->current_vs_state |= S_VS_STATE_INDEXED(!!index_size);
-   }
-
    if (sctx->current_vs_state != sctx->last_vs_state) {
       struct radeon_cmdbuf *cs = &sctx->gfx_cs;
 
       /* For the API vertex shader (VS_STATE_INDEXED, LS_OUT_*). */
-      unsigned vs_base = si_get_user_data_base(GFX_VERSION, HAS_TESS, HAS_GS, NGG,
-                                               PIPE_SHADER_VERTEX);
-      radeon_set_sh_reg(cs, vs_base + SI_SGPR_VS_STATE_BITS * 4,
-                        sctx->current_vs_state);
+      radeon_set_sh_reg(
+         cs, sctx->shader_pointers.sh_base[PIPE_SHADER_VERTEX] + SI_SGPR_VS_STATE_BITS * 4,
+         sctx->current_vs_state);
 
       /* Set CLAMP_VERTEX_COLOR and OUTPRIM in the last stage
        * before the rasterizer.
        *
        * For TES or the GS copy shader without NGG:
        */
-      if (vs_base != R_00B130_SPI_SHADER_USER_DATA_VS_0) {
+      if (sctx->shader_pointers.sh_base[PIPE_SHADER_VERTEX] != R_00B130_SPI_SHADER_USER_DATA_VS_0) {
          radeon_set_sh_reg(cs, R_00B130_SPI_SHADER_USER_DATA_VS_0 + SI_SGPR_VS_STATE_BITS * 4,
                            sctx->current_vs_state);
       }
 
       /* For NGG: */
-      if (GFX_VERSION >= GFX10 && vs_base != R_00B230_SPI_SHADER_USER_DATA_GS_0) {
+      if (sctx->screen->use_ngg &&
+          sctx->shader_pointers.sh_base[PIPE_SHADER_VERTEX] != R_00B230_SPI_SHADER_USER_DATA_GS_0) {
          radeon_set_sh_reg(cs, R_00B230_SPI_SHADER_USER_DATA_GS_0 + SI_SGPR_VS_STATE_BITS * 4,
                            sctx->current_vs_state);
       }
@@ -829,19 +699,19 @@ static bool si_prim_restart_index_changed(struct si_context *sctx, bool primitiv
 }
 
 template <chip_class GFX_VERSION, si_has_tess HAS_TESS, si_has_gs HAS_GS> ALWAYS_INLINE
-static void si_emit_ia_multi_vgt_param(struct si_context *sctx,
+static void si_emit_ia_multi_vgt_param(struct si_context *sctx, const struct pipe_draw_info *info,
                                        const struct pipe_draw_indirect_info *indirect,
                                        enum pipe_prim_type prim, unsigned num_patches,
                                        unsigned instance_count, bool primitive_restart,
-                                       unsigned min_vertex_count, ubyte vertices_per_patch)
+                                       unsigned min_vertex_count)
 {
    struct radeon_cmdbuf *cs = &sctx->gfx_cs;
    unsigned ia_multi_vgt_param;
 
    ia_multi_vgt_param =
       si_get_ia_multi_vgt_param<GFX_VERSION, HAS_TESS, HAS_GS>
-         (sctx, indirect, prim, num_patches, instance_count, primitive_restart,
-          min_vertex_count, vertices_per_patch);
+         (sctx, info, indirect, prim, num_patches, instance_count, primitive_restart,
+          min_vertex_count);
 
    /* Draw state. */
    if (ia_multi_vgt_param != sctx->last_multi_vgt_param) {
@@ -872,7 +742,7 @@ static void gfx10_emit_ge_cntl(struct si_context *sctx, unsigned num_patches)
                    S_03096C_VERT_GRP_SIZE(0) |
                    S_03096C_BREAK_WAVE_AT_EOI(key.u.tess_uses_prim_id);
       } else {
-         ge_cntl = si_get_vs_inline(sctx, HAS_TESS, HAS_GS)->current->ge_cntl;
+         ge_cntl = si_get_vs_state(sctx)->ge_cntl;
       }
    } else {
       unsigned primgroup_size;
@@ -903,25 +773,23 @@ static void gfx10_emit_ge_cntl(struct si_context *sctx, unsigned num_patches)
 }
 
 template <chip_class GFX_VERSION, si_has_tess HAS_TESS, si_has_gs HAS_GS, si_has_ngg NGG> ALWAYS_INLINE
-static void si_emit_draw_registers(struct si_context *sctx,
+static void si_emit_draw_registers(struct si_context *sctx, const struct pipe_draw_info *info,
                                    const struct pipe_draw_indirect_info *indirect,
                                    enum pipe_prim_type prim, unsigned num_patches,
-                                   unsigned instance_count, ubyte vertices_per_patch,
-                                   bool primitive_restart, unsigned restart_index,
+                                   unsigned instance_count, bool primitive_restart,
                                    unsigned min_vertex_count)
 {
    struct radeon_cmdbuf *cs = &sctx->gfx_cs;
+   unsigned vgt_prim = si_conv_pipe_prim(prim);
 
    if (GFX_VERSION >= GFX10)
       gfx10_emit_ge_cntl<GFX_VERSION, HAS_TESS, HAS_GS, NGG>(sctx, num_patches);
    else
       si_emit_ia_multi_vgt_param<GFX_VERSION, HAS_TESS, HAS_GS>
-         (sctx, indirect, prim, num_patches, instance_count, primitive_restart,
-          min_vertex_count, vertices_per_patch);
-
-   if (prim != sctx->last_prim) {
-      unsigned vgt_prim = si_conv_pipe_prim(prim);
+         (sctx, info, indirect, prim, num_patches, instance_count, primitive_restart,
+          min_vertex_count);
 
+   if (vgt_prim != sctx->last_prim) {
       if (GFX_VERSION >= GFX10)
          radeon_set_uconfig_reg(cs, R_030908_VGT_PRIMITIVE_TYPE, vgt_prim);
       else if (GFX_VERSION >= GFX7)
@@ -929,7 +797,7 @@ static void si_emit_draw_registers(struct si_context *sctx,
       else
          radeon_set_config_reg(cs, R_008958_VGT_PRIMITIVE_TYPE, vgt_prim);
 
-      sctx->last_prim = prim;
+      sctx->last_prim = vgt_prim;
    }
 
    /* Primitive restart. */
@@ -941,11 +809,10 @@ static void si_emit_draw_registers(struct si_context *sctx,
 
       sctx->last_primitive_restart_en = primitive_restart;
    }
-   if (si_prim_restart_index_changed(sctx, primitive_restart, restart_index)) {
-      radeon_set_context_reg(cs, R_02840C_VGT_MULTI_PRIM_IB_RESET_INDX, restart_index);
-      sctx->last_restart_index = restart_index;
-      if (GFX_VERSION == GFX9)
-         sctx->context_roll = true;
+   if (si_prim_restart_index_changed(sctx, primitive_restart, info->restart_index)) {
+      radeon_set_context_reg(cs, R_02840C_VGT_MULTI_PRIM_IB_RESET_INDX, info->restart_index);
+      sctx->last_restart_index = info->restart_index;
+      sctx->context_roll = true;
    }
 }
 
@@ -968,14 +835,17 @@ static void si_emit_draw_packets(struct si_context *sctx, const struct pipe_draw
                                  bool dispatch_prim_discard_cs, unsigned original_index_size)
 {
    struct radeon_cmdbuf *cs = &sctx->gfx_cs;
+   unsigned sh_base_reg = sctx->shader_pointers.sh_base[PIPE_SHADER_VERTEX];
+   bool render_cond_bit = sctx->render_cond && !sctx->render_cond_force_off;
+   uint32_t index_max_size = 0;
+   uint32_t use_opaque = 0;
+   uint64_t index_va = 0;
 
    if (unlikely(sctx->thread_trace_enabled)) {
       si_sqtt_write_event_marker(sctx, &sctx->gfx_cs, EventCmdDraw,
                                  UINT_MAX, UINT_MAX, UINT_MAX);
    }
 
-   uint32_t use_opaque = 0;
-
    if (indirect && indirect->count_from_stream_output) {
       struct si_streamout_target *t = (struct si_streamout_target *)indirect->count_from_stream_output;
 
@@ -987,9 +857,6 @@ static void si_emit_draw_packets(struct si_context *sctx, const struct pipe_draw
       indirect = NULL;
    }
 
-   uint32_t index_max_size = 0;
-   uint64_t index_va = 0;
-
    /* draw packet */
    if (index_size) {
       /* Register shadowing doesn't shadow INDEX_TYPE. */
@@ -1048,9 +915,6 @@ static void si_emit_draw_packets(struct si_context *sctx, const struct pipe_draw
          sctx->last_index_size = -1;
    }
 
-   unsigned sh_base_reg = sctx->shader_pointers.sh_base[PIPE_SHADER_VERTEX];
-   bool render_cond_bit = sctx->render_cond && !sctx->render_cond_force_off;
-
    if (indirect) {
       assert(num_draws == 1);
       uint64_t indirect_va = si_resource(indirect->buffer)->gpu_address;
@@ -1115,6 +979,8 @@ static void si_emit_draw_packets(struct si_context *sctx, const struct pipe_draw
          radeon_emit(cs, di_src_sel);
       }
    } else {
+      int base_vertex;
+
       /* Register shadowing requires that we always emit PKT3_NUM_INSTANCES. */
       if (sctx->shadowed_regs ||
           sctx->last_instance_count == SI_INSTANCE_COUNT_UNKNOWN ||
@@ -1125,7 +991,7 @@ static void si_emit_draw_packets(struct si_context *sctx, const struct pipe_draw
       }
 
       /* Base vertex and start instance. */
-      int base_vertex = original_index_size ? info->index_bias : draws[0].start;
+      base_vertex = original_index_size ? info->index_bias : draws[0].start;
 
       bool set_draw_id = sctx->vs_uses_draw_id;
       bool set_base_instance = sctx->vs_uses_base_instance;
@@ -1269,6 +1135,37 @@ static void si_emit_draw_packets(struct si_context *sctx, const struct pipe_draw
    EMIT_SQTT_END_DRAW;
 }
 
+extern "C"
+void si_emit_surface_sync(struct si_context *sctx, struct radeon_cmdbuf *cs, unsigned cp_coher_cntl)
+{
+   bool compute_ib = !sctx->has_graphics || cs == &sctx->prim_discard_compute_cs;
+
+   assert(sctx->chip_class <= GFX9);
+
+   if (sctx->chip_class == GFX9 || compute_ib) {
+      /* Flush caches and wait for the caches to assert idle. */
+      radeon_emit(cs, PKT3(PKT3_ACQUIRE_MEM, 5, 0));
+      radeon_emit(cs, cp_coher_cntl); /* CP_COHER_CNTL */
+      radeon_emit(cs, 0xffffffff);    /* CP_COHER_SIZE */
+      radeon_emit(cs, 0xffffff);      /* CP_COHER_SIZE_HI */
+      radeon_emit(cs, 0);             /* CP_COHER_BASE */
+      radeon_emit(cs, 0);             /* CP_COHER_BASE_HI */
+      radeon_emit(cs, 0x0000000A);    /* POLL_INTERVAL */
+   } else {
+      /* ACQUIRE_MEM is only required on a compute ring. */
+      radeon_emit(cs, PKT3(PKT3_SURFACE_SYNC, 3, 0));
+      radeon_emit(cs, cp_coher_cntl); /* CP_COHER_CNTL */
+      radeon_emit(cs, 0xffffffff);    /* CP_COHER_SIZE */
+      radeon_emit(cs, 0);             /* CP_COHER_BASE */
+      radeon_emit(cs, 0x0000000A);    /* POLL_INTERVAL */
+   }
+
+   /* ACQUIRE_MEM has an implicit context roll if the current context
+    * is busy. */
+   if (!compute_ib)
+      sctx->context_roll = true;
+}
+
 extern "C"
 void si_prim_discard_signal_next_compute_ib_start(struct si_context *sctx)
 {
@@ -1295,12 +1192,430 @@ void si_prim_discard_signal_next_compute_ib_start(struct si_context *sctx)
    *sctx->last_pkt3_write_data = PKT3(PKT3_NOP, 3, 0);
 }
 
+void gfx10_emit_cache_flush(struct si_context *ctx, struct radeon_cmdbuf *cs)
+{
+   uint32_t gcr_cntl = 0;
+   unsigned cb_db_event = 0;
+   unsigned flags = ctx->flags;
+
+   if (!ctx->has_graphics) {
+      /* Only process compute flags. */
+      flags &= SI_CONTEXT_INV_ICACHE | SI_CONTEXT_INV_SCACHE | SI_CONTEXT_INV_VCACHE |
+               SI_CONTEXT_INV_L2 | SI_CONTEXT_WB_L2 | SI_CONTEXT_INV_L2_METADATA |
+               SI_CONTEXT_CS_PARTIAL_FLUSH;
+   }
+
+   /* We don't need these. */
+   assert(!(flags & (SI_CONTEXT_VGT_STREAMOUT_SYNC | SI_CONTEXT_FLUSH_AND_INV_DB_META)));
+
+   if (flags & SI_CONTEXT_VGT_FLUSH) {
+      radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
+      radeon_emit(cs, EVENT_TYPE(V_028A90_VGT_FLUSH) | EVENT_INDEX(0));
+   }
+
+   if (flags & SI_CONTEXT_FLUSH_AND_INV_CB)
+      ctx->num_cb_cache_flushes++;
+   if (flags & SI_CONTEXT_FLUSH_AND_INV_DB)
+      ctx->num_db_cache_flushes++;
+
+   if (flags & SI_CONTEXT_INV_ICACHE)
+      gcr_cntl |= S_586_GLI_INV(V_586_GLI_ALL);
+   if (flags & SI_CONTEXT_INV_SCACHE) {
+      /* TODO: When writing to the SMEM L1 cache, we need to set SEQ
+       * to FORWARD when both L1 and L2 are written out (WB or INV).
+       */
+      gcr_cntl |= S_586_GL1_INV(1) | S_586_GLK_INV(1);
+   }
+   if (flags & SI_CONTEXT_INV_VCACHE)
+      gcr_cntl |= S_586_GL1_INV(1) | S_586_GLV_INV(1);
+
+   /* The L2 cache ops are:
+    * - INV: - invalidate lines that reflect memory (were loaded from memory)
+    *        - don't touch lines that were overwritten (were stored by gfx clients)
+    * - WB: - don't touch lines that reflect memory
+    *       - write back lines that were overwritten
+    * - WB | INV: - invalidate lines that reflect memory
+    *             - write back lines that were overwritten
+    *
+    * GLM doesn't support WB alone. If WB is set, INV must be set too.
+    */
+   if (flags & SI_CONTEXT_INV_L2) {
+      /* Writeback and invalidate everything in L2. */
+      gcr_cntl |= S_586_GL2_INV(1) | S_586_GL2_WB(1) | S_586_GLM_INV(1) | S_586_GLM_WB(1);
+      ctx->num_L2_invalidates++;
+   } else if (flags & SI_CONTEXT_WB_L2) {
+      gcr_cntl |= S_586_GL2_WB(1) | S_586_GLM_WB(1) | S_586_GLM_INV(1);
+   } else if (flags & SI_CONTEXT_INV_L2_METADATA) {
+      gcr_cntl |= S_586_GLM_INV(1) | S_586_GLM_WB(1);
+   }
+
+   if (flags & (SI_CONTEXT_FLUSH_AND_INV_CB | SI_CONTEXT_FLUSH_AND_INV_DB)) {
+      if (flags & SI_CONTEXT_FLUSH_AND_INV_CB) {
+         /* Flush CMASK/FMASK/DCC. Will wait for idle later. */
+         radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
+         radeon_emit(cs, EVENT_TYPE(V_028A90_FLUSH_AND_INV_CB_META) | EVENT_INDEX(0));
+      }
+      if (flags & SI_CONTEXT_FLUSH_AND_INV_DB) {
+         /* Flush HTILE. Will wait for idle later. */
+         radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
+         radeon_emit(cs, EVENT_TYPE(V_028A90_FLUSH_AND_INV_DB_META) | EVENT_INDEX(0));
+      }
+
+      /* First flush CB/DB, then L1/L2. */
+      gcr_cntl |= S_586_SEQ(V_586_SEQ_FORWARD);
+
+      if ((flags & (SI_CONTEXT_FLUSH_AND_INV_CB | SI_CONTEXT_FLUSH_AND_INV_DB)) ==
+          (SI_CONTEXT_FLUSH_AND_INV_CB | SI_CONTEXT_FLUSH_AND_INV_DB)) {
+         cb_db_event = V_028A90_CACHE_FLUSH_AND_INV_TS_EVENT;
+      } else if (flags & SI_CONTEXT_FLUSH_AND_INV_CB) {
+         cb_db_event = V_028A90_FLUSH_AND_INV_CB_DATA_TS;
+      } else if (flags & SI_CONTEXT_FLUSH_AND_INV_DB) {
+         cb_db_event = V_028A90_FLUSH_AND_INV_DB_DATA_TS;
+      } else {
+         assert(0);
+      }
+   } else {
+      /* Wait for graphics shaders to go idle if requested. */
+      if (flags & SI_CONTEXT_PS_PARTIAL_FLUSH) {
+         radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
+         radeon_emit(cs, EVENT_TYPE(V_028A90_PS_PARTIAL_FLUSH) | EVENT_INDEX(4));
+         /* Only count explicit shader flushes, not implicit ones. */
+         ctx->num_vs_flushes++;
+         ctx->num_ps_flushes++;
+      } else if (flags & SI_CONTEXT_VS_PARTIAL_FLUSH) {
+         radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
+         radeon_emit(cs, EVENT_TYPE(V_028A90_VS_PARTIAL_FLUSH) | EVENT_INDEX(4));
+         ctx->num_vs_flushes++;
+      }
+   }
+
+   if (flags & SI_CONTEXT_CS_PARTIAL_FLUSH && ctx->compute_is_busy) {
+      radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
+      radeon_emit(cs, EVENT_TYPE(V_028A90_CS_PARTIAL_FLUSH | EVENT_INDEX(4)));
+      ctx->num_cs_flushes++;
+      ctx->compute_is_busy = false;
+   }
+
+   if (cb_db_event) {
+      struct si_resource* wait_mem_scratch = unlikely(ctx->ws->cs_is_secure(cs)) ?
+        ctx->wait_mem_scratch_tmz : ctx->wait_mem_scratch;
+      /* CB/DB flush and invalidate (or possibly just a wait for a
+       * meta flush) via RELEASE_MEM.
+       *
+       * Combine this with other cache flushes when possible; this
+       * requires affected shaders to be idle, so do it after the
+       * CS_PARTIAL_FLUSH before (VS/PS partial flushes are always
+       * implied).
+       */
+      uint64_t va;
+
+      /* Do the flush (enqueue the event and wait for it). */
+      va = wait_mem_scratch->gpu_address;
+      ctx->wait_mem_number++;
+
+      /* Get GCR_CNTL fields, because the encoding is different in RELEASE_MEM. */
+      unsigned glm_wb = G_586_GLM_WB(gcr_cntl);
+      unsigned glm_inv = G_586_GLM_INV(gcr_cntl);
+      unsigned glv_inv = G_586_GLV_INV(gcr_cntl);
+      unsigned gl1_inv = G_586_GL1_INV(gcr_cntl);
+      assert(G_586_GL2_US(gcr_cntl) == 0);
+      assert(G_586_GL2_RANGE(gcr_cntl) == 0);
+      assert(G_586_GL2_DISCARD(gcr_cntl) == 0);
+      unsigned gl2_inv = G_586_GL2_INV(gcr_cntl);
+      unsigned gl2_wb = G_586_GL2_WB(gcr_cntl);
+      unsigned gcr_seq = G_586_SEQ(gcr_cntl);
+
+      gcr_cntl &= C_586_GLM_WB & C_586_GLM_INV & C_586_GLV_INV & C_586_GL1_INV & C_586_GL2_INV &
+                  C_586_GL2_WB; /* keep SEQ */
+
+      si_cp_release_mem(ctx, cs, cb_db_event,
+                        S_490_GLM_WB(glm_wb) | S_490_GLM_INV(glm_inv) | S_490_GLV_INV(glv_inv) |
+                           S_490_GL1_INV(gl1_inv) | S_490_GL2_INV(gl2_inv) | S_490_GL2_WB(gl2_wb) |
+                           S_490_SEQ(gcr_seq),
+                        EOP_DST_SEL_MEM, EOP_INT_SEL_SEND_DATA_AFTER_WR_CONFIRM,
+                        EOP_DATA_SEL_VALUE_32BIT, wait_mem_scratch, va, ctx->wait_mem_number,
+                        SI_NOT_QUERY);
+      si_cp_wait_mem(ctx, cs, va, ctx->wait_mem_number, 0xffffffff, WAIT_REG_MEM_EQUAL);
+   }
+
+   /* Ignore fields that only modify the behavior of other fields. */
+   if (gcr_cntl & C_586_GL1_RANGE & C_586_GL2_RANGE & C_586_SEQ) {
+      /* Flush caches and wait for the caches to assert idle.
+       * The cache flush is executed in the ME, but the PFP waits
+       * for completion.
+       */
+      radeon_emit(cs, PKT3(PKT3_ACQUIRE_MEM, 6, 0));
+      radeon_emit(cs, 0);          /* CP_COHER_CNTL */
+      radeon_emit(cs, 0xffffffff); /* CP_COHER_SIZE */
+      radeon_emit(cs, 0xffffff);   /* CP_COHER_SIZE_HI */
+      radeon_emit(cs, 0);          /* CP_COHER_BASE */
+      radeon_emit(cs, 0);          /* CP_COHER_BASE_HI */
+      radeon_emit(cs, 0x0000000A); /* POLL_INTERVAL */
+      radeon_emit(cs, gcr_cntl);   /* GCR_CNTL */
+   } else if (cb_db_event || (flags & (SI_CONTEXT_VS_PARTIAL_FLUSH | SI_CONTEXT_PS_PARTIAL_FLUSH |
+                                       SI_CONTEXT_CS_PARTIAL_FLUSH))) {
+      /* We need to ensure that PFP waits as well. */
+      radeon_emit(cs, PKT3(PKT3_PFP_SYNC_ME, 0, 0));
+      radeon_emit(cs, 0);
+   }
+
+   if (flags & SI_CONTEXT_START_PIPELINE_STATS) {
+      radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
+      radeon_emit(cs, EVENT_TYPE(V_028A90_PIPELINESTAT_START) | EVENT_INDEX(0));
+   } else if (flags & SI_CONTEXT_STOP_PIPELINE_STATS) {
+      radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
+      radeon_emit(cs, EVENT_TYPE(V_028A90_PIPELINESTAT_STOP) | EVENT_INDEX(0));
+   }
+
+   ctx->flags = 0;
+}
+
+extern "C"
+void si_emit_cache_flush(struct si_context *sctx, struct radeon_cmdbuf *cs)
+{
+   uint32_t flags = sctx->flags;
+
+   if (!sctx->has_graphics) {
+      /* Only process compute flags. */
+      flags &= SI_CONTEXT_INV_ICACHE | SI_CONTEXT_INV_SCACHE | SI_CONTEXT_INV_VCACHE |
+               SI_CONTEXT_INV_L2 | SI_CONTEXT_WB_L2 | SI_CONTEXT_INV_L2_METADATA |
+               SI_CONTEXT_CS_PARTIAL_FLUSH;
+   }
+
+   uint32_t cp_coher_cntl = 0;
+   const uint32_t flush_cb_db = flags & (SI_CONTEXT_FLUSH_AND_INV_CB | SI_CONTEXT_FLUSH_AND_INV_DB);
+   const bool is_barrier =
+      flush_cb_db ||
+      /* INV_ICACHE == beginning of gfx IB. Checking
+       * INV_ICACHE fixes corruption for DeusExMD with
+       * compute-based culling, but I don't know why.
+       */
+      flags & (SI_CONTEXT_INV_ICACHE | SI_CONTEXT_PS_PARTIAL_FLUSH | SI_CONTEXT_VS_PARTIAL_FLUSH) ||
+      (flags & SI_CONTEXT_CS_PARTIAL_FLUSH && sctx->compute_is_busy);
+
+   assert(sctx->chip_class <= GFX9);
+
+   if (flags & SI_CONTEXT_FLUSH_AND_INV_CB)
+      sctx->num_cb_cache_flushes++;
+   if (flags & SI_CONTEXT_FLUSH_AND_INV_DB)
+      sctx->num_db_cache_flushes++;
+
+   /* GFX6 has a bug that it always flushes ICACHE and KCACHE if either
+    * bit is set. An alternative way is to write SQC_CACHES, but that
+    * doesn't seem to work reliably. Since the bug doesn't affect
+    * correctness (it only does more work than necessary) and
+    * the performance impact is likely negligible, there is no plan
+    * to add a workaround for it.
+    */
+
+   if (flags & SI_CONTEXT_INV_ICACHE)
+      cp_coher_cntl |= S_0085F0_SH_ICACHE_ACTION_ENA(1);
+   if (flags & SI_CONTEXT_INV_SCACHE)
+      cp_coher_cntl |= S_0085F0_SH_KCACHE_ACTION_ENA(1);
+
+   if (sctx->chip_class <= GFX8) {
+      if (flags & SI_CONTEXT_FLUSH_AND_INV_CB) {
+         cp_coher_cntl |= S_0085F0_CB_ACTION_ENA(1) | S_0085F0_CB0_DEST_BASE_ENA(1) |
+                          S_0085F0_CB1_DEST_BASE_ENA(1) | S_0085F0_CB2_DEST_BASE_ENA(1) |
+                          S_0085F0_CB3_DEST_BASE_ENA(1) | S_0085F0_CB4_DEST_BASE_ENA(1) |
+                          S_0085F0_CB5_DEST_BASE_ENA(1) | S_0085F0_CB6_DEST_BASE_ENA(1) |
+                          S_0085F0_CB7_DEST_BASE_ENA(1);
+
+         /* Necessary for DCC */
+         if (sctx->chip_class == GFX8)
+            si_cp_release_mem(sctx, cs, V_028A90_FLUSH_AND_INV_CB_DATA_TS, 0, EOP_DST_SEL_MEM,
+                              EOP_INT_SEL_NONE, EOP_DATA_SEL_DISCARD, NULL, 0, 0, SI_NOT_QUERY);
+      }
+      if (flags & SI_CONTEXT_FLUSH_AND_INV_DB)
+         cp_coher_cntl |= S_0085F0_DB_ACTION_ENA(1) | S_0085F0_DB_DEST_BASE_ENA(1);
+   }
+
+   if (flags & SI_CONTEXT_FLUSH_AND_INV_CB) {
+      /* Flush CMASK/FMASK/DCC. SURFACE_SYNC will wait for idle. */
+      radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
+      radeon_emit(cs, EVENT_TYPE(V_028A90_FLUSH_AND_INV_CB_META) | EVENT_INDEX(0));
+   }
+   if (flags & (SI_CONTEXT_FLUSH_AND_INV_DB | SI_CONTEXT_FLUSH_AND_INV_DB_META)) {
+      /* Flush HTILE. SURFACE_SYNC will wait for idle. */
+      radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
+      radeon_emit(cs, EVENT_TYPE(V_028A90_FLUSH_AND_INV_DB_META) | EVENT_INDEX(0));
+   }
+
+   /* Wait for shader engines to go idle.
+    * VS and PS waits are unnecessary if SURFACE_SYNC is going to wait
+    * for everything including CB/DB cache flushes.
+    */
+   if (!flush_cb_db) {
+      if (flags & SI_CONTEXT_PS_PARTIAL_FLUSH) {
+         radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
+         radeon_emit(cs, EVENT_TYPE(V_028A90_PS_PARTIAL_FLUSH) | EVENT_INDEX(4));
+         /* Only count explicit shader flushes, not implicit ones
+          * done by SURFACE_SYNC.
+          */
+         sctx->num_vs_flushes++;
+         sctx->num_ps_flushes++;
+      } else if (flags & SI_CONTEXT_VS_PARTIAL_FLUSH) {
+         radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
+         radeon_emit(cs, EVENT_TYPE(V_028A90_VS_PARTIAL_FLUSH) | EVENT_INDEX(4));
+         sctx->num_vs_flushes++;
+      }
+   }
+
+   if (flags & SI_CONTEXT_CS_PARTIAL_FLUSH && sctx->compute_is_busy) {
+      radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
+      radeon_emit(cs, EVENT_TYPE(V_028A90_CS_PARTIAL_FLUSH) | EVENT_INDEX(4));
+      sctx->num_cs_flushes++;
+      sctx->compute_is_busy = false;
+   }
+
+   /* VGT state synchronization. */
+   if (flags & SI_CONTEXT_VGT_FLUSH) {
+      radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
+      radeon_emit(cs, EVENT_TYPE(V_028A90_VGT_FLUSH) | EVENT_INDEX(0));
+   }
+   if (flags & SI_CONTEXT_VGT_STREAMOUT_SYNC) {
+      radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
+      radeon_emit(cs, EVENT_TYPE(V_028A90_VGT_STREAMOUT_SYNC) | EVENT_INDEX(0));
+   }
+
+   /* GFX9: Wait for idle if we're flushing CB or DB. ACQUIRE_MEM doesn't
+    * wait for idle on GFX9. We have to use a TS event.
+    */
+   if (sctx->chip_class == GFX9 && flush_cb_db) {
+      uint64_t va;
+      unsigned tc_flags, cb_db_event;
+
+      /* Set the CB/DB flush event. */
+      switch (flush_cb_db) {
+      case SI_CONTEXT_FLUSH_AND_INV_CB:
+         cb_db_event = V_028A90_FLUSH_AND_INV_CB_DATA_TS;
+         break;
+      case SI_CONTEXT_FLUSH_AND_INV_DB:
+         cb_db_event = V_028A90_FLUSH_AND_INV_DB_DATA_TS;
+         break;
+      default:
+         /* both CB & DB */
+         cb_db_event = V_028A90_CACHE_FLUSH_AND_INV_TS_EVENT;
+      }
+
+      /* These are the only allowed combinations. If you need to
+       * do multiple operations at once, do them separately.
+       * All operations that invalidate L2 also seem to invalidate
+       * metadata. Volatile (VOL) and WC flushes are not listed here.
+       *
+       * TC    | TC_WB         = writeback & invalidate L2 & L1
+       * TC    | TC_WB | TC_NC = writeback & invalidate L2 for MTYPE == NC
+       *         TC_WB | TC_NC = writeback L2 for MTYPE == NC
+       * TC            | TC_NC = invalidate L2 for MTYPE == NC
+       * TC    | TC_MD         = writeback & invalidate L2 metadata (DCC, etc.)
+       * TCL1                  = invalidate L1
+       */
+      tc_flags = 0;
+
+      if (flags & SI_CONTEXT_INV_L2_METADATA) {
+         tc_flags = EVENT_TC_ACTION_ENA | EVENT_TC_MD_ACTION_ENA;
+      }
+
+      /* Ideally flush TC together with CB/DB. */
+      if (flags & SI_CONTEXT_INV_L2) {
+         /* Writeback and invalidate everything in L2 & L1. */
+         tc_flags = EVENT_TC_ACTION_ENA | EVENT_TC_WB_ACTION_ENA;
+
+         /* Clear the flags. */
+         flags &= ~(SI_CONTEXT_INV_L2 | SI_CONTEXT_WB_L2 | SI_CONTEXT_INV_VCACHE);
+         sctx->num_L2_invalidates++;
+      }
+
+      /* Do the flush (enqueue the event and wait for it). */
+      struct si_resource* wait_mem_scratch = unlikely(sctx->ws->cs_is_secure(cs)) ?
+        sctx->wait_mem_scratch_tmz : sctx->wait_mem_scratch;
+      va = wait_mem_scratch->gpu_address;
+      sctx->wait_mem_number++;
+
+      si_cp_release_mem(sctx, cs, cb_db_event, tc_flags, EOP_DST_SEL_MEM,
+                        EOP_INT_SEL_SEND_DATA_AFTER_WR_CONFIRM, EOP_DATA_SEL_VALUE_32BIT,
+                        wait_mem_scratch, va, sctx->wait_mem_number, SI_NOT_QUERY);
+      si_cp_wait_mem(sctx, cs, va, sctx->wait_mem_number, 0xffffffff, WAIT_REG_MEM_EQUAL);
+   }
+
+   /* Make sure ME is idle (it executes most packets) before continuing.
+    * This prevents read-after-write hazards between PFP and ME.
+    */
+   if (sctx->has_graphics &&
+       (cp_coher_cntl || (flags & (SI_CONTEXT_CS_PARTIAL_FLUSH | SI_CONTEXT_INV_VCACHE |
+                                   SI_CONTEXT_INV_L2 | SI_CONTEXT_WB_L2)))) {
+      radeon_emit(cs, PKT3(PKT3_PFP_SYNC_ME, 0, 0));
+      radeon_emit(cs, 0);
+   }
+
+   /* GFX6-GFX8 only:
+    *   When one of the CP_COHER_CNTL.DEST_BASE flags is set, SURFACE_SYNC
+    *   waits for idle, so it should be last. SURFACE_SYNC is done in PFP.
+    *
+    * cp_coher_cntl should contain all necessary flags except TC flags
+    * at this point.
+    *
+    * GFX6-GFX7 don't support L2 write-back.
+    */
+   if (flags & SI_CONTEXT_INV_L2 || (sctx->chip_class <= GFX7 && (flags & SI_CONTEXT_WB_L2))) {
+      /* Invalidate L1 & L2. (L1 is always invalidated on GFX6)
+       * WB must be set on GFX8+ when TC_ACTION is set.
+       */
+      si_emit_surface_sync(sctx, cs,
+                           cp_coher_cntl | S_0085F0_TC_ACTION_ENA(1) | S_0085F0_TCL1_ACTION_ENA(1) |
+                              S_0301F0_TC_WB_ACTION_ENA(sctx->chip_class >= GFX8));
+      cp_coher_cntl = 0;
+      sctx->num_L2_invalidates++;
+   } else {
+      /* L1 invalidation and L2 writeback must be done separately,
+       * because both operations can't be done together.
+       */
+      if (flags & SI_CONTEXT_WB_L2) {
+         /* WB = write-back
+          * NC = apply to non-coherent MTYPEs
+          *      (i.e. MTYPE <= 1, which is what we use everywhere)
+          *
+          * WB doesn't work without NC.
+          */
+         si_emit_surface_sync(
+            sctx, cs,
+            cp_coher_cntl | S_0301F0_TC_WB_ACTION_ENA(1) | S_0301F0_TC_NC_ACTION_ENA(1));
+         cp_coher_cntl = 0;
+         sctx->num_L2_writebacks++;
+      }
+      if (flags & SI_CONTEXT_INV_VCACHE) {
+         /* Invalidate per-CU VMEM L1. */
+         si_emit_surface_sync(sctx, cs, cp_coher_cntl | S_0085F0_TCL1_ACTION_ENA(1));
+         cp_coher_cntl = 0;
+      }
+   }
+
+   /* If TC flushes haven't cleared this... */
+   if (cp_coher_cntl)
+      si_emit_surface_sync(sctx, cs, cp_coher_cntl);
+
+   if (is_barrier)
+      si_prim_discard_signal_next_compute_ib_start(sctx);
+
+   if (flags & SI_CONTEXT_START_PIPELINE_STATS) {
+      radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
+      radeon_emit(cs, EVENT_TYPE(V_028A90_PIPELINESTAT_START) | EVENT_INDEX(0));
+   } else if (flags & SI_CONTEXT_STOP_PIPELINE_STATS) {
+      radeon_emit(cs, PKT3(PKT3_EVENT_WRITE, 0, 0));
+      radeon_emit(cs, EVENT_TYPE(V_028A90_PIPELINESTAT_STOP) | EVENT_INDEX(0));
+   }
+
+   sctx->flags = 0;
+}
+
 template <chip_class GFX_VERSION> ALWAYS_INLINE
 static bool si_upload_vertex_buffer_descriptors(struct si_context *sctx)
 {
+   unsigned i, count = sctx->num_vertex_elements;
+   uint32_t *ptr;
+
    struct si_vertex_elements *velems = sctx->vertex_elements;
    unsigned alloc_size = velems->vb_desc_list_alloc_size;
-   uint32_t *ptr;
 
    if (alloc_size) {
       /* Vertex buffer descriptors are the only ones which are uploaded
@@ -1327,13 +1642,12 @@ static bool si_upload_vertex_buffer_descriptors(struct si_context *sctx)
       sctx->prefetch_L2_mask &= ~SI_PREFETCH_VBO_DESCRIPTORS;
    }
 
-   unsigned count = sctx->num_vertex_elements;
    assert(count <= SI_MAX_ATTRIBS);
 
    unsigned first_vb_use_mask = velems->first_vb_use_mask;
    unsigned num_vbos_in_user_sgprs = sctx->screen->num_vbos_in_user_sgprs;
 
-   for (unsigned i = 0; i < count; i++) {
+   for (i = 0; i < count; i++) {
       struct pipe_vertex_buffer *vb;
       struct si_resource *buf;
       unsigned vbo_index = velems->vertex_buffer_index[i];
@@ -1476,42 +1790,36 @@ static void si_emit_all_states(struct si_context *sctx, const struct pipe_draw_i
 {
    unsigned num_patches = 0;
 
-   si_emit_rasterizer_prim_state<GFX_VERSION, HAS_TESS, HAS_GS, NGG>(sctx);
+   si_emit_rasterizer_prim_state<HAS_GS, NGG>(sctx);
    if (HAS_TESS)
-      si_emit_derived_tess_state(sctx, info->vertices_per_patch, &num_patches);
+      si_emit_derived_tess_state(sctx, info, &num_patches);
 
    /* Emit state atoms. */
    unsigned mask = sctx->dirty_atoms & ~skip_atom_mask;
-   if (mask) {
-      do {
-         sctx->atoms.array[u_bit_scan(&mask)].emit(sctx);
-      } while (mask);
+   while (mask)
+      sctx->atoms.array[u_bit_scan(&mask)].emit(sctx);
 
-      sctx->dirty_atoms &= skip_atom_mask;
-   }
+   sctx->dirty_atoms &= skip_atom_mask;
 
    /* Emit states. */
    mask = sctx->dirty_states;
-   if (mask) {
-      do {
-         unsigned i = u_bit_scan(&mask);
-         struct si_pm4_state *state = sctx->queued.array[i];
+   while (mask) {
+      unsigned i = u_bit_scan(&mask);
+      struct si_pm4_state *state = sctx->queued.array[i];
 
-         if (!state || sctx->emitted.array[i] == state)
-            continue;
-
-         si_pm4_emit(sctx, state);
-         sctx->emitted.array[i] = state;
-      } while (mask);
+      if (!state || sctx->emitted.array[i] == state)
+         continue;
 
-      sctx->dirty_states = 0;
+      si_pm4_emit(sctx, state);
+      sctx->emitted.array[i] = state;
    }
+   sctx->dirty_states = 0;
 
    /* Emit draw states. */
-   si_emit_vs_state<GFX_VERSION, HAS_TESS, HAS_GS, NGG>(sctx, info->index_size);
+   si_emit_vs_state(sctx, info);
    si_emit_draw_registers<GFX_VERSION, HAS_TESS, HAS_GS, NGG>
-         (sctx, indirect, prim, num_patches, instance_count, info->vertices_per_patch,
-          primitive_restart, info->restart_index, min_vertex_count);
+         (sctx, info, indirect, prim, num_patches, instance_count, primitive_restart,
+          min_vertex_count);
 }
 
 static bool si_all_vs_resources_read_only(struct si_context *sctx, struct pipe_resource *indexbuf)
@@ -1631,7 +1939,7 @@ static void si_draw_vbo(struct pipe_context *ctx,
     * no workaround for indirect draws, but we can at least skip
     * direct draws.
     */
-   if (GFX_VERSION <= GFX7 && unlikely(!indirect && !instance_count))
+   if (unlikely(!indirect && !instance_count))
       return;
 
    struct si_shader_selector *vs = sctx->vs_shader.cso;
@@ -1721,12 +2029,13 @@ static void si_draw_vbo(struct pipe_context *ctx,
 
    if (GFX_VERSION <= GFX9 && HAS_GS) {
       /* Determine whether the GS triangle strip adjacency fix should
-       * be applied. Rotate every other triangle if triangle strips with
-       * adjacency are fed to the GS. This doesn't work if primitive
-       * restart occurs after an odd number of triangles.
+       * be applied. Rotate every other triangle if
+       * - triangle strips with adjacency are fed to the GS and
+       * - primitive restart is disabled (the rotation doesn't help
+       *   when the restart occurs after an odd number of triangles).
        */
       bool gs_tri_strip_adj_fix =
-         !HAS_TESS && prim == PIPE_PRIM_TRIANGLE_STRIP_ADJACENCY;
+         !HAS_TESS && prim == PIPE_PRIM_TRIANGLE_STRIP_ADJACENCY && !primitive_restart;
 
       if (gs_tri_strip_adj_fix != sctx->gs_tri_strip_adj_fix) {
          sctx->gs_tri_strip_adj_fix = gs_tri_strip_adj_fix;
@@ -1783,6 +2092,7 @@ static void si_draw_vbo(struct pipe_context *ctx,
    bool dispatch_prim_discard_cs = false;
    bool prim_discard_cs_instancing = false;
    unsigned original_index_size = index_size;
+   unsigned avg_direct_count = 0;
    unsigned min_direct_count = 0;
    unsigned total_direct_count = 0;
 
@@ -1812,11 +2122,12 @@ static void si_draw_vbo(struct pipe_context *ctx,
          total_direct_count += count;
          min_direct_count = MIN2(min_direct_count, count);
       }
+      avg_direct_count = (total_direct_count / num_draws) * instance_count;
    }
 
    /* Determine if we can use the primitive discard compute shader. */
    if (ALLOW_PRIM_DISCARD_CS &&
-       (total_direct_count > sctx->prim_discard_vertex_count_threshold
+       (avg_direct_count > sctx->prim_discard_vertex_count_threshold
            ? (sctx->compute_num_verts_rejected += total_direct_count, true)
            : /* Add, then return true. */
            (sctx->compute_num_verts_ineligible += total_direct_count,
@@ -1901,10 +2212,10 @@ static void si_draw_vbo(struct pipe_context *ctx,
    if (GFX_VERSION >= GFX10) {
       struct si_shader_selector *hw_vs;
       if (NGG && !dispatch_prim_discard_cs && rast_prim == PIPE_PRIM_TRIANGLES &&
-          (hw_vs = si_get_vs_inline(sctx, HAS_TESS, HAS_GS)->cso) &&
-          (total_direct_count > hw_vs->ngg_cull_vert_threshold ||
+          (hw_vs = si_get_vs(sctx)->cso) &&
+          (avg_direct_count > hw_vs->ngg_cull_vert_threshold ||
            (!index_size &&
-            total_direct_count > hw_vs->ngg_cull_nonindexed_fast_launch_vert_threshold &&
+            avg_direct_count > hw_vs->ngg_cull_nonindexed_fast_launch_vert_threshold &&
             prim & ((1 << PIPE_PRIM_TRIANGLES) |
                     (1 << PIPE_PRIM_TRIANGLE_STRIP))))) {
          uint8_t ngg_culling = 0;
@@ -1954,6 +2265,14 @@ static void si_draw_vbo(struct pipe_context *ctx,
       }
    }
 
+   if (sctx->shader_has_inlinable_uniforms_mask &
+       sctx->inlinable_uniforms_valid_mask &
+       sctx->inlinable_uniforms_dirty_mask) {
+      sctx->do_update_shaders = true;
+      /* If inlinable uniforms are not valid, they are also not dirty, so clear all bits. */
+      sctx->inlinable_uniforms_dirty_mask = 0;
+   }
+
    if (unlikely(sctx->do_update_shaders)) {
       if (unlikely(!si_update_shaders(sctx))) {
          DRAW_CLEANUP;
@@ -1966,7 +2285,7 @@ static void si_draw_vbo(struct pipe_context *ctx,
        * This is the setting that is used by the draw.
        */
       if (GFX_VERSION >= GFX10) {
-         uint8_t ngg_culling = si_get_vs_inline(sctx, HAS_TESS, HAS_GS)->current->key.opt.ngg_culling;
+         uint8_t ngg_culling = si_get_vs(sctx)->current->key.opt.ngg_culling;
          if (GFX_VERSION == GFX10 &&
              !(old_ngg_culling & SI_NGG_CULL_GS_FAST_LAUNCH_ALL) &&
              ngg_culling & SI_NGG_CULL_GS_FAST_LAUNCH_ALL)
@@ -2068,7 +2387,8 @@ static void si_draw_vbo(struct pipe_context *ctx,
       /* Start prefetches after the draw has been started. Both will run
        * in parallel, but starting the draw first is more important.
        */
-      si_emit_prefetch_L2<GFX_VERSION, HAS_TESS, HAS_GS, NGG, false>(sctx);
+      if (GFX_VERSION >= GFX7 && sctx->prefetch_L2_mask)
+         cik_emit_prefetch_L2(sctx, false);
    } else {
       /* If we don't wait for idle, start prefetches first, then set
        * states, and draw at the end.
@@ -2077,7 +2397,8 @@ static void si_draw_vbo(struct pipe_context *ctx,
          sctx->emit_cache_flush(sctx, &sctx->gfx_cs);
 
       /* Only prefetch the API VS and VBO descriptors. */
-      si_emit_prefetch_L2<GFX_VERSION, HAS_TESS, HAS_GS, NGG, true>(sctx);
+      if (GFX_VERSION >= GFX7 && sctx->prefetch_L2_mask)
+         cik_emit_prefetch_L2(sctx, true);
 
       si_emit_all_states<GFX_VERSION, HAS_TESS, HAS_GS, NGG>
             (sctx, info, indirect, prim, instance_count, min_direct_count,
@@ -2097,7 +2418,8 @@ static void si_draw_vbo(struct pipe_context *ctx,
 
       /* Prefetch the remaining shaders after the draw has been
        * started. */
-      si_emit_prefetch_L2<GFX_VERSION, HAS_TESS, HAS_GS, NGG, false>(sctx);
+      if (GFX_VERSION >= GFX7 && sctx->prefetch_L2_mask)
+         cik_emit_prefetch_L2(sctx, false);
    }
 
    /* Clear the context roll flag after the draw call.
@@ -2113,8 +2435,8 @@ static void si_draw_vbo(struct pipe_context *ctx,
 
    /* Workaround for a VGT hang when streamout is enabled.
     * It must be done after drawing. */
-   if (((GFX_VERSION == GFX7 && sctx->family == CHIP_HAWAII) ||
-        (GFX_VERSION == GFX8 && (sctx->family == CHIP_TONGA || sctx->family == CHIP_FIJI))) &&
+   if ((GFX_VERSION == GFX7 || GFX_VERSION == GFX8) &&
+       (sctx->family == CHIP_HAWAII || sctx->family == CHIP_TONGA || sctx->family == CHIP_FIJI) &&
        si_get_strmout_en(sctx)) {
       sctx->flags |= SI_CONTEXT_VGT_STREAMOUT_SYNC;
    }
@@ -2123,8 +2445,12 @@ static void si_draw_vbo(struct pipe_context *ctx,
       sctx->num_decompress_calls++;
    } else {
       sctx->num_draw_calls++;
+      if (sctx->framebuffer.state.nr_cbufs > 1)
+         sctx->num_mrt_draw_calls++;
       if (primitive_restart)
          sctx->num_prim_restart_calls++;
+      if (G_0286E8_WAVESIZE(sctx->spi_tmpring_size))
+         sctx->num_spill_draw_calls++;
    }
 
    DRAW_CLEANUP;
diff --git a/src/gallium/drivers/radeonsi/si_state_shaders.c b/src/gallium/drivers/radeonsi/si_state_shaders.c
index 3326ad934fe..344296be09f 100644
--- a/src/gallium/drivers/radeonsi/si_state_shaders.c
+++ b/src/gallium/drivers/radeonsi/si_state_shaders.c
@@ -1744,7 +1744,7 @@ static unsigned si_get_alpha_test_func(struct si_context *sctx)
 void si_shader_selector_key_vs(struct si_context *sctx, struct si_shader_selector *vs,
                                struct si_shader_key *key, struct si_vs_prolog_bits *prolog_key)
 {
-   if (vs->info.base.vs.blit_sgprs_amd)
+   if (!sctx->vertex_elements || vs->info.base.vs.blit_sgprs_amd)
       return;
 
    struct si_vertex_elements *elts = sctx->vertex_elements;
@@ -2969,6 +2969,11 @@ static void si_update_common_shader_state(struct si_context *sctx, struct si_sha
                                 si_shader_uses_bindless_images(sctx->tcs_shader.cso) ||
                                 si_shader_uses_bindless_images(sctx->tes_shader.cso);
 
+   if (sel && sel->info.base.num_inlinable_uniforms)
+      sctx->shader_has_inlinable_uniforms_mask |= 1 << type;
+   else
+      sctx->shader_has_inlinable_uniforms_mask &= ~(1 << type);
+
    /* Invalidate inlinable uniforms. */
    sctx->inlinable_uniforms_valid_mask &= ~(1 << type);
 
@@ -2979,7 +2984,7 @@ static void si_bind_vs_shader(struct pipe_context *ctx, void *state)
 {
    struct si_context *sctx = (struct si_context *)ctx;
    struct si_shader_selector *old_hw_vs = si_get_vs(sctx)->cso;
-   struct si_shader *old_hw_vs_variant = si_get_vs(sctx)->current;
+   struct si_shader *old_hw_vs_variant = si_get_vs_state(sctx);
    struct si_shader_selector *sel = state;
 
    if (sctx->vs_shader.cso == sel)
@@ -2998,7 +3003,7 @@ static void si_bind_vs_shader(struct pipe_context *ctx, void *state)
    si_update_vs_viewport_state(sctx);
    si_update_streamout_state(sctx);
    si_update_clip_regs(sctx, old_hw_vs, old_hw_vs_variant, si_get_vs(sctx)->cso,
-                       si_get_vs(sctx)->current);
+                       si_get_vs_state(sctx));
 }
 
 static void si_update_tess_uses_prim_id(struct si_context *sctx)
@@ -3053,7 +3058,7 @@ static void si_bind_gs_shader(struct pipe_context *ctx, void *state)
 {
    struct si_context *sctx = (struct si_context *)ctx;
    struct si_shader_selector *old_hw_vs = si_get_vs(sctx)->cso;
-   struct si_shader *old_hw_vs_variant = si_get_vs(sctx)->current;
+   struct si_shader *old_hw_vs_variant = si_get_vs_state(sctx);
    struct si_shader_selector *sel = state;
    bool enable_changed = !!sctx->gs_shader.cso != !!sel;
    bool ngg_changed;
@@ -3079,7 +3084,7 @@ static void si_bind_gs_shader(struct pipe_context *ctx, void *state)
    si_update_vs_viewport_state(sctx);
    si_update_streamout_state(sctx);
    si_update_clip_regs(sctx, old_hw_vs, old_hw_vs_variant, si_get_vs(sctx)->cso,
-                       si_get_vs(sctx)->current);
+                       si_get_vs_state(sctx));
 }
 
 static void si_bind_tcs_shader(struct pipe_context *ctx, void *state)
@@ -3105,7 +3110,7 @@ static void si_bind_tes_shader(struct pipe_context *ctx, void *state)
 {
    struct si_context *sctx = (struct si_context *)ctx;
    struct si_shader_selector *old_hw_vs = si_get_vs(sctx)->cso;
-   struct si_shader *old_hw_vs_variant = si_get_vs(sctx)->current;
+   struct si_shader *old_hw_vs_variant = si_get_vs_state(sctx);
    struct si_shader_selector *sel = state;
    bool enable_changed = !!sctx->tes_shader.cso != !!sel;
 
@@ -3129,7 +3134,7 @@ static void si_bind_tes_shader(struct pipe_context *ctx, void *state)
    si_update_vs_viewport_state(sctx);
    si_update_streamout_state(sctx);
    si_update_clip_regs(sctx, old_hw_vs, old_hw_vs_variant, si_get_vs(sctx)->cso,
-                       si_get_vs(sctx)->current);
+                       si_get_vs_state(sctx));
 }
 
 static void si_bind_ps_shader(struct pipe_context *ctx, void *state)
@@ -3332,7 +3337,7 @@ static unsigned si_get_ps_input_cntl(struct si_context *sctx, struct si_shader *
 static void si_emit_spi_map(struct si_context *sctx)
 {
    struct si_shader *ps = sctx->ps_shader.current;
-   struct si_shader *vs;
+   struct si_shader *vs = si_get_vs_state(sctx);
    struct si_shader_info *psinfo = ps ? &ps->selector->info : NULL;
    unsigned i, num_interp, num_written = 0;
    unsigned spi_ps_input_cntl[32];
@@ -3340,12 +3345,6 @@ static void si_emit_spi_map(struct si_context *sctx)
    if (!ps || !ps->selector->info.num_inputs)
       return;
 
-   /* With legacy GS, only the GS copy shader contains information about param exports. */
-   if (sctx->gs_shader.cso && !sctx->ngg)
-      vs = sctx->gs_shader.cso->gs_copy_shader;
-   else
-      vs = si_get_vs(sctx)->current;
-
    num_interp = si_get_ps_num_interp(ps);
    assert(num_interp > 0);
 
@@ -3907,7 +3906,7 @@ bool si_update_shaders(struct si_context *sctx)
    struct pipe_context *ctx = (struct pipe_context *)sctx;
    struct si_compiler_ctx_state compiler_state;
    struct si_state_rasterizer *rs = sctx->queued.named.rasterizer;
-   struct si_shader *old_vs = si_get_vs(sctx)->current;
+   struct si_shader *old_vs = si_get_vs_state(sctx);
    unsigned old_kill_clip_distances = old_vs ? old_vs->key.opt.kill_clip_distances : 0;
    struct si_shader *old_ps = sctx->ps_shader.current;
    union si_vgt_stages_key key;
@@ -4039,7 +4038,7 @@ bool si_update_shaders(struct si_context *sctx)
 
    si_update_vgt_shader_config(sctx, key);
 
-   if (old_kill_clip_distances != si_get_vs(sctx)->current->key.opt.kill_clip_distances)
+   if (old_kill_clip_distances != si_get_vs_state(sctx)->key.opt.kill_clip_distances)
       si_mark_atom_dirty(sctx, &sctx->atoms.s.clip_regs);
 
    if (sctx->ps_shader.cso) {
diff --git a/src/gallium/drivers/svga/svga_tgsi_insn.c b/src/gallium/drivers/svga/svga_tgsi_insn.c
index e2d0865d944..29c5dc3548e 100644
--- a/src/gallium/drivers/svga/svga_tgsi_insn.c
+++ b/src/gallium/drivers/svga/svga_tgsi_insn.c
@@ -257,7 +257,9 @@ translate_src_register( const struct svga_shader_emitter *emit,
                   reg->Register.SwizzleZ,
                   reg->Register.SwizzleW );
 
-   /* src.mod isn't a bitfield, unfortunately */
+   /* src.mod isn't a bitfield, unfortunately:
+    * See tgsi_util_get_full_src_register_sign_mode for implementation details.
+    */
    if (reg->Register.Absolute) {
       if (reg->Register.Negate)
          src.base.srcMod = SVGA3DSRCMOD_ABSNEG;
diff --git a/src/gallium/drivers/v3d/v3d_blit.c b/src/gallium/drivers/v3d/v3d_blit.c
index cd203457292..dd5805d55a3 100644
--- a/src/gallium/drivers/v3d/v3d_blit.c
+++ b/src/gallium/drivers/v3d/v3d_blit.c
@@ -233,13 +233,18 @@ v3d_tfu(struct pipe_context *pctx,
         int msaa_scale = pdst->nr_samples > 1 ? 2 : 1;
         int width = u_minify(pdst->width0, base_level) * msaa_scale;
         int height = u_minify(pdst->height0, base_level) * msaa_scale;
-        enum pipe_format pformat;
 
         if (psrc->format != pdst->format)
                 return false;
         if (psrc->nr_samples != pdst->nr_samples)
                 return false;
 
+        uint32_t tex_format = v3d_get_tex_format(&screen->devinfo,
+                                                 pdst->format);
+
+        if (!v3d_tfu_supports_tex_format(&screen->devinfo, tex_format, for_mipmap))
+                return false;
+
         if (pdst->target != PIPE_TEXTURE_2D || psrc->target != PIPE_TEXTURE_2D)
                 return false;
 
@@ -247,31 +252,6 @@ v3d_tfu(struct pipe_context *pctx,
         if (dst_base_slice->tiling == VC5_TILING_RASTER)
                 return false;
 
-        /* When using TFU for blit, we are doing exact copies (both input and
-         * output format must be the same, no scaling, etc), so there is no
-         * pixel format conversions. Thus we can rewrite the format to use one
-         * that is TFU compatible based on its texel size.
-         */
-        if (for_mipmap) {
-                pformat = pdst->format;
-        } else {
-                switch (dst->cpp) {
-                case 16: pformat = PIPE_FORMAT_R32G32B32A32_FLOAT;   break;
-                case 8:  pformat = PIPE_FORMAT_R16G16B16A16_FLOAT;   break;
-                case 4:  pformat = PIPE_FORMAT_R32_FLOAT;            break;
-                case 2:  pformat = PIPE_FORMAT_R16_FLOAT;            break;
-                case 1:  pformat = PIPE_FORMAT_R8_UNORM;             break;
-                default: unreachable("unsupported format bit-size"); break;
-                };
-        }
-
-        uint32_t tex_format = v3d_get_tex_format(&screen->devinfo, pformat);
-
-        if (!v3d_tfu_supports_tex_format(&screen->devinfo, tex_format, for_mipmap)) {
-                assert(for_mipmap);
-                return false;
-        }
-
         v3d_flush_jobs_writing_resource(v3d, psrc, V3D_FLUSH_DEFAULT, false);
         v3d_flush_jobs_reading_resource(v3d, pdst, V3D_FLUSH_DEFAULT, false);
 
@@ -297,7 +277,7 @@ v3d_tfu(struct pipe_context *pctx,
         }
 
         uint32_t dst_offset = (dst->bo->offset +
-                               v3d_layer_offset(pdst, base_level, dst_layer));
+                               v3d_layer_offset(pdst, src_level, dst_layer));
         tfu.ioa |= dst_offset;
         if (last_level != base_level)
                 tfu.ioa |= V3D_TFU_IOA_DIMTW;
diff --git a/src/gallium/drivers/zink/driinfo_zink.h b/src/gallium/drivers/zink/driinfo_zink.h
index e637ccd4512..e1cf6d7d559 100644
--- a/src/gallium/drivers/zink/driinfo_zink.h
+++ b/src/gallium/drivers/zink/driinfo_zink.h
@@ -2,6 +2,7 @@
 
 DRI_CONF_SECTION_DEBUG
    DRI_CONF_DUAL_COLOR_BLEND_BY_LOCATION(false)
+   DRI_CONF_OPT_B(radeonsi_inline_uniforms, false, "Optimize shaders by replacing uniforms with literals")
 DRI_CONF_SECTION_END
 
 DRI_CONF_SECTION_PERFORMANCE
diff --git a/src/gallium/drivers/zink/meson.build b/src/gallium/drivers/zink/meson.build
index 293d2aacc7b..72fc4a0e47e 100644
--- a/src/gallium/drivers/zink/meson.build
+++ b/src/gallium/drivers/zink/meson.build
@@ -27,6 +27,8 @@ files_libzink = files(
   'zink_clear.c',
   'zink_compiler.c',
   'zink_context.c',
+  'zink_descriptors.c',
+  'zink_descriptors_lazy.c',
   'zink_draw.c',
   'zink_fence.c',
   'zink_format.c',
@@ -55,7 +57,7 @@ zink_instance = custom_target(
   input : ['zink_instance.py'],
   output : ['zink_instance.h', 'zink_instance.c'],
   command : [
-    prog_python, '@INPUT@', '@OUTPUT@', join_paths(meson.source_root(), 'src/vulkan/registry/vk.xml')
+    prog_python, '@INPUT@', '@OUTPUT@'
   ]
 )
 
diff --git a/src/gallium/drivers/zink/nir_lower_dynamic_bo_access.c b/src/gallium/drivers/zink/nir_lower_dynamic_bo_access.c
index d629a31e9a7..16d9683b276 100644
--- a/src/gallium/drivers/zink/nir_lower_dynamic_bo_access.c
+++ b/src/gallium/drivers/zink/nir_lower_dynamic_bo_access.c
@@ -90,14 +90,8 @@ lower_dynamic_bo_access_instr(nir_intrinsic_instr *instr, nir_builder *b)
       return false;
    b->cursor = nir_after_instr(&instr->instr);
    bool ssbo_mode = instr->intrinsic != nir_intrinsic_load_ubo && instr->intrinsic != nir_intrinsic_load_ubo_vec4;
-   unsigned first_idx = UINT_MAX, last_idx;
+   unsigned first_idx = 0, last_idx;
    if (ssbo_mode) {
-      /* ssbo bindings don't always start at 0 */
-      nir_foreach_variable_with_modes(var, b->shader, nir_var_mem_ssbo) {
-         first_idx = var->data.binding;
-         break;
-      }
-      assert(first_idx != UINT_MAX);
       last_idx = first_idx + b->shader->info.num_ssbos;
    } else {
       /* skip 0 index if uniform_0 is one we created previously */
diff --git a/src/gallium/drivers/zink/nir_to_spirv/nir_to_spirv.c b/src/gallium/drivers/zink/nir_to_spirv/nir_to_spirv.c
index 0d97a6c614e..d4c0619fdb3 100644
--- a/src/gallium/drivers/zink/nir_to_spirv/nir_to_spirv.c
+++ b/src/gallium/drivers/zink/nir_to_spirv/nir_to_spirv.c
@@ -21,6 +21,7 @@
  * USE OR OTHER DEALINGS IN THE SOFTWARE.
  */
 
+#include "zink_device_info.h"
 #include "nir_to_spirv.h"
 #include "spirv_builder.h"
 
@@ -35,6 +36,8 @@ struct ntv_context {
    void *mem_ctx;
 
    struct spirv_builder builder;
+   bool lazy;
+   bool uses_dynamic;
 
    SpvId GLSL_std_450;
 
@@ -45,7 +48,11 @@ struct ntv_context {
    size_t num_ubos;
 
    SpvId ssbos[PIPE_MAX_SHADER_BUFFERS];
+   nir_variable *ssbo_vars[PIPE_MAX_SHADER_BUFFERS];
+   uint32_t num_ssbos;
    SpvId image_types[PIPE_MAX_SAMPLERS];
+   SpvId images[PIPE_MAX_SAMPLERS];
+   SpvId sampler_types[PIPE_MAX_SAMPLERS];
    SpvId samplers[PIPE_MAX_SAMPLERS];
    unsigned char sampler_array_sizes[PIPE_MAX_SAMPLERS];
    unsigned samplers_used : PIPE_MAX_SAMPLERS;
@@ -59,6 +66,7 @@ struct ntv_context {
    size_t num_regs;
 
    struct hash_table *vars; /* nir_variable -> SpvId */
+   struct hash_table *spv_vars; /* SpvId -> nir_variable */
    struct hash_table *so_outputs; /* pipe_stream_output -> SpvId */
    unsigned outputs[VARYING_SLOT_MAX * 4];
    const struct glsl_type *so_output_gl_types[VARYING_SLOT_MAX * 4];
@@ -72,11 +80,17 @@ struct ntv_context {
    unsigned char *shader_slot_map;
    unsigned char shader_slots_reserved;
 
-   SpvId front_face_var, instance_id_var, vertex_id_var, base_instance_var,
+   SpvId front_face_var, instance_id_var, vertex_id_var,
          primitive_id_var, invocation_id_var, // geometry
          sample_mask_type, sample_id_var, sample_pos_var, sample_mask_in_var,
          tess_patch_vertices_in, tess_coord_var, // tess
-         push_const_var;
+         push_const_var,
+         workgroup_id_var, num_workgroups_var,
+         local_invocation_id_var, global_invocation_id_var,
+         local_invocation_index_var, helper_invocation_var,
+         local_group_size_var,
+         shared_block_var,
+         base_vertex_var, base_instance_var, draw_id_var;
 };
 
 static SpvId
@@ -98,6 +112,15 @@ static SpvId
 emit_binop(struct ntv_context *ctx, SpvOp op, SpvId type,
            SpvId src0, SpvId src1);
 
+
+static inline nir_variable *
+get_var_from_spvid(struct ntv_context *ctx, SpvId var_id)
+{
+   struct hash_entry *he = _mesa_hash_table_search(ctx->spv_vars, &var_id);
+   assert(he);
+   return he->data;
+}
+
 static SpvId
 emit_triop(struct ntv_context *ctx, SpvOp op, SpvId type,
            SpvId src0, SpvId src1, SpvId src2);
@@ -121,6 +144,79 @@ block_label(struct ntv_context *ctx, nir_block *block)
    return ctx->block_ids[block->index];
 }
 
+static void
+emit_access_decorations(struct ntv_context *ctx, nir_variable *var, SpvId var_id)
+{
+    enum gl_access_qualifier access = var->data.access;
+    while (access) {
+       unsigned bit = u_bit_scan(&access);
+       switch (1 << bit) {
+       case ACCESS_COHERENT:
+          //this can't be used with vulkan memory model
+          //spirv_builder_emit_decoration(&ctx->builder, var_id, SpvDecorationCoherent);
+          break;
+       case ACCESS_RESTRICT:
+          spirv_builder_emit_decoration(&ctx->builder, var_id, SpvDecorationRestrict);
+          break;
+       case ACCESS_VOLATILE:
+          //this can't be used with vulkan memory model
+          //spirv_builder_emit_decoration(&ctx->builder, var_id, SpvDecorationVolatile);
+          break;
+       case ACCESS_NON_READABLE:
+          spirv_builder_emit_decoration(&ctx->builder, var_id, SpvDecorationNonReadable);
+          break;
+       case ACCESS_NON_WRITEABLE:
+          spirv_builder_emit_decoration(&ctx->builder, var_id, SpvDecorationNonWritable);
+          break;
+       case ACCESS_NON_UNIFORM:
+          spirv_builder_emit_decoration(&ctx->builder, var_id, SpvDecorationNonUniform);
+          break;
+       case ACCESS_CAN_REORDER:
+       case ACCESS_STREAM_CACHE_POLICY:
+          /* no equivalent */
+          break;
+       default:
+          unreachable("unknown access bit");
+       }
+    }
+}
+
+static SpvOp
+get_atomic_op(nir_intrinsic_op op)
+{
+   switch (op) {
+#define CASE_ATOMIC_OP(type) \
+   case nir_intrinsic_ssbo_atomic_##type: \
+   case nir_intrinsic_image_deref_atomic_##type: \
+   case nir_intrinsic_shared_atomic_##type
+
+   CASE_ATOMIC_OP(add):
+      return SpvOpAtomicIAdd;
+   CASE_ATOMIC_OP(umin):
+      return SpvOpAtomicUMin;
+   CASE_ATOMIC_OP(imin):
+      return SpvOpAtomicSMin;
+   CASE_ATOMIC_OP(umax):
+      return SpvOpAtomicUMax;
+   CASE_ATOMIC_OP(imax):
+      return SpvOpAtomicSMax;
+   CASE_ATOMIC_OP(and):
+      return SpvOpAtomicAnd;
+   CASE_ATOMIC_OP(or):
+      return SpvOpAtomicOr;
+   CASE_ATOMIC_OP(xor):
+      return SpvOpAtomicXor;
+   CASE_ATOMIC_OP(exchange):
+      return SpvOpAtomicExchange;
+   CASE_ATOMIC_OP(comp_swap):
+      return SpvOpAtomicCompareExchange;
+   default:
+      debug_printf("%s - ", nir_intrinsic_infos[op].name);
+      unreachable("unhandled atomic op");
+   }
+   return 0;
+}
+#undef CASE_ATOMIC_OP
 static SpvId
 emit_float_const(struct ntv_context *ctx, int bit_size, double value)
 {
@@ -194,6 +290,8 @@ get_storage_class(struct nir_variable *var)
       return SpvStorageClassInput;
    case nir_var_shader_out:
       return SpvStorageClassOutput;
+   case nir_var_uniform:
+      return SpvStorageClassUniformConstant;
    default:
       unreachable("Unsupported nir_variable_mode");
    }
@@ -251,12 +349,23 @@ get_glsl_type(struct ntv_context *ctx, const struct glsl_type *type)
          glsl_get_vector_elements(type));
 
    if (glsl_type_is_array(type)) {
-      SpvId ret = spirv_builder_type_array(&ctx->builder,
-                                           get_glsl_type(ctx, glsl_get_array_element(type)),
-                                           emit_uint_const(ctx, 32, glsl_get_length(type)));
+      SpvId ret;
+      const struct glsl_type *element_gtype = glsl_get_array_element(type);
+      SpvId element_type = get_glsl_type(ctx, element_gtype);
+      if (glsl_type_is_unsized_array(type))
+         ret = spirv_builder_type_runtime_array(&ctx->builder, element_type);
+      else
+         ret = spirv_builder_type_array(&ctx->builder,
+                                        element_type,
+                                        emit_uint_const(ctx, 32, glsl_get_length(type)));
       uint32_t stride = glsl_get_explicit_stride(type);
-      if (!stride && glsl_type_is_scalar(glsl_get_array_element(type))) {
-         stride = MAX2(glsl_get_bit_size(glsl_get_array_element(type)) / 8, 1);
+      if (!stride) {
+         if (glsl_type_is_scalar(element_gtype)) {
+            stride = MAX2(glsl_get_bit_size(glsl_get_array_element(type)) / 8, 1);
+         } else if (glsl_type_is_array(element_gtype))
+            stride = glsl_get_std430_size(element_gtype, false);
+         else if (type_is_counter(element_gtype))
+            stride = glsl_atomic_size(element_gtype);
       }
       if (stride)
          spirv_builder_emit_array_stride(&ctx->builder, ret, stride);
@@ -281,9 +390,26 @@ get_glsl_type(struct ntv_context *ctx, const struct glsl_type *type)
                                                                  glsl_get_vector_elements(type)),
                                        glsl_get_matrix_columns(type));
 
+   if (type_is_counter(type))
+      return spirv_builder_type_uint(&ctx->builder, 32);
+
    unreachable("we shouldn't get here, I think...");
 }
 
+static void
+create_shared_block(struct ntv_context *ctx, unsigned shared_size)
+{
+   SpvId type = spirv_builder_type_uint(&ctx->builder, 32);
+   SpvId array = spirv_builder_type_array(&ctx->builder, type, emit_uint_const(ctx, 32, shared_size / 4));
+   spirv_builder_emit_array_stride(&ctx->builder, array, 4);
+   SpvId ptr_type = spirv_builder_type_pointer(&ctx->builder,
+                                               SpvStorageClassWorkgroup,
+                                               array);
+   ctx->shared_block_var = spirv_builder_emit_var(&ctx->builder, ptr_type, SpvStorageClassWorkgroup);
+   assert(ctx->num_entry_ifaces < ARRAY_SIZE(ctx->entry_ifaces));
+   ctx->entry_ifaces[ctx->num_entry_ifaces++] = ctx->shared_block_var;
+}
+
 static inline unsigned char
 reserve_slot(struct ntv_context *ctx)
 {
@@ -470,6 +596,10 @@ emit_output(struct ntv_context *ctx, struct nir_variable *var)
             spirv_builder_emit_builtin(&ctx->builder, var_id, SpvBuiltInFragDepth);
             break;
 
+         case FRAG_RESULT_STENCIL:
+            spirv_builder_emit_builtin(&ctx->builder, var_id, SpvBuiltInFragStencilRefEXT);
+            break;
+
          case FRAG_RESULT_SAMPLE_MASK:
             spirv_builder_emit_builtin(&ctx->builder, var_id, SpvBuiltInSampleMask);
             break;
@@ -542,68 +672,137 @@ type_to_dim(enum glsl_sampler_dim gdim, bool *is_ms)
    return SpvDim2D;
 }
 
-uint32_t
-zink_binding(gl_shader_stage stage, VkDescriptorType type, int index)
-{
-   if (stage == MESA_SHADER_NONE ||
-       stage >= MESA_SHADER_COMPUTE) {
-      unreachable("not supported");
-   } else {
-      uint32_t stage_offset = (uint32_t)stage * (PIPE_MAX_CONSTANT_BUFFERS +
-                                                 PIPE_MAX_SAMPLERS +
-                                                 PIPE_MAX_SHADER_BUFFERS);
-
-      switch (type) {
-      case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER:
-         assert(index < PIPE_MAX_CONSTANT_BUFFERS);
-         return stage_offset + index;
-
-      case VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER:
-      case VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER:
-         assert(index < PIPE_MAX_SAMPLERS);
-         return stage_offset + PIPE_MAX_CONSTANT_BUFFERS + index;
-
-      case VK_DESCRIPTOR_TYPE_STORAGE_BUFFER:
-         assert(index < PIPE_MAX_SHADER_BUFFERS);
-         return stage_offset + PIPE_MAX_CONSTANT_BUFFERS + PIPE_MAX_SHADER_SAMPLER_VIEWS + index;
-
-      default:
-         unreachable("unexpected type");
-      }
+static inline SpvImageFormat
+get_image_format(enum pipe_format format)
+{
+   switch (format) {
+   case PIPE_FORMAT_NONE:
+      return SpvImageFormatUnknown;
+   case PIPE_FORMAT_R32G32B32A32_FLOAT:
+      return SpvImageFormatRgba32f;
+   case PIPE_FORMAT_R16G16B16A16_FLOAT:
+      return SpvImageFormatRgba16f;
+   case PIPE_FORMAT_R32_FLOAT:
+      return SpvImageFormatR32f;
+   case PIPE_FORMAT_R8G8B8A8_UNORM:
+      return SpvImageFormatRgba8;
+   case PIPE_FORMAT_R8G8B8A8_SNORM:
+      return SpvImageFormatRgba8Snorm;
+   case PIPE_FORMAT_R32G32_FLOAT:
+      return SpvImageFormatRg32f;
+   case PIPE_FORMAT_R16G16_FLOAT:
+      return SpvImageFormatRg16f;
+   case PIPE_FORMAT_R11G11B10_FLOAT:
+      return SpvImageFormatR11fG11fB10f;
+   case PIPE_FORMAT_R16_FLOAT:
+      return SpvImageFormatR16f;
+   case PIPE_FORMAT_R16G16B16A16_UNORM:
+      return SpvImageFormatRgba16;
+   case PIPE_FORMAT_R10G10B10A2_UNORM:
+      return SpvImageFormatRgb10A2;
+   case PIPE_FORMAT_R16G16_UNORM:
+      return SpvImageFormatRg16;
+   case PIPE_FORMAT_R8G8_UNORM:
+      return SpvImageFormatRg8;
+   case PIPE_FORMAT_R16_UNORM:
+      return SpvImageFormatR16;
+   case PIPE_FORMAT_R8_UNORM:
+      return SpvImageFormatR8;
+   case PIPE_FORMAT_R16G16B16A16_SNORM:
+      return SpvImageFormatRgba16Snorm;
+   case PIPE_FORMAT_R16G16_SNORM:
+      return SpvImageFormatRg16Snorm;
+   case PIPE_FORMAT_R8G8_SNORM:
+      return SpvImageFormatRg8Snorm;
+   case PIPE_FORMAT_R16_SNORM:
+      return SpvImageFormatR16Snorm;
+   case PIPE_FORMAT_R8_SNORM:
+      return SpvImageFormatR8Snorm;
+   case PIPE_FORMAT_R32G32B32A32_SINT:
+      return SpvImageFormatRgba32i;
+   case PIPE_FORMAT_R16G16B16A16_SINT:
+      return SpvImageFormatRgba16i;
+   case PIPE_FORMAT_R8G8B8A8_SINT:
+      return SpvImageFormatRgba8i;
+   case PIPE_FORMAT_R32_SINT:
+      return SpvImageFormatR32i;
+   case PIPE_FORMAT_R32G32_SINT:
+      return SpvImageFormatRg32i;
+   case PIPE_FORMAT_R16G16_SINT:
+      return SpvImageFormatRg16i;
+   case PIPE_FORMAT_R8G8_SINT:
+      return SpvImageFormatRg8i;
+   case PIPE_FORMAT_R16_SINT:
+      return SpvImageFormatR16i;
+   case PIPE_FORMAT_R8_SINT:
+      return SpvImageFormatR8i;
+   case PIPE_FORMAT_R32G32B32A32_UINT:
+      return SpvImageFormatRgba32ui;
+   case PIPE_FORMAT_R16G16B16A16_UINT:
+      return SpvImageFormatRgba16ui;
+   case PIPE_FORMAT_R8G8B8A8_UINT:
+      return SpvImageFormatRgba8ui;
+   case PIPE_FORMAT_R32_UINT:
+      return SpvImageFormatR32ui;
+   case PIPE_FORMAT_R10G10B10A2_UINT:
+      return SpvImageFormatRgb10a2ui;
+   case PIPE_FORMAT_R32G32_UINT:
+      return SpvImageFormatRg32ui;
+   case PIPE_FORMAT_R16G16_UINT:
+      return SpvImageFormatRg16ui;
+   case PIPE_FORMAT_R8G8_UINT:
+      return SpvImageFormatRg8ui;
+   case PIPE_FORMAT_R16_UINT:
+      return SpvImageFormatR16ui;
+   case PIPE_FORMAT_R8_UINT:
+      return SpvImageFormatR8ui;
+   default:
+      break;
    }
+   unreachable("unknown format");
+   return SpvImageFormatUnknown;
 }
 
 static void
-emit_sampler(struct ntv_context *ctx, struct nir_variable *var)
+emit_image(struct ntv_context *ctx, struct nir_variable *var)
 {
    const struct glsl_type *type = glsl_without_array(var->type);
 
    bool is_ms;
+   bool is_sampler = glsl_type_is_sampler(type);
+
+   if (!is_sampler && !var->data.image.format) {
+      spirv_builder_emit_cap(&ctx->builder, SpvCapabilityStorageImageWriteWithoutFormat);
+      spirv_builder_emit_cap(&ctx->builder, SpvCapabilityStorageImageReadWithoutFormat);
+   }
+
    SpvDim dimension = type_to_dim(glsl_get_sampler_dim(type), &is_ms);
 
    SpvId result_type = get_glsl_basetype(ctx, glsl_get_sampler_result_type(type));
    SpvId image_type = spirv_builder_type_image(&ctx->builder, result_type,
                                                dimension, false,
                                                glsl_sampler_type_is_array(type),
-                                               is_ms, 1,
-                                               SpvImageFormatUnknown);
+                                               is_ms, 1 + !is_sampler,
+                                               get_image_format(var->data.image.format));
 
    SpvId sampled_type = spirv_builder_type_sampled_image(&ctx->builder,
                                                          image_type);
+   SpvId var_type = is_sampler ? sampled_type : image_type;
 
-   int index = var->data.binding;
-   assert(!(ctx->samplers_used & (1 << index)));
-   assert(!ctx->image_types[index]);
+   int index = var->data.driver_location;
+   assert(!is_sampler || (!(ctx->samplers_used & (1 << index))));
+   assert(!is_sampler || !ctx->sampler_types[index]);
+   assert(is_sampler || !ctx->image_types[index]);
 
    if (glsl_type_is_array(var->type)) {
-      sampled_type = spirv_builder_type_array(&ctx->builder, sampled_type,
+      var_type = spirv_builder_type_array(&ctx->builder, var_type,
                                               emit_uint_const(ctx, 32, glsl_get_aoa_size(var->type)));
-      spirv_builder_emit_array_stride(&ctx->builder, sampled_type, sizeof(void*));
+      spirv_builder_emit_array_stride(&ctx->builder, var_type, sizeof(void*));
       ctx->sampler_array_sizes[index] = glsl_get_aoa_size(var->type);
    }
    SpvId pointer_type = spirv_builder_type_pointer(&ctx->builder,
                                                    SpvStorageClassUniformConstant,
-                                                   sampled_type);
+                                                   var_type);
 
    SpvId var_id = spirv_builder_emit_var(&ctx->builder, pointer_type,
                                          SpvStorageClassUniformConstant);
@@ -611,25 +810,89 @@ emit_sampler(struct ntv_context *ctx, struct nir_variable *var)
    if (var->name)
       spirv_builder_emit_name(&ctx->builder, var_id, var->name);
 
-   ctx->image_types[index] = image_type;
-   ctx->samplers[index] = var_id;
-   ctx->samplers_used |= 1 << index;
+   if (is_sampler) {
+      ctx->sampler_types[index] = image_type;
+      ctx->samplers[index] = var_id;
+      ctx->samplers_used |= 1 << index;
+   } else {
+      ctx->image_types[index] = image_type;
+      ctx->images[index] = var_id;
+      _mesa_hash_table_insert(ctx->vars, var, (void *)(intptr_t)var_id);
+      uint32_t *key = ralloc_size(ctx->mem_ctx, sizeof(uint32_t));
+      *key = var_id;
+      _mesa_hash_table_insert(ctx->spv_vars, key, var);
+      emit_access_decorations(ctx, var, var_id);
+   }
+   assert(ctx->num_entry_ifaces < ARRAY_SIZE(ctx->entry_ifaces));
+   ctx->entry_ifaces[ctx->num_entry_ifaces++] = var_id;
+
+   if (ctx->lazy)
+      spirv_builder_emit_descriptor_set(&ctx->builder, var_id, 0);
+   else
+      spirv_builder_emit_descriptor_set(&ctx->builder, var_id, is_sampler ? ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW : ZINK_DESCRIPTOR_TYPE_IMAGE);
+   spirv_builder_emit_binding(&ctx->builder, var_id, var->data.binding);
+}
 
-   spirv_builder_emit_descriptor_set(&ctx->builder, var_id, 0);
-   int binding = zink_binding(ctx->stage,
-                              zink_sampler_type(type),
-                              var->data.binding);
-   spirv_builder_emit_binding(&ctx->builder, var_id, binding);
+static SpvId
+get_sized_uint_array_type(struct ntv_context *ctx, unsigned array_size)
+{
+   SpvId array_length = emit_uint_const(ctx, 32, array_size);
+   SpvId array_type = spirv_builder_type_array(&ctx->builder, get_uvec_type(ctx, 32, 1),
+                                            array_length);
+   spirv_builder_emit_array_stride(&ctx->builder, array_type, 4);
+   return array_type;
 }
 
-static void
-emit_ssbo(struct ntv_context *ctx, struct nir_variable *var)
+static SpvId
+get_bo_array_type(struct ntv_context *ctx, struct nir_variable *var)
 {
-   SpvId vec4_type = get_uvec_type(ctx, 32, 4);
-   SpvId array_type = spirv_builder_type_runtime_array(&ctx->builder, vec4_type);
-   spirv_builder_emit_array_stride(&ctx->builder, array_type, 16);
+   SpvId array_type;
+   SpvId uint_type = get_uvec_type(ctx, 32, 1);
+   if (type_is_counter(var->type))
+      array_type = get_glsl_type(ctx, var->type);
+   else if (glsl_type_is_unsized_array(var->type)) {
+      array_type = spirv_builder_type_runtime_array(&ctx->builder, uint_type);
+      spirv_builder_emit_array_stride(&ctx->builder, array_type, 4);
+   } else {
+      uint32_t array_size = glsl_count_attribute_slots(var->interface_type, false);
+      array_type = get_sized_uint_array_type(ctx, array_size * 4);
+   }
+   return array_type;
+}
 
-   SpvId struct_type = spirv_builder_type_struct(&ctx->builder, &array_type, 1);
+static SpvId
+get_bo_struct_type(struct ntv_context *ctx, struct nir_variable *var)
+{
+   SpvId array_type = get_bo_array_type(ctx, var);
+   bool ssbo = var->data.mode == nir_var_mem_ssbo;
+
+   // wrap UBO-array in a struct
+   SpvId runtime_array = 0;
+   if (ssbo) {
+      if (glsl_type_is_interface(var->interface_type) && !glsl_type_is_unsized_array(var->type)) {
+          const struct glsl_type *last_member = glsl_get_struct_field(var->interface_type, glsl_get_length(var->interface_type) - 1);
+          if (glsl_type_is_unsized_array(last_member)) {
+             bool is_64bit = glsl_type_is_64bit(glsl_without_array(last_member));
+             runtime_array = spirv_builder_type_runtime_array(&ctx->builder, get_uvec_type(ctx, is_64bit ? 64 : 32, 1));
+             spirv_builder_emit_array_stride(&ctx->builder, runtime_array, glsl_get_explicit_stride(last_member));
+          }
+      }
+   }
+   SpvId struct_type;
+   if (type_is_counter(var->type)) {
+      SpvId padding = var->data.offset ? get_sized_uint_array_type(ctx, var->data.offset / 4) : 0;
+      SpvId types[2];
+      if (padding)
+         types[0] = padding, types[1] = array_type;
+      else
+         types[0] = array_type;
+      struct_type = spirv_builder_type_struct(&ctx->builder, types, 1 + !!padding);
+      if (padding)
+         spirv_builder_emit_member_offset(&ctx->builder, struct_type, 1, var->data.offset);
+   } else {
+      SpvId types[] = {array_type, runtime_array};
+      struct_type = spirv_builder_type_struct(&ctx->builder, types, 1 + !!runtime_array);
+   }
    if (var->name) {
       char struct_name[100];
       snprintf(struct_name, sizeof(struct_name), "struct_%s", var->name);
@@ -639,61 +902,31 @@ emit_ssbo(struct ntv_context *ctx, struct nir_variable *var)
    spirv_builder_emit_decoration(&ctx->builder, struct_type,
                                  SpvDecorationBlock);
    spirv_builder_emit_member_offset(&ctx->builder, struct_type, 0, 0);
-
-   SpvId pointer_type = spirv_builder_type_pointer(&ctx->builder,
-                                                   SpvStorageClassStorageBuffer,
-                                                   struct_type);
-
-   SpvId var_id = spirv_builder_emit_var(&ctx->builder, pointer_type,
-                                         SpvStorageClassStorageBuffer);
-   if (var->name) {
-      char struct_name[100];
-      snprintf(struct_name, sizeof(struct_name), "%s", var->name);
-      spirv_builder_emit_name(&ctx->builder, var_id, var->name);
+   if (runtime_array) {
+      spirv_builder_emit_member_offset(&ctx->builder, struct_type, 1,
+                                      glsl_get_struct_field_offset(var->interface_type,
+                                                                   glsl_get_length(var->interface_type) - 1));
    }
 
-   assert(var->data.binding < ARRAY_SIZE(ctx->ssbos));
-   ctx->ssbos[var->data.binding] = var_id;
-
-   spirv_builder_emit_descriptor_set(&ctx->builder, var_id, 0);
-   int binding = zink_binding(ctx->stage,
-                              VK_DESCRIPTOR_TYPE_STORAGE_BUFFER,
-                              var->data.binding);
-   spirv_builder_emit_binding(&ctx->builder, var_id, binding);
+   return spirv_builder_type_pointer(&ctx->builder,
+                                                   ssbo || type_is_counter(var->type) ? SpvStorageClassStorageBuffer : SpvStorageClassUniform,
+                                                   struct_type);
 }
 
 static void
-emit_ubo(struct ntv_context *ctx, struct nir_variable *var)
+emit_bo(struct ntv_context *ctx, struct nir_variable *var)
 {
-   bool is_ubo_array = glsl_type_is_array(var->type) && glsl_type_is_interface(glsl_without_array(var->type));
+   const struct glsl_type *bare_type = glsl_without_array(var->type);
+   bool is_ubo_array = glsl_type_is_array(var->type) && glsl_type_is_interface(bare_type);
+   bool is_counter = type_is_counter(bare_type);
    /* variables accessed inside a uniform block will get merged into a big
     * memory blob and accessed by offset
     */
-   if (var->data.location && !is_ubo_array && var->type != var->interface_type)
+   if (var->data.location > 0 && !is_ubo_array && var->type != var->interface_type && !is_counter)
       return;
+   bool ssbo = var->data.mode == nir_var_mem_ssbo;
 
-   uint32_t size = glsl_count_attribute_slots(var->interface_type, false);
-   SpvId vec4_type = get_uvec_type(ctx, 32, 4);
-   SpvId array_length = emit_uint_const(ctx, 32, size);
-   SpvId array_type = spirv_builder_type_array(&ctx->builder, vec4_type,
-                                               array_length);
-   spirv_builder_emit_array_stride(&ctx->builder, array_type, 16);
-
-   // wrap UBO-array in a struct
-   SpvId struct_type = spirv_builder_type_struct(&ctx->builder, &array_type, 1);
-   if (var->name) {
-      char struct_name[100];
-      snprintf(struct_name, sizeof(struct_name), "struct_%s", var->name);
-      spirv_builder_emit_name(&ctx->builder, struct_type, struct_name);
-   }
-
-   spirv_builder_emit_decoration(&ctx->builder, struct_type,
-                                 SpvDecorationBlock);
-   spirv_builder_emit_member_offset(&ctx->builder, struct_type, 0, 0);
-
-   SpvId pointer_type = spirv_builder_type_pointer(&ctx->builder,
-                                                   SpvStorageClassUniform,
-                                                   struct_type);
+   SpvId pointer_type = get_bo_struct_type(ctx, var);
 
    /* if this is a ubo array, create a binding point for each array member:
     * 
@@ -703,37 +936,55 @@ emit_ubo(struct ntv_context *ctx, struct nir_variable *var)
 
       (also it's just easier)
     */
-   for (unsigned i = 0; i < (is_ubo_array ? glsl_get_aoa_size(var->type) : 1); i++) {
+   unsigned size = is_ubo_array ? glsl_get_aoa_size(var->type) : 1;
+   for (unsigned i = 0; i < size; i++) {
       SpvId var_id = spirv_builder_emit_var(&ctx->builder, pointer_type,
-                                            SpvStorageClassUniform);
+                                            ssbo || is_counter ? SpvStorageClassStorageBuffer : SpvStorageClassUniform);
       if (var->name) {
          char struct_name[100];
          snprintf(struct_name, sizeof(struct_name), "%s[%u]", var->name, i);
          spirv_builder_emit_name(&ctx->builder, var_id, var->name);
       }
 
-      assert(ctx->num_ubos < ARRAY_SIZE(ctx->ubos));
-      ctx->ubos[ctx->num_ubos++] = var_id;
+      if (ssbo) {
+         assert(!ctx->ssbos[ctx->num_ssbos]);
+         ctx->ssbos[ctx->num_ssbos] = var_id;
+         ctx->ssbo_vars[ctx->num_ssbos] = var;
+         ctx->num_ssbos++;
+      } else if (!is_counter) {
+         assert(ctx->num_ubos < ARRAY_SIZE(ctx->ubos));
+         ctx->ubos[ctx->num_ubos++] = var_id;
+      }
+      assert(ctx->num_entry_ifaces < ARRAY_SIZE(ctx->entry_ifaces));
+      ctx->entry_ifaces[ctx->num_entry_ifaces++] = var_id;
 
-      spirv_builder_emit_descriptor_set(&ctx->builder, var_id, 0);
-      int binding = zink_binding(ctx->stage,
-                                 VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER,
-                                 var->data.binding + i);
-      spirv_builder_emit_binding(&ctx->builder, var_id, binding);
+      if (ctx->lazy) {
+         if (ssbo || is_counter || ctx->num_ubos > 1 || !ctx->uses_dynamic)
+            spirv_builder_emit_descriptor_set(&ctx->builder, var_id, 0);
+         else {
+            /* this is the dynamic ubo */
+            spirv_builder_emit_descriptor_set(&ctx->builder, var_id, 1);
+            assert(var->data.binding + i == ctx->stage);
+         }
+      } else
+         spirv_builder_emit_descriptor_set(&ctx->builder, var_id, ssbo || is_counter ? ZINK_DESCRIPTOR_TYPE_SSBO : ZINK_DESCRIPTOR_TYPE_UBO);
+      spirv_builder_emit_binding(&ctx->builder, var_id, var->data.binding + i);
+      if (is_counter)
+         _mesa_hash_table_insert(ctx->vars, var, (void *)(intptr_t)var_id);
    }
 }
 
 static void
 emit_uniform(struct ntv_context *ctx, struct nir_variable *var)
 {
-   if (var->data.mode == nir_var_mem_ubo)
-      emit_ubo(ctx, var);
-   else if (var->data.mode == nir_var_mem_ssbo)
-      emit_ssbo(ctx, var);
+   if (var->data.mode == nir_var_mem_ubo || var->data.mode == nir_var_mem_ssbo ||
+       type_is_counter(var->type))
+      emit_bo(ctx, var);
    else {
       assert(var->data.mode == nir_var_uniform);
-      if (glsl_type_is_sampler(glsl_without_array(var->type)))
-         emit_sampler(ctx, var);
+      const struct glsl_type *type = glsl_without_array(var->type);
+      if (glsl_type_is_sampler(type) || glsl_type_is_image(type))
+         emit_image(ctx, var);
    }
 }
 
@@ -1057,11 +1308,9 @@ emit_so_outputs(struct ntv_context *ctx,
       SpvId type = get_output_type(ctx, location, so_output.num_components);
       SpvId output = 0;
       /* index is based on component, so we might have to go back a few slots to get to the base */
-      UNUSED uint32_t orig_location = location;
       while (!output)
          output = ctx->outputs[location--];
       location++;
-      assert(orig_location - location < 4);
       SpvId output_type = ctx->so_output_types[location];
       const struct glsl_type *out_type = ctx->so_output_gl_types[location];
 
@@ -1112,22 +1361,20 @@ emit_so_outputs(struct ntv_context *ctx,
 }
 
 static SpvId
-emit_atomic(struct ntv_context *ctx, SpvId op, SpvId type, SpvId src0, SpvId src1)
-{
-   if (!type) //AtomicStore
-      return spirv_builder_emit_triop(&ctx->builder, op, src0,
-                                      emit_uint_const(ctx, 32, SpvScopeWorkgroup),
-                                      emit_uint_const(ctx, 32, SpvMemorySemanticsUniformMemoryMask | SpvMemorySemanticsReleaseMask),
-                                      src1);
-   if (src1)
-      return spirv_builder_emit_quadop(&ctx->builder, op, type, src0,
-                                       emit_uint_const(ctx, 32, SpvScopeWorkgroup),
-                                       emit_uint_const(ctx, 32, SpvMemorySemanticsUniformMemoryMask | SpvMemorySemanticsReleaseMask),
-                                       src1);
-   //AtomicLoad
-   return spirv_builder_emit_triop(&ctx->builder, op, type, src0,
-                                   emit_uint_const(ctx, 32, SpvScopeWorkgroup),
-                                   emit_uint_const(ctx, 32, SpvMemorySemanticsUniformMemoryMask | SpvMemorySemanticsAcquireMask));
+emit_atomic(struct ntv_context *ctx, SpvId op, SpvId type, SpvId src0, SpvId src1, SpvId src2)
+{
+   if (op == SpvOpAtomicLoad)
+      return spirv_builder_emit_triop(&ctx->builder, op, type, src0, emit_uint_const(ctx, 32, SpvScopeWorkgroup),
+                                       emit_uint_const(ctx, 32, 0));
+   if (op == SpvOpAtomicCompareExchange)
+      return spirv_builder_emit_hexop(&ctx->builder, op, type, src0, emit_uint_const(ctx, 32, SpvScopeWorkgroup),
+                                       emit_uint_const(ctx, 32, 0),
+                                       emit_uint_const(ctx, 32, 0),
+                                       /* these params are intentionally swapped */
+                                       src2, src1);
+
+   return spirv_builder_emit_quadop(&ctx->builder, op, type, src0, emit_uint_const(ctx, 32, SpvScopeWorkgroup),
+                                    emit_uint_const(ctx, 32, 0), src1);
 }
 
 static SpvId
@@ -1617,6 +1864,8 @@ emit_alu(struct ntv_context *ctx, nir_alu_instr *alu)
       unreachable("unsupported opcode");
       return;
    }
+   if (alu->exact)
+      spirv_builder_emit_decoration(&ctx->builder, result, SpvDecorationNoContraction);
 
    store_alu_result(ctx, alu, result);
 }
@@ -1683,8 +1932,6 @@ emit_load_bo(struct ntv_context *ctx, nir_intrinsic_instr *intr)
 
    /* destination type for the load */
    SpvId type = get_dest_uvec_type(ctx, &intr->dest);
-   /* an id of the array stride in bytes */
-   SpvId vec4_size = emit_uint_const(ctx, 32, sizeof(uint32_t) * 4);
    /* an id of an array member in bytes */
    SpvId uint_size = emit_uint_const(ctx, 32, sizeof(uint32_t));
 
@@ -1696,7 +1943,7 @@ emit_load_bo(struct ntv_context *ctx, nir_intrinsic_instr *intr)
    /* our generated uniform has a memory layout like
     *
     * struct {
-    *    vec4 base[array_size];
+    *    uint base[array_size];
     * };
     *
     * where 'array_size' is set as though every member of the ubo takes up a vec4,
@@ -1709,39 +1956,28 @@ emit_load_bo(struct ntv_context *ctx, nir_intrinsic_instr *intr)
     * it may be a const value or it may be dynamic in the shader
     */
    SpvId offset = get_src(ctx, &intr->src[1]);
-   /* convert offset to an array index for 'base' to determine which vec4 to access */
-   SpvId vec_offset = emit_binop(ctx, SpvOpUDiv, uint_type, offset, vec4_size);
-   /* use the remainder to calculate the byte offset in the vec, which tells us the member
-    * that we're going to access
-    */
-   SpvId vec_member_offset = emit_binop(ctx, SpvOpUDiv, uint_type,
-                                        emit_binop(ctx, SpvOpUMod, uint_type, offset, vec4_size),
-                                        uint_size);
+   /* calculate the byte offset in the array */
+   SpvId vec_offset = emit_binop(ctx, SpvOpUDiv, uint_type, offset, uint_size);
    /* OpAccessChain takes an array of indices that drill into a hierarchy based on the type:
     * index 0 is accessing 'base'
     * index 1 is accessing 'base[index 1]'
-    * index 2 is accessing 'base[index 1][index 2]'
     *
     * we must perform the access this way in case src[1] is dynamic because there's
     * no other spirv method for using an id to access a member of a composite, as
     * (composite|vector)_extract both take literals
     */
    for (unsigned i = 0; i < num_components; i++) {
-      SpvId indices[3] = { member, vec_offset, vec_member_offset };
+      SpvId indices[2] = { member, vec_offset };
       SpvId ptr = spirv_builder_emit_access_chain(&ctx->builder, pointer_type,
                                                   bo, indices,
                                                   ARRAY_SIZE(indices));
       /* load a single value into the constituents array */
       if (ssbo)
-         constituents[i] = emit_atomic(ctx, SpvOpAtomicLoad, uint_type, ptr, 0);
+         constituents[i] = emit_atomic(ctx, SpvOpAtomicLoad, uint_type, ptr, 0, 0);
       else
          constituents[i] = spirv_builder_emit_load(&ctx->builder, uint_type, ptr);
-      /* increment to the next vec4 member index for the next load */
-      vec_member_offset = emit_binop(ctx, SpvOpIAdd, uint_type, vec_member_offset, one);
-      if (i == 3 && num_components > 4) {
-         vec_offset = emit_binop(ctx, SpvOpIAdd, uint_type, vec_offset, one);
-         vec_member_offset = emit_uint_const(ctx, 32, 0);
-      }
+      /* increment to the next member index for the next load */
+      vec_offset = emit_binop(ctx, SpvOpIAdd, uint_type, vec_offset, one);
    }
 
    /* if we're loading a 64bit value, we have to reassemble all the u32 values we've loaded into u64 values
@@ -1846,7 +2082,73 @@ emit_store_deref(struct ntv_context *ctx, nir_intrinsic_instr *intr)
    spirv_builder_emit_store(&ctx->builder, ptr, result);
 }
 
-/* FIXME: this is currently VERY specific to injected TCS usage */
+static void
+emit_load_shared(struct ntv_context *ctx, nir_intrinsic_instr *intr)
+{
+   SpvId dest_type = get_dest_type(ctx, &intr->dest, nir_type_uint);
+   unsigned num_components = nir_dest_num_components(intr->dest);
+   unsigned bit_size = nir_dest_bit_size(intr->dest);
+   bool qword = bit_size == 64;
+   SpvId uint_type = get_uvec_type(ctx, 32, 1);
+   SpvId ptr_type = spirv_builder_type_pointer(&ctx->builder,
+                                               SpvStorageClassWorkgroup,
+                                               uint_type);
+   SpvId offset = emit_binop(ctx, SpvOpUDiv, uint_type, get_src(ctx, &intr->src[0]), emit_uint_const(ctx, 32, 4));
+   SpvId constituents[num_components];
+   /* need to convert array -> vec */
+   for (unsigned i = 0; i < num_components; i++) {
+      SpvId parts[2];
+      for (unsigned j = 0; j < 1 + !!qword; j++) {
+         SpvId member = spirv_builder_emit_access_chain(&ctx->builder, ptr_type,
+                                                        ctx->shared_block_var, &offset, 1);
+         parts[j] = spirv_builder_emit_load(&ctx->builder, uint_type, member);
+         offset = emit_binop(ctx, SpvOpIAdd, uint_type, offset, emit_uint_const(ctx, 32, 1));
+      }
+      if (qword)
+         constituents[i] = spirv_builder_emit_composite_construct(&ctx->builder, get_uvec_type(ctx, 64, 1), parts, 2);
+      else
+         constituents[i] = parts[0];
+   }
+   SpvId result;
+   if (num_components > 1)
+      result = spirv_builder_emit_composite_construct(&ctx->builder, dest_type, constituents, num_components);
+   else
+      result = bitcast_to_uvec(ctx, constituents[0], bit_size, num_components);
+   store_dest(ctx, &intr->dest, result, nir_type_uint);
+}
+
+static void
+emit_store_shared(struct ntv_context *ctx, nir_intrinsic_instr *intr)
+{
+   SpvId ptr = get_src(ctx, &intr->src[0]);
+   bool qword = nir_src_bit_size(intr->src[0]) == 64;
+
+   unsigned num_writes = util_bitcount(nir_intrinsic_write_mask(intr));
+   unsigned wrmask = nir_intrinsic_write_mask(intr);
+   /* this is a partial write, so we have to loop and do a per-component write */
+   SpvId uint_type = get_uvec_type(ctx, 32, 1);
+   SpvId ptr_type = spirv_builder_type_pointer(&ctx->builder,
+                                               SpvStorageClassWorkgroup,
+                                               uint_type);
+   SpvId offset = emit_binop(ctx, SpvOpUDiv, uint_type, get_src(ctx, &intr->src[1]), emit_uint_const(ctx, 32, 4));
+
+   for (unsigned i = 0; num_writes; i++) {
+      if ((wrmask >> i) & 1) {
+         for (unsigned j = 0; j < 1 + !!qword; j++) {
+            unsigned comp = ((1 + !!qword) * i) + j;
+            SpvId shared_offset = emit_binop(ctx, SpvOpIAdd, uint_type, offset, emit_uint_const(ctx, 32, comp));
+            SpvId val = ptr;
+            if (nir_src_num_components(intr->src[0]) != 1)
+               val = spirv_builder_emit_composite_extract(&ctx->builder, uint_type, ptr, &comp, 1);
+            SpvId member = spirv_builder_emit_access_chain(&ctx->builder, ptr_type,
+                                                           ctx->shared_block_var, &shared_offset, 1);
+             spirv_builder_emit_store(&ctx->builder, member, val);
+         }
+         num_writes--;
+      }
+   }
+}
+
 static void
 emit_load_push_const(struct ntv_context *ctx, nir_intrinsic_instr *intr)
 {
@@ -1864,8 +2166,6 @@ emit_load_push_const(struct ntv_context *ctx, nir_intrinsic_instr *intr)
 
    /* destination type for the load */
    SpvId type = get_dest_uvec_type(ctx, &intr->dest);
-   /* an id of an array member in bytes */
-   SpvId uint_size = emit_uint_const(ctx, 32, sizeof(uint32_t));
    SpvId one = emit_uint_const(ctx, 32, 1);
 
    /* we grab a single array member at a time, so it's a pointer to a uint */
@@ -1873,12 +2173,9 @@ emit_load_push_const(struct ntv_context *ctx, nir_intrinsic_instr *intr)
                                                    SpvStorageClassPushConstant,
                                                    load_type);
 
-   SpvId member = emit_uint_const(ctx, 32, 0);
-   /* this is the offset (in bytes) that we're accessing:
-    * it may be a const value or it may be dynamic in the shader
-    */
-   SpvId offset = get_src(ctx, &intr->src[0]);
-   offset = emit_binop(ctx, SpvOpUDiv, uint_type, offset, uint_size);
+   SpvId member = get_src(ctx, &intr->src[0]);
+   /* reuse the offset from ZINK_PUSH_CONST_OFFSET */
+   SpvId offset = emit_uint_const(ctx, 32, 0);
    /* OpAccessChain takes an array of indices that drill into a hierarchy based on the type:
     * index 0 is accessing 'base'
     * index 1 is accessing 'base[index 1]'
@@ -2047,9 +2344,17 @@ emit_interpolate(struct ntv_context *ctx, nir_intrinsic_instr *intr)
 }
 
 static void
-emit_atomic_intrinsic(struct ntv_context *ctx, nir_intrinsic_instr *intr)
+handle_atomic_op(struct ntv_context *ctx, nir_intrinsic_instr *intr, SpvId ptr, SpvId param, SpvId param2)
+{
+   SpvId dest_type = get_dest_type(ctx, &intr->dest, nir_type_uint32);
+   SpvId result = emit_atomic(ctx, get_atomic_op(intr->intrinsic), dest_type, ptr, param, param2);
+   assert(result);
+   store_dest(ctx, &intr->dest, result, nir_type_uint);
+}
+
+static void
+emit_ssbo_atomic_intrinsic(struct ntv_context *ctx, nir_intrinsic_instr *intr)
 {
-   SpvId result = 0;
    SpvId ssbo;
    SpvId param;
    SpvId dest_type = get_dest_type(ctx, &intr->dest, nir_type_uint32);
@@ -2064,27 +2369,204 @@ emit_atomic_intrinsic(struct ntv_context *ctx, nir_intrinsic_instr *intr)
                                                    dest_type);
    SpvId uint_type = get_uvec_type(ctx, 32, 1);
    /* an id of the array stride in bytes */
-   SpvId vec4_size = emit_uint_const(ctx, 32, sizeof(uint32_t) * 4);
-   /* an id of an array member in bytes */
    SpvId uint_size = emit_uint_const(ctx, 32, sizeof(uint32_t));
    SpvId member = emit_uint_const(ctx, 32, 0);
    SpvId offset = get_src(ctx, &intr->src[1]);
-   SpvId vec_offset = emit_binop(ctx, SpvOpUDiv, uint_type, offset, vec4_size);
-   SpvId vec_member_offset = emit_binop(ctx, SpvOpUDiv, uint_type,
-                                        emit_binop(ctx, SpvOpUMod, uint_type, offset, vec4_size),
-                                        uint_size);
-   SpvId indices[3] = { member, vec_offset, vec_member_offset };
+   SpvId vec_offset = emit_binop(ctx, SpvOpUDiv, uint_type, offset, uint_size);
+   SpvId indices[] = { member, vec_offset };
    SpvId ptr = spirv_builder_emit_access_chain(&ctx->builder, pointer_type,
                                                ssbo, indices,
                                                ARRAY_SIZE(indices));
+
+   SpvId param2 = 0;
+
+   if (intr->intrinsic == nir_intrinsic_ssbo_atomic_comp_swap)
+      param2 = get_src(ctx, &intr->src[3]);
+
+   handle_atomic_op(ctx, intr, ptr, param, param2);
+}
+
+static void
+emit_shared_atomic_intrinsic(struct ntv_context *ctx, nir_intrinsic_instr *intr)
+{
+   SpvId dest_type = get_dest_type(ctx, &intr->dest, nir_type_uint32);
+   SpvId param = get_src(ctx, &intr->src[1]);
+
+   SpvId pointer_type = spirv_builder_type_pointer(&ctx->builder,
+                                                   SpvStorageClassWorkgroup,
+                                                   dest_type);
+   SpvId offset = emit_binop(ctx, SpvOpUDiv, get_uvec_type(ctx, 32, 1), get_src(ctx, &intr->src[0]), emit_uint_const(ctx, 32, 4));
+   SpvId ptr = spirv_builder_emit_access_chain(&ctx->builder, pointer_type,
+                                               ctx->shared_block_var, &offset, 1);
+
+   SpvId param2 = 0;
+
+   if (intr->intrinsic == nir_intrinsic_shared_atomic_comp_swap)
+      param2 = get_src(ctx, &intr->src[2]);
+
+   handle_atomic_op(ctx, intr, ptr, param, param2);
+}
+
+static SpvId
+get_coords(struct ntv_context *ctx, const struct glsl_type *type, nir_src *src)
+{
+   uint32_t num_coords = glsl_get_sampler_coordinate_components(type);
+   uint32_t src_components = nir_src_num_components(*src);
+
+   SpvId spv = get_src(ctx, src);
+   if (num_coords == src_components)
+      return spv;
+
+   /* need to extract the coord dimensions that the image can use */
+   SpvId vec_type = get_uvec_type(ctx, 32, num_coords);
+   if (num_coords == 1)
+      return spirv_builder_emit_vector_extract(&ctx->builder, vec_type, spv, 0);
+   uint32_t constituents[4];
+   for (unsigned i = 0; i < num_coords; i++)
+      constituents[i] = i;
+   return spirv_builder_emit_vector_shuffle(&ctx->builder, vec_type, spv, spv, constituents, num_coords);
+}
+
+static void
+emit_image_intrinsic(struct ntv_context *ctx, nir_intrinsic_instr *intr)
+{
+   SpvId img_var = get_src(ctx, &intr->src[0]);
+   SpvId sample = get_src(ctx, &intr->src[2]);
+   SpvId param = get_src(ctx, &intr->src[3]);
+   nir_variable *var = get_var_from_spvid(ctx, img_var);
+   const struct glsl_type *type = glsl_without_array(var->type);
+   SpvId coord = get_coords(ctx, type, &intr->src[1]);
+   SpvId base_type = get_glsl_basetype(ctx, glsl_get_sampler_result_type(type));
+   SpvId texel = spirv_builder_emit_image_texel_pointer(&ctx->builder, base_type, img_var, coord, sample);
+   SpvId param2 = 0;
+
+   if (intr->intrinsic == nir_intrinsic_image_deref_atomic_comp_swap)
+      param2 = get_src(ctx, &intr->src[4]);
+   handle_atomic_op(ctx, intr, texel, param, param2);
+}
+
+static void
+emit_vote(struct ntv_context *ctx, nir_intrinsic_instr *intr)
+{
+   SpvOp op;
+
    switch (intr->intrinsic) {
-   case nir_intrinsic_ssbo_atomic_add:
-      result = emit_atomic(ctx, SpvOpAtomicIAdd, dest_type, ptr, param);
+   case nir_intrinsic_vote_all:
+      op = SpvOpGroupNonUniformAll;
+      break;
+   case nir_intrinsic_vote_any:
+      op = SpvOpGroupNonUniformAny;
+      break;
+   case nir_intrinsic_vote_ieq:
+   case nir_intrinsic_vote_feq:
+      op = SpvOpGroupNonUniformAllEqual;
       break;
    default:
-      fprintf(stderr, "emit_atomic_intrinsic: not implemented (%s)\n",
-              nir_intrinsic_infos[intr->intrinsic].name);
-      unreachable("unsupported intrinsic");
+      unreachable("unknown vote intrinsic");
+   }
+   SpvId result = spirv_builder_emit_vote(&ctx->builder, op, get_src(ctx, &intr->src[0]));
+   store_dest_raw(ctx, &intr->dest, result);
+}
+
+
+static nir_intrinsic_op
+counter_op_to_atomic(nir_intrinsic_op op)
+{
+   /* from nir_lower_atomics_to_ssbo */
+   switch (op) {
+   case nir_intrinsic_memory_barrier_atomic_counter:
+      /* Atomic counters are now SSBOs so memoryBarrierAtomicCounter() is now
+       * memoryBarrierBuffer().
+       */
+      return nir_intrinsic_memory_barrier_buffer;
+      return true;
+
+   case nir_intrinsic_atomic_counter_inc_deref:
+   case nir_intrinsic_atomic_counter_add_deref:
+   case nir_intrinsic_atomic_counter_pre_dec_deref:
+   case nir_intrinsic_atomic_counter_post_dec_deref:
+      /* inc and dec get remapped to add_deref: */
+      return nir_intrinsic_ssbo_atomic_add;
+      break;
+   case nir_intrinsic_atomic_counter_read_deref:
+      return nir_intrinsic_load_ssbo;
+      break;
+   case nir_intrinsic_atomic_counter_min_deref:
+      return nir_intrinsic_ssbo_atomic_umin;
+      break;
+   case nir_intrinsic_atomic_counter_max_deref:
+      return nir_intrinsic_ssbo_atomic_umax;
+      break;
+   case nir_intrinsic_atomic_counter_and_deref:
+      return nir_intrinsic_ssbo_atomic_and;
+      break;
+   case nir_intrinsic_atomic_counter_or_deref:
+      return nir_intrinsic_ssbo_atomic_or;
+      break;
+   case nir_intrinsic_atomic_counter_xor_deref:
+      return nir_intrinsic_ssbo_atomic_xor;
+      break;
+   case nir_intrinsic_atomic_counter_exchange_deref:
+      return nir_intrinsic_ssbo_atomic_exchange;
+      break;
+   case nir_intrinsic_atomic_counter_comp_swap_deref:
+      return nir_intrinsic_ssbo_atomic_comp_swap;
+      break;
+   default:
+      break;
+   }
+   return 0;
+}
+
+static void
+handle_counter_op(struct ntv_context *ctx, nir_intrinsic_instr *intr)
+{
+   SpvId dest_type = get_dest_uvec_type(ctx, &intr->dest);
+   SpvId srcs[3] = {};
+   for (int i = 0; i < nir_intrinsic_infos[intr->intrinsic].num_srcs; i++)
+      srcs[i] = get_src(ctx, &intr->src[i]);
+
+   SpvOp op;
+   nir_intrinsic_op atomic_op = counter_op_to_atomic(intr->intrinsic);
+   if (atomic_op != nir_intrinsic_load_ssbo)
+      op = get_atomic_op(atomic_op);
+
+   SpvId result = 0;
+   switch (intr->intrinsic) {
+   case nir_intrinsic_atomic_counter_comp_swap_deref:
+   case nir_intrinsic_atomic_counter_add_deref:
+   case nir_intrinsic_atomic_counter_and_deref:
+   case nir_intrinsic_atomic_counter_exchange_deref:
+   case nir_intrinsic_atomic_counter_inc_deref:
+   case nir_intrinsic_atomic_counter_max_deref:
+   case nir_intrinsic_atomic_counter_min_deref:
+   case nir_intrinsic_atomic_counter_or_deref:
+   case nir_intrinsic_atomic_counter_xor_deref:
+      result = emit_atomic(ctx, op, dest_type,
+                         srcs[0],
+                         srcs[1] ? srcs[1] : emit_uint_const(ctx, nir_dest_bit_size(intr->dest), 1),
+                         srcs[2]);
+      break;
+   case nir_intrinsic_atomic_counter_post_dec_deref:
+      result = emit_atomic(ctx, op, dest_type, srcs[0],
+                                      emit_uint_const(ctx, nir_dest_bit_size(intr->dest), -1),
+                                      0);
+      break;
+   case nir_intrinsic_atomic_counter_pre_dec_deref:
+      result = emit_atomic(ctx, op, dest_type, srcs[0],
+                                      emit_uint_const(ctx, nir_dest_bit_size(intr->dest), -1),
+                                      0);
+      /* this intrinsic returns the decremented value, so the decrement is done
+       * manually here to avoid coherency issues related to performing an atomic load
+       * without using a barrier
+       */
+      result = emit_binop(ctx, SpvOpIAdd, dest_type, result, emit_uint_const(ctx, nir_dest_bit_size(intr->dest), -1));
+      break;
+   case nir_intrinsic_atomic_counter_read_deref:
+      result = emit_atomic(ctx, SpvOpAtomicLoad, dest_type, srcs[0], 0, 0);
+      break;
+   default:
+      unreachable("unhandled counter op");
    }
    assert(result);
    store_dest(ctx, &intr->dest, result, nir_type_uint);
@@ -2099,6 +2581,93 @@ emit_intrinsic(struct ntv_context *ctx, nir_intrinsic_instr *intr)
       emit_load_bo(ctx, intr);
       break;
 
+   /* TODO: would be great to refactor this in with emit_load_bo() */
+   case nir_intrinsic_store_ssbo: {
+      nir_const_value *const_block_index = nir_src_as_const_value(intr->src[1]);
+      assert(const_block_index);
+
+      SpvId bo = ctx->ssbos[const_block_index->u32];
+
+      unsigned bit_size = nir_src_bit_size(intr->src[0]);
+      SpvId uint_type = get_uvec_type(ctx, 32, 1);
+      SpvId one = emit_uint_const(ctx, 32, 1);
+
+      /* number of components being stored */
+      unsigned wrmask = nir_intrinsic_write_mask(intr);
+      unsigned num_writes = util_bitcount(wrmask);
+      unsigned num_components = num_writes;
+
+      /* we need to grab 2x32 to fill the 64bit value */
+      bool is_64bit = bit_size == 64;
+
+      /* an id of an array member in bytes */
+      SpvId uint_size = emit_uint_const(ctx, 32, sizeof(uint32_t));
+      /* we grab a single array member at a time, so it's a pointer to a uint */
+      SpvId pointer_type = spirv_builder_type_pointer(&ctx->builder,
+                                                      SpvStorageClassStorageBuffer,
+                                                      uint_type);
+
+      /* our generated uniform has a memory layout like
+       *
+       * struct {
+       *    uint base[array_size];
+       * };
+       *
+       * where 'array_size' is set as though every member of the ubo takes up a vec4,
+       * even if it's only a vec2 or a float.
+       *
+       * first, access 'base'
+       */
+      SpvId member = emit_uint_const(ctx, 32, 0);
+      /* this is the offset (in bytes) that we're accessing:
+       * it may be a const value or it may be dynamic in the shader
+       */
+      SpvId offset = get_src(ctx, &intr->src[2]);
+      /* calculate byte offset */
+      SpvId vec_offset = emit_binop(ctx, SpvOpUDiv, uint_type, offset, uint_size);
+
+      SpvId value = get_src(ctx, &intr->src[0]);
+      /* OpAccessChain takes an array of indices that drill into a hierarchy based on the type:
+       * index 0 is accessing 'base'
+       * index 1 is accessing 'base[index 1]'
+       * index 2 is accessing 'base[index 1][index 2]'
+       *
+       * we must perform the access this way in case src[1] is dynamic because there's
+       * no other spirv method for using an id to access a member of a composite, as
+       * (composite|vector)_extract both take literals
+       */
+      unsigned write_count = 0;
+      SpvId src_base_type = get_uvec_type(ctx, nir_src_bit_size(intr->src[0]), 1);
+      for (unsigned i = 0; write_count < num_components; i++) {
+         if (wrmask & (1 << i)) {
+            SpvId component = nir_src_num_components(intr->src[0]) > 1 ?
+                              spirv_builder_emit_composite_extract(&ctx->builder, src_base_type, value, &i, 1) :
+                              value;
+            SpvId component_split;
+            if (is_64bit)
+               component_split = emit_bitcast(ctx, get_uvec_type(ctx, 32, 2), component);
+            for (unsigned j = 0; j < 1 + !!is_64bit; j++) {
+               if (j)
+                  vec_offset = emit_binop(ctx, SpvOpIAdd, uint_type, vec_offset, one);
+               SpvId indices[] = { member, vec_offset };
+               SpvId ptr = spirv_builder_emit_access_chain(&ctx->builder, pointer_type,
+                                                           bo, indices,
+                                                           ARRAY_SIZE(indices));
+               if (is_64bit)
+                  component = spirv_builder_emit_composite_extract(&ctx->builder, uint_type, component_split, &j, 1);
+               spirv_builder_emit_atomic_store(&ctx->builder, ptr, SpvScopeWorkgroup, 0, component);
+            }
+            write_count++;
+         } else if (is_64bit)
+            /* we're doing 32bit stores here, so we need to increment correctly here */
+            vec_offset = emit_binop(ctx, SpvOpIAdd, uint_type, vec_offset, one);
+
+         /* increment to the next vec4 member index for the next store */
+         vec_offset = emit_binop(ctx, SpvOpIAdd, uint_type, vec_offset, one);
+      }
+      break;
+   }
+
    case nir_intrinsic_discard:
       emit_discard(ctx, intr);
       break;
@@ -2129,6 +2698,14 @@ emit_intrinsic(struct ntv_context *ctx, nir_intrinsic_instr *intr)
       emit_load_uint_input(ctx, intr, &ctx->instance_id_var, "gl_InstanceId", SpvBuiltInInstanceIndex);
       break;
 
+   case nir_intrinsic_load_base_vertex:
+      emit_load_uint_input(ctx, intr, &ctx->base_vertex_var, "gl_BaseVertex", SpvBuiltInBaseVertex);
+      break;
+
+   case nir_intrinsic_load_draw_id:
+      emit_load_uint_input(ctx, intr, &ctx->draw_id_var, "gl_DrawID", SpvBuiltInDrawIndex);
+      break;
+
    case nir_intrinsic_load_vertex_id:
       emit_load_uint_input(ctx, intr, &ctx->vertex_id_var, "gl_VertexId", SpvBuiltInVertexIndex);
       break;
@@ -2170,6 +2747,10 @@ emit_intrinsic(struct ntv_context *ctx, nir_intrinsic_instr *intr)
       spirv_builder_end_primitive(&ctx->builder, nir_intrinsic_stream_id(intr));
       break;
 
+   case nir_intrinsic_load_helper_invocation:
+      emit_load_vec_input(ctx, intr, &ctx->helper_invocation_var, "gl_HelperInvocation", SpvBuiltInHelperInvocation, nir_type_bool);
+      break;
+
    case nir_intrinsic_load_patch_vertices_in:
       emit_load_vec_input(ctx, intr, &ctx->tess_patch_vertices_in, "gl_PatchVerticesIn",
                           SpvBuiltInPatchVertices, nir_type_int);
@@ -2185,6 +2766,18 @@ emit_intrinsic(struct ntv_context *ctx, nir_intrinsic_instr *intr)
                                         SpvMemorySemanticsOutputMemoryMask | SpvMemorySemanticsReleaseMask);
       break;
 
+   case nir_intrinsic_memory_barrier:
+      spirv_builder_emit_memory_barrier(&ctx->builder, SpvScopeWorkgroup,
+                                        SpvMemorySemanticsImageMemoryMask | SpvMemorySemanticsUniformMemoryMask |
+                                        SpvMemorySemanticsMakeVisibleMask  | SpvMemorySemanticsAcquireReleaseMask);
+      break;
+
+   case nir_intrinsic_memory_barrier_shared:
+      spirv_builder_emit_memory_barrier(&ctx->builder, SpvScopeWorkgroup,
+                                        SpvMemorySemanticsWorkgroupMemoryMask |
+                                        SpvMemorySemanticsAcquireReleaseMask);
+      break;
+
    case nir_intrinsic_control_barrier:
       spirv_builder_emit_control_barrier(&ctx->builder, SpvScopeWorkgroup,
                                          SpvScopeWorkgroup,
@@ -2198,7 +2791,161 @@ emit_intrinsic(struct ntv_context *ctx, nir_intrinsic_instr *intr)
       break;
 
    case nir_intrinsic_ssbo_atomic_add:
-      emit_atomic_intrinsic(ctx, intr);
+   case nir_intrinsic_ssbo_atomic_umin:
+   case nir_intrinsic_ssbo_atomic_imin:
+   case nir_intrinsic_ssbo_atomic_umax:
+   case nir_intrinsic_ssbo_atomic_imax:
+   case nir_intrinsic_ssbo_atomic_and:
+   case nir_intrinsic_ssbo_atomic_or:
+   case nir_intrinsic_ssbo_atomic_xor:
+   case nir_intrinsic_ssbo_atomic_exchange:
+   case nir_intrinsic_ssbo_atomic_comp_swap:
+      emit_ssbo_atomic_intrinsic(ctx, intr);
+      break;
+
+   case nir_intrinsic_atomic_counter_add_deref:
+   case nir_intrinsic_atomic_counter_and_deref:
+   case nir_intrinsic_atomic_counter_comp_swap_deref:
+   case nir_intrinsic_atomic_counter_exchange_deref:
+   case nir_intrinsic_atomic_counter_inc_deref:
+   case nir_intrinsic_atomic_counter_max_deref:
+   case nir_intrinsic_atomic_counter_min_deref:
+   case nir_intrinsic_atomic_counter_or_deref:
+   case nir_intrinsic_atomic_counter_post_dec_deref:
+   case nir_intrinsic_atomic_counter_pre_dec_deref:
+   case nir_intrinsic_atomic_counter_read_deref:
+   case nir_intrinsic_atomic_counter_xor_deref:
+      handle_counter_op(ctx, intr);
+      break;
+
+   case nir_intrinsic_shared_atomic_add:
+   case nir_intrinsic_shared_atomic_umin:
+   case nir_intrinsic_shared_atomic_imin:
+   case nir_intrinsic_shared_atomic_umax:
+   case nir_intrinsic_shared_atomic_imax:
+   case nir_intrinsic_shared_atomic_and:
+   case nir_intrinsic_shared_atomic_or:
+   case nir_intrinsic_shared_atomic_xor:
+   case nir_intrinsic_shared_atomic_exchange:
+   case nir_intrinsic_shared_atomic_comp_swap:
+      emit_shared_atomic_intrinsic(ctx, intr);
+      break;
+
+   case nir_intrinsic_get_ssbo_size: {
+      SpvId uint_type = get_uvec_type(ctx, 32, 1);
+      nir_variable *var = ctx->ssbo_vars[nir_src_as_const_value(intr->src[0])->u32];
+      SpvId result = spirv_builder_emit_binop(&ctx->builder, SpvOpArrayLength, uint_type,
+                                              ctx->ssbos[nir_src_as_const_value(intr->src[0])->u32], 1);
+      /* this is going to be converted by nir to:
+
+         length = (buffer_size - offset) / stride
+
+        * so we need to un-convert it to avoid having the calculation performed twice
+        */
+      unsigned last_member_idx = glsl_get_length(var->interface_type) - 1;
+      const struct glsl_type *last_member = glsl_get_struct_field(var->interface_type, last_member_idx);
+      /* multiply by stride */
+      result = emit_binop(ctx, SpvOpIMul, uint_type, result, emit_uint_const(ctx, 32, glsl_get_explicit_stride(last_member)));
+      /* get total ssbo size by adding offset */
+      result = emit_binop(ctx, SpvOpIAdd, uint_type, result,
+                          emit_uint_const(ctx, 32,
+                                          glsl_get_struct_field_offset(var->interface_type, last_member_idx)));
+      store_dest(ctx, &intr->dest, result, nir_type_uint);
+      break;
+   }
+
+   case nir_intrinsic_image_deref_store: {
+      SpvId img_var = get_src(ctx, &intr->src[0]);
+      nir_variable *var = get_var_from_spvid(ctx, img_var);
+      SpvId img_type = ctx->image_types[var->data.driver_location];
+      const struct glsl_type *type = glsl_without_array(var->type);
+      SpvId base_type = get_glsl_basetype(ctx, glsl_get_sampler_result_type(type));
+      SpvId img = spirv_builder_emit_load(&ctx->builder, img_type, img_var);
+      SpvId coord = get_coords(ctx, type, &intr->src[1]);
+      SpvId texel = get_src(ctx, &intr->src[3]);
+      /* texel type must match image type */
+      texel = emit_bitcast(ctx,
+                           spirv_builder_type_vector(&ctx->builder, base_type, 4),
+                           texel);
+      spirv_builder_emit_image_write(&ctx->builder, img, coord, texel, 0, 0, 0);
+      break;
+   }
+   case nir_intrinsic_image_deref_load: {
+      SpvId img_var = get_src(ctx, &intr->src[0]);
+      nir_variable *var = get_var_from_spvid(ctx, img_var);
+      SpvId img_type = ctx->image_types[var->data.driver_location];
+      const struct glsl_type *type = glsl_without_array(var->type);
+      SpvId base_type = get_glsl_basetype(ctx, glsl_get_sampler_result_type(type));
+      SpvId img = spirv_builder_emit_load(&ctx->builder, img_type, img_var);
+      SpvId coord = get_coords(ctx, type, &intr->src[1]);
+      SpvId result = spirv_builder_emit_image_read(&ctx->builder,
+                                    spirv_builder_type_vector(&ctx->builder, base_type, nir_dest_num_components(intr->dest)),
+                                    img, coord, 0, 0, 0);
+      store_dest(ctx, &intr->dest, result, nir_type_float);
+      break;
+   }
+   case nir_intrinsic_image_deref_size: {
+      SpvId img_var = get_src(ctx, &intr->src[0]);
+      nir_variable *var = get_var_from_spvid(ctx, img_var);
+      SpvId img_type = ctx->image_types[var->data.driver_location];
+      const struct glsl_type *type = glsl_without_array(var->type);
+      SpvId img = spirv_builder_emit_load(&ctx->builder, img_type, img_var);
+      SpvId result = spirv_builder_emit_image_query_size(&ctx->builder, get_uvec_type(ctx, 32, glsl_get_sampler_coordinate_components(type)), img, 0);
+      store_dest(ctx, &intr->dest, result, nir_type_uint);
+      break;
+   }
+   case nir_intrinsic_image_deref_atomic_add:
+   case nir_intrinsic_image_deref_atomic_umin:
+   case nir_intrinsic_image_deref_atomic_imin:
+   case nir_intrinsic_image_deref_atomic_umax:
+   case nir_intrinsic_image_deref_atomic_imax:
+   case nir_intrinsic_image_deref_atomic_and:
+   case nir_intrinsic_image_deref_atomic_or:
+   case nir_intrinsic_image_deref_atomic_xor:
+   case nir_intrinsic_image_deref_atomic_exchange:
+   case nir_intrinsic_image_deref_atomic_comp_swap:
+      emit_image_intrinsic(ctx, intr);
+      break;
+
+   case nir_intrinsic_load_work_group_id:
+      emit_load_vec_input(ctx, intr, &ctx->workgroup_id_var, "gl_WorkGroupID", SpvBuiltInWorkgroupId, nir_type_uint);
+      break;
+
+   case nir_intrinsic_load_num_work_groups:
+      emit_load_vec_input(ctx, intr, &ctx->num_workgroups_var, "gl_NumWorkGroups", SpvBuiltInNumWorkgroups, nir_type_uint);
+      break;
+
+   case nir_intrinsic_load_local_invocation_id:
+      emit_load_vec_input(ctx, intr, &ctx->local_invocation_id_var, "gl_LocalInvocationID", SpvBuiltInLocalInvocationId, nir_type_uint);
+      break;
+
+   case nir_intrinsic_load_global_invocation_id:
+      emit_load_vec_input(ctx, intr, &ctx->global_invocation_id_var, "gl_GlobalInvocationID", SpvBuiltInGlobalInvocationId, nir_type_uint);
+      break;
+
+   case nir_intrinsic_load_local_invocation_index:
+      emit_load_uint_input(ctx, intr, &ctx->local_invocation_index_var, "gl_LocalInvocationIndex", SpvBuiltInLocalInvocationIndex);
+      break;
+
+   case nir_intrinsic_load_local_group_size: {
+      assert(ctx->local_group_size_var);
+      store_dest(ctx, &intr->dest, ctx->local_group_size_var, nir_type_uint);
+      break;
+   }
+
+   case nir_intrinsic_load_shared:
+      emit_load_shared(ctx, intr);
+      break;
+
+   case nir_intrinsic_store_shared:
+      emit_store_shared(ctx, intr);
+      break;
+
+   case nir_intrinsic_vote_all:
+   case nir_intrinsic_vote_any:
+   case nir_intrinsic_vote_ieq:
+   case nir_intrinsic_vote_feq:
+      emit_vote(ctx, intr);
       break;
 
    default:
@@ -2260,7 +3007,9 @@ emit_tex(struct ntv_context *ctx, nir_tex_instr *tex)
           tex->op == nir_texop_txf_ms ||
           tex->op == nir_texop_txs ||
           tex->op == nir_texop_lod ||
-          tex->op == nir_texop_tg4);
+          tex->op == nir_texop_tg4 ||
+          tex->op == nir_texop_texture_samples ||
+          tex->op == nir_texop_query_levels);
    assert(tex->texture_index == tex->sampler_index);
 
    SpvId coord = 0, proj = 0, bias = 0, lod = 0, dref = 0, dx = 0, dy = 0,
@@ -2364,7 +3113,7 @@ emit_tex(struct ntv_context *ctx, nir_tex_instr *tex)
          }
       }
    }
-   SpvId image_type = ctx->image_types[texture_index];
+   SpvId image_type = ctx->sampler_types[texture_index];
    assert(image_type);
    SpvId sampled_type = spirv_builder_type_sampled_image(&ctx->builder,
                                                          image_type);
@@ -2389,6 +3138,20 @@ emit_tex(struct ntv_context *ctx, nir_tex_instr *tex)
       store_dest(ctx, &tex->dest, result, tex->dest_type);
       return;
    }
+   if (tex->op == nir_texop_query_levels) {
+      SpvId image = spirv_builder_emit_image(&ctx->builder, image_type, load);
+      SpvId result = spirv_builder_emit_image_query_levels(&ctx->builder,
+                                                         dest_type, image);
+      store_dest(ctx, &tex->dest, result, tex->dest_type);
+      return;
+   }
+   if (tex->op == nir_texop_texture_samples) {
+      SpvId image = spirv_builder_emit_image(&ctx->builder, image_type, load);
+      SpvId result = spirv_builder_emit_unop(&ctx->builder, SpvOpImageQuerySamples,
+                                             dest_type, image);
+      store_dest(ctx, &tex->dest, result, tex->dest_type);
+      return;
+   }
 
    if (proj && coord_components > 0) {
       SpvId constituents[coord_components + 1];
@@ -2431,9 +3194,9 @@ emit_tex(struct ntv_context *ctx, nir_tex_instr *tex)
        tex->op == nir_texop_tg4) {
       SpvId image = spirv_builder_emit_image(&ctx->builder, image_type, load);
 
+      if (offset || (const_offset && tex->op == nir_texop_tg4))
+         spirv_builder_emit_cap(&ctx->builder, SpvCapabilityImageGatherExtended);
       if (tex->op == nir_texop_tg4) {
-         if (const_offset)
-            spirv_builder_emit_cap(&ctx->builder, SpvCapabilityImageGatherExtended);
          result = spirv_builder_emit_image_gather(&ctx->builder, dest_type,
                                                  load, coord, emit_uint_const(ctx, 32, tex->component),
                                                  lod, sample, const_offset, offset, dref);
@@ -2520,6 +3283,18 @@ emit_deref_var(struct ntv_context *ctx, nir_deref_instr *deref)
    struct hash_entry *he = _mesa_hash_table_search(ctx->vars, deref->var);
    assert(he);
    SpvId result = (SpvId)(intptr_t)he->data;
+   if (type_is_counter(deref->var->type)) {
+      SpvId dest_type = glsl_type_is_array(deref->var->type) ?
+                        get_glsl_type(ctx, deref->var->type) :
+                        get_dest_uvec_type(ctx, &deref->dest);
+      SpvId ptr_type = spirv_builder_type_pointer(&ctx->builder,
+                                                  SpvStorageClassStorageBuffer,
+                                                  dest_type);
+      SpvId idx[] = {
+         emit_uint_const(ctx, 32, !!deref->var->data.offset),
+      };
+      result = spirv_builder_emit_access_chain(&ctx->builder, ptr_type, result, idx, ARRAY_SIZE(idx));
+   }
    store_dest_raw(ctx, &deref->dest, result);
 }
 
@@ -2530,19 +3305,55 @@ emit_deref_array(struct ntv_context *ctx, nir_deref_instr *deref)
    nir_variable *var = nir_deref_instr_get_variable(deref);
 
    SpvStorageClass storage_class = get_storage_class(var);
+   SpvId base;
+   SpvId type;
+   switch (var->data.mode) {
+   case nir_var_shader_in:
+      base = get_src(ctx, &deref->parent);
+      type = get_glsl_type(ctx, deref->type);
+      break;
+
+   case nir_var_shader_out:
+      base = get_src(ctx, &deref->parent);
+      type = get_glsl_type(ctx, deref->type);
+      break;
+
+   case nir_var_uniform:
+      if (type_is_counter(var->type)) {
+         storage_class = SpvStorageClassStorageBuffer;
+         type = get_glsl_type(ctx, deref->type);
+         base = get_src(ctx, &deref->parent);
+      } else {
+         type = ctx->image_types[var->data.driver_location];
+         struct hash_entry *he = _mesa_hash_table_search(ctx->vars, var);
+         assert(he);
+         base = (SpvId)(intptr_t)he->data;
+      }
+      break;
+
+   default:
+      unreachable("Unsupported nir_variable_mode\n");
+   }
 
    SpvId index = get_src(ctx, &deref->arr.index);
 
    SpvId ptr_type = spirv_builder_type_pointer(&ctx->builder,
                                                storage_class,
-                                               get_glsl_type(ctx, deref->type));
+                                               type);
 
    SpvId result = spirv_builder_emit_access_chain(&ctx->builder,
                                                   ptr_type,
-                                                  get_src(ctx, &deref->parent),
+                                                  base,
                                                   &index, 1);
    /* uint is a bit of a lie here, it's really just an opaque type */
    store_dest(ctx, &deref->dest, result, nir_type_uint);
+
+   /* image ops always need to be able to get the variable to check out sampler types and such */
+   if (glsl_type_is_image(glsl_without_array(var->type))) {
+      uint32_t *key = ralloc_size(ctx->mem_ctx, sizeof(uint32_t));
+      *key = result;
+      _mesa_hash_table_insert(ctx->spv_vars, key, var);
+   }
 }
 
 static void
@@ -2810,11 +3621,15 @@ get_output_prim_type_mode(uint16_t type)
 
 struct spirv_shader *
 nir_to_spirv(struct nir_shader *s, const struct zink_so_info *so_info,
-             unsigned char *shader_slot_map, unsigned char *shader_slots_reserved)
+             unsigned char *shader_slot_map, unsigned char *shader_slots_reserved,
+             const struct zink_device_info *info,
+             bool lazy)
 {
    struct spirv_shader *ret = NULL;
 
    struct ntv_context ctx = {};
+   ctx.lazy = lazy;
+   ctx.uses_dynamic = !!s->num_uniforms;
    ctx.mem_ctx = ralloc_context(NULL);
    ctx.builder.mem_ctx = ctx.mem_ctx;
 
@@ -2847,7 +3662,17 @@ nir_to_spirv(struct nir_shader *s, const struct zink_so_info *so_info,
       unreachable("invalid stage");
    }
 
-   if (s->info.num_ssbos)
+   if (s->info.stage != MESA_SHADER_GEOMETRY) {
+      /* TODO: check vk features for this */
+      if (s->info.outputs_written & BITFIELD64_BIT(VARYING_SLOT_LAYER) ||
+          s->info.inputs_read & BITFIELD64_BIT(VARYING_SLOT_LAYER)) {
+         spirv_builder_emit_extension(&ctx.builder, "SPV_EXT_shader_viewport_index_layer");
+         spirv_builder_emit_cap(&ctx.builder, SpvCapabilityShaderViewportIndexLayerEXT);
+         spirv_builder_emit_cap(&ctx.builder, SpvCapabilityShaderLayer);
+      }
+   }
+
+   if (s->info.num_ssbos || s->info.num_abos)
       spirv_builder_emit_extension(&ctx.builder, "SPV_KHR_storage_buffer_storage_class");
 
    if (s->info.outputs_written & BITFIELD64_BIT(VARYING_SLOT_VIEWPORT)) {
@@ -2858,12 +3683,15 @@ nir_to_spirv(struct nir_shader *s, const struct zink_so_info *so_info,
    }
 
    // TODO: only enable when needed
-   if (s->info.stage == MESA_SHADER_FRAGMENT) {
+   if (s->info.stage == MESA_SHADER_FRAGMENT || s->info.num_images) {
       spirv_builder_emit_cap(&ctx.builder, SpvCapabilitySampled1D);
+      spirv_builder_emit_cap(&ctx.builder, SpvCapabilityImage1D);
       spirv_builder_emit_cap(&ctx.builder, SpvCapabilityImageQuery);
       spirv_builder_emit_cap(&ctx.builder, SpvCapabilityDerivativeControl);
       spirv_builder_emit_cap(&ctx.builder, SpvCapabilitySampleRateShading);
+      spirv_builder_emit_cap(&ctx.builder, SpvCapabilityStorageImageExtendedFormats);
    }
+
    if (s->info.bit_sizes_int & 64)
       spirv_builder_emit_cap(&ctx.builder, SpvCapabilityInt64);
    if (s->info.bit_sizes_float & 64)
@@ -2871,18 +3699,32 @@ nir_to_spirv(struct nir_shader *s, const struct zink_so_info *so_info,
 
    ctx.stage = s->info.stage;
    ctx.so_info = so_info;
-   ctx.shader_slot_map = shader_slot_map;
-   ctx.shader_slots_reserved = *shader_slots_reserved;
+
+   if (shader_slot_map) {
+      /* COMPUTE doesn't have this */
+      ctx.shader_slot_map = shader_slot_map;
+      ctx.shader_slots_reserved = *shader_slots_reserved;
+   }
    ctx.GLSL_std_450 = spirv_builder_import(&ctx.builder, "GLSL.std.450");
    spirv_builder_emit_source(&ctx.builder, SpvSourceLanguageGLSL, 450);
 
-   if (s->info.stage == MESA_SHADER_TESS_CTRL) {
+   if (s->info.stage == MESA_SHADER_TESS_CTRL || s->info.num_images) {
       /* this is required for correct barrier and io semantics */
       spirv_builder_emit_extension(&ctx.builder, "SPV_KHR_vulkan_memory_model");
       spirv_builder_emit_cap(&ctx.builder, SpvCapabilityVulkanMemoryModel);
       spirv_builder_emit_cap(&ctx.builder, SpvCapabilityVulkanMemoryModelDeviceScope);
       spirv_builder_emit_mem_model(&ctx.builder, SpvAddressingModelLogical,
                                    SpvMemoryModelVulkan);
+   } else if (s->info.stage == MESA_SHADER_COMPUTE) {
+      SpvAddressingModel model;
+      if (s->info.cs.ptr_size == 32)
+         model = SpvAddressingModelPhysical32;
+      else if (s->info.cs.ptr_size == 64)
+         model = SpvAddressingModelPhysical64;
+      else
+         model = SpvAddressingModelLogical;
+      spirv_builder_emit_mem_model(&ctx.builder, model,
+                                   SpvMemoryModelGLSL450);
    } else
       spirv_builder_emit_mem_model(&ctx.builder, SpvAddressingModelLogical,
                                    SpvMemoryModelGLSL450);
@@ -2920,6 +3762,9 @@ nir_to_spirv(struct nir_shader *s, const struct zink_so_info *so_info,
    ctx.vars = _mesa_hash_table_create(ctx.mem_ctx, _mesa_hash_pointer,
                                       _mesa_key_pointer_equal);
 
+   ctx.spv_vars = _mesa_hash_table_create(ctx.mem_ctx, _mesa_hash_u32,
+                                      _mesa_key_u32_equal);
+
    ctx.so_outputs = _mesa_hash_table_create(ctx.mem_ctx, _mesa_hash_u32,
                                             _mesa_key_u32_equal);
 
@@ -2944,11 +3789,17 @@ nir_to_spirv(struct nir_shader *s, const struct zink_so_info *so_info,
 
    switch (s->info.stage) {
    case MESA_SHADER_FRAGMENT:
+      if (s->info.outputs_written & BITFIELD64_BIT(FRAG_RESULT_STENCIL)) {
+         spirv_builder_emit_extension(&ctx.builder, "SPV_EXT_shader_stencil_export");
+         spirv_builder_emit_cap(&ctx.builder, SpvCapabilityStencilExportEXT);
+      }
       spirv_builder_emit_exec_mode(&ctx.builder, entry_point,
                                    SpvExecutionModeOriginUpperLeft);
       if (s->info.outputs_written & BITFIELD64_BIT(FRAG_RESULT_DEPTH))
          spirv_builder_emit_exec_mode(&ctx.builder, entry_point,
                                       SpvExecutionModeDepthReplacing);
+      if (s->info.fs.early_fragment_tests)
+         spirv_builder_emit_exec_mode(&ctx.builder, entry_point, SpvExecutionModeEarlyFragmentTests);
       break;
    case MESA_SHADER_TESS_CTRL:
       spirv_builder_emit_exec_mode_literal(&ctx.builder, entry_point, SpvExecutionModeOutputVertices, s->info.tess.tcs_vertices_out);
@@ -2993,6 +3844,29 @@ nir_to_spirv(struct nir_shader *s, const struct zink_so_info *so_info,
       spirv_builder_emit_exec_mode_literal(&ctx.builder, entry_point, SpvExecutionModeInvocations, s->info.gs.invocations);
       spirv_builder_emit_exec_mode_literal(&ctx.builder, entry_point, SpvExecutionModeOutputVertices, s->info.gs.vertices_out);
       break;
+   case MESA_SHADER_COMPUTE:
+      if (s->info.cs.local_size[0] || s->info.cs.local_size[1] || s->info.cs.local_size[2])
+         spirv_builder_emit_exec_mode_literal3(&ctx.builder, entry_point, SpvExecutionModeLocalSize,
+                                               (uint32_t[3]){(uint32_t)s->info.cs.local_size[0], (uint32_t)s->info.cs.local_size[1],
+                                               (uint32_t)s->info.cs.local_size[2]});
+      if (s->info.cs.shared_size)
+         create_shared_block(&ctx, s->info.cs.shared_size);
+
+      if (s->info.system_values_read & BITFIELD64_BIT(SYSTEM_VALUE_LOCAL_GROUP_SIZE)) {
+         SpvId sizes[3];
+         uint32_t ids[] = {ZINK_WORKGROUP_SIZE_X, ZINK_WORKGROUP_SIZE_Y, ZINK_WORKGROUP_SIZE_Z};
+         const char *names[] = {"x", "y", "z"};
+         for (int i = 0; i < 3; i ++) {
+            sizes[i] = spirv_builder_spec_const_uint(&ctx.builder, 32);
+            spirv_builder_emit_specid(&ctx.builder, sizes[i], ids[i]);
+            spirv_builder_emit_name(&ctx.builder, sizes[i], names[i]);
+         }
+         SpvId var_type = get_uvec_type(&ctx, 32, 3);
+         ctx.local_group_size_var = spirv_builder_spec_const_composite(&ctx.builder, var_type, sizes, 3);
+         spirv_builder_emit_name(&ctx.builder, ctx.local_group_size_var, "gl_LocalGroupSize");
+         spirv_builder_emit_builtin(&ctx.builder, ctx.local_group_size_var, SpvBuiltInWorkgroupSize);
+      }
+      break;
    default:
       break;
    }
@@ -3068,11 +3942,13 @@ nir_to_spirv(struct nir_shader *s, const struct zink_so_info *so_info,
    if (!ret->words)
       goto fail;
 
-   ret->num_words = spirv_builder_get_words(&ctx.builder, ret->words, num_words);
+   ret->num_words = spirv_builder_get_words(&ctx.builder, ret->words, num_words,
+                                            info->device_version >= VK_MAKE_VERSION(1,2,0));
    assert(ret->num_words == num_words);
 
    ralloc_free(ctx.mem_ctx);
-   *shader_slots_reserved = ctx.shader_slots_reserved;
+   if (shader_slots_reserved)
+      *shader_slots_reserved = ctx.shader_slots_reserved;
 
    return ret;
 
diff --git a/src/gallium/drivers/zink/nir_to_spirv/nir_to_spirv.h b/src/gallium/drivers/zink/nir_to_spirv/nir_to_spirv.h
index ec170a710b1..484758337c3 100644
--- a/src/gallium/drivers/zink/nir_to_spirv/nir_to_spirv.h
+++ b/src/gallium/drivers/zink/nir_to_spirv/nir_to_spirv.h
@@ -41,22 +41,28 @@ struct spirv_shader {
 
 struct nir_shader;
 struct pipe_stream_output_info;
+struct zink_device_info;
 
 struct spirv_shader *
 nir_to_spirv(struct nir_shader *s, const struct zink_so_info *so_info,
-             unsigned char *shader_slot_map, unsigned char *shader_slots_reserved);
+             unsigned char *shader_slot_map, unsigned char *shader_slots_reserved,
+             const struct zink_device_info *info,
+             bool lazy);
 
 void
 spirv_shader_delete(struct spirv_shader *s);
 
-uint32_t
-zink_binding(gl_shader_stage stage, VkDescriptorType type, int index);
+static inline bool
+type_is_counter(const struct glsl_type *type)
+{
+   return glsl_get_base_type(glsl_without_array(type)) == GLSL_TYPE_ATOMIC_UINT;
+}
 
 static inline VkDescriptorType
 zink_sampler_type(const struct glsl_type *type)
 {
    assert(glsl_type_is_sampler(type));
-   if (glsl_get_sampler_dim(type) < GLSL_SAMPLER_DIM_BUF || glsl_get_sampler_dim(type) == GLSL_SAMPLER_DIM_MS)
+   if (glsl_get_sampler_dim(type) < GLSL_SAMPLER_DIM_BUF || glsl_get_sampler_dim(type) == GLSL_SAMPLER_DIM_MS || glsl_get_sampler_dim(type) == GLSL_SAMPLER_DIM_EXTERNAL)
       return VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER;
    if (glsl_get_sampler_dim(type) == GLSL_SAMPLER_DIM_BUF)
       return VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER;
@@ -64,6 +70,18 @@ zink_sampler_type(const struct glsl_type *type)
    return 0;
 }
 
+static inline VkDescriptorType
+zink_image_type(const struct glsl_type *type)
+{
+   assert(glsl_type_is_image(type));
+   if (glsl_get_sampler_dim(type) < GLSL_SAMPLER_DIM_BUF || glsl_get_sampler_dim(type) == GLSL_SAMPLER_DIM_MS || glsl_get_sampler_dim(type) == GLSL_SAMPLER_DIM_EXTERNAL)
+      return VK_DESCRIPTOR_TYPE_STORAGE_IMAGE;
+   if (glsl_get_sampler_dim(type) == GLSL_SAMPLER_DIM_BUF)
+      return VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER;
+   unreachable("unimplemented");
+   return 0;
+}
+
 struct nir_shader;
 
 bool
diff --git a/src/gallium/drivers/zink/nir_to_spirv/spirv_builder.c b/src/gallium/drivers/zink/nir_to_spirv/spirv_builder.c
index 1254f4cd19b..9a5a4af9544 100644
--- a/src/gallium/drivers/zink/nir_to_spirv/spirv_builder.c
+++ b/src/gallium/drivers/zink/nir_to_spirv/spirv_builder.c
@@ -159,6 +159,18 @@ spirv_builder_emit_exec_mode_literal(struct spirv_builder *b, SpvId entry_point,
    spirv_buffer_emit_word(&b->exec_modes, param);
 }
 
+void
+spirv_builder_emit_exec_mode_literal3(struct spirv_builder *b, SpvId entry_point,
+                                     SpvExecutionMode exec_mode, uint32_t param[3])
+{
+   spirv_buffer_prepare(&b->exec_modes, b->mem_ctx, 6);
+   spirv_buffer_emit_word(&b->exec_modes, SpvOpExecutionMode | (6 << 16));
+   spirv_buffer_emit_word(&b->exec_modes, entry_point);
+   spirv_buffer_emit_word(&b->exec_modes, exec_mode);
+   for (unsigned i = 0; i < 3; i++)
+      spirv_buffer_emit_word(&b->exec_modes, param[i]);
+}
+
 void
 spirv_builder_emit_exec_mode(struct spirv_builder *b, SpvId entry_point,
                              SpvExecutionMode exec_mode)
@@ -202,6 +214,13 @@ spirv_builder_emit_decoration(struct spirv_builder *b, SpvId target,
    emit_decoration(b, target, decoration, NULL, 0);
 }
 
+void
+spirv_builder_emit_specid(struct spirv_builder *b, SpvId target, uint32_t id)
+{
+   uint32_t args[] = { id };
+   emit_decoration(b, target, SpvDecorationSpecId, args, ARRAY_SIZE(args));
+}
+
 void
 spirv_builder_emit_location(struct spirv_builder *b, SpvId target,
                             uint32_t location)
@@ -407,6 +426,17 @@ spirv_builder_emit_store(struct spirv_builder *b, SpvId pointer, SpvId object)
    spirv_buffer_emit_word(&b->instructions, object);
 }
 
+void
+spirv_builder_emit_atomic_store(struct spirv_builder *b, SpvId pointer, SpvScope scope, SpvMemorySemanticsMask semantics, SpvId object)
+{
+   spirv_buffer_prepare(&b->instructions, b->mem_ctx, 5);
+   spirv_buffer_emit_word(&b->instructions, SpvOpAtomicStore | (5 << 16));
+   spirv_buffer_emit_word(&b->instructions, pointer);
+   spirv_buffer_emit_word(&b->instructions, spirv_builder_const_uint(b, 32, scope));
+   spirv_buffer_emit_word(&b->instructions, spirv_builder_const_uint(b, 32, semantics));
+   spirv_buffer_emit_word(&b->instructions, object);
+}
+
 SpvId
 spirv_builder_emit_access_chain(struct spirv_builder *b, SpvId result_type,
                                 SpvId base, const SpvId indexes[],
@@ -488,6 +518,25 @@ spirv_builder_emit_quadop(struct spirv_builder *b, SpvOp op, SpvId result_type,
    return result;
 }
 
+SpvId
+spirv_builder_emit_hexop(struct spirv_builder *b, SpvOp op, SpvId result_type,
+                         SpvId operand0, SpvId operand1, SpvId operand2, SpvId operand3,
+                         SpvId operand4, SpvId operand5)
+{
+   SpvId result = spirv_builder_new_id(b);
+   spirv_buffer_prepare(&b->instructions, b->mem_ctx, 9);
+   spirv_buffer_emit_word(&b->instructions, op | (9 << 16));
+   spirv_buffer_emit_word(&b->instructions, result_type);
+   spirv_buffer_emit_word(&b->instructions, result);
+   spirv_buffer_emit_word(&b->instructions, operand0);
+   spirv_buffer_emit_word(&b->instructions, operand1);
+   spirv_buffer_emit_word(&b->instructions, operand2);
+   spirv_buffer_emit_word(&b->instructions, operand3);
+   spirv_buffer_emit_word(&b->instructions, operand4);
+   spirv_buffer_emit_word(&b->instructions, operand5);
+   return result;
+}
+
 SpvId
 spirv_builder_emit_composite_extract(struct spirv_builder *b, SpvId result_type,
                                      SpvId composite, const uint32_t indexes[],
@@ -658,6 +707,13 @@ spirv_builder_emit_kill(struct spirv_builder *b)
    spirv_buffer_emit_word(&b->instructions, SpvOpKill | (1 << 16));
 }
 
+SpvId
+spirv_builder_emit_vote(struct spirv_builder *b, SpvOp op, SpvId src)
+{
+   return spirv_builder_emit_binop(b, op, spirv_builder_type_bool(b),
+                                   spirv_builder_const_uint(b, 32, SpvScopeWorkgroup), src);
+}
+
 SpvId
 spirv_builder_emit_image_sample(struct spirv_builder *b,
                                 SpvId result_type,
@@ -741,6 +797,105 @@ spirv_builder_emit_image(struct spirv_builder *b, SpvId result_type,
    return result;
 }
 
+SpvId
+spirv_builder_emit_image_texel_pointer(struct spirv_builder *b,
+                                       SpvId result_type,
+                                       SpvId image,
+                                       SpvId coordinate,
+                                       SpvId sample)
+{
+   SpvId pointer_type = spirv_builder_type_pointer(b,
+                                                   SpvStorageClassImage,
+                                                   result_type);
+   return spirv_builder_emit_triop(b, SpvOpImageTexelPointer, pointer_type, image, coordinate, sample);
+}
+
+SpvId
+spirv_builder_emit_image_read(struct spirv_builder *b,
+                              SpvId result_type,
+                              SpvId image,
+                              SpvId coordinate,
+                              SpvId lod,
+                              SpvId sample,
+                              SpvId offset)
+{
+   SpvId result = spirv_builder_new_id(b);
+
+   SpvImageOperandsMask operand_mask = SpvImageOperandsMakeTexelVisibleMask | SpvImageOperandsNonPrivateTexelMask;
+   SpvId extra_operands[5];
+   int num_extra_operands = 1;
+   extra_operands[1] = spirv_builder_const_uint(b, 32, SpvScopeWorkgroup);
+   if (lod) {
+      extra_operands[++num_extra_operands] = lod;
+      operand_mask |= SpvImageOperandsLodMask;
+   }
+   if (sample) {
+      extra_operands[++num_extra_operands] = sample;
+      operand_mask |= SpvImageOperandsSampleMask;
+   }
+   if (offset) {
+      extra_operands[++num_extra_operands] = offset;
+      operand_mask |= SpvImageOperandsOffsetMask;
+   }
+   /* finalize num_extra_operands / extra_operands */
+   if (num_extra_operands > 0) {
+      extra_operands[0] = operand_mask;
+      num_extra_operands++;
+   }
+
+   spirv_buffer_prepare(&b->instructions, b->mem_ctx, 5 + num_extra_operands);
+   spirv_buffer_emit_word(&b->instructions, SpvOpImageRead |
+                          ((5 + num_extra_operands) << 16));
+   spirv_buffer_emit_word(&b->instructions, result_type);
+   spirv_buffer_emit_word(&b->instructions, result);
+   spirv_buffer_emit_word(&b->instructions, image);
+   spirv_buffer_emit_word(&b->instructions, coordinate);
+   for (int i = 0; i < num_extra_operands; ++i)
+      spirv_buffer_emit_word(&b->instructions, extra_operands[i]);
+   return result;
+}
+
+void
+spirv_builder_emit_image_write(struct spirv_builder *b,
+                               SpvId image,
+                               SpvId coordinate,
+                               SpvId texel,
+                               SpvId lod,
+                               SpvId sample,
+                               SpvId offset)
+{
+   SpvImageOperandsMask operand_mask = SpvImageOperandsMakeTexelAvailableMask | SpvImageOperandsNonPrivateTexelMask;
+   SpvId extra_operands[5];
+   int num_extra_operands = 1;
+   extra_operands[1] = spirv_builder_const_uint(b, 32, SpvScopeWorkgroup);
+   if (lod) {
+      extra_operands[++num_extra_operands] = lod;
+      operand_mask |= SpvImageOperandsLodMask;
+   }
+   if (sample) {
+      extra_operands[++num_extra_operands] = sample;
+      operand_mask |= SpvImageOperandsSampleMask;
+   }
+   if (offset) {
+      extra_operands[++num_extra_operands] = offset;
+      operand_mask |= SpvImageOperandsOffsetMask;
+   }
+   /* finalize num_extra_operands / extra_operands */
+   if (num_extra_operands > 0) {
+      extra_operands[0] = operand_mask;
+      num_extra_operands++;
+   }
+
+   spirv_buffer_prepare(&b->instructions, b->mem_ctx, 4 + num_extra_operands);
+   spirv_buffer_emit_word(&b->instructions, SpvOpImageWrite |
+                          ((4 + num_extra_operands) << 16));
+   spirv_buffer_emit_word(&b->instructions, image);
+   spirv_buffer_emit_word(&b->instructions, coordinate);
+   spirv_buffer_emit_word(&b->instructions, texel);
+   for (int i = 0; i < num_extra_operands; ++i)
+      spirv_buffer_emit_word(&b->instructions, extra_operands[i]);
+}
+
 SpvId
 spirv_builder_emit_image_gather(struct spirv_builder *b,
                                SpvId result_type,
@@ -875,6 +1030,14 @@ spirv_builder_emit_image_query_size(struct spirv_builder *b,
    return result;
 }
 
+SpvId
+spirv_builder_emit_image_query_levels(struct spirv_builder *b,
+                                    SpvId result_type,
+                                    SpvId image)
+{
+   return spirv_builder_emit_unop(b, SpvOpImageQueryLevels, result_type, image);
+}
+
 SpvId
 spirv_builder_emit_image_query_lod(struct spirv_builder *b,
                                     SpvId result_type,
@@ -1261,6 +1424,12 @@ spirv_builder_const_uint(struct spirv_builder *b, int width, uint64_t val)
       return emit_constant_64(b, type, val);
 }
 
+SpvId
+spirv_builder_spec_const_uint(struct spirv_builder *b, int width)
+{
+   return spirv_builder_emit_unop(b, SpvOpSpecConstant, spirv_builder_type_uint(b, width), 1);
+}
+
 SpvId
 spirv_builder_const_float(struct spirv_builder *b, int width, double val)
 {
@@ -1282,6 +1451,25 @@ spirv_builder_const_composite(struct spirv_builder *b, SpvId result_type,
                         num_constituents);
 }
 
+SpvId
+spirv_builder_spec_const_composite(struct spirv_builder *b, SpvId result_type,
+                                   const SpvId constituents[],
+                                   size_t num_constituents)
+{
+   SpvId result = spirv_builder_new_id(b);
+
+   assert(num_constituents > 0);
+   int words = 3 + num_constituents;
+   spirv_buffer_prepare(&b->instructions, b->mem_ctx, words);
+   spirv_buffer_emit_word(&b->instructions,
+                          SpvOpSpecConstantComposite | (words << 16));
+   spirv_buffer_emit_word(&b->instructions, result_type);
+   spirv_buffer_emit_word(&b->instructions, result);
+   for (int i = 0; i < num_constituents; ++i)
+      spirv_buffer_emit_word(&b->instructions, constituents[i]);
+   return result;
+}
+
 SpvId
 spirv_builder_emit_var(struct spirv_builder *b, SpvId type,
                        SpvStorageClass storage_class)
@@ -1350,13 +1538,13 @@ spirv_builder_get_num_words(struct spirv_builder *b)
 
 size_t
 spirv_builder_get_words(struct spirv_builder *b, uint32_t *words,
-                        size_t num_words)
+                        size_t num_words, bool vk_12)
 {
    assert(num_words >= spirv_builder_get_num_words(b));
 
    size_t written  = 0;
    words[written++] = SpvMagicNumber;
-   words[written++] = 0x00010000;
+   words[written++] = vk_12 ? 0x00010500 : 0x00010000;
    words[written++] = 0;
    words[written++] = b->prev_id + 1;
    words[written++] = 0;
diff --git a/src/gallium/drivers/zink/nir_to_spirv/spirv_builder.h b/src/gallium/drivers/zink/nir_to_spirv/spirv_builder.h
index 6f0281212a7..270860231f4 100644
--- a/src/gallium/drivers/zink/nir_to_spirv/spirv_builder.h
+++ b/src/gallium/drivers/zink/nir_to_spirv/spirv_builder.h
@@ -89,6 +89,9 @@ void
 spirv_builder_emit_decoration(struct spirv_builder *b, SpvId target,
                               SpvDecoration decoration);
 
+void
+spirv_builder_emit_specid(struct spirv_builder *b, SpvId target, uint32_t id);
+
 void
 spirv_builder_emit_location(struct spirv_builder *b, SpvId target,
                             uint32_t location);
@@ -144,6 +147,9 @@ void
 spirv_builder_emit_exec_mode_literal(struct spirv_builder *b, SpvId entry_point,
                                      SpvExecutionMode exec_mode, uint32_t param);
 void
+spirv_builder_emit_exec_mode_literal3(struct spirv_builder *b, SpvId entry_point,
+                                     SpvExecutionMode exec_mode, uint32_t param[3]);
+void
 spirv_builder_emit_exec_mode(struct spirv_builder *b, SpvId entry_point,
                              SpvExecutionMode exec_mode);
 
@@ -169,6 +175,9 @@ SpvId
 spirv_builder_emit_load(struct spirv_builder *b, SpvId result_type,
                         SpvId pointer);
 
+void
+spirv_builder_emit_atomic_store(struct spirv_builder *b, SpvId pointer, SpvScope scope, SpvMemorySemanticsMask semantics, SpvId object);
+
 void
 spirv_builder_emit_store(struct spirv_builder *b, SpvId pointer, SpvId object);
 
@@ -194,6 +203,10 @@ spirv_builder_emit_quadop(struct spirv_builder *b, SpvOp op, SpvId result_type,
                          SpvId operand0, SpvId operand1, SpvId operand2, SpvId operand3);
 
 SpvId
+spirv_builder_emit_hexop(struct spirv_builder *b, SpvOp op, SpvId result_type,
+                         SpvId operand0, SpvId operand1, SpvId operand2, SpvId operand3,
+                         SpvId operand4, SpvId operand5);
+SpvId
 spirv_builder_emit_composite_extract(struct spirv_builder *b, SpvId result_type,
                                      SpvId composite, const uint32_t indexes[],
                                      size_t num_indexes);
@@ -244,6 +257,8 @@ spirv_builder_set_phi_operand(struct spirv_builder *b, size_t position,
 void
 spirv_builder_emit_kill(struct spirv_builder *b);
 
+SpvId
+spirv_builder_emit_vote(struct spirv_builder *b, SpvOp op, SpvId src);
 
 SpvId
 spirv_builder_emit_image_sample(struct spirv_builder *b,
@@ -263,6 +278,31 @@ SpvId
 spirv_builder_emit_image(struct spirv_builder *b, SpvId result_type,
                          SpvId sampled_image);
 
+SpvId
+spirv_builder_emit_image_texel_pointer(struct spirv_builder *b,
+                                       SpvId result_type,
+                                       SpvId image,
+                                       SpvId coordinate,
+                                       SpvId sample);
+
+SpvId
+spirv_builder_emit_image_read(struct spirv_builder *b,
+                              SpvId result_type,
+                              SpvId image,
+                              SpvId coordinate,
+                              SpvId lod,
+                              SpvId sample,
+                              SpvId offset);
+
+void
+spirv_builder_emit_image_write(struct spirv_builder *b,
+                               SpvId image,
+                               SpvId coordinate,
+                               SpvId texel,
+                               SpvId lod,
+                               SpvId sample,
+                               SpvId offset);
+
 SpvId
 spirv_builder_emit_image_fetch(struct spirv_builder *b,
                                SpvId result_type,
@@ -290,6 +330,11 @@ spirv_builder_emit_image_query_size(struct spirv_builder *b,
                                     SpvId image,
                                     SpvId lod);
 
+SpvId
+spirv_builder_emit_image_query_levels(struct spirv_builder *b,
+                                    SpvId result_type,
+                                    SpvId image);
+
 SpvId
 spirv_builder_emit_image_query_lod(struct spirv_builder *b,
                                     SpvId result_type,
@@ -361,6 +406,9 @@ spirv_builder_const_int(struct spirv_builder *b, int width, int64_t val);
 SpvId
 spirv_builder_const_uint(struct spirv_builder *b, int width, uint64_t val);
 
+SpvId
+spirv_builder_spec_const_uint(struct spirv_builder *b, int width);
+
 SpvId
 spirv_builder_const_float(struct spirv_builder *b, int width, double val);
 
@@ -369,6 +417,11 @@ spirv_builder_const_composite(struct spirv_builder *b, SpvId result_type,
                               const SpvId constituents[],
                               size_t num_constituents);
 
+SpvId
+spirv_builder_spec_const_composite(struct spirv_builder *b, SpvId result_type,
+                                   const SpvId constituents[],
+                                   size_t num_constituents);
+
 SpvId
 spirv_builder_emit_var(struct spirv_builder *b, SpvId type,
                        SpvStorageClass storage_class);
@@ -387,7 +440,7 @@ spirv_builder_get_num_words(struct spirv_builder *b);
 
 size_t
 spirv_builder_get_words(struct spirv_builder *b, uint32_t *words,
-                        size_t num_words);
+                        size_t num_words, bool vk_12);
 
 void
 spirv_builder_emit_vertex(struct spirv_builder *b, uint32_t stream);
diff --git a/src/gallium/drivers/zink/zink_batch.c b/src/gallium/drivers/zink/zink_batch.c
index bb8d8f364b8..eb6b9000023 100644
--- a/src/gallium/drivers/zink/zink_batch.c
+++ b/src/gallium/drivers/zink/zink_batch.c
@@ -9,91 +9,280 @@
 #include "zink_resource.h"
 #include "zink_screen.h"
 
+#include "util/u_cpu_detect.h"
 #include "util/hash_table.h"
 #include "util/u_debug.h"
 #include "util/set.h"
 
+#include "wsi_common.h"
+
 void
-zink_batch_release(struct zink_screen *screen, struct zink_batch *batch)
+debug_describe_zink_batch_state(char *buf, const struct zink_batch_state *ptr)
 {
-   zink_fence_reference(screen, &batch->fence, NULL);
+   sprintf(buf, "zink_batch_state");
+}
 
-   zink_framebuffer_reference(screen, &batch->fb, NULL);
-   set_foreach(batch->programs, entry) {
-      struct zink_gfx_program *prog = (struct zink_gfx_program*)entry->key;
-      zink_gfx_program_reference(screen, &prog, NULL);
+void
+zink_reset_batch_state(struct zink_context *ctx, struct zink_batch_state *bs)
+{
+   struct zink_screen *screen = zink_screen(ctx->base.screen);
+
+   if (vkResetCommandPool(screen->dev, bs->cmdpool, 0) != VK_SUCCESS)
+      fprintf(stderr, "vkResetCommandPool failed\n");
+
+   zink_fence_clear_resources(screen, &bs->fence);
+
+   set_foreach_remove(bs->active_queries, entry) {
+      struct zink_query *query = (void*)entry->key;
+      zink_prune_query(screen, query);
    }
-   _mesa_set_clear(batch->programs, NULL);
 
-   /* unref all used resources */
-   set_foreach(batch->resources, entry) {
-      struct pipe_resource *pres = (struct pipe_resource *)entry->key;
-      pipe_resource_reference(&pres, NULL);
+   set_foreach_remove(bs->samplers, entry) {
+      struct zink_sampler *sampler = (struct zink_sampler*)entry->key;
+      zink_batch_usage_unset(&sampler->batch_uses, bs->fence.batch_id);
+      zink_sampler_reference(ctx, &sampler, NULL);
    }
-   _mesa_set_clear(batch->resources, NULL);
 
-   /* unref all used sampler-views */
-   set_foreach(batch->sampler_views, entry) {
-      struct pipe_sampler_view *pres = (struct pipe_sampler_view *)entry->key;
-      pipe_sampler_view_reference(&pres, NULL);
+   set_foreach_remove(bs->surfaces, entry) {
+      struct zink_surface *surf = (struct zink_surface *)entry->key;
+      zink_batch_usage_unset(&surf->batch_uses, bs->fence.batch_id);
+      pipe_surface_reference((struct pipe_surface**)&surf, NULL);
+   }
+   set_foreach_remove(bs->bufferviews, entry) {
+      struct zink_buffer_view *buffer_view = (struct zink_buffer_view *)entry->key;
+      zink_batch_usage_unset(&buffer_view->batch_uses, bs->fence.batch_id);
+      zink_buffer_view_reference(ctx, &buffer_view, NULL);
    }
-   _mesa_set_clear(batch->sampler_views, NULL);
 
-   util_dynarray_foreach(&batch->zombie_samplers, VkSampler, samp) {
-      vkDestroySampler(screen->dev, *samp, NULL);
+   screen->batch_descriptor_reset(screen, bs);
+
+   set_foreach_remove(bs->programs, entry) {
+      struct zink_program *pg = (struct zink_program*)entry->key;
+      if (pg->is_compute) {
+         struct zink_compute_program *comp = (struct zink_compute_program*)pg;
+         bool in_use = comp == ctx->curr_compute;
+         if (zink_compute_program_reference(screen, &comp, NULL) && in_use)
+            ctx->curr_compute = NULL;
+      } else {
+         struct zink_gfx_program *prog = (struct zink_gfx_program*)pg;
+         bool in_use = prog == ctx->curr_program;
+         if (zink_gfx_program_reference(screen, &prog, NULL) && in_use)
+            ctx->curr_program = NULL;
+      }
    }
-   util_dynarray_clear(&batch->zombie_samplers);
+   zink_framebuffer_reference(screen, &bs->fb, NULL);
+
+   bs->flush_res = NULL;
+
+   bs->descs_used = 0;
+   ctx->resource_size -= bs->resource_size;
+   bs->resource_size = 0;
+   /* only reset submitted here so that tc fence desync can pick up the 'completed' flag
+    * before the state is reused
+    */
+   bs->fence.submitted = false;
+   ctx->last_finished = MAX2(bs->fence.batch_id, ctx->last_finished);
+   bs->fence.batch_id = 0;
 }
 
-static void
-reset_batch(struct zink_context *ctx, struct zink_batch *batch)
+void
+zink_clear_batch_state(struct zink_context *ctx, struct zink_batch_state *bs)
 {
-   struct zink_screen *screen = zink_screen(ctx->base.screen);
-   batch->descs_left = ZINK_BATCH_DESC_SIZE;
+   bs->fence.completed = true;
+   zink_reset_batch_state(ctx, bs);
+}
 
-   // cmdbuf hasn't been submitted before
-   if (!batch->fence)
+void
+zink_batch_reset_all(struct zink_context *ctx)
+{
+   simple_mtx_lock(&ctx->batch_mtx);
+   hash_table_foreach(&ctx->batch_states, entry) {
+      struct zink_batch_state *bs = entry->data;
+      bs->fence.completed = true;
+      zink_reset_batch_state(ctx, bs);
+      _mesa_hash_table_remove(&ctx->batch_states, entry);
+      util_dynarray_append(&ctx->free_batch_states, struct zink_batch_state *, bs);
+   }
+   simple_mtx_unlock(&ctx->batch_mtx);
+}
+
+void
+zink_batch_state_destroy(struct zink_screen *screen, struct zink_batch_state *bs)
+{
+   if (!bs)
       return;
 
-   zink_fence_finish(screen, batch->fence, PIPE_TIMEOUT_INFINITE);
-   zink_batch_release(screen, batch);
+   util_queue_fence_destroy(&bs->flush_completed);
+
+   if (bs->fence.fence)
+      vkDestroyFence(screen->dev, bs->fence.fence, NULL);
 
-   if (vkResetDescriptorPool(screen->dev, batch->descpool, 0) != VK_SUCCESS)
-      fprintf(stderr, "vkResetDescriptorPool failed\n");
+   if (bs->cmdbuf)
+      vkFreeCommandBuffers(screen->dev, bs->cmdpool, 1, &bs->cmdbuf);
+   if (bs->cmdpool)
+      vkDestroyCommandPool(screen->dev, bs->cmdpool, NULL);
+
+   _mesa_set_destroy(bs->fence.resources, NULL);
+   _mesa_set_destroy(bs->samplers, NULL);
+   _mesa_set_destroy(bs->surfaces, NULL);
+   _mesa_set_destroy(bs->bufferviews, NULL);
+   _mesa_set_destroy(bs->programs, NULL);
+   _mesa_set_destroy(bs->active_queries, NULL);
+   screen->batch_descriptor_deinit(screen, bs);
+   simple_mtx_destroy(&bs->fence.resource_mtx);
+   ralloc_free(bs);
+}
+
+static struct zink_batch_state *
+create_batch_state(struct zink_context *ctx)
+{
+   struct zink_screen *screen = zink_screen(ctx->base.screen);
+   struct zink_batch_state *bs = rzalloc(NULL, struct zink_batch_state);
+   bs->have_timelines = ctx->have_timelines;
+   if (ctx->have_timelines)
+      bs->sem = ctx->batch.sem;
+   VkCommandPoolCreateInfo cpci = {};
+   cpci.sType = VK_STRUCTURE_TYPE_COMMAND_POOL_CREATE_INFO;
+   cpci.queueFamilyIndex = screen->gfx_queue;
+   cpci.flags = VK_COMMAND_POOL_CREATE_RESET_COMMAND_BUFFER_BIT;
+   if (vkCreateCommandPool(screen->dev, &cpci, NULL, &bs->cmdpool) != VK_SUCCESS)
+      goto fail;
+
+   VkCommandBufferAllocateInfo cbai = {};
+   cbai.sType = VK_STRUCTURE_TYPE_COMMAND_BUFFER_ALLOCATE_INFO;
+   cbai.commandPool = bs->cmdpool;
+   cbai.level = VK_COMMAND_BUFFER_LEVEL_PRIMARY;
+   cbai.commandBufferCount = 1;
+
+   if (vkAllocateCommandBuffers(screen->dev, &cbai, &bs->cmdbuf) != VK_SUCCESS)
+      goto fail;
+
+#define SET_CREATE_OR_FAIL(ptr) \
+   ptr = _mesa_pointer_set_create(bs); \
+   if (!ptr) \
+      goto fail
+
+   pipe_reference_init(&bs->reference, 1);
+   SET_CREATE_OR_FAIL(bs->fence.resources);
+   SET_CREATE_OR_FAIL(bs->samplers);
+   SET_CREATE_OR_FAIL(bs->surfaces);
+   SET_CREATE_OR_FAIL(bs->bufferviews);
+   SET_CREATE_OR_FAIL(bs->programs);
+   SET_CREATE_OR_FAIL(bs->active_queries);
+   if (!screen->batch_descriptor_init(bs))
+      goto fail;
+
+   VkFenceCreateInfo fci = {};
+   fci.sType = VK_STRUCTURE_TYPE_FENCE_CREATE_INFO;
+
+   if (vkCreateFence(screen->dev, &fci, NULL, &bs->fence.fence) != VK_SUCCESS)
+      goto fail;
+
+   simple_mtx_init(&bs->fence.resource_mtx, mtx_plain);
+   util_queue_fence_init(&bs->flush_completed);
+
+   return bs;
+fail:
+   zink_batch_state_destroy(screen, bs);
+   return NULL;
+}
+
+static bool
+find_unused_state(struct hash_entry *entry)
+{
+   struct zink_fence *fence = entry->data;
+   /* we can't reset these from fence_finish because threads */
+   bool completed = p_atomic_read(&fence->completed);
+   bool submitted = p_atomic_read(&fence->submitted);
+   return submitted && completed;
+}
+
+static struct zink_batch_state *
+get_batch_state(struct zink_context *ctx, struct zink_batch *batch)
+{
+   struct zink_batch_state *bs = NULL;
+
+   simple_mtx_lock(&ctx->batch_mtx);
+   if (util_dynarray_num_elements(&ctx->free_batch_states, struct zink_batch_state*))
+      bs = util_dynarray_pop(&ctx->free_batch_states, struct zink_batch_state*);
+   if (!bs) {
+      struct hash_entry *he = _mesa_hash_table_random_entry(&ctx->batch_states, find_unused_state);
+      if (he) { //there may not be any entries available
+         bs = he->data;
+         _mesa_hash_table_remove(&ctx->batch_states, he);
+      }
+   }
+   simple_mtx_unlock(&ctx->batch_mtx);
+   if (bs)
+      zink_reset_batch_state(ctx, bs);
+   else {
+      if (!batch->state) {
+         /* this is batch init, so create a few more states for later use */
+         for (int i = 0; i < 3; i++) {
+            struct zink_batch_state *state = create_batch_state(ctx);
+            util_dynarray_append(&ctx->free_batch_states, struct zink_batch_state *, state);
+         }
+      }
+      bs = create_batch_state(ctx);
+   }
+   return bs;
+}
+
+void
+zink_reset_batch(struct zink_context *ctx, struct zink_batch *batch)
+{
+   struct zink_screen *screen = zink_screen(ctx->base.screen);
+   bool fresh = !batch->state;
+
+   if (fresh && ctx->have_timelines) {
+      VkSemaphoreCreateInfo sci = {};
+      VkSemaphoreTypeCreateInfo tci = {};
+      sci.pNext = &tci;
+      sci.sType = VK_STRUCTURE_TYPE_SEMAPHORE_CREATE_INFO;
+      tci.sType = VK_STRUCTURE_TYPE_SEMAPHORE_TYPE_CREATE_INFO;
+      tci.semaphoreType = VK_SEMAPHORE_TYPE_TIMELINE;
+      if (vkCreateSemaphore(screen->dev, &sci, NULL, &batch->sem) != VK_SUCCESS)
+         ctx->have_timelines = false;
+   }
+
+   batch->state = get_batch_state(ctx, batch);
+   assert(batch->state);
+
+   batch->has_work = false;
 }
 
 void
 zink_start_batch(struct zink_context *ctx, struct zink_batch *batch)
 {
-   reset_batch(ctx, batch);
+   zink_reset_batch(ctx, batch);
 
    VkCommandBufferBeginInfo cbbi = {};
    cbbi.sType = VK_STRUCTURE_TYPE_COMMAND_BUFFER_BEGIN_INFO;
    cbbi.flags = VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT;
-   if (vkBeginCommandBuffer(batch->cmdbuf, &cbbi) != VK_SUCCESS)
+   if (vkBeginCommandBuffer(batch->state->cmdbuf, &cbbi) != VK_SUCCESS)
       debug_printf("vkBeginCommandBuffer failed\n");
 
+   batch->state->fence.batch_id = ctx->curr_batch;
+   batch->state->fence.completed = false;
+   if (ctx->last_fence) {
+      struct zink_batch_state *last_state = zink_batch_state(ctx->last_fence);
+      batch->last_batch_id = last_state->fence.batch_id;
+   } else {
+      /* TODO: move to wsi */
+      if (zink_screen(ctx->base.screen)->threaded) {
+         util_queue_init(&batch->flush_queue, "zfq", 8, 1, 0);
+      }
+   }
    if (!ctx->queries_disabled)
       zink_resume_queries(ctx, batch);
 }
 
-void
-zink_end_batch(struct zink_context *ctx, struct zink_batch *batch)
+static void
+submit_queue(void *data, int thread_index)
 {
-   if (!ctx->queries_disabled)
-      zink_suspend_queries(ctx, batch);
-
-   if (vkEndCommandBuffer(batch->cmdbuf) != VK_SUCCESS) {
-      debug_printf("vkEndCommandBuffer failed\n");
-      return;
-   }
-
-   assert(batch->fence == NULL);
-   batch->fence = zink_create_fence(ctx->base.screen, batch);
-   if (!batch->fence)
-      return;
-
+   struct zink_batch_state *bs = data;
    VkSubmitInfo si = {};
+   uint64_t batch_id = bs->fence.batch_id;
    si.sType = VK_STRUCTURE_TYPE_SUBMIT_INFO;
    si.waitSemaphoreCount = 0;
    si.pWaitSemaphores = NULL;
@@ -101,65 +290,200 @@ zink_end_batch(struct zink_context *ctx, struct zink_batch *batch)
    si.pSignalSemaphores = NULL;
    si.pWaitDstStageMask = NULL;
    si.commandBufferCount = 1;
-   si.pCommandBuffers = &batch->cmdbuf;
+   si.pCommandBuffers = &bs->cmdbuf;
+
+   VkTimelineSemaphoreSubmitInfo tsi = {};
+   if (bs->have_timelines) {
+      tsi.sType = VK_STRUCTURE_TYPE_TIMELINE_SEMAPHORE_SUBMIT_INFO;
+      si.pNext = &tsi;
+      tsi.signalSemaphoreValueCount = 1;
+      tsi.pSignalSemaphoreValues = &batch_id;
+      si.signalSemaphoreCount = 1;
+      si.pSignalSemaphores = &bs->sem;
+   }
+
+   struct wsi_memory_signal_submit_info mem_signal = {
+      .sType = VK_STRUCTURE_TYPE_WSI_MEMORY_SIGNAL_SUBMIT_INFO_MESA,
+      .pNext = si.pNext,
+   };
 
-   if (vkQueueSubmit(ctx->queue, 1, &si, batch->fence->fence) != VK_SUCCESS) {
+   if (bs->flush_res) {
+      mem_signal.memory = bs->flush_res->obj->mem;
+      si.pNext = &mem_signal;
+   }
+
+   if (vkQueueSubmit(bs->queue, 1, &si, bs->fence.fence) != VK_SUCCESS) {
       debug_printf("ZINK: vkQueueSubmit() failed\n");
-      ctx->is_device_lost = true;
+      bs->is_device_lost = true;
+   }
+   p_atomic_set(&bs->fence.submitted, true);
+}
 
-      if (ctx->reset.reset) {
-         ctx->reset.reset(ctx->reset.data, PIPE_GUILTY_CONTEXT_RESET);
-      }
+void
+zink_end_batch(struct zink_context *ctx, struct zink_batch *batch)
+{
+   if (!ctx->queries_disabled)
+      zink_suspend_queries(ctx, batch);
+
+   if (vkEndCommandBuffer(batch->state->cmdbuf) != VK_SUCCESS) {
+      debug_printf("vkEndCommandBuffer failed\n");
+      return;
+   }
+
+   vkResetFences(zink_screen(ctx->base.screen)->dev, 1, &batch->state->fence.fence);
+   simple_mtx_lock(&ctx->batch_mtx);
+   ctx->last_fence = &batch->state->fence;
+   _mesa_hash_table_insert_pre_hashed(&ctx->batch_states, batch->state->fence.batch_id, (void*)(uintptr_t)batch->state->fence.batch_id, batch->state);
+   simple_mtx_unlock(&ctx->batch_mtx);
+   ctx->resource_size += batch->state->resource_size;
+
+   if (util_queue_is_initialized(&batch->flush_queue)) {
+      batch->state->queue = batch->thread_queue;
+      util_queue_add_job(&batch->flush_queue, batch->state, &batch->state->flush_completed,
+                         submit_queue, NULL, 0);
+   } else {
+      batch->state->queue = batch->queue;
+      submit_queue(batch->state, 0);
    }
 }
 
 void
 zink_batch_reference_resource_rw(struct zink_batch *batch, struct zink_resource *res, bool write)
 {
-   unsigned mask = write ? ZINK_RESOURCE_ACCESS_WRITE : ZINK_RESOURCE_ACCESS_READ;
-
    /* u_transfer_helper unrefs the stencil buffer when the depth buffer is unrefed,
     * so we add an extra ref here to the stencil buffer to compensate
     */
-   struct zink_resource *stencil;
+   struct zink_resource *stencil = NULL;
 
-   zink_get_depth_stencil_resources((struct pipe_resource*)res, NULL, &stencil);
+   if (!res->obj->is_buffer && res->aspect == (VK_IMAGE_ASPECT_DEPTH_BIT | VK_IMAGE_ASPECT_STENCIL_BIT))
+      zink_get_depth_stencil_resources((struct pipe_resource*)res, NULL, &stencil);
 
-
-   struct set_entry *entry = _mesa_set_search(batch->resources, res);
-   if (!entry) {
-      entry = _mesa_set_add(batch->resources, res);
-      pipe_reference(NULL, &res->base.reference);
-      if (stencil)
-         pipe_reference(NULL, &stencil->base.reference);
+   /* if the resource already has usage of any sort set for this batch, we can skip hashing */
+   if (res->obj->reads.usage != batch->state->fence.batch_id &&
+       res->obj->writes.usage != batch->state->fence.batch_id) {
+      bool found = false;
+      _mesa_set_search_and_add(batch->state->fence.resources, res->obj, &found);
+      if (!found) {
+         pipe_reference(NULL, &res->obj->reference);
+         if (!batch->last_batch_id || !zink_batch_usage_matches(&res->obj->reads, batch->last_batch_id))
+            /* only add resource usage if it's "new" usage, though this only checks the most recent usage
+             * and not all pending usages
+             */
+            batch->state->resource_size += res->obj->size;
+         if (stencil) {
+            pipe_reference(NULL, &stencil->base.b.reference);
+            if (!batch->last_batch_id || !zink_batch_usage_matches(&stencil->obj->reads, batch->last_batch_id))
+               batch->state->resource_size += stencil->obj->size;
+         }
+      }
+       }
+   if (write) {
+      if (res->obj->writes.usage != batch->state->fence.batch_id) {
+         if (stencil)
+            zink_batch_usage_set(&stencil->obj->writes, batch->state->fence.batch_id);
+         zink_batch_usage_set(&res->obj->writes, batch->state->fence.batch_id);
+      }
+   } else {
+      if (res->obj->reads.usage != batch->state->fence.batch_id) {
+         if (stencil)
+            zink_batch_usage_set(&stencil->obj->reads, batch->state->fence.batch_id);
+         zink_batch_usage_set(&res->obj->reads, batch->state->fence.batch_id);
+      }
    }
-   /* the batch_uses value for this batch is guaranteed to not be in use now because
-    * reset_batch() waits on the fence and removes access before resetting
-    */
-   res->batch_uses[batch->batch_id] |= mask;
 
-   if (stencil)
-      stencil->batch_uses[batch->batch_id] |= mask;
+   batch->has_work = true;
+}
+
+bool
+batch_ptr_add_usage(struct zink_batch *batch, struct set *s, void *ptr, struct zink_batch_usage *u);
+
+bool
+batch_ptr_add_usage(struct zink_batch *batch, struct set *s, void *ptr, struct zink_batch_usage *u)
+{
+   bool found = false;
+   if (u->usage == batch->state->fence.batch_id)
+      return false;
+   _mesa_set_search_and_add(s, ptr, &found);
+   assert(!found);
+   zink_batch_usage_set(u, batch->state->fence.batch_id);
+   return true;
+}
+
+void
+zink_batch_reference_bufferview(struct zink_batch *batch, struct zink_buffer_view *buffer_view)
+{
+   if (!batch_ptr_add_usage(batch, batch->state->bufferviews, buffer_view, &buffer_view->batch_uses))
+      return;
+   pipe_reference(NULL, &buffer_view->reference);
+   batch->has_work = true;
+}
+
+void
+zink_batch_reference_surface(struct zink_batch *batch, struct zink_surface *surface)
+{
+   if (!batch_ptr_add_usage(batch, batch->state->surfaces, surface, &surface->batch_uses))
+      return;
+   struct pipe_surface *surf = NULL;
+   pipe_surface_reference(&surf, &surface->base);
+   batch->has_work = true;
 }
 
 void
 zink_batch_reference_sampler_view(struct zink_batch *batch,
                                   struct zink_sampler_view *sv)
 {
-   struct set_entry *entry = _mesa_set_search(batch->sampler_views, sv);
-   if (!entry) {
-      entry = _mesa_set_add(batch->sampler_views, sv);
-      pipe_reference(NULL, &sv->base.reference);
-   }
+   if (sv->base.target == PIPE_BUFFER)
+      zink_batch_reference_bufferview(batch, sv->buffer_view);
+   else
+      zink_batch_reference_surface(batch, sv->image_view);
+}
+
+void
+zink_batch_reference_sampler(struct zink_batch *batch,
+                             struct zink_sampler *sampler)
+{
+   if (!batch_ptr_add_usage(batch, batch->state->samplers, sampler, &sampler->batch_uses))
+      return;
+   pipe_reference(NULL, &sampler->reference);
+   batch->has_work = true;
 }
 
 void
 zink_batch_reference_program(struct zink_batch *batch,
-                             struct zink_gfx_program *prog)
+                             struct zink_program *pg)
 {
-   struct set_entry *entry = _mesa_set_search(batch->programs, prog);
-   if (!entry) {
-      entry = _mesa_set_add(batch->programs, prog);
-      pipe_reference(NULL, &prog->reference);
-   }
+   if (!batch_ptr_add_usage(batch, batch->state->programs, pg, &pg->batch_uses))
+      return;
+   pipe_reference(NULL, &pg->reference);
+   batch->has_work = true;
+}
+
+void
+zink_batch_reference_image_view(struct zink_batch *batch,
+                                struct zink_image_view *image_view)
+{
+   if (image_view->base.resource->target == PIPE_BUFFER)
+      zink_batch_reference_bufferview(batch, image_view->buffer_view);
+   else
+      zink_batch_reference_surface(batch, image_view->surface);
+}
+
+void
+zink_batch_usage_set(struct zink_batch_usage *u, uint32_t batch_id)
+{
+   p_atomic_set(&u->usage, batch_id);
+}
+
+bool
+zink_batch_usage_matches(struct zink_batch_usage *u, uint32_t batch_id)
+{
+   uint32_t usage = p_atomic_read(&u->usage);
+   return usage == batch_id;
+}
+
+bool
+zink_batch_usage_exists(struct zink_batch_usage *u)
+{
+   uint32_t usage = p_atomic_read(&u->usage);
+   return !!usage;
 }
diff --git a/src/gallium/drivers/zink/zink_batch.h b/src/gallium/drivers/zink/zink_batch.h
index ee0367ca924..ae8d5a601b2 100644
--- a/src/gallium/drivers/zink/zink_batch.h
+++ b/src/gallium/drivers/zink/zink_batch.h
@@ -29,41 +29,97 @@
 #include "util/list.h"
 #include "util/u_dynarray.h"
 
+#include "zink_fence.h"
+
+struct pipe_reference;
+
+struct zink_buffer_view;
 struct zink_context;
-struct zink_fence;
+struct zink_descriptor_set;
 struct zink_framebuffer;
-struct zink_gfx_program;
+struct zink_image_view;
+struct zink_program;
 struct zink_render_pass;
 struct zink_resource;
-struct zink_screen;
+struct zink_sampler;
 struct zink_sampler_view;
+struct zink_surface;
 
-#define ZINK_BATCH_DESC_SIZE 1000
 
-struct zink_batch {
-   unsigned batch_id : 2;
+struct zink_batch_usage {
+   /* this has to be atomic for fence access, so we can't use a bitmask and make everything neat */
+   uint32_t usage;
+};
+
+struct zink_batch_state {
+   struct zink_fence fence;
+   struct pipe_reference reference;
+   VkCommandPool cmdpool;
    VkCommandBuffer cmdbuf;
-   VkDescriptorPool descpool;
-   int descs_left;
-   struct zink_fence *fence;
+
+   VkQueue queue; //duplicated from batch for threading
+   VkSemaphore sem;
+
+   struct util_queue_fence flush_completed; //TODO: move to wsi
+
+   struct zink_resource *flush_res;
+
+   unsigned short descs_used; //number of descriptors currently allocated
 
    struct zink_framebuffer *fb;
    struct set *programs;
 
-   struct set *resources;
-   struct set *sampler_views;
-
-   struct util_dynarray zombie_samplers;
+   struct set *samplers;
+   struct set *surfaces;
+   struct set *bufferviews;
 
    struct set *active_queries; /* zink_query objects which were active at some point in this batch */
 
+   struct zink_batch_descriptor_data *dd;
+
+   VkDeviceSize resource_size;
+
+   bool is_device_lost;
+   bool have_timelines;
+};
+
+struct zink_batch {
+   struct zink_batch_state *state;
+
+   uint32_t last_batch_id;
+   VkQueue queue; //gfx+compute
+   VkQueue thread_queue; //gfx+compute
+   VkSemaphore sem;
+   struct util_queue flush_queue; //TODO: move to wsi
+
+   bool has_work;
    bool in_rp; //renderpass is currently active
 };
 
-/* release all resources attached to batch */
+
+static inline struct zink_batch_state *
+zink_batch_state(struct zink_fence *fence)
+{
+   return (struct zink_batch_state *)fence;
+}
+
 void
-zink_batch_release(struct zink_screen *screen, struct zink_batch *batch);
+zink_reset_batch_state(struct zink_context *ctx, struct zink_batch_state *bs);
 
+void
+zink_clear_batch_state(struct zink_context *ctx, struct zink_batch_state *bs);
+
+void
+zink_batch_reset_all(struct zink_context *ctx);
+
+void
+zink_batch_state_destroy(struct zink_screen *screen, struct zink_batch_state *bs);
+
+void
+zink_batch_state_clear_resources(struct zink_screen *screen, struct zink_batch_state *bs);
+
+void
+zink_reset_batch(struct zink_context *ctx, struct zink_batch *batch);
 void
 zink_start_batch(struct zink_context *ctx, struct zink_batch *batch);
 
@@ -78,8 +134,48 @@ zink_batch_reference_resource_rw(struct zink_batch *batch,
 void
 zink_batch_reference_sampler_view(struct zink_batch *batch,
                                   struct zink_sampler_view *sv);
+void
+zink_batch_reference_sampler(struct zink_batch *batch,
+                                  struct zink_sampler *sampler);
 
 void
 zink_batch_reference_program(struct zink_batch *batch,
-                             struct zink_gfx_program *prog);
+                             struct zink_program *pg);
+
+void
+zink_batch_reference_image_view(struct zink_batch *batch,
+                                struct zink_image_view *image_view);
+
+void
+zink_batch_reference_bufferview(struct zink_batch *batch, struct zink_buffer_view *buffer_view);
+void
+zink_batch_reference_surface(struct zink_batch *batch, struct zink_surface *surface);
+void
+debug_describe_zink_batch_state(char *buf, const struct zink_batch_state *ptr);
+
+static inline void
+zink_batch_state_reference(struct zink_screen *screen,
+                           struct zink_batch_state **dst,
+                           struct zink_batch_state *src)
+{
+   struct zink_batch_state *old_dst = dst ? *dst : NULL;
+
+   if (pipe_reference_described(old_dst ? &old_dst->reference : NULL, src ? &src->reference : NULL,
+                                (debug_reference_descriptor)debug_describe_zink_batch_state))
+      zink_batch_state_destroy(screen, old_dst);
+   if (dst) *dst = src;
+}
+
+void
+zink_batch_usage_set(struct zink_batch_usage *u, uint32_t batch_id);
+bool
+zink_batch_usage_matches(struct zink_batch_usage *u, uint32_t batch_id);
+bool
+zink_batch_usage_exists(struct zink_batch_usage *u);
+
+static inline void
+zink_batch_usage_unset(struct zink_batch_usage *u, uint32_t batch_id)
+{
+   p_atomic_cmpxchg(&u->usage, batch_id, 0);
+}
 #endif
diff --git a/src/gallium/drivers/zink/zink_blit.c b/src/gallium/drivers/zink/zink_blit.c
index c91068cd45d..2db57e4d291 100644
--- a/src/gallium/drivers/zink/zink_blit.c
+++ b/src/gallium/drivers/zink/zink_blit.c
@@ -4,6 +4,7 @@
 #include "zink_screen.h"
 
 #include "util/u_blitter.h"
+#include "util/u_rect.h"
 #include "util/u_surface.h"
 #include "util/format/u_format.h"
 
@@ -33,13 +34,18 @@ blit_resolve(struct zink_context *ctx, const struct pipe_blit_info *info)
    if (src->format != zink_get_format(screen, info->src.format) ||
        dst->format != zink_get_format(screen, info->dst.format))
       return false;
+   if (info->dst.resource->target == PIPE_BUFFER)
+      util_range_add(info->dst.resource, &dst->valid_buffer_range,
+                     info->dst.box.x, info->dst.box.x + info->dst.box.width);
 
+   zink_fb_clears_apply_or_discard(ctx, info->dst.resource, zink_rect_from_box(&info->dst.box), false);
+   zink_fb_clears_apply_region(ctx, info->src.resource, zink_rect_from_box(&info->src.box));
    struct zink_batch *batch = zink_batch_no_rp(ctx);
 
    zink_batch_reference_resource_rw(batch, src, false);
    zink_batch_reference_resource_rw(batch, dst, true);
 
-   zink_resource_setup_transfer_layouts(batch, src, dst);
+   zink_resource_setup_transfer_layouts(ctx, src, dst);
 
    VkImageResolve region = {};
 
@@ -48,7 +54,7 @@ blit_resolve(struct zink_context *ctx, const struct pipe_blit_info *info)
    region.srcOffset.x = info->src.box.x;
    region.srcOffset.y = info->src.box.y;
 
-   if (src->base.array_size > 1) {
+   if (src->base.b.array_size > 1) {
       region.srcOffset.z = 0;
       region.srcSubresource.baseArrayLayer = info->src.box.z;
       region.srcSubresource.layerCount = info->src.box.depth;
@@ -64,7 +70,7 @@ blit_resolve(struct zink_context *ctx, const struct pipe_blit_info *info)
    region.dstOffset.x = info->dst.box.x;
    region.dstOffset.y = info->dst.box.y;
 
-   if (dst->base.array_size > 1) {
+   if (dst->base.b.array_size > 1) {
       region.dstOffset.z = 0;
       region.dstSubresource.baseArrayLayer = info->dst.box.z;
       region.dstSubresource.layerCount = info->dst.box.depth;
@@ -78,8 +84,8 @@ blit_resolve(struct zink_context *ctx, const struct pipe_blit_info *info)
    region.extent.width = info->dst.box.width;
    region.extent.height = info->dst.box.height;
    region.extent.depth = info->dst.box.depth;
-   vkCmdResolveImage(batch->cmdbuf, src->image, src->layout,
-                     dst->image, dst->layout,
+   vkCmdResolveImage(batch->state->cmdbuf, src->obj->image, src->layout,
+                     dst->obj->image, dst->layout,
                      1, &region);
 
    return true;
@@ -102,6 +108,11 @@ blit_native(struct zink_context *ctx, const struct pipe_blit_info *info)
        info->dst.format != info->src.format)
       return false;
 
+   if (util_format_is_argb(info->src.format) || util_format_is_abgr(info->src.format))
+      return false;
+   if (util_format_is_argb(info->dst.format) || util_format_is_abgr(info->dst.format))
+      return false;
+
    /* vkCmdBlitImage must not be used for multisampled source or destination images. */
    if (info->src.resource->nr_samples > 1 || info->dst.resource->nr_samples > 1)
       return false;
@@ -114,12 +125,16 @@ blit_native(struct zink_context *ctx, const struct pipe_blit_info *info)
        dst->format != zink_get_format(screen, info->dst.format))
       return false;
 
+   zink_fb_clears_apply_or_discard(ctx, info->dst.resource, zink_rect_from_box(&info->dst.box), false);
+   zink_fb_clears_apply_region(ctx, info->src.resource, zink_rect_from_box(&info->src.box));
    struct zink_batch *batch = zink_batch_no_rp(ctx);
    zink_batch_reference_resource_rw(batch, src, false);
    zink_batch_reference_resource_rw(batch, dst, true);
 
-   zink_resource_setup_transfer_layouts(batch, src, dst);
-
+   zink_resource_setup_transfer_layouts(ctx, src, dst);
+   if (info->dst.resource->target == PIPE_BUFFER)
+      util_range_add(info->dst.resource, &dst->valid_buffer_range,
+                     info->dst.box.x, info->dst.box.x + info->dst.box.width);
    VkImageBlit region = {};
    region.srcSubresource.aspectMask = src->aspect;
    region.srcSubresource.mipLevel = info->src.level;
@@ -128,7 +143,7 @@ blit_native(struct zink_context *ctx, const struct pipe_blit_info *info)
    region.srcOffsets[1].x = info->src.box.x + info->src.box.width;
    region.srcOffsets[1].y = info->src.box.y + info->src.box.height;
 
-   if (src->base.array_size > 1) {
+   if (src->base.b.array_size > 1) {
       region.srcOffsets[0].z = 0;
       region.srcOffsets[1].z = 1;
       region.srcSubresource.baseArrayLayer = info->src.box.z;
@@ -147,7 +162,7 @@ blit_native(struct zink_context *ctx, const struct pipe_blit_info *info)
    region.dstOffsets[1].x = info->dst.box.x + info->dst.box.width;
    region.dstOffsets[1].y = info->dst.box.y + info->dst.box.height;
 
-   if (dst->base.array_size > 1) {
+   if (dst->base.b.array_size > 1) {
       region.dstOffsets[0].z = 0;
       region.dstOffsets[1].z = 1;
       region.dstSubresource.baseArrayLayer = info->dst.box.z;
@@ -159,8 +174,8 @@ blit_native(struct zink_context *ctx, const struct pipe_blit_info *info)
       region.dstSubresource.layerCount = 1;
    }
 
-   vkCmdBlitImage(batch->cmdbuf, src->image, src->layout,
-                  dst->image, dst->layout,
+   vkCmdBlitImage(batch->state->cmdbuf, src->obj->image, src->layout,
+                  dst->obj->image, dst->layout,
                   1, &region,
                   zink_filter(info->filter));
 
@@ -172,13 +187,20 @@ zink_blit(struct pipe_context *pctx,
           const struct pipe_blit_info *info)
 {
    struct zink_context *ctx = zink_context(pctx);
-   if (info->src.resource->nr_samples > 1 &&
-       info->dst.resource->nr_samples <= 1) {
-      if (blit_resolve(ctx, info))
-         return;
-   } else {
-      if (blit_native(ctx, info))
-         return;
+   const struct util_format_description *desc = util_format_description(info->src.format);
+   if (desc->nr_channels != 4 || desc->layout != UTIL_FORMAT_LAYOUT_PLAIN ||
+       (desc->nr_channels == 4 && desc->channel[3].type != UTIL_FORMAT_TYPE_VOID)) {
+      /* we can't blit RGBX formats directly since they're emulated
+       * so we have to use sampler views
+       */
+      if (info->src.resource->nr_samples > 1 &&
+          info->dst.resource->nr_samples <= 1) {
+         if (blit_resolve(ctx, info))
+            return;
+      } else {
+         if (blit_native(ctx, info))
+            return;
+      }
    }
 
    struct zink_resource *src = zink_resource(info->src.resource);
@@ -203,18 +225,12 @@ zink_blit(struct pipe_context *pctx,
       return;
    }
 
-   util_blitter_save_vertex_elements(ctx->blitter, ctx->element_state);
-   util_blitter_save_viewport(ctx->blitter, ctx->viewport_states);
-
-   util_blitter_save_fragment_sampler_states(ctx->blitter,
-                                             ctx->num_samplers[PIPE_SHADER_FRAGMENT],
-                                             ctx->sampler_states[PIPE_SHADER_FRAGMENT]);
-   util_blitter_save_fragment_sampler_views(ctx->blitter,
-                                            ctx->num_image_views[PIPE_SHADER_FRAGMENT],
-                                            ctx->image_views[PIPE_SHADER_FRAGMENT]);
-   util_blitter_save_fragment_constant_buffer_slot(ctx->blitter, ctx->ubos[PIPE_SHADER_FRAGMENT]);
-   util_blitter_save_vertex_buffer_slot(ctx->blitter, ctx->buffers);
-   zink_blit_begin(ctx, ZINK_BLIT_SAVE_FB | ZINK_BLIT_SAVE_FS);
+   zink_fb_clears_apply_or_discard(ctx, info->dst.resource, zink_rect_from_box(&info->dst.box), true);
+
+   if (info->dst.resource->target == PIPE_BUFFER)
+      util_range_add(info->dst.resource, &dst->valid_buffer_range,
+                     info->dst.box.x, info->dst.box.x + info->dst.box.width);
+   zink_blit_begin(ctx, ZINK_BLIT_SAVE_FB | ZINK_BLIT_SAVE_FS | ZINK_BLIT_SAVE_TEXTURES);
 
    util_blitter_blit(ctx->blitter, info);
 }
@@ -223,6 +239,10 @@ zink_blit(struct pipe_context *pctx,
 void
 zink_blit_begin(struct zink_context *ctx, enum zink_blit_flags flags)
 {
+   util_blitter_save_vertex_elements(ctx->blitter, ctx->element_state);
+   util_blitter_save_viewport(ctx->blitter, ctx->vp_state.viewport_states);
+
+   util_blitter_save_vertex_buffer_slot(ctx->blitter, ctx->vertex_buffers);
    util_blitter_save_vertex_shader(ctx->blitter, ctx->gfx_stages[PIPE_SHADER_VERTEX]);
    util_blitter_save_tessctrl_shader(ctx->blitter, ctx->gfx_stages[PIPE_SHADER_TESS_CTRL]);
    util_blitter_save_tesseval_shader(ctx->blitter, ctx->gfx_stages[PIPE_SHADER_TESS_EVAL]);
@@ -231,11 +251,12 @@ zink_blit_begin(struct zink_context *ctx, enum zink_blit_flags flags)
    util_blitter_save_so_targets(ctx->blitter, ctx->num_so_targets, ctx->so_targets);
 
    if (flags & ZINK_BLIT_SAVE_FS) {
+      util_blitter_save_fragment_constant_buffer_slot(ctx->blitter, ctx->ubos[PIPE_SHADER_FRAGMENT]);
       util_blitter_save_blend(ctx->blitter, ctx->gfx_pipeline_state.blend_state);
       util_blitter_save_depth_stencil_alpha(ctx->blitter, ctx->dsa_state);
       util_blitter_save_stencil_ref(ctx->blitter, &ctx->stencil_ref);
       util_blitter_save_sample_mask(ctx->blitter, ctx->gfx_pipeline_state.sample_mask);
-      util_blitter_save_scissor(ctx->blitter, ctx->scissor_states);
+      util_blitter_save_scissor(ctx->blitter, ctx->vp_state.scissor_states);
       /* also util_blitter_save_window_rectangles when we have that? */
 
       util_blitter_save_fragment_shader(ctx->blitter, ctx->gfx_stages[PIPE_SHADER_FRAGMENT]);
@@ -243,4 +264,43 @@ zink_blit_begin(struct zink_context *ctx, enum zink_blit_flags flags)
 
    if (flags & ZINK_BLIT_SAVE_FB)
       util_blitter_save_framebuffer(ctx->blitter, &ctx->fb_state);
+
+
+   if (flags & ZINK_BLIT_SAVE_TEXTURES) {
+      util_blitter_save_fragment_sampler_states(ctx->blitter,
+                                                ctx->num_samplers[PIPE_SHADER_FRAGMENT],
+                                                (void**)ctx->sampler_states[PIPE_SHADER_FRAGMENT]);
+      util_blitter_save_fragment_sampler_views(ctx->blitter,
+                                               ctx->num_sampler_views[PIPE_SHADER_FRAGMENT],
+                                               ctx->sampler_views[PIPE_SHADER_FRAGMENT]);
+   }
+}
+
+bool
+zink_blit_region_fills(struct u_rect region, unsigned width, unsigned height)
+{
+   struct u_rect intersect = {0, width, 0, height};
+
+   if (!u_rect_test_intersection(&region, &intersect))
+      /* is this even a thing? */
+      return false;
+
+    u_rect_find_intersection(&region, &intersect);
+    if (intersect.x0 != 0 || intersect.y0 != 0 ||
+        intersect.x1 != width || intersect.y1 != height)
+       return false;
+
+   return false;
+}
+
+bool
+zink_blit_region_covers(struct u_rect region, struct u_rect covers)
+{
+   struct u_rect intersect;
+   if (!u_rect_test_intersection(&region, &covers))
+      return false;
+
+    u_rect_union(&intersect, &region, &covers);
+    return intersect.x0 == covers.x0 && intersect.y0 == covers.y0 &&
+           intersect.x1 == covers.x1 && intersect.y1 == covers.y1;
 }
diff --git a/src/gallium/drivers/zink/zink_clear.c b/src/gallium/drivers/zink/zink_clear.c
index daa60558e00..3e422046d8d 100644
--- a/src/gallium/drivers/zink/zink_clear.c
+++ b/src/gallium/drivers/zink/zink_clear.c
@@ -26,6 +26,7 @@
 #include "zink_screen.h"
 
 #include "util/u_blitter.h"
+#include "util/u_dynarray.h"
 #include "util/format/u_format.h"
 #include "util/format_srgb.h"
 #include "util/u_framebuffer.h"
@@ -36,6 +37,8 @@
 static inline bool
 check_3d_layers(struct pipe_surface *psurf)
 {
+   if (psurf->texture->target != PIPE_TEXTURE_3D)
+      return true;
    /* SPEC PROBLEM:
     * though the vk spec doesn't seem to explicitly address this, currently drivers
     * are claiming that all 3D images have a single "3D" layer regardless of layercount,
@@ -49,6 +52,12 @@ check_3d_layers(struct pipe_surface *psurf)
    return true;
 }
 
+static inline bool
+scissor_states_equal(const struct pipe_scissor_state *a, const struct pipe_scissor_state *b)
+{
+   return a->minx == b->minx && a->miny == b->miny && a->maxx == b->maxx && a->maxy == b->maxy;
+}
+
 static void
 clear_in_rp(struct pipe_context *pctx,
            unsigned buffers,
@@ -58,8 +67,8 @@ clear_in_rp(struct pipe_context *pctx,
 {
    struct zink_context *ctx = zink_context(pctx);
    struct pipe_framebuffer_state *fb = &ctx->fb_state;
-
-   struct zink_batch *batch = zink_batch_rp(ctx);
+   struct zink_resource *resources[PIPE_MAX_COLOR_BUFS + 1] = {};
+   int res_count = 0;
 
    VkClearAttachment attachments[1 + PIPE_MAX_COLOR_BUFS];
    int num_attachments = 0;
@@ -79,6 +88,9 @@ clear_in_rp(struct pipe_context *pctx,
          attachments[num_attachments].colorAttachment = i;
          attachments[num_attachments].clearValue.color = color;
          ++num_attachments;
+         struct zink_resource *res = (struct zink_resource*)fb->cbufs[i]->texture;
+         zink_resource_image_barrier(ctx, NULL, res, VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL, 0, 0);
+         resources[res_count++] = res;
       }
    }
 
@@ -93,6 +105,9 @@ clear_in_rp(struct pipe_context *pctx,
       attachments[num_attachments].clearValue.depthStencil.depth = depth;
       attachments[num_attachments].clearValue.depthStencil.stencil = stencil;
       ++num_attachments;
+      struct zink_resource *res = (struct zink_resource*)fb->zsbuf->texture;
+      zink_resource_image_barrier(ctx, NULL, res, VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL, 0, 0);
+      resources[res_count++] = res;
    }
 
    VkClearRect cr = {};
@@ -107,12 +122,16 @@ clear_in_rp(struct pipe_context *pctx,
    }
    cr.baseArrayLayer = 0;
    cr.layerCount = util_framebuffer_get_num_layers(fb);
-   vkCmdClearAttachments(batch->cmdbuf, num_attachments, attachments, 1, &cr);
+   struct zink_batch *batch = zink_batch_rp(ctx);
+   for (int i = 0; i < res_count; i++)
+      zink_batch_reference_resource_rw(batch, resources[i], true);
+   vkCmdClearAttachments(batch->state->cmdbuf, num_attachments, attachments, 1, &cr);
 }
 
 static void
-clear_color_no_rp(struct zink_batch *batch, struct zink_resource *res, const union pipe_color_union *pcolor, unsigned level, unsigned layer, unsigned layerCount)
+clear_color_no_rp(struct zink_context *ctx, struct zink_resource *res, const union pipe_color_union *pcolor, unsigned level, unsigned layer, unsigned layerCount)
 {
+   struct zink_batch *batch = zink_batch_no_rp(ctx);
    VkImageSubresourceRange range = {};
    range.baseMipLevel = level;
    range.levelCount = 1;
@@ -126,14 +145,17 @@ clear_color_no_rp(struct zink_batch *batch, struct zink_resource *res, const uni
    color.float32[2] = pcolor->f[2];
    color.float32[3] = pcolor->f[3];
 
-   if (res->layout != VK_IMAGE_LAYOUT_GENERAL && res->layout != VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL)
-      zink_resource_barrier(batch->cmdbuf, res, range.aspectMask, VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL);
-   vkCmdClearColorImage(batch->cmdbuf, res->image, res->layout, &color, 1, &range);
+   if (zink_resource_image_needs_barrier(res, VK_IMAGE_LAYOUT_GENERAL, 0, 0) &&
+       zink_resource_image_needs_barrier(res, VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL, 0, 0))
+      zink_resource_image_barrier(ctx, NULL, res, VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL, 0, 0);
+   zink_batch_reference_resource_rw(batch, res, true);
+   vkCmdClearColorImage(batch->state->cmdbuf, res->obj->image, res->layout, &color, 1, &range);
 }
 
 static void
-clear_zs_no_rp(struct zink_batch *batch, struct zink_resource *res, VkImageAspectFlags aspects, double depth, unsigned stencil, unsigned level, unsigned layer, unsigned layerCount)
+clear_zs_no_rp(struct zink_context *ctx, struct zink_resource *res, VkImageAspectFlags aspects, double depth, unsigned stencil, unsigned level, unsigned layer, unsigned layerCount)
 {
+   struct zink_batch *batch = zink_batch_no_rp(ctx);
    VkImageSubresourceRange range = {};
    range.baseMipLevel = level;
    range.levelCount = 1;
@@ -143,31 +165,32 @@ clear_zs_no_rp(struct zink_batch *batch, struct zink_resource *res, VkImageAspec
 
    VkClearDepthStencilValue zs_value = {depth, stencil};
 
-   if (res->layout != VK_IMAGE_LAYOUT_GENERAL && res->layout != VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL)
-      zink_resource_barrier(batch->cmdbuf, res, res->aspect, VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL);
-   vkCmdClearDepthStencilImage(batch->cmdbuf, res->image, res->layout, &zs_value, 1, &range);
+   if (zink_resource_image_needs_barrier(res, VK_IMAGE_LAYOUT_GENERAL, 0, 0) &&
+       zink_resource_image_needs_barrier(res, VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL, 0, 0))
+      zink_resource_image_barrier(ctx, NULL, res, VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL, 0, 0);
+   zink_batch_reference_resource_rw(batch, res, true);
+   vkCmdClearDepthStencilImage(batch->state->cmdbuf, res->obj->image, res->layout, &zs_value, 1, &range);
 }
 
-static bool
-clear_needs_rp(unsigned width, unsigned height, struct u_rect *region)
-{
-   struct u_rect intersect = {0, width, 0, height};
-
-   /* FIXME: this is very inefficient; if no renderpass has been started yet,
-    * we should record the clear if it's full-screen, and apply it as we
-    * start the render-pass. Otherwise we can do a partial out-of-renderpass
-    * clear.
-    */
-   if (!u_rect_test_intersection(region, &intersect))
-      /* is this even a thing? */
-      return true;
 
-    u_rect_find_intersection(region, &intersect);
-    if (intersect.x0 != 0 || intersect.y0 != 0 ||
-        intersect.x1 != width || intersect.y1 != height)
-       return true;
 
-   return false;
+static struct zink_framebuffer_clear_data *
+get_clear_data(struct zink_context *ctx, struct zink_framebuffer_clear *fb_clear, const struct pipe_scissor_state *scissor_state)
+{
+   struct zink_framebuffer_clear_data *clear = NULL;
+   unsigned num_clears = zink_fb_clear_count(fb_clear);
+   if (num_clears) {
+      struct zink_framebuffer_clear_data *last_clear = zink_fb_clear_element(fb_clear, num_clears - 1);
+      /* if we're completely overwriting the previous clear, merge this into the previous clear */
+      if (!scissor_state || (last_clear->has_scissor && scissor_states_equal(&last_clear->scissor, scissor_state)))
+         clear = last_clear;
+   }
+   if (!clear) {
+      struct zink_framebuffer_clear_data cd = {};
+      util_dynarray_append(&fb_clear->clears, struct zink_framebuffer_clear_data, cd);
+      clear = zink_fb_clear_element(fb_clear, zink_fb_clear_count(fb_clear) - 1);
+   }
+   return clear;
 }
 
 void
@@ -179,16 +202,16 @@ zink_clear(struct pipe_context *pctx,
 {
    struct zink_context *ctx = zink_context(pctx);
    struct pipe_framebuffer_state *fb = &ctx->fb_state;
-   struct zink_batch *batch = zink_curr_batch(ctx);
+   struct zink_batch *batch = &ctx->batch;
    bool needs_rp = false;
 
    if (scissor_state) {
       struct u_rect scissor = {scissor_state->minx, scissor_state->maxx, scissor_state->miny, scissor_state->maxy};
-      needs_rp = clear_needs_rp(fb->width, fb->height, &scissor);
+      needs_rp = !zink_blit_region_fills(scissor, fb->width, fb->height);
    }
 
 
-   if (needs_rp || batch->in_rp || ctx->render_condition_active) {
+   if (batch->in_rp) {
       clear_in_rp(pctx, buffers, scissor_state, pcolor, depth, stencil);
       return;
    }
@@ -197,45 +220,132 @@ zink_clear(struct pipe_context *pctx,
       for (unsigned i = 0; i < fb->nr_cbufs; i++) {
          if ((buffers & (PIPE_CLEAR_COLOR0 << i)) && fb->cbufs[i]) {
             struct pipe_surface *psurf = fb->cbufs[i];
-
-            if (psurf->texture->target == PIPE_TEXTURE_3D && !check_3d_layers(psurf)) {
-               clear_in_rp(pctx, buffers, scissor_state, pcolor, depth, stencil);
-               return;
-            }
-            struct zink_resource *res = zink_resource(psurf->texture);
-            union pipe_color_union color = *pcolor;
-            if (psurf->format != res->base.format &&
-                !util_format_is_srgb(psurf->format) && util_format_is_srgb(res->base.format)) {
-               /* if SRGB mode is disabled for the fb with a backing srgb image then we have to
-                * convert this to srgb color
-                */
-               color.f[0] = util_format_srgb_to_linear_float(pcolor->f[0]);
-               color.f[1] = util_format_srgb_to_linear_float(pcolor->f[1]);
-               color.f[2] = util_format_srgb_to_linear_float(pcolor->f[2]);
-            }
-            clear_color_no_rp(batch, zink_resource(fb->cbufs[i]->texture), &color,
-                              psurf->u.tex.level, psurf->u.tex.first_layer,
-                              psurf->u.tex.last_layer - psurf->u.tex.first_layer + 1);
+            struct zink_framebuffer_clear *fb_clear = &ctx->fb_clears[i];
+            struct zink_framebuffer_clear_data *clear = get_clear_data(ctx, fb_clear, needs_rp ? scissor_state : NULL);
+
+            ctx->clears_enabled |= PIPE_CLEAR_COLOR0 << i;
+            clear->conditional = ctx->render_condition_active;
+            clear->has_scissor = needs_rp;
+            if (scissor_state && needs_rp)
+               clear->scissor = *scissor_state;
+            clear->color.color = *pcolor;
+            clear->color.srgb = psurf->format != psurf->texture->format &&
+                                !util_format_is_srgb(psurf->format) && util_format_is_srgb(psurf->texture->format);
          }
       }
    }
 
    if (buffers & PIPE_CLEAR_DEPTHSTENCIL && fb->zsbuf) {
-      if (fb->zsbuf->texture->target == PIPE_TEXTURE_3D && !check_3d_layers(fb->zsbuf)) {
-         clear_in_rp(pctx, buffers, scissor_state, pcolor, depth, stencil);
-         return;
-      }
-      VkImageAspectFlags aspects = 0;
+      struct zink_framebuffer_clear *fb_clear = &ctx->fb_clears[PIPE_MAX_COLOR_BUFS];
+      struct zink_framebuffer_clear_data *clear = get_clear_data(ctx, fb_clear, needs_rp ? scissor_state : NULL);
+      ctx->clears_enabled |= PIPE_CLEAR_DEPTHSTENCIL;
+      clear->conditional = ctx->render_condition_active;
+      clear->has_scissor = needs_rp;
+      if (scissor_state && needs_rp)
+         clear->scissor = *scissor_state;
       if (buffers & PIPE_CLEAR_DEPTH)
-         aspects |= VK_IMAGE_ASPECT_DEPTH_BIT;
+         clear->zs.depth = depth;
       if (buffers & PIPE_CLEAR_STENCIL)
-         aspects |= VK_IMAGE_ASPECT_STENCIL_BIT;
-      clear_zs_no_rp(batch, zink_resource(fb->zsbuf->texture), aspects,
-                     depth, stencil, fb->zsbuf->u.tex.level, fb->zsbuf->u.tex.first_layer,
-                     fb->zsbuf->u.tex.last_layer - fb->zsbuf->u.tex.first_layer + 1);
+         clear->zs.stencil = stencil;
+      clear->zs.bits |= (buffers & PIPE_CLEAR_DEPTHSTENCIL);
    }
 }
 
+static inline bool
+colors_equal(union pipe_color_union *a, union pipe_color_union *b)
+{
+   return a->ui[0] == b->ui[0] && a->ui[1] == b->ui[1] && a->ui[2] == b->ui[2] && a->ui[3] == b->ui[3];
+}
+
+void
+zink_clear_framebuffer(struct zink_context *ctx, unsigned clear_buffers)
+{
+   unsigned to_clear = 0;
+   struct pipe_framebuffer_state *fb_state = &ctx->fb_state;
+   while (clear_buffers) {
+      struct zink_framebuffer_clear *color_clear = NULL;
+      struct zink_framebuffer_clear *zs_clear = NULL;
+      unsigned num_clears = 0;
+      for (int i = 0; i < fb_state->nr_cbufs && clear_buffers >= PIPE_CLEAR_COLOR0; i++) {
+         struct zink_framebuffer_clear *fb_clear = &ctx->fb_clears[i];
+         /* these need actual clear calls inside the rp */
+         if (!(clear_buffers & (PIPE_CLEAR_COLOR0 << i)))
+            continue;
+         if (color_clear) {
+            /* different number of clears -> do another clear */
+            //XXX: could potentially merge "some" of the clears into this one for a very, very small optimization
+            if (num_clears != zink_fb_clear_count(fb_clear))
+               goto out;
+            /* compare all the clears to determine if we can batch these buffers together */
+            for (int j = !zink_fb_clear_first_needs_explicit(fb_clear); j < num_clears; j++) {
+               struct zink_framebuffer_clear_data *a = zink_fb_clear_element(color_clear, j);
+               struct zink_framebuffer_clear_data *b = zink_fb_clear_element(fb_clear, j);
+               /* scissors don't match, fire this one off */
+               if (a->has_scissor != b->has_scissor || (a->has_scissor && !scissor_states_equal(&a->scissor, &b->scissor)))
+                  goto out;
+
+               /* colors don't match, fire this one off */
+               if (!colors_equal(&a->color.color, &b->color.color))
+                  goto out;
+            }
+         } else {
+            color_clear = fb_clear;
+            num_clears = zink_fb_clear_count(fb_clear);
+         }
+
+         clear_buffers &= ~(PIPE_CLEAR_COLOR0 << i);
+         to_clear |= (PIPE_CLEAR_COLOR0 << i);
+      }
+      clear_buffers &= ~PIPE_CLEAR_COLOR;
+      if (clear_buffers & PIPE_CLEAR_DEPTHSTENCIL) {
+         struct zink_framebuffer_clear *fb_clear = &ctx->fb_clears[PIPE_MAX_COLOR_BUFS];
+         if (color_clear) {
+            if (num_clears != zink_fb_clear_count(fb_clear))
+               goto out;
+            /* compare all the clears to determine if we can batch these buffers together */
+            for (int j = !zink_fb_clear_first_needs_explicit(fb_clear); j < zink_fb_clear_count(color_clear); j++) {
+               struct zink_framebuffer_clear_data *a = zink_fb_clear_element(color_clear, j);
+               struct zink_framebuffer_clear_data *b = zink_fb_clear_element(fb_clear, j);
+               /* scissors don't match, fire this one off */
+               if (a->has_scissor != b->has_scissor || (a->has_scissor && !scissor_states_equal(&a->scissor, &b->scissor)))
+                  goto out;
+            }
+         }
+         zs_clear = fb_clear;
+         to_clear |= (clear_buffers & PIPE_CLEAR_DEPTHSTENCIL);
+         clear_buffers &= ~PIPE_CLEAR_DEPTHSTENCIL;
+      }
+out:
+      if (to_clear) {
+         if (num_clears) {
+            for (int j = !zink_fb_clear_first_needs_explicit(color_clear); j < num_clears; j++) {
+               struct zink_framebuffer_clear_data *clear = zink_fb_clear_element(color_clear, j);
+               struct zink_framebuffer_clear_data *zsclear = NULL;
+               if (zs_clear)
+                  zsclear = zink_fb_clear_element(zs_clear, j);
+               zink_clear(&ctx->base, to_clear,
+                          clear->has_scissor ? &clear->scissor : NULL,
+                          &clear->color.color,
+                          zsclear ? zsclear->zs.depth : 0,
+                          zsclear ? zsclear->zs.stencil : 0);
+            }
+         } else {
+            for (int j = !zink_fb_clear_first_needs_explicit(zs_clear); j < zink_fb_clear_count(zs_clear); j++) {
+               struct zink_framebuffer_clear_data *clear = zink_fb_clear_element(zs_clear, j);
+               zink_clear(&ctx->base, to_clear,
+                          clear->has_scissor ? &clear->scissor : NULL,
+                          NULL,
+                          clear->zs.depth,
+                          clear->zs.stencil);
+            }
+         }
+      }
+      to_clear = 0;
+   }
+   for (int i = 0; i < ARRAY_SIZE(ctx->fb_clears); i++)
+       zink_fb_clear_reset(ctx, i);
+}
+
 static struct pipe_surface *
 create_clear_surface(struct pipe_context *pctx, struct pipe_resource *pres, unsigned level, const struct pipe_box *box)
 {
@@ -258,9 +368,9 @@ zink_clear_texture(struct pipe_context *pctx,
    struct zink_context *ctx = zink_context(pctx);
    struct zink_resource *res = zink_resource(pres);
    struct pipe_screen *pscreen = pctx->screen;
-   struct u_rect region = {box->x, box->x + box->width, box->y, box->y + box->height};
-   bool needs_rp = clear_needs_rp(pres->width0, pres->height0, &region) || ctx->render_condition_active;
-   struct zink_batch *batch = zink_curr_batch(ctx);
+   struct u_rect region = zink_rect_from_box(box);
+   bool needs_rp = !zink_blit_region_fills(region, pres->width0, pres->height0) || ctx->render_condition_active;
+   struct zink_batch *batch = &ctx->batch;
    struct pipe_surface *surf = NULL;
 
    if (res->aspect & VK_IMAGE_ASPECT_COLOR_BIT) {
@@ -270,12 +380,14 @@ zink_clear_texture(struct pipe_context *pctx,
 
       if (pscreen->is_format_supported(pscreen, pres->format, pres->target, 0, 0,
                                       PIPE_BIND_RENDER_TARGET) && !needs_rp && !batch->in_rp) {
-         clear_color_no_rp(batch, res, &color, level, box->z, box->depth);
+         clear_color_no_rp(ctx, res, &color, level, box->z, box->depth);
       } else {
          surf = create_clear_surface(pctx, pres, level, box);
          zink_blit_begin(ctx, ZINK_BLIT_SAVE_FB | ZINK_BLIT_SAVE_FS);
          util_clear_render_target(pctx, surf, &color, box->x, box->y, box->width, box->height);
       }
+      if (res->base.b.target == PIPE_BUFFER)
+         util_range_add(&res->base.b, &res->valid_buffer_range, box->x, box->x + box->width);
    } else {
       float depth = 0.0;
       uint8_t stencil = 0;
@@ -287,7 +399,7 @@ zink_clear_texture(struct pipe_context *pctx,
          util_format_unpack_s_8uint(pres->format, &stencil, data, 1);
 
       if (!needs_rp && !batch->in_rp)
-         clear_zs_no_rp(batch, res, res->aspect, depth, stencil, level, box->z, box->depth);
+         clear_zs_no_rp(ctx, res, res->aspect, depth, stencil, level, box->z, box->depth);
       else {
          unsigned flags = 0;
          if (res->aspect & VK_IMAGE_ASPECT_DEPTH_BIT)
@@ -301,3 +413,207 @@ zink_clear_texture(struct pipe_context *pctx,
    }
    pipe_surface_reference(&surf, NULL);
 }
+
+bool
+zink_fb_clear_needs_explicit(struct zink_framebuffer_clear *fb_clear)
+{
+   if (zink_fb_clear_count(fb_clear) != 1)
+      return true;
+   return zink_fb_clear_element_needs_explicit(zink_fb_clear_element(fb_clear, 0));
+}
+
+bool
+zink_fb_clear_first_needs_explicit(struct zink_framebuffer_clear *fb_clear)
+{
+   if (!zink_fb_clear_count(fb_clear))
+      return false;
+   return zink_fb_clear_element_needs_explicit(zink_fb_clear_element(fb_clear, 0));
+}
+
+static void
+fb_clears_apply_internal(struct zink_context *ctx, struct pipe_resource *pres, int i)
+{
+   struct zink_framebuffer_clear *fb_clear = &ctx->fb_clears[i];
+
+   if (!zink_fb_clear_enabled(ctx, i))
+      return;
+   if (zink_resource(pres)->aspect == VK_IMAGE_ASPECT_COLOR_BIT) {
+      assert(!ctx->batch.in_rp);
+      if (zink_fb_clear_needs_explicit(fb_clear) || !check_3d_layers(ctx->fb_state.cbufs[i]))
+         /* this will automatically trigger all the clears */
+         zink_batch_rp(ctx);
+      else {
+         struct pipe_surface *psurf = ctx->fb_state.cbufs[i];
+         struct zink_framebuffer_clear_data *clear = zink_fb_clear_element(fb_clear, 0);
+         union pipe_color_union color = clear->color.color;
+         if (clear->color.srgb) {
+            /* if SRGB mode is disabled for the fb with a backing srgb image then we have to
+             * convert this to srgb color
+             */
+            color.f[0] = util_format_srgb_to_linear_float(clear->color.color.f[0]);
+            color.f[1] = util_format_srgb_to_linear_float(clear->color.color.f[1]);
+            color.f[2] = util_format_srgb_to_linear_float(clear->color.color.f[2]);
+         }
+
+         clear_color_no_rp(ctx, zink_resource(pres), &color,
+                                psurf->u.tex.level, psurf->u.tex.first_layer,
+                                psurf->u.tex.last_layer - psurf->u.tex.first_layer + 1);
+      }
+      zink_fb_clear_reset(ctx, i);
+      return;
+   } else {
+      assert(!ctx->batch.in_rp);
+      if (zink_fb_clear_needs_explicit(fb_clear) || !check_3d_layers(ctx->fb_state.zsbuf))
+         /* this will automatically trigger all the clears */
+         zink_batch_rp(ctx);
+      else {
+         struct pipe_surface *psurf = ctx->fb_state.zsbuf;
+         struct zink_framebuffer_clear_data *clear = zink_fb_clear_element(fb_clear, 0);
+         VkImageAspectFlags aspects = 0;
+         if (clear->zs.bits & PIPE_CLEAR_DEPTH)
+            aspects |= VK_IMAGE_ASPECT_DEPTH_BIT;
+         if (clear->zs.bits & PIPE_CLEAR_STENCIL)
+            aspects |= VK_IMAGE_ASPECT_STENCIL_BIT;
+         clear_zs_no_rp(ctx, zink_resource(pres), aspects, clear->zs.depth, clear->zs.stencil,
+                             psurf->u.tex.level, psurf->u.tex.first_layer,
+                             psurf->u.tex.last_layer - psurf->u.tex.first_layer + 1);
+      }
+   }
+   zink_fb_clear_reset(ctx, i);
+}
+
+void
+zink_fb_clear_reset(struct zink_context *ctx, unsigned i)
+{
+   util_dynarray_fini(&ctx->fb_clears[i].clears);
+   if (i == PIPE_MAX_COLOR_BUFS)
+      ctx->clears_enabled &= ~PIPE_CLEAR_DEPTHSTENCIL;
+   else
+      ctx->clears_enabled &= ~(PIPE_CLEAR_COLOR0 << i);
+}
+
+void
+zink_fb_clears_apply(struct zink_context *ctx, struct pipe_resource *pres)
+{
+   if (zink_resource(pres)->aspect == VK_IMAGE_ASPECT_COLOR_BIT) {
+      for (int i = 0; i < ctx->fb_state.nr_cbufs; i++) {
+         if (ctx->fb_state.cbufs[i] && ctx->fb_state.cbufs[i]->texture == pres) {
+            fb_clears_apply_internal(ctx, pres, i);
+            return;
+         }
+      }
+   } else {
+      if (ctx->fb_state.zsbuf && ctx->fb_state.zsbuf->texture == pres) {
+         fb_clears_apply_internal(ctx, pres, PIPE_MAX_COLOR_BUFS);
+      }
+   }
+}
+
+void
+zink_fb_clears_discard(struct zink_context *ctx, struct pipe_resource *pres)
+{
+   if (zink_resource(pres)->aspect == VK_IMAGE_ASPECT_COLOR_BIT) {
+      for (int i = 0; i < ctx->fb_state.nr_cbufs; i++) {
+         if (ctx->fb_state.cbufs[i] && ctx->fb_state.cbufs[i]->texture == pres) {
+            if (zink_fb_clear_enabled(ctx, i)) {
+               zink_fb_clear_reset(ctx, i);
+               return;
+            }
+         }
+      }
+   } else {
+      if (zink_fb_clear_enabled(ctx, PIPE_MAX_COLOR_BUFS) && ctx->fb_state.zsbuf && ctx->fb_state.zsbuf->texture == pres) {
+         int i = PIPE_MAX_COLOR_BUFS;
+         zink_fb_clear_reset(ctx, i);
+      }
+   }
+}
+
+void
+zink_clear_apply_conditionals(struct zink_context *ctx)
+{
+   for (int i = 0; i < ARRAY_SIZE(ctx->fb_clears); i++) {
+      struct zink_framebuffer_clear *fb_clear = &ctx->fb_clears[i];
+      if (!zink_fb_clear_enabled(ctx, i))
+         continue;
+      for (int j = 0; j < zink_fb_clear_count(fb_clear); j++) {
+         struct zink_framebuffer_clear_data *clear = zink_fb_clear_element(fb_clear, j);
+         if (clear->conditional) {
+            struct pipe_surface *surf;
+            if (i < PIPE_MAX_COLOR_BUFS)
+               surf = ctx->fb_state.cbufs[i];
+            else
+               surf = ctx->fb_state.zsbuf;
+            if (surf)
+               fb_clears_apply_internal(ctx, surf->texture, i);
+            else
+               zink_fb_clear_reset(ctx, i);
+            break;
+         }
+      }
+   }
+}
+
+static void
+fb_clears_apply_or_discard_internal(struct zink_context *ctx, struct pipe_resource *pres, struct u_rect region, bool discard_only, bool invert, int i)
+{
+   struct zink_framebuffer_clear *fb_clear = &ctx->fb_clears[i];
+   if (zink_fb_clear_enabled(ctx, i)) {
+      if (zink_blit_region_fills(region, pres->width0, pres->height0)) {
+         if (invert)
+            fb_clears_apply_internal(ctx, pres, i);
+         else
+            /* we know we can skip these */
+            zink_fb_clears_discard(ctx, pres);
+         return;
+      }
+      for (int j = 0; j < zink_fb_clear_count(fb_clear); j++) {
+         struct zink_framebuffer_clear_data *clear = zink_fb_clear_element(fb_clear, j);
+         struct u_rect scissor = {clear->scissor.minx, clear->scissor.maxx,
+                                  clear->scissor.miny, clear->scissor.maxy};
+         if (!clear->has_scissor || zink_blit_region_covers(region, scissor)) {
+            /* this is a clear that isn't fully covered by our pending write */
+            if (!discard_only)
+               fb_clears_apply_internal(ctx, pres, i);
+            return;
+         }
+      }
+      if (!invert)
+         /* if we haven't already returned, then we know we can discard */
+         zink_fb_clears_discard(ctx, pres);
+   }
+}
+
+void
+zink_fb_clears_apply_or_discard(struct zink_context *ctx, struct pipe_resource *pres, struct u_rect region, bool discard_only)
+{
+   if (zink_resource(pres)->aspect == VK_IMAGE_ASPECT_COLOR_BIT) {
+      for (int i = 0; i < ctx->fb_state.nr_cbufs; i++) {
+         if (ctx->fb_state.cbufs[i] && ctx->fb_state.cbufs[i]->texture == pres) {
+            fb_clears_apply_or_discard_internal(ctx, pres, region, discard_only, false, i);
+            return;
+         }
+      }
+   }  else {
+      if (zink_fb_clear_enabled(ctx, PIPE_MAX_COLOR_BUFS) && ctx->fb_state.zsbuf && ctx->fb_state.zsbuf->texture == pres) {
+         fb_clears_apply_or_discard_internal(ctx, pres, region, discard_only, false, PIPE_MAX_COLOR_BUFS);
+      }
+   }
+}
+
+void
+zink_fb_clears_apply_region(struct zink_context *ctx, struct pipe_resource *pres, struct u_rect region)
+{
+   if (zink_resource(pres)->aspect == VK_IMAGE_ASPECT_COLOR_BIT) {
+      for (int i = 0; i < ctx->fb_state.nr_cbufs; i++) {
+         if (ctx->fb_state.cbufs[i] && ctx->fb_state.cbufs[i]->texture == pres) {
+            fb_clears_apply_or_discard_internal(ctx, pres, region, false, true, i);
+            return;
+         }
+      }
+   }  else {
+      if (ctx->fb_state.zsbuf && ctx->fb_state.zsbuf->texture == pres) {
+         fb_clears_apply_or_discard_internal(ctx, pres, region, false, true, PIPE_MAX_COLOR_BUFS);
+      }
+   }
+}
diff --git a/src/gallium/drivers/zink/zink_clear.h b/src/gallium/drivers/zink/zink_clear.h
new file mode 100644
index 00000000000..a9f0c13979e
--- /dev/null
+++ b/src/gallium/drivers/zink/zink_clear.h
@@ -0,0 +1,112 @@
+/*
+ * Copyright  2020 Mike Blumenkrantz
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ * 
+ * Authors:
+ *    Mike Blumenkrantz <michael.blumenkrantz@gmail.com>
+ */
+
+#include "util/u_dynarray.h"
+#include "pipe/p_state.h"
+#include <vulkan/vulkan.h>
+#include "util/u_rect.h"
+
+struct zink_context;
+struct zink_resource;
+
+struct zink_framebuffer_clear_data {
+   union {
+      struct {
+         union pipe_color_union color;
+         bool srgb;
+      } color;
+      struct {
+         float depth;
+         unsigned stencil;
+         uint8_t bits : 2; // PIPE_CLEAR_DEPTH, PIPE_CLEAR_STENCIL
+      } zs;
+   };
+   struct pipe_scissor_state scissor;
+   bool has_scissor;
+   bool conditional;
+};
+
+struct zink_framebuffer_clear {
+   struct util_dynarray clears;
+};
+
+void
+zink_clear(struct pipe_context *pctx,
+           unsigned buffers,
+           const struct pipe_scissor_state *scissor_state,
+           const union pipe_color_union *pcolor,
+           double depth, unsigned stencil);
+void
+zink_clear_texture(struct pipe_context *ctx,
+                   struct pipe_resource *p_res,
+                   unsigned level,
+                   const struct pipe_box *box,
+                   const void *data);
+
+bool
+zink_fb_clear_needs_explicit(struct zink_framebuffer_clear *fb_clear);
+
+bool
+zink_fb_clear_first_needs_explicit(struct zink_framebuffer_clear *fb_clear);
+
+void
+zink_clear_framebuffer(struct zink_context *ctx, unsigned clear_buffers);
+
+static inline struct zink_framebuffer_clear_data *
+zink_fb_clear_element(struct zink_framebuffer_clear *fb_clear, int idx)
+{
+   return util_dynarray_element(&fb_clear->clears, struct zink_framebuffer_clear_data, idx);
+}
+
+static inline unsigned
+zink_fb_clear_count(struct zink_framebuffer_clear *fb_clear)
+{
+   return fb_clear ? util_dynarray_num_elements(&fb_clear->clears, struct zink_framebuffer_clear_data) : 0;
+}
+
+void
+zink_fb_clear_reset(struct zink_context *ctx, unsigned idx);
+
+static inline bool
+zink_fb_clear_element_needs_explicit(struct zink_framebuffer_clear_data *clear)
+{
+   return clear->has_scissor || clear->conditional;
+}
+
+void
+zink_clear_apply_conditionals(struct zink_context *ctx);
+
+void
+zink_fb_clears_apply(struct zink_context *ctx, struct pipe_resource *pres);
+
+void
+zink_fb_clears_discard(struct zink_context *ctx, struct pipe_resource *pres);
+
+void
+zink_fb_clears_apply_or_discard(struct zink_context *ctx, struct pipe_resource *pres, struct u_rect region, bool discard_only);
+
+void
+zink_fb_clears_apply_region(struct zink_context *ctx, struct pipe_resource *pres, struct u_rect region);
diff --git a/src/gallium/drivers/zink/zink_compiler.c b/src/gallium/drivers/zink/zink_compiler.c
index cc2e178de92..6d948d60028 100644
--- a/src/gallium/drivers/zink/zink_compiler.c
+++ b/src/gallium/drivers/zink/zink_compiler.c
@@ -38,6 +38,50 @@
 
 #include "util/u_memory.h"
 
+static void
+create_vs_pushconst(nir_shader *nir)
+{
+   nir_variable *vs_pushconst;
+   /* create compatible layout for the ntv push constant loader */
+   struct glsl_struct_field *fields = rzalloc_array(nir, struct glsl_struct_field, 2);
+   fields[0].type = glsl_array_type(glsl_uint_type(), 1, 0);
+   fields[0].name = ralloc_asprintf(nir, "draw_mode_is_indexed");
+   fields[0].offset = offsetof(struct zink_gfx_push_constant, draw_mode_is_indexed);
+   fields[1].type = glsl_array_type(glsl_uint_type(), 1, 0);
+   fields[1].name = ralloc_asprintf(nir, "draw_id");
+   fields[1].offset = offsetof(struct zink_gfx_push_constant, draw_id);
+   vs_pushconst = nir_variable_create(nir, nir_var_mem_push_const,
+                                                 glsl_struct_type(fields, 2, "struct", false), "vs_pushconst");
+   vs_pushconst->data.location = INT_MAX; //doesn't really matter
+}
+
+static void
+create_cs_pushconst(nir_shader *nir)
+{
+   nir_variable *cs_pushconst;
+   /* create compatible layout for the ntv push constant loader */
+   struct glsl_struct_field *fields = rzalloc_size(nir, 1 * sizeof(struct glsl_struct_field));
+   fields[0].type = glsl_array_type(glsl_uint_type(), 1, 0);
+   fields[0].name = ralloc_asprintf(nir, "work_dim");
+   fields[0].offset = 0;
+   cs_pushconst = nir_variable_create(nir, nir_var_mem_push_const,
+                                                 glsl_struct_type(fields, 1, "struct", false), "cs_pushconst");
+   cs_pushconst->data.location = INT_MAX; //doesn't really matter
+}
+
+static bool
+reads_draw_params(nir_shader *shader)
+{
+   return (shader->info.system_values_read & (1ull << SYSTEM_VALUE_BASE_VERTEX)) ||
+          (shader->info.system_values_read & (1ull << SYSTEM_VALUE_DRAW_ID));
+}
+
+static bool
+reads_work_dim(nir_shader *shader)
+{
+   return shader->info.system_values_read & (1ull << SYSTEM_VALUE_WORK_DIM);
+}
+
 static bool
 lower_discard_if_instr(nir_intrinsic_instr *instr, nir_builder *b)
 {
@@ -122,6 +166,42 @@ lower_discard_if(nir_shader *shader)
    return progress;
 }
 
+static bool
+lower_work_dim_instr(nir_builder *b, nir_instr *in, void *data)
+{
+   if (in->type != nir_instr_type_intrinsic)
+      return false;
+   nir_intrinsic_instr *instr = nir_instr_as_intrinsic(in);
+   if (instr->intrinsic != nir_intrinsic_load_work_dim)
+      return false;
+
+   if (instr->intrinsic == nir_intrinsic_load_work_dim) {
+      b->cursor = nir_after_instr(&instr->instr);
+      nir_intrinsic_instr *load = nir_intrinsic_instr_create(b->shader, nir_intrinsic_load_push_constant);
+      load->src[0] = nir_src_for_ssa(nir_imm_int(b, 0));
+      nir_intrinsic_set_range(load, 3 * sizeof(uint32_t));
+      load->num_components = 1;
+      nir_ssa_dest_init(&load->instr, &load->dest, 1, 32, "work_dim");
+      nir_builder_instr_insert(b, &load->instr);
+
+      nir_ssa_def_rewrite_uses(&instr->dest.ssa, nir_src_for_ssa(&load->dest.ssa));
+   }
+
+   return true;
+}
+
+static bool
+lower_work_dim(nir_shader *shader)
+{
+   if (shader->info.stage != MESA_SHADER_COMPUTE)
+      return false;
+
+   if (!reads_work_dim(shader))
+      return false;
+
+   return nir_shader_instructions_pass(shader, lower_work_dim_instr, nir_metadata_dominance, NULL);
+}
+
 static bool
 lower_64bit_vertex_attribs_instr(nir_builder *b, nir_instr *instr, void *data)
 {
@@ -201,6 +281,73 @@ lower_64bit_vertex_attribs(nir_shader *shader)
    return nir_shader_instructions_pass(shader, lower_64bit_vertex_attribs_instr, nir_metadata_dominance, NULL);
 }
 
+static bool
+lower_draw_params_instr(nir_builder *b, nir_instr *in, void *data)
+{
+   if (in->type != nir_instr_type_intrinsic)
+      return false;
+   nir_intrinsic_instr *instr = nir_instr_as_intrinsic(in);
+   if (instr->intrinsic != nir_intrinsic_load_base_vertex &&
+       instr->intrinsic != nir_intrinsic_load_draw_id)
+      return false;
+
+   if (instr->intrinsic == nir_intrinsic_load_base_vertex) {
+      b->cursor = nir_after_instr(&instr->instr);
+      nir_intrinsic_instr *load = nir_intrinsic_instr_create(b->shader, nir_intrinsic_load_push_constant);
+      load->src[0] = nir_src_for_ssa(nir_imm_int(b, 0));
+      nir_intrinsic_set_range(load, 4);
+      load->num_components = 1;
+      nir_ssa_dest_init(&load->instr, &load->dest, 1, 32, "draw_mode_is_indexed");
+      nir_builder_instr_insert(b, &load->instr);
+
+      nir_ssa_def *composite = nir_build_alu(b, nir_op_bcsel,
+                                             nir_build_alu(b, nir_op_ieq, &load->dest.ssa, nir_imm_int(b, 1), NULL, NULL),
+                                             &instr->dest.ssa,
+                                             nir_imm_int(b, 0),
+                                             NULL);
+
+      nir_ssa_def_rewrite_uses_after(&instr->dest.ssa, nir_src_for_ssa(composite), composite->parent_instr);
+   } else {
+      b->cursor = nir_before_instr(&instr->instr);
+      nir_intrinsic_instr *load = nir_intrinsic_instr_create(b->shader, nir_intrinsic_load_push_constant);
+      load->src[0] = nir_src_for_ssa(nir_imm_int(b, 1));
+      nir_intrinsic_set_range(load, 4);
+      load->num_components = 1;
+      nir_ssa_dest_init(&load->instr, &load->dest, 1, 32, "draw_id");
+      nir_builder_instr_insert(b, &load->instr);
+
+      nir_ssa_def_rewrite_uses(&instr->dest.ssa, nir_src_for_ssa(&load->dest.ssa));
+   }
+
+   return true;
+}
+
+static bool
+lower_draw_params(nir_shader *shader)
+{
+   if (shader->info.stage != MESA_SHADER_VERTEX)
+      return false;
+
+   if (!reads_draw_params(shader))
+      return false;
+
+   return nir_shader_instructions_pass(shader, lower_draw_params_instr, nir_metadata_dominance, NULL);
+}
+
+static bool
+lower_dual_blend(nir_shader *shader)
+{
+   bool progress = false;
+   nir_variable *var = nir_find_variable_with_location(shader, nir_var_shader_out, FRAG_RESULT_DATA1);
+   if (var) {
+      var->data.location = FRAG_RESULT_DATA0;
+      var->data.index = 1;
+      progress = true;
+   }
+   nir_shader_preserve_all_metadata(shader);
+   return progress;
+}
+
 void
 zink_screen_init_compiler(struct zink_screen *screen)
 {
@@ -319,28 +466,64 @@ zink_shader_compile(struct zink_screen *screen, struct zink_shader *zs, struct z
 {
    VkShaderModule mod = VK_NULL_HANDLE;
    void *streamout = NULL;
-   nir_shader *nir = zs->nir;
+   nir_shader *needs_free = NULL, *nir = zs->nir;
+   bool want_halfz = false;
+
+   if (key) {
+      nir_lower_tex_options tex_opts = {};
+      tex_opts.saturate_s = key->base.gl_clamp[0];
+      tex_opts.saturate_r = key->base.gl_clamp[1];
+      tex_opts.saturate_t = key->base.gl_clamp[2];
+      if (tex_opts.saturate_s || tex_opts.saturate_r || tex_opts.saturate_t) {
+         if (!needs_free)
+            needs_free = nir = nir_shader_clone(NULL, nir);
+         NIR_PASS_V(nir, nir_lower_tex, &tex_opts);
+         optimize_nir(nir);
+      }
+      if (key->inline_uniforms) {
+         if (!needs_free)
+            needs_free = nir = nir_shader_clone(NULL, nir);
+         NIR_PASS_V(nir, nir_inline_uniforms,
+                    nir->info.num_inlinable_uniforms,
+                    key->base.inlined_uniform_values,
+                    nir->info.inlinable_uniform_dw_offsets);
+
+         optimize_nir(nir);
+
+         /* This must be done again. */
+         NIR_PASS_V(nir, nir_io_add_const_offset_to_base, nir_var_shader_in |
+                                                          nir_var_shader_out);
+      }
+   }
+
    /* TODO: use a separate mem ctx here for ralloc */
    if (zs->has_geometry_shader) {
       if (zs->nir->info.stage == MESA_SHADER_GEOMETRY) {
          streamout = &zs->streamout;
-         NIR_PASS_V(nir, nir_lower_clip_halfz);
+         want_halfz = true;
       }
    } else if (zs->has_tess_shader) {
       if (zs->nir->info.stage == MESA_SHADER_TESS_EVAL) {
          streamout = &zs->streamout;
-         NIR_PASS_V(nir, nir_lower_clip_halfz);
+         want_halfz = true;
       }
    } else {
       streamout = &zs->streamout;
+      want_halfz = true;
+   }
+   /* only run this if we aren't already using halfz */
+   if (want_halfz && key && !zink_vs_key(key)->clip_halfz) {
+      if (!needs_free)
+         needs_free = nir = nir_shader_clone(NULL, nir);
       NIR_PASS_V(nir, nir_lower_clip_halfz);
    }
    if (!zs->streamout.so_info_slots)
        streamout = NULL;
    if (zs->nir->info.stage == MESA_SHADER_FRAGMENT) {
-      nir = nir_shader_clone(NULL, nir);
       if (!zink_fs_key(key)->samples &&
           nir->info.outputs_written & BITFIELD64_BIT(FRAG_RESULT_SAMPLE_MASK)) {
+         if (!needs_free)
+            needs_free = nir = nir_shader_clone(NULL, nir);
          /* VK will always use gl_SampleMask[] values even if sample count is 0,
           * so we need to skip this write here to mimic GL's behavior of ignoring it
           */
@@ -352,8 +535,11 @@ zink_shader_compile(struct zink_screen *screen, struct zink_shader *zs, struct z
          NIR_PASS_V(nir, nir_remove_dead_variables, nir_var_shader_temp, NULL);
          optimize_nir(nir);
       }
+      if (zink_fs_key(key)->force_dual_color_blend && nir->info.outputs_written & BITFIELD64_BIT(FRAG_RESULT_DATA1))
+         NIR_PASS_V(nir, lower_dual_blend);
    }
-   struct spirv_shader *spirv = nir_to_spirv(nir, streamout, shader_slot_map, shader_slots_reserved);
+   NIR_PASS_V(nir, nir_convert_from_ssa, true);
+   struct spirv_shader *spirv = nir_to_spirv(nir, streamout, shader_slot_map, shader_slots_reserved, &screen->info, screen->lazy_descriptors);
    assert(spirv);
 
    if (zink_debug & ZINK_DEBUG_SPIRV) {
@@ -376,8 +562,8 @@ zink_shader_compile(struct zink_screen *screen, struct zink_shader *zs, struct z
    if (vkCreateShaderModule(screen->dev, &smci, NULL, &mod) != VK_SUCCESS)
       mod = VK_NULL_HANDLE;
 
-   if (zs->nir->info.stage == MESA_SHADER_FRAGMENT)
-      ralloc_free(nir);
+   if (needs_free)
+      ralloc_free(needs_free);
 
    /* TODO: determine if there's any reason to cache spirv output? */
    ralloc_free(spirv);
@@ -408,6 +594,26 @@ lower_baseinstance(nir_shader *shader)
 
 bool nir_lower_dynamic_bo_access(nir_shader *shader);
 
+static bool
+fixup_counter_locations(nir_shader *shader)
+{
+   unsigned last_binding = 0;
+   unsigned last_location = 0;
+   if (!shader->info.num_abos)
+      return false;
+   nir_foreach_variable_with_modes(var, shader, nir_var_uniform) {
+      if (!type_is_counter(var->type))
+         continue;
+      var->data.binding += shader->info.num_ssbos;
+      if (var->data.binding != last_binding) {
+         last_binding = var->data.binding;
+         last_location = 0;
+      }
+      var->data.location = last_location++;
+   }
+   return true;
+}
+
 struct zink_shader *
 zink_shader_create(struct zink_screen *screen, struct nir_shader *nir,
                    const struct pipe_stream_output_info *so_info)
@@ -418,21 +624,15 @@ zink_shader_create(struct zink_screen *screen, struct nir_shader *nir,
    ret->shader_id = p_atomic_inc_return(&screen->shader_id);
    ret->programs = _mesa_pointer_set_create(NULL);
 
-   if (!screen->info.feats.features.shaderImageGatherExtended) {
-      nir_lower_tex_options tex_opts = {};
-      tex_opts.lower_tg4_offsets = true;
-      NIR_PASS_V(nir, nir_lower_tex, &tex_opts);
-   }
+   if (nir->info.stage == MESA_SHADER_VERTEX)
+      create_vs_pushconst(nir);
+   else if (nir->info.stage == MESA_SHADER_COMPUTE)
+      create_cs_pushconst(nir);
 
-   /* only do uniforms -> ubo if we have uniforms, otherwise we're just
-    * screwing with the bindings for no reason
-    */
-   if (nir->num_uniforms)
-      NIR_PASS_V(nir, nir_lower_uniforms_to_ubo, 16);
    if (nir->info.stage < MESA_SHADER_FRAGMENT)
       have_psiz = check_psiz(nir);
-   if (nir->info.stage == MESA_SHADER_GEOMETRY)
-      NIR_PASS_V(nir, nir_lower_gs_intrinsics, nir_lower_gs_intrinsics_per_stream);
+   NIR_PASS_V(nir, lower_draw_params);
+   NIR_PASS_V(nir, lower_work_dim);
    NIR_PASS_V(nir, nir_lower_regs_to_ssa);
    NIR_PASS_V(nir, lower_baseinstance);
    optimize_nir(nir);
@@ -440,9 +640,7 @@ zink_shader_create(struct zink_screen *screen, struct nir_shader *nir,
    NIR_PASS_V(nir, lower_discard_if);
    NIR_PASS_V(nir, nir_lower_fragcolor);
    NIR_PASS_V(nir, lower_64bit_vertex_attribs);
-   if (nir->info.num_ubos || nir->info.num_ssbos)
-      NIR_PASS_V(nir, nir_lower_dynamic_bo_access);
-   NIR_PASS_V(nir, nir_convert_from_ssa, true);
+   NIR_PASS_V(nir, fixup_counter_locations);
 
    if (zink_debug & ZINK_DEBUG_NIR) {
       fprintf(stderr, "NIR shader:\n---8<---\n");
@@ -450,8 +648,6 @@ zink_shader_create(struct zink_screen *screen, struct nir_shader *nir,
       fprintf(stderr, "---8<---\n");
    }
 
-   ret->num_bindings = 0;
-   uint32_t cur_ubo = 0;
    /* UBO buffers are zero-indexed, but buffer 0 is always the one created by nir_lower_uniforms_to_ubo,
     * which means there is no buffer 0 if there are no uniforms
     */
@@ -459,16 +655,26 @@ zink_shader_create(struct zink_screen *screen, struct nir_shader *nir,
    /* need to set up var->data.binding for UBOs, which means we need to start at
     * the "first" UBO, which is at the end of the list
     */
+   int ssbo_array_index = 0;
+   int abo_array_index = nir->info.num_ssbos;
+   uint32_t (*binding_func)(gl_shader_stage stage, VkDescriptorType type, int index) = (screen->lazy_descriptors ? zink_binding_lazy : zink_binding);
    foreach_list_typed_reverse(nir_variable, var, node, &nir->variables) {
       if (_nir_shader_variable_has_mode(var, nir_var_uniform |
                                         nir_var_mem_ubo |
                                         nir_var_mem_ssbo)) {
+         enum zink_descriptor_type ztype;
+         const struct glsl_type *type = glsl_without_array(var->type);
+         bool is_counter = type_is_counter(type);
          if (var->data.mode == nir_var_mem_ubo) {
             /* ignore variables being accessed if they aren't the base of the UBO */
-            bool ubo_array = glsl_type_is_array(var->type) && glsl_type_is_interface(glsl_without_array(var->type));
-            if (var->data.location && !ubo_array && var->type != var->interface_type)
+            bool ubo_array = glsl_type_is_array(var->type) && glsl_type_is_interface(type);
+            if (var->data.location > 0 && !ubo_array && var->type != var->interface_type)
                continue;
-            var->data.binding = cur_ubo;
+            ztype = ZINK_DESCRIPTOR_TYPE_UBO;
+            var->data.driver_location = ret->num_bindings[ztype];
+            var->data.binding = !ubo_index ? nir->info.stage : binding_func(nir->info.stage,
+                                                              VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER,
+                                                              var->data.driver_location);
             /* if this is a ubo array, create a binding point for each array member:
              * 
                "For uniform blocks declared as arrays, each individual array element
@@ -478,41 +684,59 @@ zink_shader_create(struct zink_screen *screen, struct nir_shader *nir,
                (also it's just easier)
              */
             for (unsigned i = 0; i < (ubo_array ? glsl_get_aoa_size(var->type) : 1); i++) {
-
-               int binding = zink_binding(nir->info.stage,
-                                          VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER,
-                                          cur_ubo++);
-               ret->bindings[ret->num_bindings].index = ubo_index++;
-               ret->bindings[ret->num_bindings].binding = binding;
-               ret->bindings[ret->num_bindings].type = VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER;
-               ret->bindings[ret->num_bindings].size = 1;
-               ret->num_bindings++;
+               VkDescriptorType vktype = !ubo_index ? VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER_DYNAMIC : VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER;
+               int binding = var->data.binding + i;
+               ret->bindings[ztype][ret->num_bindings[ztype]].index = ubo_index++;
+               ret->bindings[ztype][ret->num_bindings[ztype]].binding = binding;
+               ret->bindings[ztype][ret->num_bindings[ztype]].type = vktype;
+               ret->bindings[ztype][ret->num_bindings[ztype]].size = 1;
+               ret->ubos_used |= (1 << ret->bindings[ztype][ret->num_bindings[ztype]].index);
+               ret->num_bindings[ztype]++;
+            }
+         } else if (var->data.mode == nir_var_mem_ssbo || is_counter) {
+            /* same-ish mechanics as ubos */
+            bool bo_array = glsl_type_is_array(var->type) && glsl_type_is_interface(type);
+            if (var->data.location > 0 && !bo_array && !is_counter)
+               continue;
+            if (!var->data.explicit_binding) {
+               var->data.driver_location = is_counter ? abo_array_index : ssbo_array_index;
+            } else
+               var->data.driver_location = var->data.binding;
+            ztype = ZINK_DESCRIPTOR_TYPE_SSBO;
+            var->data.binding = binding_func(nir->info.stage,
+                                             VK_DESCRIPTOR_TYPE_STORAGE_BUFFER,
+                                             var->data.driver_location);
+            for (unsigned i = 0; i < (bo_array ? glsl_get_aoa_size(var->type) : 1); i++) {
+               int binding = var->data.binding + i;
+               if (!is_counter)
+                  ret->bindings[ztype][ret->num_bindings[ztype]].index = ssbo_array_index++;
+               else
+                  ret->bindings[ztype][ret->num_bindings[ztype]].index = var->data.explicit_binding ? var->data.driver_location : abo_array_index++ ;
+               ret->ssbos_used |= (1 << ret->bindings[ztype][ret->num_bindings[ztype]].index);
+               ret->bindings[ztype][ret->num_bindings[ztype]].binding = binding;
+               ret->bindings[ztype][ret->num_bindings[ztype]].type = VK_DESCRIPTOR_TYPE_STORAGE_BUFFER;
+               ret->bindings[ztype][ret->num_bindings[ztype]].size = 1;
+               ret->num_bindings[ztype]++;
             }
-         } else if (var->data.mode == nir_var_mem_ssbo) {
-            int binding = zink_binding(nir->info.stage,
-                                       VK_DESCRIPTOR_TYPE_STORAGE_BUFFER,
-                                       var->data.binding);
-            ret->bindings[ret->num_bindings].index = var->data.binding;
-            ret->bindings[ret->num_bindings].binding = binding;
-            ret->bindings[ret->num_bindings].type = VK_DESCRIPTOR_TYPE_STORAGE_BUFFER;
-            ret->bindings[ret->num_bindings].size = 1;
-            ret->num_bindings++;
          } else {
             assert(var->data.mode == nir_var_uniform);
-            const struct glsl_type *type = glsl_without_array(var->type);
-            if (glsl_type_is_sampler(type)) {
-               VkDescriptorType vktype = zink_sampler_type(type);
-               int binding = zink_binding(nir->info.stage,
-                                          vktype,
-                                          var->data.binding);
-               ret->bindings[ret->num_bindings].index = var->data.binding;
-               ret->bindings[ret->num_bindings].binding = binding;
-               ret->bindings[ret->num_bindings].type = vktype;
+            if (glsl_type_is_sampler(type) || glsl_type_is_image(type)) {
+               VkDescriptorType vktype = glsl_type_is_image(type) ? zink_image_type(type) : zink_sampler_type(type);
+               if (vktype == VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER)
+                  ret->num_texel_buffers++;
+               ztype = zink_desc_type_from_vktype(vktype);
+               var->data.driver_location = var->data.binding;
+               var->data.binding = binding_func(nir->info.stage,
+                                                vktype,
+                                                var->data.driver_location);
+               ret->bindings[ztype][ret->num_bindings[ztype]].index = var->data.driver_location;
+               ret->bindings[ztype][ret->num_bindings[ztype]].binding = var->data.binding;
+               ret->bindings[ztype][ret->num_bindings[ztype]].type = vktype;
                if (glsl_type_is_array(var->type))
-                  ret->bindings[ret->num_bindings].size = glsl_get_aoa_size(var->type);
+                  ret->bindings[ztype][ret->num_bindings[ztype]].size = glsl_get_aoa_size(var->type);
                else
-                  ret->bindings[ret->num_bindings].size = 1;
-               ret->num_bindings++;
+                  ret->bindings[ztype][ret->num_bindings[ztype]].size = 1;
+               ret->num_bindings[ztype]++;
             }
          }
       }
@@ -529,18 +753,55 @@ zink_shader_create(struct zink_screen *screen, struct nir_shader *nir,
    return ret;
 }
 
+void
+zink_shader_finalize(struct pipe_screen *pscreen, void *nirptr, bool optimize)
+{
+   struct zink_screen *screen = zink_screen(pscreen);
+   nir_shader *nir = nirptr;
+
+   if (!screen->info.feats.features.shaderImageGatherExtended) {
+      nir_lower_tex_options tex_opts = {};
+      tex_opts.lower_tg4_offsets = true;
+      NIR_PASS_V(nir, nir_lower_tex, &tex_opts);
+   }
+   /* only do uniforms -> ubo if we have uniforms, otherwise we're just
+    * screwing with the bindings for no reason
+    */
+   if (nir->num_uniforms)
+      NIR_PASS_V(nir, nir_lower_uniforms_to_ubo, 16);
+   if (nir->info.stage == MESA_SHADER_GEOMETRY)
+      NIR_PASS_V(nir, nir_lower_gs_intrinsics, nir_lower_gs_intrinsics_per_stream);
+   optimize_nir(nir);
+   if (nir->info.num_ubos || nir->info.num_ssbos)
+      NIR_PASS_V(nir, nir_lower_dynamic_bo_access);
+   nir_shader_gather_info(nir, nir_shader_get_entrypoint(nir));
+   if (screen->driconf.inline_uniforms)
+      nir_find_inlinable_uniforms(nir);
+}
+
 void
 zink_shader_free(struct zink_context *ctx, struct zink_shader *shader)
 {
    struct zink_screen *screen = zink_screen(ctx->base.screen);
    set_foreach(shader->programs, entry) {
-      struct zink_gfx_program *prog = (void*)entry->key;
-      _mesa_hash_table_remove_key(ctx->program_cache, prog->shaders);
-      prog->shaders[pipe_shader_type_from_mesa(shader->nir->info.stage)] = NULL;
-      if (shader->nir->info.stage == MESA_SHADER_TESS_EVAL && shader->generated)
+      if (shader->nir->info.stage == MESA_SHADER_COMPUTE) {
+         struct zink_compute_program *comp = (void*)entry->key;
+         _mesa_hash_table_remove_key(ctx->compute_program_cache, &comp->shader->shader_id);
+         comp->shader = NULL;
+         bool in_use = comp == ctx->curr_compute;
+         if (zink_compute_program_reference(screen, &comp, NULL) && in_use)
+            ctx->curr_compute = NULL;
+      } else {
+         struct zink_gfx_program *prog = (void*)entry->key;
+         _mesa_hash_table_remove_key(ctx->program_cache, prog->shaders);
+         prog->shaders[pipe_shader_type_from_mesa(shader->nir->info.stage)] = NULL;
+         if (shader->nir->info.stage == MESA_SHADER_TESS_EVAL && shader->generated)
             /* automatically destroy generated tcs shaders when tes is destroyed */
             zink_shader_free(ctx, shader->generated);
-      zink_gfx_program_reference(screen, &prog, NULL);
+         bool in_use = prog == ctx->curr_program;
+         if (zink_gfx_program_reference(screen, &prog, NULL) && in_use)
+            ctx->curr_program = NULL;
+      }
    }
    _mesa_set_destroy(shader->programs, NULL);
    free(shader->streamout.so_info_slots);
@@ -628,19 +889,23 @@ zink_shader_tcs_create(struct zink_context *ctx, struct zink_shader *vs)
    gl_TessLevelOuter->data.patch = 1;
 
    /* hacks so we can size these right for now */
-   struct glsl_struct_field *fields = ralloc_size(nir, 2 * sizeof(struct glsl_struct_field));
-   fields[0].type = glsl_array_type(glsl_uint_type(), 2, 0);
-   fields[0].name = ralloc_asprintf(nir, "gl_TessLevelInner");
+   struct glsl_struct_field *fields = rzalloc_array(nir, struct glsl_struct_field, 3);
+   /* just use a single blob for padding here because it's easier */
+   fields[0].type = glsl_array_type(glsl_uint_type(), offsetof(struct zink_gfx_push_constant, default_inner_level) / 4, 0);
+   fields[0].name = ralloc_asprintf(nir, "padding");
    fields[0].offset = 0;
-   fields[1].type = glsl_array_type(glsl_uint_type(), 4, 0);
-   fields[1].name = ralloc_asprintf(nir, "gl_TessLevelOuter");
-   fields[1].offset = 8;
+   fields[1].type = glsl_array_type(glsl_uint_type(), 2, 0);
+   fields[1].name = ralloc_asprintf(nir, "gl_TessLevelInner");
+   fields[1].offset = offsetof(struct zink_gfx_push_constant, default_inner_level);
+   fields[2].type = glsl_array_type(glsl_uint_type(), 4, 0);
+   fields[2].name = ralloc_asprintf(nir, "gl_TessLevelOuter");
+   fields[2].offset = offsetof(struct zink_gfx_push_constant, default_outer_level);
    nir_variable *pushconst = nir_variable_create(nir, nir_var_mem_push_const,
-                                                 glsl_struct_type(fields, 2, "struct", false), "pushconst");
+                                                 glsl_struct_type(fields, 3, "struct", false), "pushconst");
    pushconst->data.location = VARYING_SLOT_VAR0;
 
-   nir_ssa_def *load_inner = nir_load_push_constant(&b, 2, 32, nir_imm_int(&b, 0), .base = 0, .range = 8);
-   nir_ssa_def *load_outer = nir_load_push_constant(&b, 4, 32, nir_imm_int(&b, 8), .base = 8, .range = 16);
+   nir_ssa_def *load_inner = nir_load_push_constant(&b, 2, 32, nir_imm_int(&b, 1), .base = 1, .range = 8);
+   nir_ssa_def *load_outer = nir_load_push_constant(&b, 4, 32, nir_imm_int(&b, 2), .base = 2, .range = 16);
 
    for (unsigned i = 0; i < 2; i++) {
       nir_deref_instr *store_idx = nir_build_deref_array_imm(&b, nir_build_deref_var(&b, gl_TessLevelInner), i);
@@ -664,3 +929,72 @@ zink_shader_tcs_create(struct zink_context *ctx, struct zink_shader *vs)
    ret->is_generated = true;
    return ret;
 }
+
+uint32_t
+zink_binding(gl_shader_stage stage, VkDescriptorType type, int index)
+{
+   if (stage == MESA_SHADER_NONE) {
+      unreachable("not supported");
+   } else {
+      switch (type) {
+      case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER:
+      case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER_DYNAMIC:
+         assert(index < PIPE_MAX_CONSTANT_BUFFERS);
+         return (stage * PIPE_MAX_CONSTANT_BUFFERS) + index;
+
+      case VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER:
+      case VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER:
+         assert(index < PIPE_MAX_SAMPLERS);
+         return (stage * PIPE_MAX_SAMPLERS) + index;
+
+      case VK_DESCRIPTOR_TYPE_STORAGE_BUFFER:
+         assert(index < PIPE_MAX_SHADER_BUFFERS);
+         return (stage * PIPE_MAX_SHADER_BUFFERS) + index;
+
+      case VK_DESCRIPTOR_TYPE_STORAGE_IMAGE:
+      case VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER:
+         assert(index < PIPE_MAX_SHADER_IMAGES);
+         return (stage * PIPE_MAX_SHADER_IMAGES) + index;
+
+      default:
+         unreachable("unexpected type");
+      }
+   }
+}
+
+uint32_t
+zink_binding_lazy(gl_shader_stage stage, VkDescriptorType type, int index)
+{
+   if (stage == MESA_SHADER_NONE) {
+      unreachable("not supported");
+   } else {
+      uint32_t stage_offset = (uint32_t)stage * (PIPE_MAX_CONSTANT_BUFFERS +
+                                           PIPE_MAX_SAMPLERS +
+                                           PIPE_MAX_SHADER_BUFFERS +
+                                           PIPE_MAX_SHADER_IMAGES);
+
+      switch (type) {
+      case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER:
+      case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER_DYNAMIC:
+         assert(index < PIPE_MAX_CONSTANT_BUFFERS);
+         return stage_offset + index;
+
+      case VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER:
+      case VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER:
+         assert(index < PIPE_MAX_SAMPLERS);
+         return stage_offset + PIPE_MAX_CONSTANT_BUFFERS + index;
+
+      case VK_DESCRIPTOR_TYPE_STORAGE_BUFFER:
+         assert(index < PIPE_MAX_SHADER_BUFFERS);
+         return stage_offset + PIPE_MAX_CONSTANT_BUFFERS + PIPE_MAX_SHADER_SAMPLER_VIEWS + index;
+
+      case VK_DESCRIPTOR_TYPE_STORAGE_IMAGE:
+      case VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER:
+         assert(index < PIPE_MAX_SHADER_IMAGES);
+         return stage_offset + PIPE_MAX_CONSTANT_BUFFERS + PIPE_MAX_SHADER_SAMPLER_VIEWS + PIPE_MAX_SHADER_IMAGES + index;
+
+      default:
+         unreachable("unexpected type");
+      }
+   }
+}
diff --git a/src/gallium/drivers/zink/zink_compiler.h b/src/gallium/drivers/zink/zink_compiler.h
index afda418ebe2..9f772ec11d2 100644
--- a/src/gallium/drivers/zink/zink_compiler.h
+++ b/src/gallium/drivers/zink/zink_compiler.h
@@ -31,6 +31,11 @@
 #include "compiler/shader_info.h"
 
 #include <vulkan/vulkan.h>
+#include "zink_descriptors.h"
+
+#define ZINK_WORKGROUP_SIZE_X 1
+#define ZINK_WORKGROUP_SIZE_Y 2
+#define ZINK_WORKGROUP_SIZE_Z 3
 
 struct pipe_screen;
 struct zink_context;
@@ -69,8 +74,11 @@ struct zink_shader {
       int binding;
       VkDescriptorType type;
       unsigned char size;
-   } bindings[PIPE_MAX_CONSTANT_BUFFERS + PIPE_MAX_SAMPLERS];
-   size_t num_bindings;
+   } bindings[ZINK_DESCRIPTOR_TYPES][32];
+   size_t num_bindings[ZINK_DESCRIPTOR_TYPES];
+   unsigned num_texel_buffers;
+   uint32_t ubos_used; // bitfield of which ubo indices are used
+   uint32_t ssbos_used; // bitfield of which ssbo indices are used
    struct set *programs;
 
    bool has_tess_shader; // vertex shaders need to know if a tesseval shader exists
@@ -92,10 +100,24 @@ struct zink_shader *
 zink_shader_create(struct zink_screen *screen, struct nir_shader *nir,
                  const struct pipe_stream_output_info *so_info);
 
+void
+zink_shader_finalize(struct pipe_screen *pscreen, void *nirptr, bool optimize);
+
 void
 zink_shader_free(struct zink_context *ctx, struct zink_shader *shader);
 
 struct zink_shader *
 zink_shader_tcs_create(struct zink_context *ctx, struct zink_shader *vs);
 
+static inline bool
+zink_shader_descriptor_is_buffer(struct zink_shader *zs, enum zink_descriptor_type type, unsigned i)
+{
+   return zs->bindings[type][i].type == VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER ||
+          zs->bindings[type][i].type == VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER;
+}
+
+uint32_t
+zink_binding(gl_shader_stage stage, VkDescriptorType type, int index);
+uint32_t
+zink_binding_lazy(gl_shader_stage stage, VkDescriptorType type, int index);
 #endif
diff --git a/src/gallium/drivers/zink/zink_context.c b/src/gallium/drivers/zink/zink_context.c
index 0c057b85451..708490a9c80 100644
--- a/src/gallium/drivers/zink/zink_context.c
+++ b/src/gallium/drivers/zink/zink_context.c
@@ -40,49 +40,109 @@
 #include "indices/u_primconvert.h"
 #include "util/u_blitter.h"
 #include "util/u_debug.h"
+#include "util/format_srgb.h"
 #include "util/format/u_format.h"
 #include "util/u_framebuffer.h"
 #include "util/u_helpers.h"
 #include "util/u_inlines.h"
-
+#include "util/u_sampler.h"
 #include "nir.h"
 
 #include "util/u_memory.h"
 #include "util/u_upload_mgr.h"
 
+#define XXH_INLINE_ALL
+#include "util/xxhash.h"
+
+static void
+incr_curr_batch(struct zink_context *ctx)
+{
+   ctx->curr_batch++;
+   if (!ctx->curr_batch)
+      ctx->curr_batch = 1;
+}
+
+void
+debug_describe_zink_sampler(char *buf, const struct zink_sampler *ptr)
+{
+   sprintf(buf, "zink_sampler");
+}
+
+void
+debug_describe_zink_buffer_view(char *buf, const struct zink_buffer_view *ptr)
+{
+   sprintf(buf, "zink_buffer_view");
+}
+
 static void
 zink_context_destroy(struct pipe_context *pctx)
 {
    struct zink_context *ctx = zink_context(pctx);
    struct zink_screen *screen = zink_screen(pctx->screen);
 
-   if (vkQueueWaitIdle(ctx->queue) != VK_SUCCESS)
+   if (ctx->batch.queue && vkQueueWaitIdle(ctx->batch.queue) != VK_SUCCESS)
       debug_printf("vkQueueWaitIdle failed\n");
 
    util_blitter_destroy(ctx->blitter);
 
-   pipe_resource_reference(&ctx->dummy_buffer, NULL);
+   pipe_resource_reference(&ctx->dummy_vertex_buffer, NULL);
+   pipe_resource_reference(&ctx->dummy_xfb_buffer, NULL);
+   if (ctx->batch.sem)
+      vkDestroySemaphore(screen->dev, ctx->batch.sem, NULL);
+
+   if (ctx->tc)
+      util_queue_destroy(&ctx->batch.flush_queue);
+
    for (unsigned i = 0; i < ARRAY_SIZE(ctx->null_buffers); i++)
       pipe_resource_reference(&ctx->null_buffers[i], NULL);
 
-   for (int i = 0; i < ARRAY_SIZE(ctx->batches); ++i) {
-      zink_batch_release(screen, &ctx->batches[i]);
-      util_dynarray_fini(&ctx->batches[i].zombie_samplers);
-      vkDestroyDescriptorPool(screen->dev, ctx->batches[i].descpool, NULL);
-      vkFreeCommandBuffers(screen->dev, ctx->cmdpool, 1, &ctx->batches[i].cmdbuf);
+   pipe_resource_reference(&ctx->dummy_vertex_buffer, NULL);
+   pipe_resource_reference(&ctx->dummy_xfb_buffer, NULL);
+
+   simple_mtx_destroy(&ctx->batch_mtx);
+   zink_clear_batch_state(ctx, ctx->batch.state);
+   zink_fence_clear_resources(screen, &ctx->batch.state->fence);
+   zink_batch_state_reference(zink_screen(pctx->screen), &ctx->batch.state, NULL);
+   hash_table_foreach(&ctx->batch_states, entry) {
+      struct zink_batch_state *bs = entry->data;
+      zink_clear_batch_state(ctx, bs);
+      zink_fence_clear_resources(screen, &bs->fence);
+      zink_batch_state_reference(zink_screen(pctx->screen), &bs, NULL);
+   }
+   util_dynarray_foreach(&ctx->free_batch_states, struct zink_batch_state*, bs) {
+      zink_clear_batch_state(ctx, *bs);
+      zink_batch_state_reference(zink_screen(pctx->screen), bs, NULL);
+   }
+
+   hash_table_foreach(&ctx->surface_cache, entry) {
+      struct pipe_surface* sf = (struct pipe_surface*)entry->data;
+      pipe_resource_reference(&sf->texture, NULL);
+      pipe_surface_reference(&sf, NULL);
+   }
 
-      _mesa_set_destroy(ctx->batches[i].resources, NULL);
-      _mesa_set_destroy(ctx->batches[i].sampler_views, NULL);
-      _mesa_set_destroy(ctx->batches[i].programs, NULL);
+   hash_table_foreach(&ctx->framebuffer_cache, entry) {
+      struct zink_framebuffer* fb = (struct zink_framebuffer*)entry->data;
+      zink_framebuffer_reference(zink_screen(pctx->screen), &fb, NULL);
+      _mesa_hash_table_remove(&ctx->framebuffer_cache, entry);
    }
-   vkDestroyCommandPool(screen->dev, ctx->cmdpool, NULL);
 
    util_primconvert_destroy(ctx->primconvert);
    u_upload_destroy(pctx->stream_uploader);
+   u_upload_destroy(pctx->const_uploader);
    slab_destroy_child(&ctx->transfer_pool);
    _mesa_hash_table_destroy(ctx->program_cache, NULL);
    _mesa_hash_table_destroy(ctx->render_pass_cache, NULL);
-   FREE(ctx);
+   slab_destroy_child(&ctx->transfer_pool_unsync);
+
+   screen->descriptors_deinit(ctx);
+
+   zink_descriptor_layouts_deinit(ctx);
+
+   simple_mtx_destroy(&ctx->framebuffer_mtx);
+   simple_mtx_destroy(&ctx->surface_mtx);
+   simple_mtx_destroy(&ctx->bufferview_mtx);
+
+   ralloc_free(ctx);
 }
 
 static enum pipe_reset_status
@@ -131,17 +191,21 @@ sampler_mipmap_mode(enum pipe_tex_mipfilter filter)
 }
 
 static VkSamplerAddressMode
-sampler_address_mode(enum pipe_tex_wrap filter)
+sampler_address_mode(enum pipe_tex_wrap filter, bool clamp_to_border)
 {
    switch (filter) {
    case PIPE_TEX_WRAP_REPEAT: return VK_SAMPLER_ADDRESS_MODE_REPEAT;
-   case PIPE_TEX_WRAP_CLAMP: return VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE; /* not technically correct, but kinda works */
+   case PIPE_TEX_WRAP_CLAMP: return clamp_to_border ?
+                                    VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_BORDER :
+                                    VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE;
    case PIPE_TEX_WRAP_CLAMP_TO_EDGE: return VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_EDGE;
    case PIPE_TEX_WRAP_CLAMP_TO_BORDER: return VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_BORDER;
    case PIPE_TEX_WRAP_MIRROR_REPEAT: return VK_SAMPLER_ADDRESS_MODE_MIRRORED_REPEAT;
-   case PIPE_TEX_WRAP_MIRROR_CLAMP: return VK_SAMPLER_ADDRESS_MODE_MIRROR_CLAMP_TO_EDGE; /* not technically correct, but kinda works */
+   case PIPE_TEX_WRAP_MIRROR_CLAMP: return clamp_to_border ?
+                                    VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_BORDER :
+                                    VK_SAMPLER_ADDRESS_MODE_MIRROR_CLAMP_TO_EDGE;
    case PIPE_TEX_WRAP_MIRROR_CLAMP_TO_EDGE: return VK_SAMPLER_ADDRESS_MODE_MIRROR_CLAMP_TO_EDGE;
-   case PIPE_TEX_WRAP_MIRROR_CLAMP_TO_BORDER: return VK_SAMPLER_ADDRESS_MODE_MIRROR_CLAMP_TO_EDGE; /* not technically correct, but kinda works */
+   case PIPE_TEX_WRAP_MIRROR_CLAMP_TO_BORDER: return VK_SAMPLER_ADDRESS_MODE_CLAMP_TO_BORDER; /* not correct at all */
    }
    unreachable("unexpected wrap");
 }
@@ -163,32 +227,70 @@ compare_op(enum pipe_compare_func op)
 }
 
 static inline bool
-wrap_needs_border_color(unsigned wrap)
+wrap_needs_border_color(unsigned wrap, bool clamp_to_border)
 {
-   return wrap == PIPE_TEX_WRAP_CLAMP || wrap == PIPE_TEX_WRAP_CLAMP_TO_BORDER ||
-          wrap == PIPE_TEX_WRAP_MIRROR_CLAMP || wrap == PIPE_TEX_WRAP_MIRROR_CLAMP_TO_BORDER;
+   return (clamp_to_border && (wrap == PIPE_TEX_WRAP_CLAMP || wrap == PIPE_TEX_WRAP_MIRROR_CLAMP)) ||
+          wrap == PIPE_TEX_WRAP_CLAMP_TO_BORDER || wrap == PIPE_TEX_WRAP_MIRROR_CLAMP_TO_BORDER;
 }
 
-struct zink_sampler_state {
-   VkSampler sampler;
-   bool custom_border_color;
-};
+static VkBorderColor
+get_border_color(union pipe_color_union *color, bool is_integer)
+{
+   if (is_integer) {
+      if (color->ui[0] == 0 && color->ui[1] == 0 && color->ui[2] == 0 && color->ui[3] == 0)
+         return VK_BORDER_COLOR_INT_TRANSPARENT_BLACK;
+      if (color->ui[0] == 0 && color->ui[1] == 0 && color->ui[2] == 0 && color->ui[3] == 1)
+         return VK_BORDER_COLOR_INT_OPAQUE_BLACK;
+      if (color->ui[0] == 1 && color->ui[1] == 1 && color->ui[2] == 1 && color->ui[3] == 1)
+         return VK_BORDER_COLOR_INT_OPAQUE_WHITE;
+      return VK_BORDER_COLOR_INT_CUSTOM_EXT;
+   }
 
-static void *
-zink_create_sampler_state(struct pipe_context *pctx,
-                          const struct pipe_sampler_state *state)
+   if (color->f[0] == 0 && color->f[1] == 0 && color->f[2] == 0 && color->f[3] == 0)
+      return VK_BORDER_COLOR_FLOAT_TRANSPARENT_BLACK;
+   if (color->f[0] == 0 && color->f[1] == 0 && color->f[2] == 0 && color->f[3] == 1)
+      return VK_BORDER_COLOR_FLOAT_OPAQUE_BLACK;
+   if (color->f[0] == 1 && color->f[1] == 1 && color->f[2] == 1 && color->f[3] == 1)
+      return VK_BORDER_COLOR_FLOAT_OPAQUE_WHITE;
+   return VK_BORDER_COLOR_FLOAT_CUSTOM_EXT;
+}
+
+static VkSampler
+create_sampler(struct pipe_context *pctx, const struct pipe_sampler_state *state,
+               const struct zink_sampler_key *key, bool *custom_border_color)
 {
    struct zink_screen *screen = zink_screen(pctx->screen);
-   bool need_custom = false;
+   const struct zink_sampler_state *sampler_state = (struct zink_sampler_state*)state;
+   VkSampler sampler = VK_NULL_HANDLE;
+
+   bool can_linear = key->can_linear || key->has_depth;
 
    VkSamplerCreateInfo sci = {};
    VkSamplerCustomBorderColorCreateInfoEXT cbci = {};
    sci.sType = VK_STRUCTURE_TYPE_SAMPLER_CREATE_INFO;
-   sci.magFilter = zink_filter(state->mag_img_filter);
-   sci.minFilter = zink_filter(state->min_img_filter);
 
-   if (state->min_mip_filter != PIPE_TEX_MIPFILTER_NONE) {
-      sci.mipmapMode = sampler_mipmap_mode(state->min_mip_filter);
+   bool clamp_to_border = can_linear && state->min_img_filter != PIPE_TEX_FILTER_NEAREST;
+   sci.addressModeU = sampler_address_mode(state->wrap_s, clamp_to_border);
+   sci.addressModeV = sampler_address_mode(state->wrap_t, clamp_to_border);
+   sci.addressModeW = sampler_address_mode(state->wrap_r, clamp_to_border);
+
+   if (key->has_depth &&
+       !key->can_linear &&
+       state->wrap_s != PIPE_TEX_WRAP_CLAMP &&
+       state->wrap_s != PIPE_TEX_WRAP_MIRROR_CLAMP &&
+       state->wrap_t != PIPE_TEX_WRAP_CLAMP &&
+       state->wrap_t != PIPE_TEX_WRAP_MIRROR_CLAMP &&
+       state->wrap_r != PIPE_TEX_WRAP_CLAMP &&
+       state->wrap_r != PIPE_TEX_WRAP_MIRROR_CLAMP) {
+      sci.magFilter = sci.minFilter = VK_FILTER_NEAREST;
+   } else {
+      sci.magFilter = zink_filter(can_linear ? state->mag_img_filter : PIPE_TEX_FILTER_NEAREST);
+      sci.minFilter = zink_filter(can_linear ? state->min_img_filter : PIPE_TEX_FILTER_NEAREST);
+   }
+
+   unsigned min_mip_filter = can_linear ? state->min_mip_filter : PIPE_TEX_MIPFILTER_NEAREST;
+   if (min_mip_filter != PIPE_TEX_MIPFILTER_NONE) {
+      sci.mipmapMode = sampler_mipmap_mode(min_mip_filter);
       sci.minLod = state->min_lod;
       sci.maxLod = state->max_lod;
    } else {
@@ -196,52 +298,358 @@ zink_create_sampler_state(struct pipe_context *pctx,
       sci.minLod = 0;
       sci.maxLod = 0;
    }
-
-   sci.addressModeU = sampler_address_mode(state->wrap_s);
-   sci.addressModeV = sampler_address_mode(state->wrap_t);
-   sci.addressModeW = sampler_address_mode(state->wrap_r);
    sci.mipLodBias = state->lod_bias;
 
-   need_custom |= wrap_needs_border_color(state->wrap_s);
-   need_custom |= wrap_needs_border_color(state->wrap_t);
-   need_custom |= wrap_needs_border_color(state->wrap_r);
-
-   if (state->compare_mode == PIPE_TEX_COMPARE_NONE)
-      sci.compareOp = VK_COMPARE_OP_NEVER;
-   else {
+   if (state->compare_mode == PIPE_TEX_COMPARE_NONE) {
+      if (key->has_depth && !key->can_linear &&
+          (sci.magFilter == VK_FILTER_LINEAR ||
+           sci.minFilter == VK_FILTER_LINEAR)) {
+         /* If the format being queried is a depth/stencil format,
+          * this bit only specifies that the depth aspect (not the stencil aspect)
+          * of an image of this format supports linear filtering,
+          * and that linear filtering of the depth aspect is supported whether
+          * depth compare is enabled in the sampler or not.
+          * If this bit is not present, linear filtering with depth compare disabled
+          * is unsupported and linear filtering with depth compare enabled is supported
+          */
+         sci.compareOp = VK_COMPARE_OP_ALWAYS;
+         sci.compareEnable = VK_TRUE;
+      } else
+         sci.compareOp = VK_COMPARE_OP_NEVER;
+   } else {
       sci.compareOp = compare_op(state->compare_func);
       sci.compareEnable = VK_TRUE;
    }
 
-   if (screen->info.have_EXT_custom_border_color &&
-       screen->info.border_color_feats.customBorderColorWithoutFormat && need_custom) {
-      cbci.sType = VK_STRUCTURE_TYPE_SAMPLER_CUSTOM_BORDER_COLOR_CREATE_INFO_EXT;
-      cbci.format = VK_FORMAT_UNDEFINED;
+   if (sampler_state->custom_border_color) {
       /* these are identical unions */
       memcpy(&cbci.customBorderColor, &state->border_color, sizeof(union pipe_color_union));
-      sci.pNext = &cbci;
-      sci.borderColor = VK_BORDER_COLOR_INT_CUSTOM_EXT;
-      UNUSED uint32_t check = p_atomic_inc_return(&screen->cur_custom_border_color_samplers);
-      assert(check <= screen->info.border_color_props.maxCustomBorderColorSamplers);
-   } else
-      sci.borderColor = VK_BORDER_COLOR_FLOAT_TRANSPARENT_BLACK; // TODO with custom shader if we're super interested?
+      if (key->argb)
+         u_sampler_format_swizzle_color_argb((union pipe_color_union*)&cbci.customBorderColor, key->is_integer);
+      else if (key->abgr)
+         u_sampler_format_swizzle_color_abgr((union pipe_color_union*)&cbci.customBorderColor, key->is_integer);
+   }
+
+   sci.borderColor = get_border_color((union pipe_color_union*)&cbci.customBorderColor, key->is_integer);
+   if (sci.borderColor > VK_BORDER_COLOR_INT_OPAQUE_WHITE &&
+       sampler_state->custom_border_color) {
+      if (screen->info.have_EXT_custom_border_color) {
+         cbci.sType = VK_STRUCTURE_TYPE_SAMPLER_CUSTOM_BORDER_COLOR_CREATE_INFO_EXT;
+         cbci.format = VK_FORMAT_UNDEFINED;
+         sci.pNext = &cbci;
+         UNUSED uint32_t check = p_atomic_inc_return(&screen->cur_custom_border_color_samplers);
+         assert(check <= screen->info.border_color_props.maxCustomBorderColorSamplers);
+         *custom_border_color = true;
+      } else
+         sci.borderColor = VK_BORDER_COLOR_FLOAT_TRANSPARENT_BLACK; // TODO with custom shader if we're super interested?
+   }
+
    sci.unnormalizedCoordinates = !state->normalized_coords;
 
    if (state->max_anisotropy > 1) {
       sci.maxAnisotropy = state->max_anisotropy;
       sci.anisotropyEnable = VK_TRUE;
    }
+   vkCreateSampler(screen->dev, &sci, NULL, &sampler);
+   return sampler;
+}
+
+static uint32_t
+hash_sampler(const void *key)
+{
+   const struct zink_sampler_key *k = key;
+   if (k->argb)
+      return _mesa_hash_data(key, offsetof(struct zink_sampler_key, argb));
+   /* skip swizzle if it isn't relevant */
+   return _mesa_hash_data(key, offsetof(struct zink_sampler_key, swizzle));
+}
+
+static bool
+equals_sampler(const void *a, const void *b)
+{
+   const struct zink_sampler_key *a_k = a, *b_k = b;
+   if (a == b)
+      return true;
+   if (a_k->argb != b_k->argb || a_k->abgr != b_k->abgr)
+      return false;
+   size_t size = a_k->argb || a_k->abgr ? offsetof(struct zink_sampler_key, argb) : offsetof(struct zink_sampler_key, swizzle);
+   return a == b || !memcmp(a, b, size);
+}
 
-   struct zink_sampler_state *sampler = CALLOC_STRUCT(zink_sampler_state);
+static struct zink_sampler *
+get_sampler(struct zink_context *ctx, struct zink_sampler_state *state, const struct zink_sampler_key *key)
+{
+   if (state->last_sampler && equals_sampler(&state->last_sampler->key, key))
+      return state->last_sampler;
+   uint32_t hash = hash_sampler(key);
+   struct hash_entry *he = _mesa_hash_table_search_pre_hashed(&state->samplers, hash, key);
+   if (he)
+      return (struct zink_sampler*)he->data;
+
+   struct zink_sampler *sampler = rzalloc(NULL, struct zink_sampler);
    if (!sampler)
       return NULL;
+   pipe_reference_init(&sampler->reference, 1);
+   sampler->sampler = create_sampler(&ctx->base, &state->base, key, &sampler->custom_border_color);
+   if (!sampler->sampler) {
+       ralloc_free(sampler);
+       return NULL;
+   }
+   util_dynarray_init(&sampler->desc_set_refs.refs, sampler);
+   sampler->key = *key;
+   _mesa_hash_table_insert_pre_hashed(&state->samplers, hash, &sampler->key, sampler);
+   return sampler;
+}
+
+static void
+zink_delete_sampler_state(struct pipe_context *pctx,
+                          void *psampler_state)
+{
+   struct zink_sampler_state *sampler_state = psampler_state;
+   hash_table_foreach(&sampler_state->samplers, he) {
+      struct zink_sampler *sampler = (struct zink_sampler *)he->data;
+      zink_sampler_reference(zink_context(pctx), &sampler, NULL);
+      _mesa_hash_table_remove(&sampler_state->samplers, he);
+   }
+   ralloc_free(sampler_state);
+}
 
-   if (vkCreateSampler(screen->dev, &sci, NULL, &sampler->sampler) != VK_SUCCESS) {
-      FREE(sampler);
+static void *
+zink_create_sampler_state(struct pipe_context *pctx,
+                          const struct pipe_sampler_state *state)
+{
+   struct zink_sampler_state *sampler_state = rzalloc(NULL, struct zink_sampler_state);
+   if (!sampler_state)
       return NULL;
+   memcpy(&sampler_state->base, state, sizeof(struct pipe_sampler_state));
+   _mesa_hash_table_init(&sampler_state->samplers, sampler_state, hash_sampler, equals_sampler);
+
+   bool clamp_to_border = state->min_img_filter != PIPE_TEX_FILTER_NEAREST;
+   sampler_state->custom_border_color |= wrap_needs_border_color(state->wrap_s, clamp_to_border);
+   sampler_state->custom_border_color |= wrap_needs_border_color(state->wrap_t, clamp_to_border);
+   sampler_state->custom_border_color |= wrap_needs_border_color(state->wrap_r, clamp_to_border);
+
+   return sampler_state;
+}
+
+static VkImageLayout
+get_layout_for_binding(struct zink_resource *res, enum zink_descriptor_type type)
+{
+   if (res->obj->is_buffer)
+      return 0;
+   switch (type) {
+   case ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW:
+      return res->bind_history & BITFIELD64_BIT(ZINK_DESCRIPTOR_TYPE_IMAGE) ?
+             VK_IMAGE_LAYOUT_GENERAL : VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL;
+   case ZINK_DESCRIPTOR_TYPE_IMAGE:
+      return VK_IMAGE_LAYOUT_GENERAL;
+   default:
+      break;
    }
+   return 0;
+}
 
-   return sampler;
+static struct zink_surface *
+get_imageview_for_binding(struct zink_context *ctx, enum pipe_shader_type stage, enum zink_descriptor_type type, unsigned idx)
+{
+   switch (type) {
+   case ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW: {
+      struct zink_sampler_view *sampler_view = zink_sampler_view(ctx->sampler_views[stage][idx]);
+      return sampler_view->base.texture ? sampler_view->image_view : NULL;
+   }
+   case ZINK_DESCRIPTOR_TYPE_IMAGE: {
+      struct zink_image_view *image_view = &ctx->image_views[stage][idx];
+      return image_view->base.resource ? image_view->surface : NULL;
+   }
+   default:
+      break;
+   }
+   unreachable("ACK");
+   return VK_NULL_HANDLE;
+}
+
+static struct zink_buffer_view *
+get_bufferview_for_binding(struct zink_context *ctx, enum pipe_shader_type stage, enum zink_descriptor_type type, unsigned idx)
+{
+   switch (type) {
+   case ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW: {
+      struct zink_sampler_view *sampler_view = zink_sampler_view(ctx->sampler_views[stage][idx]);
+      return sampler_view->base.texture ? sampler_view->buffer_view : NULL;
+   }
+   case ZINK_DESCRIPTOR_TYPE_IMAGE: {
+      struct zink_image_view *image_view = &ctx->image_views[stage][idx];
+      return image_view->base.resource ? image_view->buffer_view : NULL;
+   }
+   default:
+      break;
+   }
+   unreachable("ACK");
+   return VK_NULL_HANDLE;
+}
+
+static void
+update_descriptor_state(struct zink_context *ctx, enum pipe_shader_type shader, enum zink_descriptor_type type, unsigned slot)
+{
+   if (!zink_screen(ctx->base.screen)->lazy_descriptors)
+      return;
+   bool have_null_descriptors = zink_screen(ctx->base.screen)->info.rb2_feats.nullDescriptor;
+   VkBuffer null_buffer = zink_resource(ctx->dummy_vertex_buffer)->obj->buffer;
+   struct zink_resource *res = zink_get_resource_for_descriptor(ctx, type, shader, slot);
+   ctx->di.descriptor_res[type][shader][slot] = res;
+   switch (type) {
+   case ZINK_DESCRIPTOR_TYPE_UBO:
+      ctx->di.ubos[shader][slot].offset = ctx->ubos[shader][slot].buffer_offset;
+      if (res) {
+         ctx->di.ubos[shader][slot].buffer = res->obj->buffer;
+         ctx->di.ubos[shader][slot].range = ctx->ubos[shader][slot].buffer_size;
+      } else {
+         ctx->di.ubos[shader][slot].buffer = have_null_descriptors ? VK_NULL_HANDLE : null_buffer;
+         ctx->di.ubos[shader][slot].range = VK_WHOLE_SIZE;
+      }
+      if (!slot) {
+         if (res)
+            ctx->di.push_valid |= BITFIELD64_BIT(shader);
+         else
+            ctx->di.push_valid &= ~BITFIELD64_BIT(shader);
+      }
+      break;
+   case ZINK_DESCRIPTOR_TYPE_SSBO:
+      ctx->di.ssbos[shader][slot].offset = ctx->ssbos[shader][slot].buffer_offset;
+      if (res) {
+         ctx->di.ssbos[shader][slot].buffer = res->obj->buffer;
+         ctx->di.ssbos[shader][slot].range = ctx->ssbos[shader][slot].buffer_size;
+      } else {
+         ctx->di.ssbos[shader][slot].buffer = have_null_descriptors ? VK_NULL_HANDLE : null_buffer;
+         ctx->di.ssbos[shader][slot].range = VK_WHOLE_SIZE;
+      }
+      break;
+   case ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW: {
+      struct zink_sampler *sampler = NULL;
+      /* ignore samplers with pending updates */
+      if (slot < PIPE_MAX_SAMPLERS)
+         sampler = ctx->samplers[shader][slot];
+      if (res) {
+         if (res->obj->is_buffer) {
+            struct zink_buffer_view *bv = get_bufferview_for_binding(ctx, shader, type, slot);
+            ctx->di.tbos[shader][slot] = bv->buffer_view;
+            ctx->di.sampler_surfaces[shader][slot].bufferview = bv;
+         } else {
+            struct zink_surface *surface = get_imageview_for_binding(ctx, shader, type, slot);
+            ctx->di.textures[shader][slot].imageLayout = get_layout_for_binding(res, type);
+            ctx->di.textures[shader][slot].imageView = surface->image_view;
+            ctx->di.sampler_surfaces[shader][slot].surface = surface;
+         }
+      } else {
+         ctx->di.textures[shader][slot].imageView = VK_NULL_HANDLE;
+         ctx->di.textures[shader][slot].imageLayout = VK_IMAGE_LAYOUT_UNDEFINED;
+         ctx->di.tbos[shader][slot] = VK_NULL_HANDLE;
+      }
+      if (sampler)
+         ctx->di.textures[shader][slot].sampler = sampler->sampler;
+      else if (!(ctx->dirty_samplers[shader] & BITFIELD64_BIT(slot)))
+         ctx->di.textures[shader][slot].sampler = VK_NULL_HANDLE;
+      break;
+   }
+   case ZINK_DESCRIPTOR_TYPE_IMAGE: {
+      if (res) {
+         if (res->obj->is_buffer) {
+            struct zink_buffer_view *bv = get_bufferview_for_binding(ctx, shader, type, slot);
+            ctx->di.texel_images[shader][slot] = bv->buffer_view;
+            ctx->di.image_surfaces[shader][slot].bufferview = bv;
+         } else {
+            struct zink_surface *surface = get_imageview_for_binding(ctx, shader, type, slot);
+            ctx->di.images[shader][slot].imageLayout = get_layout_for_binding(res, type);
+            ctx->di.images[shader][slot].imageView = surface->image_view;
+            ctx->di.image_surfaces[shader][slot].surface = surface;
+         }
+      } else {
+         memset(&ctx->di.images[shader][slot], 0, sizeof(ctx->di.images[shader][slot]));
+         ctx->di.texel_images[shader][slot] = VK_NULL_HANDLE;
+      }
+      break;
+   }
+   default:
+      break;
+   }
+}
+
+
+void
+zink_update_samplers(struct zink_context *ctx, bool is_compute)
+{
+   struct zink_screen *screen = zink_screen(ctx->base.screen);
+   enum pipe_shader_type stage = is_compute ? PIPE_SHADER_COMPUTE : PIPE_SHADER_VERTEX;
+   for (; stage < PIPE_SHADER_TYPES; stage++) {
+      if (!ctx->dirty_samplers[stage])
+         continue;
+      uint32_t sampler_gl_clamp[3];
+      for (int i = 0; i < 3; i++)
+         sampler_gl_clamp[i] = ctx->sampler_gl_clamp[stage][i];
+      while (ctx->dirty_samplers[stage]) {
+         unsigned slot = u_bit_scan(&ctx->dirty_samplers[stage]);
+         struct zink_sampler_state *sampler_state = ctx->sampler_states[stage][slot];
+         struct pipe_sampler_view *psampler_view = ctx->sampler_views[stage][slot];
+
+         ctx->sampler_gl_clamp[stage][0] &= ~BITFIELD64_BIT(slot);
+         ctx->sampler_gl_clamp[stage][1] &= ~BITFIELD64_BIT(slot);
+         ctx->sampler_gl_clamp[stage][2] &= ~BITFIELD64_BIT(slot);
+
+         if (!sampler_state)
+            continue;
+
+         enum pipe_format format = 0;
+         bool can_linear = true;
+         bool has_depth = false;
+         if (psampler_view) {
+            struct zink_surface *surf = zink_sampler_view(psampler_view)->image_view;
+            struct zink_resource *res = zink_resource(psampler_view->texture);
+            VkFormatProperties props = screen->format_props[surf->base.format];
+            format = surf->base.format;
+            if (res->aspect & VK_IMAGE_ASPECT_DEPTH_BIT)
+               has_depth = util_format_has_depth(util_format_description(format));
+            can_linear =  (res->optimal_tiling && props.optimalTilingFeatures & VK_FORMAT_FEATURE_SAMPLED_IMAGE_FILTER_LINEAR_BIT) ||
+                           (!res->optimal_tiling && props.linearTilingFeatures & VK_FORMAT_FEATURE_SAMPLED_IMAGE_FILTER_LINEAR_BIT);
+         }
+         struct zink_sampler_key key = {
+            .has_depth = has_depth,
+            .is_integer = util_format_is_pure_integer(format),
+            .swizzle = {
+               psampler_view ? psampler_view->swizzle_r : PIPE_SWIZZLE_X,
+               psampler_view ? psampler_view->swizzle_g : PIPE_SWIZZLE_Y,
+               psampler_view ? psampler_view->swizzle_b : PIPE_SWIZZLE_Z,
+               psampler_view ? psampler_view->swizzle_a : PIPE_SWIZZLE_W,
+            },
+            .can_linear = can_linear,
+            .argb = util_format_is_argb(format),
+            .abgr = util_format_is_abgr(format),
+         };
+         struct zink_sampler *sampler = ctx->samplers[stage][slot];
+         ctx->samplers[stage][slot] = sampler_state->last_sampler = get_sampler(ctx, sampler_state, &key);
+
+         if (sampler != ctx->samplers[stage][slot]) {
+            update_descriptor_state(ctx, stage, ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW, slot);
+            screen->context_invalidate_descriptor_state(ctx, stage, ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW, slot, 1);
+         }
+
+         bool clamp_to_border = sampler_state->base.min_img_filter != PIPE_TEX_FILTER_NEAREST;
+
+         if (sampler_state->custom_border_color && clamp_to_border) {
+            if (sampler_state->base.wrap_s == PIPE_TEX_WRAP_CLAMP ||
+                sampler_state->base.wrap_s == PIPE_TEX_WRAP_MIRROR_CLAMP)
+               ctx->sampler_gl_clamp[stage][0] |= BITFIELD64_BIT(slot);
+            if (sampler_state->base.wrap_r == PIPE_TEX_WRAP_CLAMP ||
+                sampler_state->base.wrap_r == PIPE_TEX_WRAP_MIRROR_CLAMP)
+               ctx->sampler_gl_clamp[stage][1] |= BITFIELD64_BIT(slot);
+            if (sampler_state->base.wrap_t == PIPE_TEX_WRAP_CLAMP ||
+                sampler_state->base.wrap_t == PIPE_TEX_WRAP_MIRROR_CLAMP)
+               ctx->sampler_gl_clamp[stage][2] |= BITFIELD64_BIT(slot);
+         }
+      }
+      for (int i = 0; i < 3; i++) {
+         if (sampler_gl_clamp[i] != ctx->sampler_gl_clamp[stage][i])
+            ctx->dirty_shader_stages |= BITFIELD64_BIT(stage);
+      }
+   }
 }
 
 static void
@@ -252,28 +660,37 @@ zink_bind_sampler_states(struct pipe_context *pctx,
                          void **samplers)
 {
    struct zink_context *ctx = zink_context(pctx);
+   uint32_t usage = zink_program_get_descriptor_usage(ctx, shader, ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW);
+   bool update = false;
    for (unsigned i = 0; i < num_samplers; ++i) {
-      VkSampler *sampler = samplers[i];
-      ctx->sampler_states[shader][start_slot + i] = sampler;
-      ctx->samplers[shader][start_slot + i] = sampler ? *sampler : VK_NULL_HANDLE;
+      struct zink_sampler_state *a = ctx->sampler_states[shader][start_slot + i];
+      struct zink_sampler_state *b = samplers[i];
+      if (usage & BITFIELD64_BIT(start_slot + i))
+         update |= a != b;
+      ctx->sampler_states[shader][start_slot + i] = samplers[i];
+      if (b)
+         ctx->dirty_samplers[shader] |= BITFIELD64_BIT(start_slot + i);
+      else {
+         ctx->dirty_samplers[shader] &= ~BITFIELD64_BIT(start_slot + i);
+         update_descriptor_state(ctx, shader, ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW, start_slot + i);
+      }
    }
    ctx->num_samplers[shader] = start_slot + num_samplers;
+   if (update)
+      zink_screen(pctx->screen)->context_invalidate_descriptor_state(ctx, shader, ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW, start_slot, num_samplers);
 }
 
-static void
-zink_delete_sampler_state(struct pipe_context *pctx,
-                          void *sampler_state)
+void
+zink_destroy_sampler(struct zink_context *ctx, struct zink_sampler *sampler)
 {
-   struct zink_sampler_state *sampler = sampler_state;
-   struct zink_batch *batch = zink_curr_batch(zink_context(pctx));
-   util_dynarray_append(&batch->zombie_samplers, VkSampler,
-                        sampler->sampler);
+   struct zink_screen *screen = zink_screen(ctx->base.screen);
+   zink_descriptor_set_refs_clear(&sampler->desc_set_refs, sampler);
    if (sampler->custom_border_color)
-      p_atomic_dec(&zink_screen(pctx->screen)->cur_custom_border_color_samplers);
-   FREE(sampler);
+      p_atomic_dec(&screen->cur_custom_border_color_samplers);
+   vkDestroySampler(screen->dev, sampler->sampler, NULL);
+   ralloc_free(sampler);
 }
 
-
 static VkImageViewType
 image_view_type(enum pipe_texture_target target)
 {
@@ -320,6 +737,53 @@ sampler_aspect_from_format(enum pipe_format fmt)
      return VK_IMAGE_ASPECT_COLOR_BIT;
 }
 
+static uint32_t
+hash_bufferview(void *bvci)
+{
+   size_t offset = offsetof(VkBufferViewCreateInfo, flags);
+   return _mesa_hash_data((char*)bvci + offset, sizeof(VkBufferViewCreateInfo) - offset);
+}
+
+static struct zink_buffer_view *
+get_buffer_view(struct zink_context *ctx, struct zink_resource *res, enum pipe_format format, uint32_t offset, uint32_t range)
+{
+   struct zink_screen *screen = zink_screen(ctx->base.screen);
+   struct zink_buffer_view *buffer_view;
+   VkBufferViewCreateInfo bvci = {};
+   bvci.sType = VK_STRUCTURE_TYPE_BUFFER_VIEW_CREATE_INFO;
+   bvci.buffer = res->obj->buffer;
+   bvci.format = zink_get_format(screen, format);
+   assert(bvci.format);
+   bvci.offset = offset;
+   bvci.range = range;
+
+   uint32_t hash = hash_bufferview(&bvci);
+   simple_mtx_lock(&ctx->bufferview_mtx);
+   struct hash_entry *he = _mesa_hash_table_search_pre_hashed(&ctx->bufferview_cache, hash, &bvci);
+   simple_mtx_unlock(&ctx->bufferview_mtx);
+   if (he) {
+      buffer_view = he->data;
+      p_atomic_inc(&buffer_view->reference.count);
+   } else {
+      VkBufferView view;
+      if (vkCreateBufferView(screen->dev, &bvci, NULL, &view) != VK_SUCCESS)
+         return NULL;
+      buffer_view = CALLOC_STRUCT(zink_buffer_view);
+      if (!buffer_view) {
+         vkDestroyBufferView(screen->dev, view, NULL);
+         return NULL;
+      }
+      pipe_reference_init(&buffer_view->reference, 1);
+      buffer_view->bvci = bvci;
+      buffer_view->buffer_view = view;
+      buffer_view->hash = hash;
+      simple_mtx_lock(&ctx->bufferview_mtx);
+      _mesa_hash_table_insert_pre_hashed(&ctx->bufferview_cache, hash, &buffer_view->bvci, buffer_view);
+      simple_mtx_unlock(&ctx->bufferview_mtx);
+   }
+   return buffer_view;
+}
+
 static struct pipe_sampler_view *
 zink_create_sampler_view(struct pipe_context *pctx, struct pipe_resource *pres,
                          const struct pipe_sampler_view *state)
@@ -327,7 +791,7 @@ zink_create_sampler_view(struct pipe_context *pctx, struct pipe_resource *pres,
    struct zink_screen *screen = zink_screen(pctx->screen);
    struct zink_resource *res = zink_resource(pres);
    struct zink_sampler_view *sampler_view = CALLOC_STRUCT(zink_sampler_view);
-   VkResult err;
+   bool err;
 
    sampler_view->base = *state;
    sampler_view->base.texture = NULL;
@@ -338,55 +802,103 @@ zink_create_sampler_view(struct pipe_context *pctx, struct pipe_resource *pres,
    if (state->target != PIPE_BUFFER) {
       VkImageViewCreateInfo ivci = {};
       ivci.sType = VK_STRUCTURE_TYPE_IMAGE_VIEW_CREATE_INFO;
-      ivci.image = res->image;
+      ivci.image = res->obj->image;
       ivci.viewType = image_view_type(state->target);
 
-      ivci.components.r = component_mapping(state->swizzle_r);
-      ivci.components.g = component_mapping(state->swizzle_g);
-      ivci.components.b = component_mapping(state->swizzle_b);
-      ivci.components.a = component_mapping(state->swizzle_a);
+      ivci.components.r = component_mapping(sampler_view->base.swizzle_r);
+      ivci.components.g = component_mapping(sampler_view->base.swizzle_g);
+      ivci.components.b = component_mapping(sampler_view->base.swizzle_b);
+      ivci.components.a = component_mapping(sampler_view->base.swizzle_a);
+
       ivci.subresourceRange.aspectMask = sampler_aspect_from_format(state->format);
+      enum pipe_format format = state->format;
       /* samplers for stencil aspects of packed formats need to always use stencil type */
       if (ivci.subresourceRange.aspectMask == VK_IMAGE_ASPECT_STENCIL_BIT) {
          ivci.format = VK_FORMAT_S8_UINT;
+         format = PIPE_FORMAT_S8_UINT;
          ivci.components.g = VK_COMPONENT_SWIZZLE_R;
-      } else
+      } else if (ivci.subresourceRange.aspectMask == VK_IMAGE_ASPECT_DEPTH_BIT) {
+         format = util_format_get_depth_only(format);
+         ivci.format = zink_get_format(screen, format);
+      } else {
          ivci.format = zink_get_format(screen, state->format);
+         /* if we have e.g., R8G8B8X8, then we have to ignore alpha since we're just emulating
+          * these formats
+          */
+         const struct util_format_description *desc = util_format_description(state->format);
+         if (ivci.subresourceRange.aspectMask == VK_IMAGE_ASPECT_COLOR_BIT) {
+             if (util_format_is_rgba8_variant(desc)) {
+                if (desc->channel[0].type == UTIL_FORMAT_TYPE_VOID)
+                   sampler_view->base.swizzle_r = PIPE_SWIZZLE_1;
+                if (desc->channel[1].type == UTIL_FORMAT_TYPE_VOID)
+                   sampler_view->base.swizzle_g = PIPE_SWIZZLE_1;
+                if (desc->channel[2].type == UTIL_FORMAT_TYPE_VOID)
+                   sampler_view->base.swizzle_b = PIPE_SWIZZLE_1;
+                if (desc->channel[3].type == UTIL_FORMAT_TYPE_VOID)
+                   sampler_view->base.swizzle_a = PIPE_SWIZZLE_1;
+                ivci.components.r = component_mapping(sampler_view->base.swizzle_r);
+                ivci.components.g = component_mapping(sampler_view->base.swizzle_g);
+                ivci.components.b = component_mapping(sampler_view->base.swizzle_b);
+                ivci.components.a = component_mapping(sampler_view->base.swizzle_a);
+             }
+         }
+      }
       assert(ivci.format);
 
       ivci.subresourceRange.baseMipLevel = state->u.tex.first_level;
+      ivci.subresourceRange.levelCount = 1;
       ivci.subresourceRange.baseArrayLayer = state->u.tex.first_layer;
       ivci.subresourceRange.levelCount = state->u.tex.last_level - state->u.tex.first_level + 1;
       ivci.subresourceRange.layerCount = state->u.tex.last_layer - state->u.tex.first_layer + 1;
+      if (pres->target == PIPE_TEXTURE_CUBE ||
+          pres->target == PIPE_TEXTURE_CUBE_ARRAY) {
+         if (ivci.subresourceRange.layerCount != 6)
+            ivci.subresourceRange.layerCount = VK_REMAINING_ARRAY_LAYERS;
+      }
 
-      err = vkCreateImageView(screen->dev, &ivci, NULL, &sampler_view->image_view);
+      struct pipe_surface templ = {};
+      templ.u.tex.level = state->u.tex.first_level;
+      templ.format = format;
+      templ.u.tex.first_layer = state->u.tex.first_layer;
+      templ.u.tex.last_layer = state->u.tex.last_layer;
+      sampler_view->image_view = (struct zink_surface*)zink_get_surface(zink_context(pctx), pres, &templ, &ivci);
+      err = !sampler_view->image_view;
    } else {
-      VkBufferViewCreateInfo bvci = {};
-      bvci.sType = VK_STRUCTURE_TYPE_BUFFER_VIEW_CREATE_INFO;
-      bvci.buffer = res->buffer;
-      bvci.format = zink_get_format(screen, state->format);
-      assert(bvci.format);
-      bvci.offset = state->u.buf.offset;
-      bvci.range = state->u.buf.size;
-
-      err = vkCreateBufferView(screen->dev, &bvci, NULL, &sampler_view->buffer_view);
+      sampler_view->buffer_view = get_buffer_view(zink_context(pctx), res, state->format, state->u.buf.offset, state->u.buf.size);
+      err = !sampler_view->buffer_view;
    }
-   if (err != VK_SUCCESS) {
+   if (err) {
       FREE(sampler_view);
       return NULL;
    }
+   util_dynarray_init(&sampler_view->desc_set_refs.refs, NULL);
    return &sampler_view->base;
 }
 
+void
+zink_destroy_buffer_view(struct zink_context *ctx, struct zink_buffer_view *buffer_view)
+{
+   simple_mtx_lock(&ctx->bufferview_mtx);
+   struct hash_entry *he = _mesa_hash_table_search_pre_hashed(&ctx->bufferview_cache, buffer_view->hash, &buffer_view->bvci);
+   assert(he);
+   _mesa_hash_table_remove(&ctx->bufferview_cache, he);
+   simple_mtx_unlock(&ctx->bufferview_mtx);
+   vkDestroyBufferView(zink_screen(ctx->base.screen)->dev, buffer_view->buffer_view, NULL);
+   FREE(buffer_view);
+}
+
 static void
 zink_sampler_view_destroy(struct pipe_context *pctx,
                           struct pipe_sampler_view *pview)
 {
    struct zink_sampler_view *view = zink_sampler_view(pview);
+   zink_descriptor_set_refs_clear(&view->desc_set_refs, view);
    if (pview->texture->target == PIPE_BUFFER)
-      vkDestroyBufferView(zink_screen(pctx->screen)->dev, view->buffer_view, NULL);
-   else
-      vkDestroyImageView(zink_screen(pctx->screen)->dev, view->image_view, NULL);
+      zink_buffer_view_reference(zink_context(pctx), &view->buffer_view, NULL);
+   else {
+      struct pipe_surface *psurf = &view->image_view->base;
+      pipe_surface_reference(&psurf, NULL);
+   }
    pipe_resource_reference(&pview->texture, NULL);
    FREE(view);
 }
@@ -477,21 +989,16 @@ zink_set_vertex_buffers(struct pipe_context *pctx,
 {
    struct zink_context *ctx = zink_context(pctx);
 
-   if (buffers) {
+   if (buffers && !zink_screen(pctx->screen)->info.have_EXT_extended_dynamic_state) {
       for (int i = 0; i < num_buffers; ++i) {
          const struct pipe_vertex_buffer *vb = buffers + i;
-         struct zink_resource *res = zink_resource(vb->buffer.resource);
-         if (res && res->needs_xfb_barrier) {
-            /* if we're binding a previously-used xfb buffer, we need cmd buffer synchronization to ensure
-             * that we use the right buffer data
-             */
-            pctx->flush(pctx, NULL, 0);
-            res->needs_xfb_barrier = false;
-         }
+         if (ctx->gfx_pipeline_state.bindings[start_slot + i].stride != vb->stride)
+            ctx->gfx_pipeline_state.dirty = true;
+         ctx->gfx_pipeline_state.bindings[start_slot + i].stride = vb->stride;
       }
    }
 
-   util_set_vertex_buffers_mask(ctx->buffers, &ctx->buffers_enabled_mask,
+   util_set_vertex_buffers_mask(ctx->vertex_buffers, &ctx->gfx_pipeline_state.vertex_buffers_enabled_mask,
                                 buffers, start_slot, num_buffers);
 }
 
@@ -503,21 +1010,15 @@ zink_set_viewport_states(struct pipe_context *pctx,
 {
    struct zink_context *ctx = zink_context(pctx);
 
-   for (unsigned i = 0; i < num_viewports; ++i) {
-      VkViewport viewport = {
-         state[i].translate[0] - state[i].scale[0],
-         state[i].translate[1] - state[i].scale[1],
-         state[i].scale[0] * 2,
-         state[i].scale[1] * 2,
-         state[i].translate[2] - state[i].scale[2],
-         state[i].translate[2] + state[i].scale[2]
-      };
-      ctx->viewport_states[start_slot + i] = state[i];
-      ctx->viewports[start_slot + i] = viewport;
-   }
-   if (ctx->gfx_pipeline_state.num_viewports != start_slot + num_viewports)
-      ctx->gfx_pipeline_state.dirty = true;
-   ctx->gfx_pipeline_state.num_viewports = start_slot + num_viewports;
+   for (unsigned i = 0; i < num_viewports; ++i)
+      ctx->vp_state.viewport_states[start_slot + i] = state[i];
+   ctx->vp_state.num_viewports = start_slot + num_viewports;
+
+   if (!zink_screen(pctx->screen)->info.have_EXT_extended_dynamic_state) {
+      if (ctx->gfx_pipeline_state.num_viewports != ctx->vp_state.num_viewports)
+         ctx->gfx_pipeline_state.dirty = true;
+      ctx->gfx_pipeline_state.num_viewports = ctx->vp_state.num_viewports;
+   }
 }
 
 static void
@@ -527,16 +1028,20 @@ zink_set_scissor_states(struct pipe_context *pctx,
 {
    struct zink_context *ctx = zink_context(pctx);
 
-   for (unsigned i = 0; i < num_scissors; i++) {
-      VkRect2D scissor;
+   for (unsigned i = 0; i < num_scissors; i++)
+      ctx->vp_state.scissor_states[start_slot + i] = states[i];
+}
+
+static void
+zink_set_inlinable_constants(struct pipe_context *pctx,
+                             enum pipe_shader_type shader,
+                             uint num_values, uint32_t *values)
+{
+   struct zink_context *ctx = (struct zink_context *)pctx;
 
-      scissor.offset.x = states[i].minx;
-      scissor.offset.y = states[i].miny;
-      scissor.extent.width = states[i].maxx - states[i].minx;
-      scissor.extent.height = states[i].maxy - states[i].miny;
-      ctx->scissor_states[start_slot + i] = states[i];
-      ctx->scissors[start_slot + i] = scissor;
-   }
+   memcpy(ctx->inlinable_uniforms[shader], values, num_values * 4);
+   ctx->inlinable_uniforms_dirty_mask |= 1 << shader;
+   ctx->inlinable_uniforms_valid_mask |= 1 << shader;
 }
 
 static void
@@ -545,7 +1050,12 @@ zink_set_constant_buffer(struct pipe_context *pctx,
                          const struct pipe_constant_buffer *cb)
 {
    struct zink_context *ctx = zink_context(pctx);
+   uint32_t usage = zink_program_get_descriptor_usage(ctx, shader, ZINK_DESCRIPTOR_TYPE_UBO);
+   bool update = false;
 
+   struct zink_resource *res = zink_resource(ctx->ubos[shader][index].buffer);
+   if (res)
+      res->bind_count[shader == PIPE_SHADER_COMPUTE]--;
    if (cb) {
       struct pipe_resource *buffer = cb->buffer;
       unsigned offset = cb->buffer_offset;
@@ -555,6 +1065,15 @@ zink_set_constant_buffer(struct pipe_context *pctx,
                        screen->info.props.limits.minUniformBufferOffsetAlignment,
                        cb->user_buffer, &offset, &buffer);
       }
+      struct zink_resource *new_res = zink_resource(buffer);
+      if (new_res) {
+         new_res->bind_history |= BITFIELD64_BIT(ZINK_DESCRIPTOR_TYPE_UBO);
+         new_res->bind_stages |= 1 << shader;
+         new_res->bind_count[shader == PIPE_SHADER_COMPUTE]++;
+      }
+      update |= (index && ctx->ubos[shader][index].buffer_offset != offset) ||
+                !!res != !!buffer || (res && res->obj->buffer != new_res->obj->buffer) ||
+                ctx->ubos[shader][index].buffer_size != cb->buffer_size;
 
       pipe_resource_reference(&ctx->ubos[shader][index].buffer, buffer);
       ctx->ubos[shader][index].buffer_offset = offset;
@@ -568,7 +1087,17 @@ zink_set_constant_buffer(struct pipe_context *pctx,
       ctx->ubos[shader][index].buffer_offset = 0;
       ctx->ubos[shader][index].buffer_size = 0;
       ctx->ubos[shader][index].user_buffer = NULL;
+
+      update = (usage & BITFIELD64_BIT(index));
    }
+   if (index == 0) {
+      /* Invalidate current inlinable uniforms. */
+      ctx->inlinable_uniforms_valid_mask &= ~(1 << shader);
+   }
+   update_descriptor_state(ctx, shader, ZINK_DESCRIPTOR_TYPE_UBO, index);
+
+   if (update)
+      zink_screen(pctx->screen)->context_invalidate_descriptor_state(ctx, shader, ZINK_DESCRIPTOR_TYPE_UBO, index, 1);
 }
 
 static void
@@ -579,24 +1108,121 @@ zink_set_shader_buffers(struct pipe_context *pctx,
                         unsigned writable_bitmask)
 {
    struct zink_context *ctx = zink_context(pctx);
+   uint32_t usage = zink_program_get_descriptor_usage(ctx, p_stage, ZINK_DESCRIPTOR_TYPE_SSBO);
+   bool update = false;
 
    unsigned modified_bits = u_bit_consecutive(start_slot, count);
-   ctx->writable_ssbos &= ~modified_bits;
-   ctx->writable_ssbos |= writable_bitmask << start_slot;
+   unsigned old_writable_mask = ctx->writable_ssbos[p_stage];
+   ctx->writable_ssbos[p_stage] &= ~modified_bits;
+   ctx->writable_ssbos[p_stage] |= writable_bitmask << start_slot;
 
    for (unsigned i = 0; i < count; i++) {
       struct pipe_shader_buffer *ssbo = &ctx->ssbos[p_stage][start_slot + i];
+      if (ssbo->buffer) {
+         zink_resource(ssbo->buffer)->bind_count[p_stage == PIPE_SHADER_COMPUTE]--;
+         if (old_writable_mask & BITFIELD64_BIT(start_slot + i))
+            zink_resource(ssbo->buffer)->write_bind_count[p_stage == PIPE_SHADER_COMPUTE]--;
+      }
       if (buffers && buffers[i].buffer) {
-         struct zink_resource *res = zink_resource(buffers[i].buffer);
-         pipe_resource_reference(&ssbo->buffer, &res->base);
+         struct zink_resource *res = (void *) buffers[i].buffer;
+         res->bind_history |= BITFIELD64_BIT(ZINK_DESCRIPTOR_TYPE_SSBO);
+         res->bind_stages |= 1 << p_stage;
+         res->bind_count[p_stage == PIPE_SHADER_COMPUTE]++;
+         if (ctx->writable_ssbos[p_stage] & BITFIELD64_BIT(start_slot + i))
+            res->write_bind_count[p_stage == PIPE_SHADER_COMPUTE]++;
+         pipe_resource_reference(&ssbo->buffer, &res->base.b);
          ssbo->buffer_offset = buffers[i].buffer_offset;
-         ssbo->buffer_size = MIN2(buffers[i].buffer_size, res->size - ssbo->buffer_offset);
+         ssbo->buffer_size = MIN2(buffers[i].buffer_size, res->obj->size - ssbo->buffer_offset);
+         util_range_add(&res->base.b, &res->valid_buffer_range, ssbo->buffer_offset,
+                        ssbo->buffer_offset + ssbo->buffer_size);
+         update = true;
       } else {
          pipe_resource_reference(&ssbo->buffer, NULL);
          ssbo->buffer_offset = 0;
          ssbo->buffer_size = 0;
+         update |= (usage & BITFIELD64_BIT(start_slot + i));
+      }
+      update_descriptor_state(ctx, p_stage, ZINK_DESCRIPTOR_TYPE_SSBO, start_slot + i);
+   }
+   if (update)
+      zink_screen(pctx->screen)->context_invalidate_descriptor_state(ctx, p_stage, ZINK_DESCRIPTOR_TYPE_SSBO, start_slot, count);
+}
+
+static void
+zink_set_shader_images(struct pipe_context *pctx,
+                       enum pipe_shader_type p_stage,
+                       unsigned start_slot, unsigned count,
+                       const struct pipe_image_view *images)
+{
+   struct zink_context *ctx = zink_context(pctx);
+   uint32_t usage = zink_program_get_descriptor_usage(ctx, p_stage, ZINK_DESCRIPTOR_TYPE_IMAGE);
+   bool update = false;
+   for (unsigned i = 0; i < count; i++) {
+      uint32_t bit = BITFIELD64_BIT(start_slot + i);
+      struct zink_image_view *image_view = &ctx->image_views[p_stage][start_slot + i];
+      if (image_view->base.resource) {
+         zink_resource(image_view->base.resource)->bind_count[p_stage == PIPE_SHADER_COMPUTE]--;
+         if (image_view->base.access & PIPE_IMAGE_ACCESS_WRITE)
+            zink_resource(image_view->base.resource)->write_bind_count[p_stage == PIPE_SHADER_COMPUTE]--;
+         if (image_view->base.resource->target != PIPE_BUFFER)
+            zink_resource(image_view->base.resource)->image_bind_count[p_stage == PIPE_SHADER_COMPUTE]--;
+      }
+      if (images && images[i].resource) {
+         util_dynarray_init(&image_view->desc_set_refs.refs, NULL);
+         struct zink_resource *res = (void *) images[i].resource;
+         if (!zink_resource_object_init_storage(ctx, res)) {
+            debug_printf("couldn't create storage image!");
+            continue;
+         }
+         res->bind_history |= BITFIELD64_BIT(ZINK_DESCRIPTOR_TYPE_IMAGE);
+         res->bind_stages |= 1 << p_stage;
+         res->bind_count[p_stage == PIPE_SHADER_COMPUTE]++;
+         util_copy_image_view(&image_view->base, images + i);
+         if (image_view->base.access & PIPE_IMAGE_ACCESS_WRITE)
+            zink_resource(image_view->base.resource)->write_bind_count[p_stage == PIPE_SHADER_COMPUTE]++;
+         if (images[i].resource->target == PIPE_BUFFER) {
+            image_view->buffer_view = get_buffer_view(ctx, res, images[i].format, images[i].u.buf.offset, images[i].u.buf.size);
+            assert(image_view->buffer_view);
+            util_range_add(&res->base.b, &res->valid_buffer_range, images[i].u.buf.offset,
+                           images[i].u.buf.offset + images[i].u.buf.size);
+         } else {
+            struct pipe_surface tmpl = {};
+            tmpl.format = images[i].format;
+            tmpl.nr_samples = 1;
+            tmpl.u.tex.level = images[i].u.tex.level;
+            tmpl.u.tex.first_layer = images[i].u.tex.first_layer;
+            tmpl.u.tex.last_layer = images[i].u.tex.last_layer;
+            image_view->surface = (void*)pctx->create_surface(pctx, &res->base.b, &tmpl);
+            assert(image_view->surface);
+            res->image_bind_count[p_stage == PIPE_SHADER_COMPUTE]++;
+         }
+         update = true;
+      } else if (image_view->base.resource) {
+         zink_descriptor_set_refs_clear(&image_view->desc_set_refs, image_view);
+         if (image_view->base.resource->target == PIPE_BUFFER)
+            zink_buffer_view_reference(ctx, &image_view->buffer_view, NULL);
+         else
+            pipe_surface_reference((struct pipe_surface**)&image_view->surface, NULL);
+         pipe_resource_reference(&image_view->base.resource, NULL);
+         image_view->base.resource = NULL;
+         image_view->surface = NULL;
+         update |= (usage & bit);
       }
+      if (image_view->base.resource)
+         ctx->di.valid_images[p_stage] |= bit;
+      else
+         ctx->di.valid_images[p_stage] &= ~bit;
+      update_descriptor_state(ctx, p_stage, ZINK_DESCRIPTOR_TYPE_IMAGE, start_slot + i);
    }
+   if (update)
+      zink_screen(pctx->screen)->context_invalidate_descriptor_state(ctx, p_stage, ZINK_DESCRIPTOR_TYPE_IMAGE, start_slot, count);
+}
+
+static void
+sampler_view_buffer_clear(struct zink_context *ctx, struct zink_sampler_view *sampler_view)
+{
+   zink_descriptor_set_refs_clear(&sampler_view->desc_set_refs, sampler_view);
+   zink_buffer_view_reference(ctx, &sampler_view->buffer_view, NULL);
 }
 
 static void
@@ -607,13 +1233,52 @@ zink_set_sampler_views(struct pipe_context *pctx,
                        struct pipe_sampler_view **views)
 {
    struct zink_context *ctx = zink_context(pctx);
+   assert(views);
+   uint32_t usage = zink_program_get_descriptor_usage(ctx, shader_type, ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW);
+   bool update = false;
    for (unsigned i = 0; i < num_views; ++i) {
-      struct pipe_sampler_view *pview = views ? views[i] : NULL;
-      pipe_sampler_view_reference(
-         &ctx->image_views[shader_type][start_slot + i],
-         pview);
+      struct zink_sampler_view *a = zink_sampler_view(ctx->sampler_views[shader_type][start_slot + i]);
+      struct zink_sampler_view *b = zink_sampler_view(views[i]);
+      if (a && a->base.texture)
+         zink_resource(a->base.texture)->bind_count[shader_type == PIPE_SHADER_COMPUTE]--;
+      if (b && b->base.texture) {
+         struct zink_resource *res = zink_resource(b->base.texture);
+         if (res->base.b.target == PIPE_BUFFER &&
+             res->bind_history & BITFIELD64_BIT(ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW)) {
+            /* if this resource has been rebound while it wasn't set here,
+             * its backing resource will have changed and thus we need to update
+             * the bufferview
+             */
+            struct zink_buffer_view *buffer_view = get_buffer_view(ctx, res, b->base.format, b->base.u.buf.offset, b->base.u.buf.size);
+            if (buffer_view == b->buffer_view)
+               p_atomic_dec(&buffer_view->reference.count);
+            else {
+               sampler_view_buffer_clear(ctx, b);
+               b->buffer_view = buffer_view;
+            }
+         } else if (!res->obj->is_buffer) {
+             if (res->obj->simage)
+                zink_rebind_surface(ctx, &b->image_view->base);
+
+             ctx->dirty_samplers[shader_type] |= BITFIELD64_BIT(start_slot + i);
+         }
+         res->bind_count[shader_type == PIPE_SHADER_COMPUTE]++;
+         res->bind_history |= BITFIELD64_BIT(ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW);
+         res->bind_stages |= 1 << shader_type;
+      }
+      if (!zink_screen(ctx->base.screen)->lazy_descriptors) {
+         bool is_buffer = zink_program_descriptor_is_buffer(ctx, shader_type, ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW, start_slot + i);
+         uint32_t hash_a = zink_get_sampler_view_hash(ctx, a, is_buffer);
+         uint32_t hash_b = zink_get_sampler_view_hash(ctx, b, is_buffer);
+         if (usage & BITFIELD64_BIT(start_slot + i))
+            update |= !!a != !!b || hash_a != hash_b;
+      }
+      pipe_sampler_view_reference(&ctx->sampler_views[shader_type][start_slot + i], views[i]);
+      update_descriptor_state(ctx, shader_type, ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW, start_slot + i);
    }
-   ctx->num_image_views[shader_type] = start_slot + num_views;
+   ctx->num_sampler_views[shader_type] = start_slot + num_views;
+   if (update)
+      zink_screen(pctx->screen)->context_invalidate_descriptor_state(ctx, shader_type, ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW, start_slot, num_views);
 }
 
 static void
@@ -650,10 +1315,8 @@ hash_render_pass_state(const void *key)
 static bool
 equals_render_pass_state(const void *a, const void *b)
 {
-   const struct zink_render_pass_state *s_a = a, *s_b = b;
-   if (s_a->num_rts != s_b->num_rts)
-      return false;
-   return memcmp(a, b, offsetof(struct zink_render_pass_state, rts) + sizeof(s_a->rts[0]) * s_a->num_rts) == 0;
+   struct zink_render_pass_state *s = (struct zink_render_pass_state*)a;
+   return memcmp(a, b, offsetof(struct zink_render_pass_state, rts) + sizeof(s->rts[0]) * s->num_rts) == 0;
 }
 
 static struct zink_render_pass *
@@ -662,6 +1325,8 @@ get_render_pass(struct zink_context *ctx)
    struct zink_screen *screen = zink_screen(ctx->base.screen);
    const struct pipe_framebuffer_state *fb = &ctx->fb_state;
    struct zink_render_pass_state state = { 0 };
+   uint32_t clears = 0;
+   state.swapchain_init = ctx->new_swapchain;
 
    for (int i = 0; i < fb->nr_cbufs; i++) {
       struct pipe_surface *surf = fb->cbufs[i];
@@ -669,6 +1334,9 @@ get_render_pass(struct zink_context *ctx)
          state.rts[i].format = zink_get_format(screen, surf->format);
          state.rts[i].samples = surf->texture->nr_samples > 0 ? surf->texture->nr_samples :
                                                        VK_SAMPLE_COUNT_1_BIT;
+         state.rts[i].clear_color = zink_fb_clear_enabled(ctx, i) && !zink_fb_clear_first_needs_explicit(&ctx->fb_clears[i]);
+         clears |= !!state.rts[i].clear_color ? BITFIELD64_BIT(i) : 0;
+         state.rts[i].swapchain = surf->texture->bind & PIPE_BIND_SCANOUT;
       } else {
          state.rts[i].format = VK_FORMAT_R8_UINT;
          state.rts[i].samples = MAX2(fb->samples, 1);
@@ -679,28 +1347,65 @@ get_render_pass(struct zink_context *ctx)
 
    if (fb->zsbuf) {
       struct zink_resource *zsbuf = zink_resource(fb->zsbuf->texture);
+      struct zink_framebuffer_clear *fb_clear = &ctx->fb_clears[PIPE_MAX_COLOR_BUFS];
       state.rts[fb->nr_cbufs].format = zsbuf->format;
-      state.rts[fb->nr_cbufs].samples = zsbuf->base.nr_samples > 0 ? zsbuf->base.nr_samples : VK_SAMPLE_COUNT_1_BIT;
+      state.rts[fb->nr_cbufs].samples = zsbuf->base.b.nr_samples > 0 ? zsbuf->base.b.nr_samples : VK_SAMPLE_COUNT_1_BIT;
+      state.rts[fb->nr_cbufs].clear_color = zink_fb_clear_enabled(ctx, PIPE_MAX_COLOR_BUFS) &&
+                                            !zink_fb_clear_first_needs_explicit(fb_clear) &&
+                                            (zink_fb_clear_element(fb_clear, 0)->zs.bits & PIPE_CLEAR_DEPTH);
+      state.rts[fb->nr_cbufs].clear_stencil = zink_fb_clear_enabled(ctx, PIPE_MAX_COLOR_BUFS) &&
+                                              !zink_fb_clear_first_needs_explicit(fb_clear) &&
+                                              (zink_fb_clear_element(fb_clear, 0)->zs.bits & PIPE_CLEAR_STENCIL);
+      clears |= state.rts[fb->nr_cbufs].clear_color || state.rts[fb->nr_cbufs].clear_stencil ? BITFIELD64_BIT(fb->nr_cbufs) : 0;;
       state.num_rts++;
    }
    state.have_zsbuf = fb->zsbuf != NULL;
-
+#ifndef NDEBUG
+      state.clears = clears;
+#endif
    uint32_t hash = hash_render_pass_state(&state);
    struct hash_entry *entry = _mesa_hash_table_search_pre_hashed(ctx->render_pass_cache, hash,
                                                                  &state);
-   if (!entry) {
-      struct zink_render_pass *rp;
+   struct zink_render_pass *rp;
+   if (entry) {
+      rp = entry->data;
+      assert(rp->state.clears == clears);
+   } else {
       rp = zink_create_render_pass(screen, &state);
-      entry = _mesa_hash_table_insert_pre_hashed(ctx->render_pass_cache, hash, &rp->state, rp);
-      if (!entry)
+      if (!_mesa_hash_table_insert_pre_hashed(ctx->render_pass_cache, hash, &rp->state, rp))
          return NULL;
    }
+   return rp;
+}
+
+static bool
+equals_ivci(const void *a, const void *b)
+{
+   return memcmp(a, b, sizeof(VkImageViewCreateInfo)) == 0;
+}
 
-   return entry->data;
+static bool
+equals_bvci(const void *a, const void *b)
+{
+   return memcmp(a, b, sizeof(VkBufferViewCreateInfo)) == 0;
+}
+
+static uint32_t
+hash_framebuffer_state(const void *key)
+{
+   struct zink_framebuffer_state* s = (struct zink_framebuffer_state*)key;
+   return _mesa_hash_data(key, offsetof(struct zink_framebuffer_state, attachments) + sizeof(s->attachments[0]) * s->num_attachments);
+}
+
+static bool
+equals_framebuffer_state(const void *a, const void *b)
+{
+   struct zink_framebuffer_state *s = (struct zink_framebuffer_state*)a;
+   return memcmp(a, b, offsetof(struct zink_framebuffer_state, attachments) + sizeof(s->attachments[0]) * s->num_attachments) == 0;
 }
 
 static struct zink_framebuffer *
-create_framebuffer(struct zink_context *ctx)
+get_framebuffer(struct zink_context *ctx)
 {
    struct zink_screen *screen = zink_screen(ctx->base.screen);
    struct pipe_surface *attachments[PIPE_MAX_COLOR_BUFS + 1] = {};
@@ -724,8 +1429,19 @@ create_framebuffer(struct zink_context *ctx)
    state.layers = MAX2(util_framebuffer_get_num_layers(&ctx->fb_state), 1);
    state.samples = ctx->fb_state.samples;
 
-   struct zink_framebuffer *fb = zink_create_framebuffer(ctx, &state, attachments);
-   zink_init_framebuffer(screen, fb, get_render_pass(ctx));
+   struct zink_framebuffer *fb;
+   simple_mtx_lock(&ctx->framebuffer_mtx);
+   struct hash_entry *entry = _mesa_hash_table_search(&ctx->framebuffer_cache, &state);
+   if (entry) {
+      fb = (void*)entry->data;
+      struct zink_framebuffer *fb_ref = NULL;
+      /* this gains 1 ref every time we reuse it */
+      zink_framebuffer_reference(screen, &fb_ref, fb);
+   } else {
+      fb = zink_create_framebuffer(ctx, &state, &attachments[0]);
+      _mesa_hash_table_insert(&ctx->framebuffer_cache, &fb->state, fb);
+   }
+   simple_mtx_unlock(&ctx->framebuffer_mtx);
    return fb;
 }
 
@@ -738,26 +1454,40 @@ framebuffer_state_buffer_barriers_setup(struct zink_context *ctx,
       if (!surf)
          surf = ctx->framebuffer->null_surface;
       struct zink_resource *res = zink_resource(surf->texture);
-      if (res->layout != VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL)
-         zink_resource_barrier(batch->cmdbuf, res, res->aspect,
-                               VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL);
+      if (!ctx->new_swapchain || !(res->base.b.bind & PIPE_BIND_SCANOUT))
+         zink_resource_image_barrier(ctx, NULL, res, VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL, 0, 0);
+      if (res->base.b.bind & PIPE_BIND_SCANOUT) {
+         /* this will be transitioned to PRESENT at the end of the rp */
+         res->layout = VK_IMAGE_LAYOUT_PRESENT_SRC_KHR;
+         res->access = 0;
+         res->access_stage = 0;
+      }
    }
 
    if (state->zsbuf) {
       struct zink_resource *res = zink_resource(state->zsbuf->texture);
-      if (res->layout != VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL)
-         zink_resource_barrier(batch->cmdbuf, res, res->aspect,
-                               VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL);
+      zink_resource_image_barrier(ctx, NULL, res, VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL, 0, 0);
    }
 }
 
+static void
+setup_framebuffer(struct zink_context *ctx)
+{
+   struct zink_screen *screen = zink_screen(ctx->base.screen);
+   zink_init_framebuffer(screen, ctx->framebuffer, get_render_pass(ctx));
+
+   if (ctx->framebuffer->rp != ctx->gfx_pipeline_state.render_pass)
+      ctx->gfx_pipeline_state.dirty = true;
+   ctx->gfx_pipeline_state.render_pass = ctx->framebuffer->rp;
+}
+
 void
 zink_begin_render_pass(struct zink_context *ctx, struct zink_batch *batch)
 {
    struct zink_screen *screen = zink_screen(ctx->base.screen);
-   assert(batch == zink_curr_batch(ctx));
-   assert(ctx->gfx_pipeline_state.render_pass);
 
+   setup_framebuffer(ctx);
+   assert(ctx->gfx_pipeline_state.render_pass);
    struct pipe_framebuffer_state *fb_state = &ctx->fb_state;
 
    VkRenderPassBeginInfo rpbi = {};
@@ -767,52 +1497,120 @@ zink_begin_render_pass(struct zink_context *ctx, struct zink_batch *batch)
    rpbi.renderArea.offset.y = 0;
    rpbi.renderArea.extent.width = fb_state->width;
    rpbi.renderArea.extent.height = fb_state->height;
-   rpbi.clearValueCount = 0;
-   rpbi.pClearValues = NULL;
+
+   VkClearValue clears[PIPE_MAX_COLOR_BUFS + 1] = {};
+   unsigned clear_buffers = 0;
+   uint32_t clear_validate = 0;
+   for (int i = 0; i < fb_state->nr_cbufs; i++) {
+      /* these are no-ops */
+      if (!fb_state->cbufs[i] || !zink_fb_clear_enabled(ctx, i))
+         continue;
+      /* these need actual clear calls inside the rp */
+      struct zink_framebuffer_clear_data *clear = zink_fb_clear_element(&ctx->fb_clears[i], 0);
+      if (zink_fb_clear_needs_explicit(&ctx->fb_clears[i])) {
+         clear_buffers |= (PIPE_CLEAR_COLOR0 << i);
+         if (zink_fb_clear_count(&ctx->fb_clears[i]) < 2 ||
+             zink_fb_clear_element_needs_explicit(clear))
+            continue;
+      }
+      /* we now know there's one clear that can be done here */
+      if (clear->color.srgb) {
+         clears[i].color.float32[0] = util_format_srgb_to_linear_float(clear->color.color.f[0]);
+         clears[i].color.float32[1] = util_format_srgb_to_linear_float(clear->color.color.f[1]);
+         clears[i].color.float32[2] = util_format_srgb_to_linear_float(clear->color.color.f[2]);
+      } else {
+         clears[i].color.float32[0] = clear->color.color.f[0];
+         clears[i].color.float32[1] = clear->color.color.f[1];
+         clears[i].color.float32[2] = clear->color.color.f[2];
+      }
+      clears[i].color.float32[3] = clear->color.color.f[3];
+      rpbi.clearValueCount = i + 1;
+      clear_validate |= BITFIELD64_BIT(i);
+      assert(ctx->framebuffer->rp->state.clears);
+   }
+   if (fb_state->zsbuf && zink_fb_clear_enabled(ctx, PIPE_MAX_COLOR_BUFS)) {
+      struct zink_framebuffer_clear *fb_clear = &ctx->fb_clears[PIPE_MAX_COLOR_BUFS];
+      struct zink_framebuffer_clear_data *clear = zink_fb_clear_element(fb_clear, 0);
+      if (!zink_fb_clear_element_needs_explicit(clear)) {
+         clears[fb_state->nr_cbufs].depthStencil.depth = clear->zs.depth;
+         clears[fb_state->nr_cbufs].depthStencil.stencil = clear->zs.stencil;
+         rpbi.clearValueCount = fb_state->nr_cbufs + 1;
+         clear_validate |= BITFIELD64_BIT(fb_state->nr_cbufs);
+         assert(ctx->framebuffer->rp->state.clears);
+      }
+      if (zink_fb_clear_needs_explicit(fb_clear)) {
+         for (int j = !zink_fb_clear_element_needs_explicit(clear); j < zink_fb_clear_count(fb_clear); j++)
+            clear_buffers |= zink_fb_clear_element(fb_clear, j)->zs.bits;
+      }
+   }
+   assert(clear_validate == ctx->framebuffer->rp->state.clears);
+   rpbi.pClearValues = &clears[0];
    rpbi.framebuffer = ctx->framebuffer->fb;
 
    assert(ctx->gfx_pipeline_state.render_pass && ctx->framebuffer);
-   assert(!batch->fb || batch->fb == ctx->framebuffer);
 
    framebuffer_state_buffer_barriers_setup(ctx, fb_state, batch);
 
-   zink_framebuffer_reference(screen, &batch->fb, ctx->framebuffer);
-   for (struct zink_surface **surf = (struct zink_surface **)batch->fb->surfaces; *surf; surf++)
-      zink_batch_reference_resource_rw(batch, zink_resource((*surf)->base.texture), true);
+   zink_framebuffer_reference(screen, &batch->state->fb, ctx->framebuffer);
+   for (int i = 0; i < batch->state->fb->state.num_attachments; i++) {
+      if (batch->state->fb->surfaces[i]) {
+         struct zink_surface *surf = zink_surface(batch->state->fb->surfaces[i]);
+         zink_batch_reference_resource_rw(batch, zink_resource(surf->base.texture), true);
+         zink_batch_reference_surface(batch, surf);
+      }
+   }
 
-   vkCmdBeginRenderPass(batch->cmdbuf, &rpbi, VK_SUBPASS_CONTENTS_INLINE);
+   vkCmdBeginRenderPass(batch->state->cmdbuf, &rpbi, VK_SUBPASS_CONTENTS_INLINE);
    batch->in_rp = true;
+   ctx->new_swapchain = false;
+
+   zink_clear_framebuffer(ctx, clear_buffers);
 }
 
 static void
 zink_end_render_pass(struct zink_context *ctx, struct zink_batch *batch)
 {
    if (batch->in_rp)
-      vkCmdEndRenderPass(batch->cmdbuf);
+      vkCmdEndRenderPass(batch->state->cmdbuf);
    batch->in_rp = false;
 }
 
 static void
-flush_batch(struct zink_context *ctx)
+sync_flush(struct zink_context *ctx, struct zink_batch_state *bs)
+{
+   if (util_queue_is_initialized(&ctx->batch.flush_queue))
+      util_queue_fence_wait(&bs->flush_completed);
+}
+
+static void
+flush_batch(struct zink_context *ctx, bool sync)
 {
-   struct zink_batch *batch = zink_curr_batch(ctx);
+   struct zink_batch *batch = &ctx->batch;
    zink_end_render_pass(ctx, batch);
    zink_end_batch(ctx, batch);
 
-   ctx->curr_batch++;
-   if (ctx->curr_batch == ARRAY_SIZE(ctx->batches))
-      ctx->curr_batch = 0;
+   if (sync)
+      sync_flush(ctx, ctx->batch.state);
+
+   if (ctx->batch.state->is_device_lost && ctx->reset.reset) {
+      ctx->is_device_lost = true;
+      ctx->reset.reset(ctx->reset.data, PIPE_GUILTY_CONTEXT_RESET);
+   } else {
+      incr_curr_batch(ctx);
 
-   zink_start_batch(ctx, zink_curr_batch(ctx));
+      zink_start_batch(ctx, batch);
+      if (zink_screen(ctx->base.screen)->info.have_EXT_transform_feedback && ctx->num_so_targets)
+         ctx->dirty_so_targets = true;
+   }
 }
 
 struct zink_batch *
 zink_batch_rp(struct zink_context *ctx)
 {
-   struct zink_batch *batch = zink_curr_batch(ctx);
+   struct zink_batch *batch = &ctx->batch;
    if (!batch->in_rp) {
       zink_begin_render_pass(ctx, batch);
-      assert(batch->fb && batch->fb->rp);
+      assert(batch->state->fb && batch->state->fb->rp);
    }
    return batch;
 }
@@ -820,44 +1618,99 @@ zink_batch_rp(struct zink_context *ctx)
 struct zink_batch *
 zink_batch_no_rp(struct zink_context *ctx)
 {
-   struct zink_batch *batch = zink_curr_batch(ctx);
-   if (batch->in_rp) {
-      /* flush batch and get a new one */
-      flush_batch(ctx);
-      batch = zink_curr_batch(ctx);
-      assert(!batch->in_rp);
-   }
+   struct zink_batch *batch = &ctx->batch;
+   zink_end_render_pass(ctx, batch);
+   assert(!batch->in_rp);
    return batch;
 }
 
-static void
-zink_set_framebuffer_state(struct pipe_context *pctx,
-                           const struct pipe_framebuffer_state *state)
+void
+zink_flush_queue(struct zink_context *ctx)
 {
-   struct zink_context *ctx = zink_context(pctx);
-   struct zink_screen *screen = zink_screen(pctx->screen);
+   flush_batch(ctx, true);
+}
 
-   util_copy_framebuffer_state(&ctx->fb_state, state);
+static bool
+rebind_fb_surface(struct zink_context *ctx, struct pipe_surface *surf, struct zink_resource *match_res)
+{
+   if (!surf)
+      return false;
+   struct zink_resource *surf_res = zink_resource(surf->texture);
+   if ((match_res == surf_res) || surf_res->obj->simage)
+      return zink_rebind_surface(ctx, surf);
+   return false;
+}
+
+static bool
+rebind_fb_state(struct zink_context *ctx, struct zink_resource *match_res, bool from_set_fb)
+{
+   bool rebind = false;
+   for (int i = 0; i < ctx->fb_state.nr_cbufs; i++) {
+      rebind |= rebind_fb_surface(ctx, ctx->fb_state.cbufs[i], match_res);
+      if (from_set_fb && ctx->fb_state.cbufs[i] && ctx->fb_state.cbufs[i]->texture->bind & PIPE_BIND_SCANOUT)
+         ctx->new_swapchain = true;
+   }
+   rebind |= rebind_fb_surface(ctx, ctx->fb_state.zsbuf, match_res);
+   return rebind;
+}
+
+static void
+zink_set_framebuffer_state(struct pipe_context *pctx,
+                           const struct pipe_framebuffer_state *state)
+{
+   struct zink_context *ctx = zink_context(pctx);
 
-   struct zink_framebuffer *fb = ctx->framebuffer;
-   /* explicitly unref previous fb to ensure it gets destroyed */
-   if (fb)
-      zink_framebuffer_reference(screen, &fb, NULL);
-   fb = create_framebuffer(ctx);
-   zink_framebuffer_reference(screen, &ctx->framebuffer, fb);
-   ctx->gfx_pipeline_state.render_pass = fb->rp;
+   for (int i = 0; i < ctx->fb_state.nr_cbufs; i++) {
+      struct pipe_surface *surf = ctx->fb_state.cbufs[i];
+      if (surf &&
+          (!state->cbufs[i] || i >= state->nr_cbufs ||
+           surf->texture != state->cbufs[i]->texture ||
+           surf->format != state->cbufs[i]->format ||
+           memcmp(&surf->u, &state->cbufs[i]->u, sizeof(union pipe_surface_desc))))
+         zink_fb_clears_apply(ctx, surf->texture);
+   }
+   if (ctx->fb_state.zsbuf) {
+      struct pipe_surface *surf = ctx->fb_state.zsbuf;
+      if (!state->zsbuf || surf->texture != state->zsbuf->texture ||
+          memcmp(&surf->u, &state->zsbuf->u, sizeof(union pipe_surface_desc)))
+      zink_fb_clears_apply(ctx, ctx->fb_state.zsbuf->texture);
+   }
+
+   util_copy_framebuffer_state(&ctx->fb_state, state);
+   rebind_fb_state(ctx, NULL, true);
+   /* get_framebuffer adds a ref if the fb is reused;
+    * always do get_framebuffer first to avoid deleting the same fb
+    * we're about to use
+    */
+   struct zink_framebuffer *fb = get_framebuffer(ctx);
+   if (ctx->framebuffer) {
+      simple_mtx_lock(&ctx->framebuffer_mtx);
+      struct hash_entry *he = _mesa_hash_table_search(&ctx->framebuffer_cache, &ctx->framebuffer->state);
+      if (ctx->framebuffer && !ctx->framebuffer->state.num_attachments) {
+         /* if this has no attachments then its lifetime has ended */
+         _mesa_hash_table_remove(&ctx->framebuffer_cache, he);
+         he = NULL;
+      }
+      /* this loses 1 ref every time we unset it */
+      if (zink_framebuffer_reference(zink_screen(pctx->screen), &ctx->framebuffer, NULL) && he)
+         _mesa_hash_table_remove(&ctx->framebuffer_cache, he);
+      simple_mtx_unlock(&ctx->framebuffer_mtx);
+   }
+   ctx->framebuffer = fb;
 
    uint8_t rast_samples = util_framebuffer_get_num_samples(state);
    /* in vulkan, gl_SampleMask needs to be explicitly ignored for sampleCount == 1 */
    if ((ctx->gfx_pipeline_state.rast_samples > 1) != (rast_samples > 1))
       ctx->dirty_shader_stages |= 1 << PIPE_SHADER_FRAGMENT;
+   if (ctx->gfx_pipeline_state.rast_samples != rast_samples)
+      ctx->gfx_pipeline_state.dirty = true;
    ctx->gfx_pipeline_state.rast_samples = rast_samples;
+   if (ctx->gfx_pipeline_state.num_attachments != state->nr_cbufs)
+      ctx->gfx_pipeline_state.dirty = true;
    ctx->gfx_pipeline_state.num_attachments = state->nr_cbufs;
-   ctx->gfx_pipeline_state.dirty = true;
-
-   struct zink_batch *batch = zink_batch_no_rp(ctx);
 
-   framebuffer_state_buffer_barriers_setup(ctx, state, batch);
+   /* need to ensure we start a new rp on next draw */
+   zink_batch_no_rp(ctx);
 }
 
 static void
@@ -891,6 +1744,7 @@ access_src_flags(VkImageLayout layout)
    case VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL:
       return VK_ACCESS_DEPTH_STENCIL_ATTACHMENT_READ_BIT;
 
+   case VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL:
    case VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL:
       return VK_ACCESS_SHADER_READ_BIT;
 
@@ -903,6 +1757,9 @@ access_src_flags(VkImageLayout layout)
    case VK_IMAGE_LAYOUT_PREINITIALIZED:
       return VK_ACCESS_HOST_WRITE_BIT;
 
+   case VK_IMAGE_LAYOUT_PRESENT_SRC_KHR:
+      return 0;
+
    default:
       unreachable("unexpected layout");
    }
@@ -929,9 +1786,15 @@ access_dst_flags(VkImageLayout layout)
    case VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL:
       return VK_ACCESS_TRANSFER_READ_BIT;
 
+   case VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL:
+      return VK_ACCESS_SHADER_READ_BIT;
+
    case VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL:
       return VK_ACCESS_TRANSFER_WRITE_BIT;
 
+   case VK_IMAGE_LAYOUT_PRESENT_SRC_KHR:
+      return 0;
+
    default:
       unreachable("unexpected layout");
    }
@@ -955,6 +1818,7 @@ pipeline_dst_stage(VkImageLayout layout)
       return VK_PIPELINE_STAGE_ALL_COMMANDS_BIT;
 
    case VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL:
+   case VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL:
       return VK_PIPELINE_STAGE_FRAGMENT_SHADER_BIT;
 
    default:
@@ -962,58 +1826,92 @@ pipeline_dst_stage(VkImageLayout layout)
    }
 }
 
-static VkPipelineStageFlags
-pipeline_src_stage(VkImageLayout layout)
+#define ALL_READ_ACCESS_FLAGS \
+    (VK_ACCESS_INDIRECT_COMMAND_READ_BIT | \
+    VK_ACCESS_INDEX_READ_BIT | \
+    VK_ACCESS_VERTEX_ATTRIBUTE_READ_BIT | \
+    VK_ACCESS_UNIFORM_READ_BIT | \
+    VK_ACCESS_INPUT_ATTACHMENT_READ_BIT | \
+    VK_ACCESS_SHADER_READ_BIT | \
+    VK_ACCESS_COLOR_ATTACHMENT_READ_BIT | \
+    VK_ACCESS_DEPTH_STENCIL_ATTACHMENT_READ_BIT | \
+    VK_ACCESS_TRANSFER_READ_BIT |\
+    VK_ACCESS_HOST_READ_BIT |\
+    VK_ACCESS_MEMORY_READ_BIT |\
+    VK_ACCESS_TRANSFORM_FEEDBACK_COUNTER_READ_BIT_EXT |\
+    VK_ACCESS_CONDITIONAL_RENDERING_READ_BIT_EXT |\
+    VK_ACCESS_COLOR_ATTACHMENT_READ_NONCOHERENT_BIT_EXT |\
+    VK_ACCESS_ACCELERATION_STRUCTURE_READ_BIT_KHR |\
+    VK_ACCESS_SHADING_RATE_IMAGE_READ_BIT_NV |\
+    VK_ACCESS_FRAGMENT_DENSITY_MAP_READ_BIT_EXT |\
+    VK_ACCESS_COMMAND_PREPROCESS_READ_BIT_NV |\
+    VK_ACCESS_ACCELERATION_STRUCTURE_READ_BIT_NV |\
+    VK_ACCESS_ACCELERATION_STRUCTURE_WRITE_BIT_NV)
+
+
+bool
+zink_resource_access_is_write(VkAccessFlags flags)
 {
-   switch (layout) {
-   case VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL:
-      return VK_PIPELINE_STAGE_COLOR_ATTACHMENT_OUTPUT_BIT;
-   case VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL:
-      return VK_PIPELINE_STAGE_EARLY_FRAGMENT_TESTS_BIT;
-
-   case VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL:
-      return VK_PIPELINE_STAGE_TRANSFER_BIT;
-   case VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL:
-      return VK_PIPELINE_STAGE_TRANSFER_BIT;
-
-   case VK_IMAGE_LAYOUT_GENERAL:
-      return VK_PIPELINE_STAGE_ALL_COMMANDS_BIT;
-
-   case VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL:
-      return VK_PIPELINE_STAGE_VERTEX_SHADER_BIT;
-
-   default:
-      return VK_PIPELINE_STAGE_TOP_OF_PIPE_BIT;
-   }
+   return (flags & ALL_READ_ACCESS_FLAGS) != flags;
 }
 
+bool
+zink_resource_image_needs_barrier(struct zink_resource *res, VkImageLayout new_layout, VkAccessFlags flags, VkPipelineStageFlags pipeline)
+{
+   if (!pipeline)
+      pipeline = pipeline_dst_stage(new_layout);
+   if (!flags)
+      flags = access_dst_flags(new_layout);
+   return res->layout != new_layout || (res->access_stage & pipeline) != pipeline ||
+          (res->access & flags) != flags ||
+          (zink_resource_access_is_write(flags) && util_bitcount(flags) > 1);
+}
 
-void
-zink_resource_barrier(VkCommandBuffer cmdbuf, struct zink_resource *res,
-                      VkImageAspectFlags aspect, VkImageLayout new_layout)
+bool
+zink_resource_image_barrier_init(VkImageMemoryBarrier *imb, struct zink_resource *res, VkImageLayout new_layout, VkAccessFlags flags, VkPipelineStageFlags pipeline)
 {
+   if (!pipeline)
+      pipeline = pipeline_dst_stage(new_layout);
+   if (!flags)
+      flags = access_dst_flags(new_layout);
+
    VkImageSubresourceRange isr = {
-      aspect,
+      res->aspect,
       0, VK_REMAINING_MIP_LEVELS,
       0, VK_REMAINING_ARRAY_LAYERS
    };
-
-   VkImageMemoryBarrier imb = {
+   *imb = (VkImageMemoryBarrier){
       VK_STRUCTURE_TYPE_IMAGE_MEMORY_BARRIER,
       NULL,
-      access_src_flags(res->layout),
-      access_dst_flags(new_layout),
+      res->access ? res->access : access_src_flags(res->layout),
+      flags,
       res->layout,
       new_layout,
       VK_QUEUE_FAMILY_IGNORED,
       VK_QUEUE_FAMILY_IGNORED,
-      res->image,
+      res->obj->image,
       isr
    };
+   return zink_resource_image_needs_barrier(res, new_layout, flags, pipeline);
+}
+
+void
+zink_resource_image_barrier(struct zink_context *ctx, struct zink_batch *batch, struct zink_resource *res,
+                      VkImageLayout new_layout, VkAccessFlags flags, VkPipelineStageFlags pipeline)
+{
+   VkImageMemoryBarrier imb;
+   if (!zink_resource_image_barrier_init(&imb, res, new_layout, flags, pipeline))
+      return;
+   if (!pipeline)
+      pipeline = pipeline_dst_stage(new_layout);
+   /* only barrier if we're changing layout or doing something besides read -> read */
+   batch = zink_batch_no_rp(ctx);
+   assert(!batch->in_rp);
+   assert(new_layout);
    vkCmdPipelineBarrier(
-      cmdbuf,
-      pipeline_src_stage(res->layout),
-      pipeline_dst_stage(new_layout),
+      batch->state->cmdbuf,
+      res->access_stage ? res->access_stage : VK_PIPELINE_STAGE_TOP_OF_PIPE_BIT,
+      pipeline,
       0,
       0, NULL,
       0, NULL,
@@ -1021,6 +1919,120 @@ zink_resource_barrier(VkCommandBuffer cmdbuf, struct zink_resource *res,
    );
 
    res->layout = new_layout;
+   res->access_stage = pipeline;
+   res->access = imb.dstAccessMask;
+}
+
+
+VkPipelineStageFlags
+zink_pipeline_flags_from_stage(VkShaderStageFlagBits stage)
+{
+   switch (stage) {
+   case VK_SHADER_STAGE_VERTEX_BIT:
+      return VK_PIPELINE_STAGE_VERTEX_SHADER_BIT;
+   case VK_SHADER_STAGE_FRAGMENT_BIT:
+      return VK_PIPELINE_STAGE_FRAGMENT_SHADER_BIT;
+   case VK_SHADER_STAGE_GEOMETRY_BIT:
+      return VK_PIPELINE_STAGE_GEOMETRY_SHADER_BIT;
+   case VK_SHADER_STAGE_TESSELLATION_CONTROL_BIT:
+      return VK_PIPELINE_STAGE_TESSELLATION_CONTROL_SHADER_BIT;
+   case VK_SHADER_STAGE_TESSELLATION_EVALUATION_BIT:
+      return VK_PIPELINE_STAGE_TESSELLATION_EVALUATION_SHADER_BIT;
+   case VK_SHADER_STAGE_COMPUTE_BIT:
+      return VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT;
+   default:
+      unreachable("unknown shader stage bit");
+   }
+}
+
+static VkPipelineStageFlags
+pipeline_access_stage(VkAccessFlags flags)
+{
+   switch (flags) {
+   case VK_ACCESS_UNIFORM_READ_BIT:
+   case VK_ACCESS_SHADER_READ_BIT:
+   case VK_ACCESS_SHADER_WRITE_BIT:
+      return VK_PIPELINE_STAGE_TASK_SHADER_BIT_NV |
+             VK_PIPELINE_STAGE_MESH_SHADER_BIT_NV |
+             VK_PIPELINE_STAGE_RAY_TRACING_SHADER_BIT_KHR |
+             VK_PIPELINE_STAGE_VERTEX_SHADER_BIT |
+             VK_PIPELINE_STAGE_TESSELLATION_CONTROL_SHADER_BIT |
+             VK_PIPELINE_STAGE_TESSELLATION_EVALUATION_SHADER_BIT |
+             VK_PIPELINE_STAGE_GEOMETRY_SHADER_BIT |
+             VK_PIPELINE_STAGE_FRAGMENT_SHADER_BIT |
+             VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT;
+   default:
+      return VK_PIPELINE_STAGE_TRANSFER_BIT;
+   }
+}
+
+bool
+zink_resource_buffer_needs_barrier(struct zink_resource *res, VkAccessFlags flags, VkPipelineStageFlags pipeline)
+{
+   if (!pipeline)
+      pipeline = pipeline_access_stage(flags);
+   return (res->access_stage & pipeline) != pipeline || (res->access & flags) != flags ||
+          (zink_resource_access_is_write(flags) && util_bitcount(flags) > 1);
+}
+
+bool
+zink_resource_buffer_barrier_init(VkBufferMemoryBarrier *bmb, struct zink_resource *res, VkAccessFlags flags, VkPipelineStageFlags pipeline)
+{
+   if (!pipeline)
+      pipeline = pipeline_access_stage(flags);
+   *bmb = (VkBufferMemoryBarrier){
+      VK_STRUCTURE_TYPE_BUFFER_MEMORY_BARRIER,
+      NULL,
+      res->access,
+      flags,
+      VK_QUEUE_FAMILY_IGNORED,
+      VK_QUEUE_FAMILY_IGNORED,
+      res->obj->buffer,
+      res->obj->offset,
+      res->base.b.width0
+   };
+   return zink_resource_buffer_needs_barrier(res, flags, pipeline);
+}
+
+void
+zink_resource_buffer_barrier(struct zink_context *ctx, struct zink_batch *batch, struct zink_resource *res, VkAccessFlags flags, VkPipelineStageFlags pipeline)
+{
+   VkBufferMemoryBarrier bmb;
+   if (!zink_resource_buffer_barrier_init(&bmb, res, flags, pipeline))
+      return;
+   if (!pipeline)
+      pipeline = pipeline_access_stage(flags);
+   /* only barrier if we're changing layout or doing something besides read -> read */
+   batch = zink_batch_no_rp(ctx);
+   assert(!batch->in_rp);
+   vkCmdPipelineBarrier(
+      batch->state->cmdbuf,
+      res->access_stage ? res->access_stage : pipeline_access_stage(res->access),
+      pipeline,
+      0,
+      0, NULL,
+      1, &bmb,
+      0, NULL
+   );
+   res->access = bmb.dstAccessMask;
+   res->access_stage = pipeline;
+}
+
+bool
+zink_resource_needs_barrier(struct zink_resource *res, VkImageLayout layout, VkAccessFlags flags, VkPipelineStageFlags pipeline)
+{
+   if (res->base.b.target == PIPE_BUFFER)
+      return zink_resource_buffer_needs_barrier(res, flags, pipeline);
+   return zink_resource_image_needs_barrier(res, layout, flags, pipeline);
+}
+
+void
+zink_resource_barrier(struct zink_context *ctx, struct zink_batch *batch, struct zink_resource *res, VkImageLayout layout, VkAccessFlags flags, VkPipelineStageFlags pipeline)
+{
+   if (res->base.b.target == PIPE_BUFFER)
+      zink_resource_buffer_barrier(ctx, batch, res, flags, pipeline);
+   else
+      zink_resource_image_barrier(ctx, batch, res, layout, flags, pipeline);
 }
 
 VkShaderStageFlagBits
@@ -1040,13 +2052,27 @@ zink_shader_stage(enum pipe_shader_type type)
 static uint32_t
 hash_gfx_program(const void *key)
 {
-   return _mesa_hash_data(key, sizeof(struct zink_shader *) * (ZINK_SHADER_COUNT));
+   const struct zink_shader **shaders = (void*)key;
+   uint32_t hash = 0;
+   unsigned zero = 0;
+   /* pointers can be recycled, so we need to check the shader ids */
+   for (unsigned i = 0; i < ZINK_SHADER_COUNT; i++)
+      hash = XXH32(shaders[i] ? &shaders[i]->shader_id : &zero, sizeof(unsigned), hash);
+   return hash;
 }
 
 static bool
 equals_gfx_program(const void *a, const void *b)
 {
-   return memcmp(a, b, sizeof(struct zink_shader *) * (ZINK_SHADER_COUNT)) == 0;
+   const struct zink_shader **left = (void*)a, **right = (void*)b;
+   /* if any shaders are set/unset or shader ids don't match then these aren't equal */
+   for (unsigned i = 0; i < ZINK_SHADER_COUNT; i++) {
+      if (!!left[i] != !!right[i])
+         return false;
+      if (left[i] && right[i] && left[i]->shader_id != right[i]->shader_id)
+         return false;
+   }
+   return true;
 }
 
 static void
@@ -1055,48 +2081,405 @@ zink_flush(struct pipe_context *pctx,
            enum pipe_flush_flags flags)
 {
    struct zink_context *ctx = zink_context(pctx);
+   bool deferred = flags & PIPE_FLUSH_DEFERRED;
+   bool deferred_fence = false;
+   struct zink_batch *batch = &ctx->batch;
+   struct zink_fence *fence = NULL;
+   struct zink_screen *screen = zink_screen(ctx->base.screen);
 
-   struct zink_batch *batch = zink_curr_batch(ctx);
-   flush_batch(ctx);
+   if (!deferred && ctx->clears_enabled) {
+      /* start rp to do all the clears */
+      zink_begin_render_pass(ctx, batch);
+   }
 
-   if (zink_screen(pctx->screen)->info.have_EXT_transform_feedback && ctx->num_so_targets)
-      ctx->dirty_so_targets = true;
+   if (!batch->has_work) {
+       if (pfence) {
+          /* reuse last fence */
+          fence = ctx->last_fence;
+       }
+       if (!deferred) {
+          struct zink_batch_state *last = zink_batch_state(ctx->last_fence);
+          if (last) {
+             sync_flush(ctx, last);
+             if (last->is_device_lost && ctx->reset.reset) {
+                ctx->is_device_lost = true;
+                ctx->reset.reset(ctx->reset.data, PIPE_GUILTY_CONTEXT_RESET);
+             }
+          }
+       }
+   } else {
+      fence = &batch->state->fence;
+      if (ctx->flush_res) {
+         batch->state->flush_res = ctx->flush_res;
+         ctx->flush_res = NULL;
+      }
+      if (deferred && !(flags & PIPE_FLUSH_FENCE_FD) && pfence)
+         deferred_fence = true;
+      else
+         flush_batch(ctx, true);
+   }
 
-   if (pfence)
-      zink_fence_reference(zink_screen(pctx->screen),
-                           (struct zink_fence **)pfence,
-                           batch->fence);
-
-   /* HACK:
-    * For some strange reason, we need to finish before presenting, or else
-    * we start rendering on top of the back-buffer for the next frame. This
-    * seems like a bug in the DRI-driver to me, because we really should
-    * be properly protected by fences here, and the back-buffer should
-    * either be swapped with the front-buffer, or blitted from. But for
-    * some strange reason, neither of these things happen.
-    */
-   if (flags & PIPE_FLUSH_END_OF_FRAME)
-      pctx->screen->fence_finish(pctx->screen, pctx,
-                                 (struct pipe_fence_handle *)batch->fence,
-                                 PIPE_TIMEOUT_INFINITE);
+   if (pfence) {
+      struct zink_tc_fence *mfence;
+
+      if (flags & TC_FLUSH_ASYNC) {
+         mfence = zink_tc_fence(*pfence);
+         assert(mfence);
+      } else {
+         mfence = zink_create_tc_fence();
+
+         screen->base.fence_reference(&screen->base, pfence, NULL);
+         *pfence = (struct pipe_fence_handle *)mfence;
+      }
+
+      zink_batch_state_reference(screen, NULL, zink_batch_state(fence));
+      mfence->fence = fence;
+      if (fence)
+         mfence->batch_id = fence->batch_id;
+
+      if (deferred_fence) {
+         assert(fence);
+         mfence->deferred_ctx = pctx;
+         mfence->deferred_id = fence->batch_id;
+      }
+
+      if (!fence || flags & TC_FLUSH_ASYNC) {
+         if (!util_queue_fence_is_signalled(&mfence->ready))
+            util_queue_fence_signal(&mfence->ready);
+      }
+   }
+   if (fence && !(flags & (PIPE_FLUSH_DEFERRED | PIPE_FLUSH_ASYNC)))
+      sync_flush(ctx, zink_batch_state(fence));
+   if (flags & PIPE_FLUSH_END_OF_FRAME && !(flags & TC_FLUSH_ASYNC) && !deferred) {
+      if (!ctx->first_frame)
+         zink_vkfence_wait(screen, fence, PIPE_TIMEOUT_INFINITE);
+      ctx->first_frame = true;
+   }
+}
+
+void
+zink_maybe_flush_or_stall(struct zink_context *ctx)
+{
+   struct zink_screen *screen = zink_screen(ctx->base.screen);
+   /* flush anytime our total batch memory usage is potentially >= 1/10 of total system memory */
+   if (ctx->batch.state->resource_size >= screen->total_mem / 10)
+      flush_batch(ctx, true);
+
+   if (ctx->resource_size >= screen->total_mem / 10 || _mesa_hash_table_num_entries(&ctx->batch_states) > 10) {
+      sync_flush(ctx, zink_batch_state(ctx->last_fence));
+      zink_vkfence_wait(screen, ctx->last_fence, PIPE_TIMEOUT_INFINITE);
+      zink_batch_reset_all(ctx);
+   }
 }
 
 void
 zink_fence_wait(struct pipe_context *pctx)
 {
-   struct pipe_fence_handle *fence = NULL;
-   pctx->flush(pctx, &fence, PIPE_FLUSH_HINT_FINISH);
-   if (fence) {
-      pctx->screen->fence_finish(pctx->screen, NULL, fence,
-                                 PIPE_TIMEOUT_INFINITE);
-      pctx->screen->fence_reference(pctx->screen, &fence, NULL);
+   struct zink_context *ctx = zink_context(pctx);
+
+   if (ctx->batch.has_work)
+      pctx->flush(pctx, NULL, PIPE_FLUSH_HINT_FINISH);
+   if (ctx->last_fence) {
+      sync_flush(ctx, zink_batch_state(ctx->last_fence));
+      zink_vkfence_wait(zink_screen(ctx->base.screen), ctx->last_fence, PIPE_TIMEOUT_INFINITE);
+      zink_batch_reset_all(ctx);
+   }
+}
+
+static bool
+timeline_wait(struct zink_context *ctx, uint32_t batch_id, uint64_t timeout)
+{
+   struct zink_screen *screen = zink_screen(ctx->base.screen);
+   VkSemaphoreWaitInfo wi = {};
+   wi.sType = VK_STRUCTURE_TYPE_SEMAPHORE_WAIT_INFO;
+   wi.semaphoreCount = 1;
+   wi.pSemaphores = &ctx->batch.sem;
+   uint64_t batch_id64 = batch_id;
+   wi.pValues = &batch_id64;
+   bool success = screen->vk_WaitSemaphores(screen->dev, &wi, timeout);
+   if (success)
+      ctx->last_finished = MAX2(batch_id, ctx->last_finished);
+   return success;
+}
+
+void
+zink_wait_on_batch(struct zink_context *ctx, uint32_t batch_id)
+{
+   struct zink_batch_state *bs = ctx->batch.state;
+   assert(bs);
+   if (!batch_id || bs->fence.batch_id == batch_id)
+      /* not submitted yet */
+      flush_batch(ctx, true);
+   if (ctx->have_timelines) {
+      timeline_wait(ctx, batch_id, UINT64_MAX);
+      return;
+   }
+   simple_mtx_lock(&ctx->batch_mtx);
+   struct zink_fence *fence;
+
+   assert(batch_id || ctx->last_fence);
+   if (ctx->last_fence && (!batch_id || batch_id == zink_batch_state(ctx->last_fence)->fence.batch_id))
+      fence = ctx->last_fence;
+   else {
+      struct hash_entry *he = _mesa_hash_table_search_pre_hashed(&ctx->batch_states, batch_id, (void*)(uintptr_t)batch_id);
+      /* if we can't find it, it must have finished already */
+      if (!he) {
+         simple_mtx_unlock(&ctx->batch_mtx);
+         return;
+      }
+      fence = he->data;
+   }
+   simple_mtx_unlock(&ctx->batch_mtx);
+   assert(fence);
+   sync_flush(ctx, zink_batch_state(fence));
+   if (zink_vkfence_wait(zink_screen(ctx->base.screen), fence, PIPE_TIMEOUT_INFINITE))
+      ctx->last_finished = MAX2(fence->batch_id, ctx->last_finished);
+}
+
+bool
+zink_check_batch_completion(struct zink_context *ctx, uint32_t batch_id)
+{
+   assert(batch_id);
+   struct zink_batch_state *bs = ctx->batch.state;
+   assert(bs);
+   if (bs->fence.batch_id == batch_id)
+      /* not submitted yet */
+      return false;
+
+   if (ctx->last_finished >= batch_id)
+      return true;
+
+   if (ctx->have_timelines)
+      return timeline_wait(ctx, batch_id, 0);
+   struct zink_fence *fence;
+
+   simple_mtx_lock(&ctx->batch_mtx);
+
+   if (ctx->last_fence && batch_id == zink_batch_state(ctx->last_fence)->fence.batch_id)
+      fence = ctx->last_fence;
+   else {
+      struct hash_entry *he = _mesa_hash_table_search_pre_hashed(&ctx->batch_states, batch_id, (void*)(uintptr_t)batch_id);
+      /* if we can't find it, it must have finished already */
+      if (!he) {
+         simple_mtx_unlock(&ctx->batch_mtx);
+         return true;
+      }
+      fence = he->data;
+   }
+   simple_mtx_unlock(&ctx->batch_mtx);
+   assert(fence);
+   if (util_queue_is_initialized(&ctx->batch.flush_queue) &&
+       !util_queue_fence_is_signalled(&zink_batch_state(fence)->flush_completed))
+      return false;
+   bool success = zink_vkfence_wait(zink_screen(ctx->base.screen), fence, 0);
+   if (success)
+      ctx->last_finished = MAX2(fence->batch_id, ctx->last_finished);
+   return success;
+}
+
+static void
+zink_texture_barrier(struct pipe_context *pctx, unsigned flags)
+{
+   struct zink_context *ctx = zink_context(pctx);
+   for (unsigned i = 0; i < ctx->framebuffer->state.num_attachments; i++) {
+      struct zink_surface *surf = zink_surface(ctx->framebuffer->surfaces[i]);
+      if (!surf)
+         continue;
+      struct zink_resource *res = zink_resource(surf->base.texture);
+      if (res->aspect == VK_IMAGE_ASPECT_COLOR_BIT)
+         zink_resource_image_barrier(ctx, NULL, res, VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL,
+                                     VK_ACCESS_COLOR_ATTACHMENT_READ_BIT,
+                                     VK_PIPELINE_STAGE_COLOR_ATTACHMENT_OUTPUT_BIT);
+      else
+         zink_resource_image_barrier(ctx, NULL, res, VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL,
+                                     VK_ACCESS_DEPTH_STENCIL_ATTACHMENT_READ_BIT,
+                                     VK_PIPELINE_STAGE_EARLY_FRAGMENT_TESTS_BIT);
+   }
+}
+
+static inline void
+mem_barrier(struct zink_batch *batch, VkPipelineStageFlags stage, VkAccessFlags src, VkAccessFlags dst)
+{
+   VkMemoryBarrier mb;
+   mb.sType = VK_STRUCTURE_TYPE_MEMORY_BARRIER;
+   mb.pNext = NULL;
+   mb.srcAccessMask = src;
+   mb.dstAccessMask = dst;
+   vkCmdPipelineBarrier(batch->state->cmdbuf, stage, stage, 0, 1, &mb, 0, NULL, 0, NULL);
+}
+
+static void
+zink_memory_barrier(struct pipe_context *pctx, unsigned flags)
+{
+   struct zink_context *ctx = zink_context(pctx);
+
+   VkPipelineStageFlags all_flags = VK_PIPELINE_STAGE_VERTEX_SHADER_BIT |
+                                    VK_PIPELINE_STAGE_TESSELLATION_CONTROL_SHADER_BIT |
+                                    VK_PIPELINE_STAGE_TESSELLATION_EVALUATION_SHADER_BIT |
+                                    VK_PIPELINE_STAGE_GEOMETRY_SHADER_BIT |
+                                    VK_PIPELINE_STAGE_FRAGMENT_SHADER_BIT |
+                                    VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT;
+
+   if (!(flags & ~PIPE_BARRIER_UPDATE))
+      return;
+
+   struct zink_batch *batch = &ctx->batch;
+   zink_end_render_pass(ctx, batch);
+
+   if (flags & PIPE_BARRIER_MAPPED_BUFFER) {
+      /* TODO: this should flush all persistent buffers in use as I think */
+   }
+
+   if (flags & (PIPE_BARRIER_SHADER_BUFFER | PIPE_BARRIER_TEXTURE |
+                PIPE_BARRIER_IMAGE | PIPE_BARRIER_GLOBAL_BUFFER))
+      mem_barrier(batch, all_flags, VK_ACCESS_SHADER_READ_BIT | VK_ACCESS_SHADER_WRITE_BIT,
+                                    VK_ACCESS_SHADER_READ_BIT | VK_ACCESS_SHADER_WRITE_BIT);
+
+   if (flags & (PIPE_BARRIER_QUERY_BUFFER | PIPE_BARRIER_IMAGE))
+      mem_barrier(batch, VK_PIPELINE_STAGE_TRANSFER_BIT,
+                  VK_ACCESS_TRANSFER_READ_BIT | VK_ACCESS_TRANSFER_WRITE_BIT,
+                  VK_ACCESS_TRANSFER_READ_BIT | VK_ACCESS_TRANSFER_WRITE_BIT);
+
+   if (flags & PIPE_BARRIER_VERTEX_BUFFER)
+      mem_barrier(batch, VK_PIPELINE_STAGE_VERTEX_INPUT_BIT,
+                  VK_ACCESS_VERTEX_ATTRIBUTE_READ_BIT,
+                  VK_ACCESS_VERTEX_ATTRIBUTE_READ_BIT);
+
+   if (flags & PIPE_BARRIER_INDEX_BUFFER)
+      mem_barrier(batch, VK_PIPELINE_STAGE_VERTEX_INPUT_BIT,
+                  VK_ACCESS_INDEX_READ_BIT,
+                  VK_ACCESS_INDEX_READ_BIT);
+
+   if (flags & (PIPE_BARRIER_CONSTANT_BUFFER | PIPE_BARRIER_IMAGE))
+      mem_barrier(batch, all_flags,
+                  VK_ACCESS_UNIFORM_READ_BIT,
+                  VK_ACCESS_UNIFORM_READ_BIT);
+
+   if (flags & PIPE_BARRIER_INDIRECT_BUFFER)
+      mem_barrier(batch, VK_PIPELINE_STAGE_DRAW_INDIRECT_BIT,
+                  VK_ACCESS_INDIRECT_COMMAND_READ_BIT,
+                  VK_ACCESS_INDIRECT_COMMAND_READ_BIT);
+
+   if (flags & PIPE_BARRIER_FRAMEBUFFER) {
+      mem_barrier(batch, VK_PIPELINE_STAGE_COLOR_ATTACHMENT_OUTPUT_BIT,
+                  VK_ACCESS_COLOR_ATTACHMENT_WRITE_BIT,
+                  VK_ACCESS_COLOR_ATTACHMENT_READ_BIT);
+      mem_barrier(batch, VK_PIPELINE_STAGE_EARLY_FRAGMENT_TESTS_BIT,
+                  VK_ACCESS_DEPTH_STENCIL_ATTACHMENT_WRITE_BIT,
+                  VK_ACCESS_DEPTH_STENCIL_ATTACHMENT_READ_BIT);
    }
+   if (flags & PIPE_BARRIER_STREAMOUT_BUFFER)
+      mem_barrier(batch, VK_PIPELINE_STAGE_TRANSFORM_FEEDBACK_BIT_EXT,
+                  VK_ACCESS_TRANSFORM_FEEDBACK_WRITE_BIT_EXT |
+                  VK_ACCESS_TRANSFORM_FEEDBACK_COUNTER_WRITE_BIT_EXT,
+                  VK_ACCESS_TRANSFORM_FEEDBACK_COUNTER_READ_BIT_EXT);
 }
 
 static void
 zink_flush_resource(struct pipe_context *pipe,
                     struct pipe_resource *resource)
 {
+   struct zink_context *ctx = zink_context(pipe);
+   struct zink_resource *res = zink_resource(resource);
+   ctx->flush_res = res;
+}
+
+void
+zink_copy_buffer(struct zink_context *ctx, struct zink_batch *batch, struct zink_resource *dst, struct zink_resource *src,
+                 unsigned dst_offset, unsigned src_offset, unsigned size)
+{
+   VkBufferCopy region;
+   region.srcOffset = src_offset;
+   region.dstOffset = dst_offset;
+   region.size = size;
+
+   if (!batch)
+      batch = zink_batch_no_rp(ctx);
+   assert(!batch->in_rp);
+   zink_batch_reference_resource_rw(batch, src, false);
+   zink_batch_reference_resource_rw(batch, dst, true);
+   util_range_add(&dst->base.b, &dst->valid_buffer_range, dst_offset, dst_offset + size);
+   zink_resource_buffer_barrier(ctx, batch, src, VK_ACCESS_TRANSFER_READ_BIT, 0);
+   zink_resource_buffer_barrier(ctx, batch, dst, VK_ACCESS_TRANSFER_WRITE_BIT, 0);
+   vkCmdCopyBuffer(batch->state->cmdbuf, src->obj->buffer, dst->obj->buffer, 1, &region);
+}
+
+void
+zink_copy_image_buffer(struct zink_context *ctx, struct zink_batch *batch, struct zink_resource *dst, struct zink_resource *src,
+                       unsigned dst_level, unsigned dstx, unsigned dsty, unsigned dstz,
+                       unsigned src_level, const struct pipe_box *src_box, enum pipe_map_flags map_flags)
+{
+   struct zink_resource *img = dst->base.b.target == PIPE_BUFFER ? src : dst;
+   struct zink_resource *buf = dst->base.b.target == PIPE_BUFFER ? dst : src;
+
+   if (!batch)
+      batch = zink_batch_no_rp(ctx);
+
+   bool buf2img = buf == src;
+
+   if (buf2img) {
+      zink_resource_image_barrier(ctx, batch, img, VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL, 0, 0);
+      zink_resource_buffer_barrier(ctx, batch, buf, VK_ACCESS_TRANSFER_READ_BIT, VK_PIPELINE_STAGE_TRANSFER_BIT);
+   } else {
+      zink_resource_image_barrier(ctx, batch, img, VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL, 0, 0);
+      zink_resource_buffer_barrier(ctx, batch, buf, VK_ACCESS_TRANSFER_WRITE_BIT, VK_PIPELINE_STAGE_TRANSFER_BIT);
+      util_range_add(&dst->base.b, &dst->valid_buffer_range, dstx, dstx + src_box->width);
+   }
+
+   VkBufferImageCopy region = {};
+   region.bufferOffset = buf2img ? src_box->x : dstx;
+   region.bufferRowLength = 0;
+   region.bufferImageHeight = 0;
+   region.imageSubresource.mipLevel = buf2img ? dst_level : src_level;
+   region.imageSubresource.layerCount = 1;
+   if (img->base.b.array_size > 1) {
+      region.imageSubresource.baseArrayLayer = buf2img ? dstz : src_box->z;
+      region.imageSubresource.layerCount = src_box->depth;
+      region.imageExtent.depth = 1;
+   } else {
+      region.imageOffset.z = buf2img ? dstz : src_box->z;
+      region.imageExtent.depth = src_box->depth;
+   }
+   region.imageOffset.x = buf2img ? dstx : src_box->x;
+   region.imageOffset.y = buf2img ? dsty : src_box->y;
+
+   region.imageExtent.width = src_box->width;
+   region.imageExtent.height = src_box->height;
+
+   zink_batch_reference_resource_rw(batch, img, buf2img);
+   zink_batch_reference_resource_rw(batch, buf, !buf2img);
+
+   /* we're using u_transfer_helper_deinterleave, which means we'll be getting PIPE_MAP_* usage
+    * to indicate whether to copy either the depth or stencil aspects
+    */
+   unsigned aspects = 0;
+   if (map_flags) {
+      assert((map_flags & (PIPE_MAP_DEPTH_ONLY | PIPE_MAP_STENCIL_ONLY)) !=
+             (PIPE_MAP_DEPTH_ONLY | PIPE_MAP_STENCIL_ONLY));
+      if (map_flags & PIPE_MAP_DEPTH_ONLY)
+         aspects = VK_IMAGE_ASPECT_DEPTH_BIT;
+      else if (map_flags & PIPE_MAP_STENCIL_ONLY)
+         aspects = VK_IMAGE_ASPECT_STENCIL_BIT;
+   }
+   if (!aspects)
+      aspects = img->aspect;
+   while (aspects) {
+      int aspect = 1 << u_bit_scan(&aspects);
+      region.imageSubresource.aspectMask = aspect;
+
+      /* this may or may not work with multisampled depth/stencil buffers depending on the driver implementation:
+       *
+       * srcImage must have a sample count equal to VK_SAMPLE_COUNT_1_BIT
+       * - vkCmdCopyImageToBuffer spec
+       *
+       * dstImage must have a sample count equal to VK_SAMPLE_COUNT_1_BIT
+       * - vkCmdCopyBufferToImage spec
+       */
+      if (buf2img)
+         vkCmdCopyBufferToImage(batch->state->cmdbuf, buf->obj->buffer, img->obj->image, img->layout, 1, &region);
+      else
+         vkCmdCopyImageToBuffer(batch->state->cmdbuf, img->obj->image, img->layout, buf->obj->buffer, 1, &region);
+   }
 }
 
 static void
@@ -1109,10 +2492,10 @@ zink_resource_copy_region(struct pipe_context *pctx,
    struct zink_resource *dst = zink_resource(pdst);
    struct zink_resource *src = zink_resource(psrc);
    struct zink_context *ctx = zink_context(pctx);
-   if (dst->base.target != PIPE_BUFFER && src->base.target != PIPE_BUFFER) {
+   if (dst->base.b.target != PIPE_BUFFER && src->base.b.target != PIPE_BUFFER) {
       VkImageCopy region = {};
-      if (util_format_get_num_planes(src->base.format) == 1 &&
-          util_format_get_num_planes(dst->base.format) == 1) {
+      if (util_format_get_num_planes(src->base.b.format) == 1 &&
+          util_format_get_num_planes(dst->base.b.format) == 1) {
       /* If neither the calling commands srcImage nor the calling commands dstImage
        * has a multi-planar image format then the aspectMask member of srcSubresource
        * and dstSubresource must match
@@ -1123,10 +2506,13 @@ zink_resource_copy_region(struct pipe_context *pctx,
       } else
          unreachable("planar formats not yet handled");
 
+      zink_fb_clears_apply_or_discard(ctx, pdst, (struct u_rect){dstx, dstx + src_box->width, dsty, dsty + src_box->height}, false);
+      zink_fb_clears_apply_region(ctx, psrc, zink_rect_from_box(src_box));
+
       region.srcSubresource.aspectMask = src->aspect;
       region.srcSubresource.mipLevel = src_level;
       region.srcSubresource.layerCount = 1;
-      if (src->base.array_size > 1) {
+      if (src->base.b.array_size > 1) {
          region.srcSubresource.baseArrayLayer = src_box->z;
          region.srcSubresource.layerCount = src_box->depth;
          region.extent.depth = 1;
@@ -1141,7 +2527,7 @@ zink_resource_copy_region(struct pipe_context *pctx,
 
       region.dstSubresource.aspectMask = dst->aspect;
       region.dstSubresource.mipLevel = dst_level;
-      if (dst->base.array_size > 1) {
+      if (dst->base.b.array_size > 1) {
          region.dstSubresource.baseArrayLayer = dstz;
          region.dstSubresource.layerCount = src_box->depth;
       } else {
@@ -1158,24 +2544,15 @@ zink_resource_copy_region(struct pipe_context *pctx,
       zink_batch_reference_resource_rw(batch, src, false);
       zink_batch_reference_resource_rw(batch, dst, true);
 
-      zink_resource_setup_transfer_layouts(batch, src, dst);
-      vkCmdCopyImage(batch->cmdbuf, src->image, src->layout,
-                     dst->image, dst->layout,
+      zink_resource_setup_transfer_layouts(ctx, src, dst);
+      vkCmdCopyImage(batch->state->cmdbuf, src->obj->image, src->layout,
+                     dst->obj->image, dst->layout,
                      1, &region);
-   } else if (dst->base.target == PIPE_BUFFER &&
-              src->base.target == PIPE_BUFFER) {
-      VkBufferCopy region;
-      region.srcOffset = src_box->x;
-      region.dstOffset = dstx;
-      region.size = src_box->width;
-
-      struct zink_batch *batch = zink_batch_no_rp(ctx);
-      zink_batch_reference_resource_rw(batch, src, false);
-      zink_batch_reference_resource_rw(batch, dst, true);
-
-      vkCmdCopyBuffer(batch->cmdbuf, src->buffer, dst->buffer, 1, &region);
+   } else if (dst->base.b.target == PIPE_BUFFER &&
+              src->base.b.target == PIPE_BUFFER) {
+      zink_copy_buffer(ctx, NULL, dst, src, dstx, src_box->x, src_box->width);
    } else
-      debug_printf("zink: TODO resource copy\n");
+      zink_copy_image_buffer(ctx, NULL, dst, src, dst_level, dstx, dsty, dstz, src_level, src_box, 0);
 }
 
 static struct pipe_stream_output_target *
@@ -1205,6 +2582,9 @@ zink_create_stream_output_target(struct pipe_context *pctx,
    t->base.buffer_offset = buffer_offset;
    t->base.buffer_size = buffer_size;
 
+   struct zink_resource *res = zink_resource(pres);
+   util_range_add(pres, &res->valid_buffer_range, buffer_offset,
+                  buffer_offset + buffer_size);
    return &t->base;
 }
 
@@ -1231,29 +2611,154 @@ zink_set_stream_output_targets(struct pipe_context *pctx,
          pipe_so_target_reference(&ctx->so_targets[i], NULL);
       ctx->num_so_targets = 0;
    } else {
-      for (unsigned i = 0; i < num_targets; i++)
+      for (unsigned i = 0; i < num_targets; i++) {
+         struct zink_so_target *t = zink_so_target(targets[i]);
          pipe_so_target_reference(&ctx->so_targets[i], targets[i]);
+         if (!t)
+            continue;
+         struct zink_resource *res = zink_resource(t->counter_buffer);
+         if (offsets[0] == (unsigned)-1)
+            ctx->xfb_barrier |= zink_resource_buffer_needs_barrier(res,
+                                                                   VK_ACCESS_TRANSFORM_FEEDBACK_COUNTER_READ_BIT_EXT,
+                                                                   VK_PIPELINE_STAGE_DRAW_INDIRECT_BIT);
+         else
+            ctx->xfb_barrier |= zink_resource_buffer_needs_barrier(res,
+                                                                   VK_ACCESS_TRANSFORM_FEEDBACK_COUNTER_WRITE_BIT_EXT,
+                                                                   VK_PIPELINE_STAGE_TRANSFORM_FEEDBACK_BIT_EXT);
+      }
       for (unsigned i = num_targets; i < ctx->num_so_targets; i++)
          pipe_so_target_reference(&ctx->so_targets[i], NULL);
       ctx->num_so_targets = num_targets;
 
-      /* emit memory barrier on next draw for synchronization */
-      if (offsets[0] == (unsigned)-1)
-         ctx->xfb_barrier = true;
       /* TODO: possibly avoid rebinding on resume if resuming from same buffers? */
       ctx->dirty_so_targets = true;
    }
 }
 
+void
+zink_rebind_framebuffer(struct zink_context *ctx, struct zink_resource *res)
+{
+   if (!ctx->framebuffer)
+      return;
+   for (unsigned i = 0; i < ctx->framebuffer->state.num_attachments; i++) {
+      if (!ctx->framebuffer->surfaces[i] ||
+          zink_resource(ctx->framebuffer->surfaces[i]->texture) != res)
+         continue;
+      zink_rebind_surface(ctx, ctx->framebuffer->surfaces[i]);
+      zink_batch_no_rp(ctx);
+   }
+   if (rebind_fb_state(ctx, res, false))
+      zink_batch_no_rp(ctx);
+}
+
+static void
+rebind_buffer(struct zink_context *ctx, struct zink_resource *res)
+{
+   for (unsigned shader = 0; shader < PIPE_SHADER_TYPES; shader++) {
+      if (!(res->bind_stages & (1 << shader)))
+         continue;
+      for (enum zink_descriptor_type type = 0; type < ZINK_DESCRIPTOR_TYPES; type++) {
+         if (!(res->bind_history & BITFIELD64_BIT(type)))
+            continue;
+
+         uint32_t usage = zink_program_get_descriptor_usage(ctx, shader, type);
+         while (usage) {
+            const int i = u_bit_scan(&usage);
+            struct zink_resource *cres = zink_get_resource_for_descriptor(ctx, type, shader, i);
+            if (res != cres)
+               continue;
+
+            switch (type) {
+            case ZINK_DESCRIPTOR_TYPE_SSBO: {
+               struct pipe_shader_buffer *ssbo = &ctx->ssbos[shader][i];
+               util_range_add(&res->base.b, &res->valid_buffer_range, ssbo->buffer_offset,
+                              ssbo->buffer_offset + ssbo->buffer_size);
+               break;
+            }
+            case ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW: {
+               struct zink_sampler_view *sampler_view = zink_sampler_view(ctx->sampler_views[shader][i]);
+               sampler_view_buffer_clear(ctx, sampler_view);
+               sampler_view->buffer_view = get_buffer_view(ctx, res, sampler_view->base.format,
+                                                           sampler_view->base.u.buf.offset, sampler_view->base.u.buf.size);
+               break;
+            }
+            case ZINK_DESCRIPTOR_TYPE_IMAGE: {
+               struct zink_image_view *image_view = &ctx->image_views[shader][i];
+               zink_descriptor_set_refs_clear(&image_view->desc_set_refs, image_view);
+               zink_buffer_view_reference(ctx, &image_view->buffer_view, NULL);
+               if (!zink_resource_object_init_storage(ctx, res)) {
+                  debug_printf("couldn't create storage image!");
+                  continue;
+               }
+               image_view->buffer_view = get_buffer_view(ctx, res, image_view->base.format,
+                                                         image_view->base.u.buf.offset, image_view->base.u.buf.size);
+               assert(image_view->buffer_view);
+               util_range_add(&res->base.b, &res->valid_buffer_range, image_view->base.u.buf.offset,
+                              image_view->base.u.buf.offset + image_view->base.u.buf.size);
+               break;
+            }
+            default:
+               break;
+            }
+
+            zink_screen(ctx->base.screen)->context_invalidate_descriptor_state(ctx, shader, type, i, 1);
+            update_descriptor_state(ctx, shader, type, i);
+         }
+      }
+   }
+}
+
+static void
+rebind_image(struct zink_context *ctx, struct zink_resource *res)
+{
+    zink_rebind_framebuffer(ctx, res);
+    if (!res->image_bind_count)
+       return;
+    for (unsigned i = 0; i < PIPE_SHADER_TYPES; i++) {
+       if (!ctx->di.valid_images[i])
+          continue;
+       unsigned last = util_last_bit(ctx->di.valid_images[i]);
+       for (unsigned j = 0; j < last; j++) {
+          if (zink_resource(ctx->image_views[i][j].base.resource) == res) {
+             zink_screen(ctx->base.screen)->context_invalidate_descriptor_state(ctx, i, ZINK_DESCRIPTOR_TYPE_IMAGE, j, 1);
+             update_descriptor_state(ctx, i, ZINK_DESCRIPTOR_TYPE_IMAGE, j);
+          }
+       }
+    }
+}
+
+void
+zink_resource_rebind(struct zink_context *ctx, struct zink_resource *res)
+{
+   if (res->base.b.target == PIPE_BUFFER)
+      rebind_buffer(ctx, res);
+   else
+      rebind_image(ctx, res);
+}
+
+static void
+zink_context_replace_buffer_storage(struct pipe_context *pctx, struct pipe_resource *dst, struct pipe_resource *src)
+{
+   struct zink_resource *d = zink_resource(dst);
+   struct zink_resource *s = zink_resource(src);
+
+   assert(d->internal_format == s->internal_format);
+   zink_resource_object_reference(zink_screen(pctx->screen), &d->obj, s->obj);
+   d->access = s->access;
+   d->access_stage = s->access_stage;
+   zink_resource_rebind(zink_context(pctx), d);
+}
+
 struct pipe_context *
 zink_context_create(struct pipe_screen *pscreen, void *priv, unsigned flags)
 {
    struct zink_screen *screen = zink_screen(pscreen);
-   struct zink_context *ctx = CALLOC_STRUCT(zink_context);
+   struct zink_context *ctx = rzalloc(NULL, struct zink_context);
    if (!ctx)
       goto fail;
 
    ctx->gfx_pipeline_state.dirty = true;
+   ctx->compute_pipeline_state.dirty = true;
 
    ctx->base.screen = pscreen;
    ctx->base.priv = priv;
@@ -1279,8 +2784,10 @@ zink_context_create(struct pipe_screen *pscreen, void *priv, unsigned flags)
    ctx->base.set_vertex_buffers = zink_set_vertex_buffers;
    ctx->base.set_viewport_states = zink_set_viewport_states;
    ctx->base.set_scissor_states = zink_set_scissor_states;
+   ctx->base.set_inlinable_constants = zink_set_inlinable_constants;
    ctx->base.set_constant_buffer = zink_set_constant_buffer;
    ctx->base.set_shader_buffers = zink_set_shader_buffers;
+   ctx->base.set_shader_images = zink_set_shader_images;
    ctx->base.set_framebuffer_state = zink_set_framebuffer_state;
    ctx->base.set_stencil_ref = zink_set_stencil_ref;
    ctx->base.set_clip_state = zink_set_clip_state;
@@ -1293,7 +2800,11 @@ zink_context_create(struct pipe_screen *pscreen, void *priv, unsigned flags)
    ctx->base.clear_texture = zink_clear_texture;
 
    ctx->base.draw_vbo = zink_draw_vbo;
+   ctx->base.launch_grid = zink_launch_grid;
+   ctx->base.fence_server_sync = zink_fence_server_sync;
    ctx->base.flush = zink_flush;
+   ctx->base.memory_barrier = zink_memory_barrier;
+   ctx->base.texture_barrier = zink_texture_barrier;
 
    ctx->base.resource_copy_region = zink_resource_copy_region;
    ctx->base.blit = zink_blit;
@@ -1306,10 +2817,24 @@ zink_context_create(struct pipe_screen *pscreen, void *priv, unsigned flags)
    zink_context_resource_init(&ctx->base);
    zink_context_query_init(&ctx->base);
 
+   _mesa_hash_table_init(&ctx->framebuffer_cache, ctx, hash_framebuffer_state, equals_framebuffer_state);
+   _mesa_hash_table_init(&ctx->surface_cache, ctx, NULL, equals_ivci);
+   _mesa_hash_table_init(&ctx->bufferview_cache, ctx, NULL, equals_bvci);
+   simple_mtx_init(&ctx->framebuffer_mtx, mtx_plain);
+   simple_mtx_init(&ctx->surface_mtx, mtx_plain);
+   simple_mtx_init(&ctx->bufferview_mtx, mtx_plain);
+   util_dynarray_init(&ctx->free_batch_states, ctx);
+   _mesa_hash_table_init(&ctx->batch_states, ctx, NULL, _mesa_key_pointer_equal);
+
+   ctx->gfx_pipeline_state.have_EXT_extended_dynamic_state = screen->info.have_EXT_extended_dynamic_state;
+
    slab_create_child(&ctx->transfer_pool, &screen->transfer_pool);
+   slab_create_child(&ctx->transfer_pool_unsync, &screen->transfer_pool);
 
    ctx->base.stream_uploader = u_upload_create_default(&ctx->base);
-   ctx->base.const_uploader = ctx->base.stream_uploader;
+   ctx->base.const_uploader = u_upload_create_default(&ctx->base);
+   for (int i = 0; i < ARRAY_SIZE(ctx->fb_clears); i++)
+      util_dynarray_init(&ctx->fb_clears[i].clears, ctx);
 
    int prim_hwsupport = 1 << PIPE_PRIM_POINTS |
                         1 << PIPE_PRIM_LINES |
@@ -1327,81 +2852,64 @@ zink_context_create(struct pipe_screen *pscreen, void *priv, unsigned flags)
    if (!ctx->blitter)
       goto fail;
 
-   VkCommandPoolCreateInfo cpci = {};
-   cpci.sType = VK_STRUCTURE_TYPE_COMMAND_POOL_CREATE_INFO;
-   cpci.queueFamilyIndex = screen->gfx_queue;
-   cpci.flags = VK_COMMAND_POOL_CREATE_RESET_COMMAND_BUFFER_BIT;
-   if (vkCreateCommandPool(screen->dev, &cpci, NULL, &ctx->cmdpool) != VK_SUCCESS)
-      goto fail;
-
-   VkCommandBufferAllocateInfo cbai = {};
-   cbai.sType = VK_STRUCTURE_TYPE_COMMAND_BUFFER_ALLOCATE_INFO;
-   cbai.commandPool = ctx->cmdpool;
-   cbai.level = VK_COMMAND_BUFFER_LEVEL_PRIMARY;
-   cbai.commandBufferCount = 1;
-
-   VkDescriptorPoolSize sizes[] = {
-      {VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER,         ZINK_BATCH_DESC_SIZE},
-      {VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER,   ZINK_BATCH_DESC_SIZE},
-      {VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER, ZINK_BATCH_DESC_SIZE},
-      {VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER,   ZINK_BATCH_DESC_SIZE},
-      {VK_DESCRIPTOR_TYPE_STORAGE_IMAGE,          ZINK_BATCH_DESC_SIZE},
-      {VK_DESCRIPTOR_TYPE_STORAGE_BUFFER,         ZINK_BATCH_DESC_SIZE},
-   };
-   VkDescriptorPoolCreateInfo dpci = {};
-   dpci.sType = VK_STRUCTURE_TYPE_DESCRIPTOR_POOL_CREATE_INFO;
-   dpci.pPoolSizes = sizes;
-   dpci.poolSizeCount = ARRAY_SIZE(sizes);
-   dpci.flags = VK_DESCRIPTOR_POOL_CREATE_FREE_DESCRIPTOR_SET_BIT;
-   dpci.maxSets = ZINK_BATCH_DESC_SIZE;
-
-   for (int i = 0; i < ARRAY_SIZE(ctx->batches); ++i) {
-      if (vkAllocateCommandBuffers(screen->dev, &cbai, &ctx->batches[i].cmdbuf) != VK_SUCCESS)
-         goto fail;
-
-      ctx->batches[i].resources = _mesa_pointer_set_create(NULL);
-      ctx->batches[i].sampler_views = _mesa_pointer_set_create(NULL);
-      ctx->batches[i].programs = _mesa_pointer_set_create(NULL);
-
-      if (!ctx->batches[i].resources || !ctx->batches[i].sampler_views ||
-          !ctx->batches[i].programs)
-         goto fail;
-
-      util_dynarray_init(&ctx->batches[i].zombie_samplers, NULL);
-
-      if (vkCreateDescriptorPool(screen->dev, &dpci, 0,
-                                 &ctx->batches[i].descpool) != VK_SUCCESS)
-         goto fail;
-
-      ctx->batches[i].batch_id = i;
-   }
-
-   vkGetDeviceQueue(screen->dev, screen->gfx_queue, 0, &ctx->queue);
-
    ctx->program_cache = _mesa_hash_table_create(NULL,
                                                 hash_gfx_program,
                                                 equals_gfx_program);
+   ctx->compute_program_cache = _mesa_hash_table_create(NULL,
+                                                _mesa_hash_uint,
+                                                _mesa_key_uint_equal);
    ctx->render_pass_cache = _mesa_hash_table_create(NULL,
                                                     hash_render_pass_state,
                                                     equals_render_pass_state);
-   if (!ctx->program_cache || !ctx->render_pass_cache)
+   if (!ctx->program_cache || !ctx->compute_program_cache || !ctx->render_pass_cache)
       goto fail;
 
    const uint8_t data[] = { 0 };
-   ctx->dummy_buffer = pipe_buffer_create_with_data(&ctx->base,
+   ctx->dummy_vertex_buffer = pipe_buffer_create_with_data(&ctx->base,
       PIPE_BIND_VERTEX_BUFFER, PIPE_USAGE_IMMUTABLE, sizeof(data), data);
-   if (!ctx->dummy_buffer)
+   if (!ctx->dummy_vertex_buffer)
+      goto fail;
+   ctx->dummy_xfb_buffer = pipe_buffer_create_with_data(&ctx->base,
+      PIPE_BIND_STREAM_OUTPUT, PIPE_USAGE_DEFAULT, sizeof(data), data);
+   if (!ctx->dummy_xfb_buffer)
       goto fail;
 
-   /* start the first batch */
-   zink_start_batch(ctx, zink_curr_batch(ctx));
+   if (!zink_descriptor_layouts_init(ctx))
+      goto fail;
 
-   return &ctx->base;
+   if (!screen->descriptors_init(ctx)) {
+      zink_screen_init_descriptor_funcs(screen, true);
+      if (!screen->descriptors_init(ctx))
+         goto fail;
+   }
+   vkGetDeviceQueue(screen->dev, screen->gfx_queue, 0, &ctx->batch.queue);
+   if (screen->threaded && screen->max_queues > 1)
+      vkGetDeviceQueue(screen->dev, screen->gfx_queue, 1, &ctx->batch.thread_queue);
+   else
+      ctx->batch.thread_queue = ctx->batch.queue;
 
-fail:
-   if (ctx) {
-      vkDestroyCommandPool(screen->dev, ctx->cmdpool, NULL);
-      FREE(ctx);
+   ctx->have_timelines = !!screen->vk_WaitSemaphores;
+   simple_mtx_init(&ctx->batch_mtx, mtx_plain);
+   incr_curr_batch(ctx);
+   zink_start_batch(ctx, &ctx->batch);
+   if (!ctx->batch.state)
+      goto fail;
+   if (!(flags & PIPE_CONTEXT_PREFER_THREADED) || flags & PIPE_CONTEXT_COMPUTE_ONLY) {
+      return &ctx->base;
+   }
+
+   struct threaded_context *tc = (struct threaded_context*)threaded_context_create(&ctx->base, &screen->transfer_pool,
+                                                     zink_context_replace_buffer_storage,
+                                                     zink_create_tc_fence_for_tc, &ctx->tc);
+
+   if (tc && (struct zink_context*)tc != ctx) {
+      tc->bytes_mapped_limit = screen->total_mem / 4;
    }
+
+   return (struct pipe_context*)tc;
+
+fail:
+   if (ctx)
+      zink_context_destroy(&ctx->base);
    return NULL;
 }
diff --git a/src/gallium/drivers/zink/zink_context.h b/src/gallium/drivers/zink/zink_context.h
index 7b83aa147af..81dbc3c7847 100644
--- a/src/gallium/drivers/zink/zink_context.h
+++ b/src/gallium/drivers/zink/zink_context.h
@@ -24,14 +24,27 @@
 #ifndef ZINK_CONTEXT_H
 #define ZINK_CONTEXT_H
 
+#define ZINK_SHADER_COUNT (PIPE_SHADER_TYPES - 1)
+#define ZINK_DEFAULT_MAX_DESCS 5000
+
+#include "zink_clear.h"
 #include "zink_pipeline.h"
 #include "zink_batch.h"
+#include "zink_compiler.h"
+#include "zink_descriptors.h"
+#include "zink_helpers.h"
+#include "zink_surface.h"
 
 #include "pipe/p_context.h"
 #include "pipe/p_state.h"
+#include "util/u_inlines.h"
+#include "util/u_rect.h"
+#include "util/u_threaded_context.h"
 
 #include "util/slab.h"
 #include "util/list.h"
+#include "util/u_dynarray.h"
+#include "util/simple_mtx.h"
 
 #include <vulkan/vulkan.h>
 
@@ -46,20 +59,66 @@ struct zink_rasterizer_state;
 struct zink_resource;
 struct zink_vertex_elements_state;
 
+
 enum zink_blit_flags {
    ZINK_BLIT_NORMAL = 1 << 0,
    ZINK_BLIT_SAVE_FS = 1 << 1,
    ZINK_BLIT_SAVE_FB = 1 << 2,
+   ZINK_BLIT_SAVE_TEXTURES = 1 << 3,
+};
+
+struct zink_buffer_view {
+   struct pipe_reference reference;
+   VkBufferViewCreateInfo bvci;
+   VkBufferView buffer_view;
+   uint32_t hash;
+   struct zink_batch_usage batch_uses;
 };
 
 struct zink_sampler_view {
    struct pipe_sampler_view base;
+   struct zink_descriptor_refs desc_set_refs;
    union {
-      VkImageView image_view;
-      VkBufferView buffer_view;
+      struct zink_surface *image_view;
+      struct zink_buffer_view *buffer_view;
    };
 };
 
+struct zink_image_view {
+   struct pipe_image_view base;
+   struct zink_descriptor_refs desc_set_refs;
+   union {
+      struct zink_surface *surface;
+      struct zink_buffer_view *buffer_view;
+   };
+};
+
+struct zink_sampler_key {
+   bool can_linear;
+   bool is_integer;
+   bool has_depth;
+   unsigned char swizzle[4];
+   bool argb;
+   bool abgr;
+};
+
+struct zink_sampler {
+   struct pipe_reference reference;
+   struct zink_descriptor_refs desc_set_refs;
+   VkSampler sampler;
+   struct zink_batch_usage batch_uses;
+   struct zink_sampler_key key;
+   bool custom_border_color;
+};
+
+struct zink_sampler_state {
+   struct pipe_sampler_state base;
+   struct zink_sampler *last_sampler;
+   /* zink_sampler_key -> zink_sampler */
+   struct hash_table samplers;
+   bool custom_border_color;
+};
+
 static inline struct zink_sampler_view *
 zink_sampler_view(struct pipe_sampler_view *pview)
 {
@@ -80,56 +139,97 @@ zink_so_target(struct pipe_stream_output_target *so_target)
    return (struct zink_so_target *)so_target;
 }
 
-#define ZINK_SHADER_COUNT (PIPE_SHADER_TYPES - 1)
+struct zink_viewport_state {
+   struct pipe_viewport_state viewport_states[PIPE_MAX_VIEWPORTS];
+   struct pipe_scissor_state scissor_states[PIPE_MAX_VIEWPORTS];
+   uint8_t num_viewports;
+};
+
+
+union zink_descriptor_surface {
+   struct zink_surface *surface;
+   struct zink_buffer_view *bufferview;
+};
 
 struct zink_context {
    struct pipe_context base;
+   struct threaded_context *tc;
    struct slab_child_pool transfer_pool;
+   struct slab_child_pool transfer_pool_unsync;
    struct blitter_context *blitter;
 
    struct pipe_device_reset_callback reset;
-
-   VkCommandPool cmdpool;
-   struct zink_batch batches[4];
    bool is_device_lost;
-   unsigned curr_batch;
 
-   VkQueue queue;
+   uint32_t curr_batch; //the current batch id
+   uint32_t last_finished;
+   struct zink_batch batch;
+   simple_mtx_t batch_mtx;
+   struct zink_fence *last_fence; //the last command buffer submitted
+   struct hash_table batch_states; //submitted batch states
+   struct util_dynarray free_batch_states; //unused batch states
+   VkDeviceSize resource_size; //the accumulated size of resources in submitted buffers
+   struct zink_resource *flush_res;
+
+   unsigned shader_has_inlinable_uniforms_mask;
+   unsigned inlinable_uniforms_dirty_mask;
+   unsigned inlinable_uniforms_valid_mask;
+   uint32_t inlinable_uniforms[PIPE_SHADER_TYPES][MAX_INLINABLE_UNIFORMS];
 
    struct pipe_constant_buffer ubos[PIPE_SHADER_TYPES][PIPE_MAX_CONSTANT_BUFFERS];
    struct pipe_shader_buffer ssbos[PIPE_SHADER_TYPES][PIPE_MAX_SHADER_BUFFERS];
-   uint32_t writable_ssbos;
+   uint32_t writable_ssbos[PIPE_SHADER_TYPES];
+   struct zink_image_view image_views[PIPE_SHADER_TYPES][PIPE_MAX_SHADER_IMAGES];
+
    struct pipe_framebuffer_state fb_state;
 
    struct zink_vertex_elements_state *element_state;
    struct zink_rasterizer_state *rast_state;
    struct zink_depth_stencil_alpha_state *dsa_state;
 
+   struct hash_table desc_set_layouts[ZINK_DESCRIPTOR_TYPES];
+
    struct zink_shader *gfx_stages[ZINK_SHADER_COUNT];
    struct zink_gfx_pipeline_state gfx_pipeline_state;
    struct hash_table *program_cache;
    struct zink_gfx_program *curr_program;
 
-   unsigned dirty_shader_stages : 6; /* mask of changed shader stages */
+   struct zink_descriptor_data *descriptor_data;
+
+   struct zink_shader *compute_stage;
+   struct zink_compute_pipeline_state compute_pipeline_state;
+   struct hash_table *compute_program_cache;
+   struct zink_compute_program *curr_compute;
+
+   unsigned dirty_shader_stages : 7; /* mask of changed shader stages */
 
    struct hash_table *render_pass_cache;
+   bool new_swapchain;
 
    struct primconvert_context *primconvert;
 
    struct zink_framebuffer *framebuffer;
+   struct zink_framebuffer_clear fb_clears[PIPE_MAX_COLOR_BUFS + 1];
+   uint16_t clears_enabled;
 
-   struct pipe_viewport_state viewport_states[PIPE_MAX_VIEWPORTS];
-   struct pipe_scissor_state scissor_states[PIPE_MAX_VIEWPORTS];
-   VkViewport viewports[PIPE_MAX_VIEWPORTS];
-   VkRect2D scissors[PIPE_MAX_VIEWPORTS];
-   struct pipe_vertex_buffer buffers[PIPE_MAX_ATTRIBS];
-   uint32_t buffers_enabled_mask;
+   struct pipe_vertex_buffer vertex_buffers[PIPE_MAX_ATTRIBS];
+
+   struct hash_table surface_cache;
+   simple_mtx_t surface_mtx;
+   struct hash_table bufferview_cache;
+   simple_mtx_t bufferview_mtx;
+   struct hash_table framebuffer_cache;
+   simple_mtx_t framebuffer_mtx;
 
-   void *sampler_states[PIPE_SHADER_TYPES][PIPE_MAX_SAMPLERS];
-   VkSampler samplers[PIPE_SHADER_TYPES][PIPE_MAX_SAMPLERS];
+   struct zink_sampler_state *sampler_states[PIPE_SHADER_TYPES][PIPE_MAX_SAMPLERS];
+   struct zink_sampler *samplers[PIPE_SHADER_TYPES][PIPE_MAX_SAMPLERS];
    unsigned num_samplers[PIPE_SHADER_TYPES];
-   struct pipe_sampler_view *image_views[PIPE_SHADER_TYPES][PIPE_MAX_SAMPLERS];
-   unsigned num_image_views[PIPE_SHADER_TYPES];
+   unsigned dirty_samplers[PIPE_SHADER_TYPES];
+   unsigned sampler_gl_clamp[PIPE_SHADER_TYPES][3]; // s,r,t
+   struct pipe_sampler_view *sampler_views[PIPE_SHADER_TYPES][PIPE_MAX_SAMPLERS];
+   unsigned num_sampler_views[PIPE_SHADER_TYPES];
+
+   struct zink_viewport_state vp_state;
 
    float line_width;
    float blend_constants[4];
@@ -148,13 +248,37 @@ struct zink_context {
    struct list_head primitives_generated_queries;
    bool queries_disabled, render_condition_active;
 
-   struct pipe_resource *dummy_buffer;
+   struct pipe_resource *dummy_vertex_buffer;
+   struct pipe_resource *dummy_xfb_buffer;
    struct pipe_resource *null_buffers[5]; /* used to create zink_framebuffer->null_surface, one buffer per samplecount */
 
+   struct {
+      /* descriptor info */
+      VkDescriptorBufferInfo ubos[PIPE_SHADER_TYPES][PIPE_MAX_CONSTANT_BUFFERS];
+      uint32_t push_valid;
+
+      VkDescriptorBufferInfo ssbos[PIPE_SHADER_TYPES][PIPE_MAX_SHADER_BUFFERS];
+
+      VkDescriptorImageInfo textures[PIPE_SHADER_TYPES][PIPE_MAX_SAMPLERS];
+      VkBufferView tbos[PIPE_SHADER_TYPES][PIPE_MAX_SAMPLERS];
+
+      VkDescriptorImageInfo images[PIPE_SHADER_TYPES][PIPE_MAX_SHADER_IMAGES];
+      VkBufferView texel_images[PIPE_SHADER_TYPES][PIPE_MAX_SHADER_IMAGES];
+      uint32_t valid_images[PIPE_SHADER_TYPES];
+
+      struct zink_resource *descriptor_res[ZINK_DESCRIPTOR_TYPES][PIPE_SHADER_TYPES][PIPE_MAX_SAMPLERS];
+      union zink_descriptor_surface sampler_surfaces[PIPE_SHADER_TYPES][PIPE_MAX_SAMPLERS];
+      union zink_descriptor_surface image_surfaces[PIPE_SHADER_TYPES][PIPE_MAX_SHADER_IMAGES];
+   } di;
+
+   unsigned num_persistent_maps;
+
    uint32_t num_so_targets;
    struct pipe_stream_output_target *so_targets[PIPE_MAX_SO_OUTPUTS];
    bool dirty_so_targets;
    bool xfb_barrier;
+   bool first_frame;
+   bool have_timelines;
 };
 
 static inline struct zink_context *
@@ -163,11 +287,12 @@ zink_context(struct pipe_context *context)
    return (struct zink_context *)context;
 }
 
-static inline struct zink_batch *
-zink_curr_batch(struct zink_context *ctx)
+static inline bool
+zink_fb_clear_enabled(const struct zink_context *ctx, unsigned idx)
 {
-   assert(ctx->curr_batch < ARRAY_SIZE(ctx->batches));
-   return ctx->batches + ctx->curr_batch;
+   if (idx == PIPE_MAX_COLOR_BUFS)
+      return ctx->clears_enabled & PIPE_CLEAR_DEPTHSTENCIL;
+   return ctx->clears_enabled & (PIPE_CLEAR_COLOR0 << idx);
 }
 
 struct zink_batch *
@@ -180,13 +305,48 @@ void
 zink_fence_wait(struct pipe_context *ctx);
 
 void
-zink_resource_barrier(VkCommandBuffer cmdbuf, struct zink_resource *res,
-                      VkImageAspectFlags aspect, VkImageLayout new_layout);
+zink_wait_on_batch(struct zink_context *ctx, uint32_t batch_id);
+
+bool
+zink_check_batch_completion(struct zink_context *ctx, uint32_t batch_id);
+
+void
+zink_flush_queue(struct zink_context *ctx);
+
+void
+zink_maybe_flush_or_stall(struct zink_context *ctx);
+
+bool
+zink_resource_access_is_write(VkAccessFlags flags);
+
+bool
+zink_resource_buffer_needs_barrier(struct zink_resource *res, VkAccessFlags flags, VkPipelineStageFlags pipeline);
+
+bool
+zink_resource_buffer_barrier_init(VkBufferMemoryBarrier *bmb, struct zink_resource *res, VkAccessFlags flags, VkPipelineStageFlags pipeline);
+
+void
+zink_resource_buffer_barrier(struct zink_context *ctx, struct zink_batch *batch, struct zink_resource *res, VkAccessFlags flags, VkPipelineStageFlags pipeline);
+
+bool
+zink_resource_image_needs_barrier(struct zink_resource *res, VkImageLayout new_layout, VkAccessFlags flags, VkPipelineStageFlags pipeline);
+bool
+zink_resource_image_barrier_init(VkImageMemoryBarrier *imb, struct zink_resource *res, VkImageLayout new_layout, VkAccessFlags flags, VkPipelineStageFlags pipeline);
+void
+zink_resource_image_barrier(struct zink_context *ctx, struct zink_batch *batch, struct zink_resource *res,
+                      VkImageLayout new_layout, VkAccessFlags flags, VkPipelineStageFlags pipeline);
+
+bool
+zink_resource_needs_barrier(struct zink_resource *res, VkImageLayout layout, VkAccessFlags flags, VkPipelineStageFlags pipeline);
+void
+zink_resource_barrier(struct zink_context *ctx, struct zink_batch *batch, struct zink_resource *res, VkImageLayout layout, VkAccessFlags flags, VkPipelineStageFlags pipeline);
 
  void
  zink_begin_render_pass(struct zink_context *ctx,
                         struct zink_batch *batch);
 
+VkPipelineStageFlags
+zink_pipeline_flags_from_stage(VkShaderStageFlagBits stage);
 
 VkShaderStageFlagBits
 zink_shader_stage(enum pipe_shader_type type);
@@ -204,18 +364,23 @@ void
 zink_blit(struct pipe_context *pctx,
           const struct pipe_blit_info *info);
 
+bool
+zink_blit_region_fills(struct u_rect region, unsigned width, unsigned height);
+
+bool
+zink_blit_region_covers(struct u_rect region, struct u_rect covers);
+
+static inline struct u_rect
+zink_rect_from_box(const struct pipe_box *box)
+{
+   return (struct u_rect){box->x, box->x + box->width, box->y, box->y + box->height};
+}
+
 void
-zink_clear(struct pipe_context *pctx,
-           unsigned buffers,
-           const struct pipe_scissor_state *scissor_state,
-           const union pipe_color_union *pcolor,
-           double depth, unsigned stencil);
+zink_resource_rebind(struct zink_context *ctx, struct zink_resource *res);
+
 void
-zink_clear_texture(struct pipe_context *ctx,
-                   struct pipe_resource *p_res,
-                   unsigned level,
-                   const struct pipe_box *box,
-                   const void *data);
+zink_rebind_framebuffer(struct zink_context *ctx, struct zink_resource *res);
 
 void
 zink_draw_vbo(struct pipe_context *pctx,
@@ -224,4 +389,56 @@ zink_draw_vbo(struct pipe_context *pctx,
               const struct pipe_draw_start_count *draws,
               unsigned num_draws);
 
+void
+zink_launch_grid(struct pipe_context *pctx, const struct pipe_grid_info *info);
+
+void
+zink_copy_buffer(struct zink_context *ctx, struct zink_batch *batch, struct zink_resource *dst, struct zink_resource *src,
+                 unsigned dst_offset, unsigned src_offset, unsigned size);
+
+void
+zink_copy_image_buffer(struct zink_context *ctx, struct zink_batch *batch, struct zink_resource *dst, struct zink_resource *src,
+                       unsigned dst_level, unsigned dstx, unsigned dsty, unsigned dstz,
+                       unsigned src_level, const struct pipe_box *src_box, enum pipe_map_flags map_flags);
+
+void
+zink_destroy_sampler(struct zink_context *ctx, struct zink_sampler *sampler);
+void
+debug_describe_zink_sampler(char *buf, const struct zink_sampler *ptr);
+
+static inline void
+zink_sampler_reference(struct zink_context *ctx,
+                             struct zink_sampler **dst,
+                             struct zink_sampler *src)
+{
+   struct zink_sampler *old_dst = dst ? *dst : NULL;
+
+   if (pipe_reference_described(old_dst ? &old_dst->reference : NULL, &src->reference,
+                                (debug_reference_descriptor)debug_describe_zink_sampler))
+      zink_destroy_sampler(ctx, old_dst);
+   if (dst) *dst = src;
+}
+
+void
+zink_update_samplers(struct zink_context *ctx, bool is_compute);
+
+void
+zink_destroy_buffer_view(struct zink_context *ctx, struct zink_buffer_view *buffer_view);
+
+void
+debug_describe_zink_buffer_view(char *buf, const struct zink_buffer_view *ptr);
+
+static inline void
+zink_buffer_view_reference(struct zink_context *ctx,
+                             struct zink_buffer_view **dst,
+                             struct zink_buffer_view *src)
+{
+   struct zink_buffer_view *old_dst = dst ? *dst : NULL;
+
+   if (pipe_reference_described(old_dst ? &old_dst->reference : NULL, &src->reference,
+                                (debug_reference_descriptor)debug_describe_zink_buffer_view))
+      zink_destroy_buffer_view(ctx, old_dst);
+   if (dst) *dst = src;
+}
+
 #endif
diff --git a/src/gallium/drivers/zink/zink_descriptors.c b/src/gallium/drivers/zink/zink_descriptors.c
new file mode 100644
index 00000000000..f803e3b354e
--- /dev/null
+++ b/src/gallium/drivers/zink/zink_descriptors.c
@@ -0,0 +1,1810 @@
+/*
+ * Copyright  2020 Mike Blumenkrantz
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ * 
+ * Authors:
+ *    Mike Blumenkrantz <michael.blumenkrantz@gmail.com>
+ */
+
+#include "tgsi/tgsi_from_mesa.h"
+
+
+
+#include "zink_context.h"
+#include "zink_descriptors.h"
+#include "zink_program.h"
+#include "zink_resource.h"
+#include "zink_screen.h"
+
+#define XXH_INLINE_ALL
+#include "util/xxhash.h"
+
+
+struct zink_descriptor_pool {
+   struct pipe_reference reference;
+   enum zink_descriptor_type type;
+   struct hash_table *desc_sets;
+   struct hash_table *free_desc_sets;
+   struct util_dynarray alloc_desc_sets;
+   VkDescriptorPool descpool;
+   struct zink_descriptor_pool_key key;
+   unsigned num_resources;
+   unsigned num_sets_allocated;
+   simple_mtx_t mtx;
+};
+
+struct zink_descriptor_set {
+   struct zink_descriptor_pool *pool;
+   struct pipe_reference reference; //incremented for batch usage
+   VkDescriptorSet desc_set;
+   uint32_t hash;
+   bool invalid;
+   bool punted;
+   bool recycled;
+   struct zink_descriptor_state_key key;
+   struct util_dynarray barriers;
+   struct zink_batch_usage batch_uses;
+#ifndef NDEBUG
+   /* for extra debug asserts */
+   unsigned num_resources;
+#endif
+   union {
+      struct zink_resource_object **res_objs;
+      struct zink_image_view **image_views;
+      struct {
+         struct zink_sampler_view **sampler_views;
+         struct zink_sampler **samplers;
+      };
+   };
+};
+
+
+struct zink_descriptor_data {
+   struct zink_descriptor_state gfx_descriptor_states[ZINK_SHADER_COUNT]; // keep incremental hashes here
+   struct zink_descriptor_state descriptor_states[2]; // gfx, compute
+   struct hash_table *descriptor_pools[ZINK_DESCRIPTOR_TYPES];
+};
+
+struct zink_program_descriptor_data {
+   struct zink_descriptor_pool *pool[ZINK_DESCRIPTOR_TYPES];
+   struct zink_descriptor_set *last_set[ZINK_DESCRIPTOR_TYPES];
+};
+
+struct zink_batch_descriptor_data {
+   struct set *desc_sets;
+};
+
+bool
+batch_ptr_add_usage(struct zink_batch *batch, struct set *s, void *ptr, struct zink_batch_usage *u);
+
+static bool
+batch_add_desc_set(struct zink_batch *batch, struct zink_descriptor_set *zds)
+{
+   if (!batch_ptr_add_usage(batch, batch->state->dd->desc_sets, zds, &zds->batch_uses))
+      return false;
+   pipe_reference(NULL, &zds->reference);
+   return true;
+}
+
+static void
+debug_describe_zink_descriptor_pool(char *buf, const struct zink_descriptor_pool *ptr)
+{
+   sprintf(buf, "zink_descriptor_pool");
+}
+
+static bool
+desc_state_equal(const void *a, const void *b)
+{
+   const struct zink_descriptor_state_key *a_k = (void*)a;
+   const struct zink_descriptor_state_key *b_k = (void*)b;
+
+   for (unsigned i = 0; i < ZINK_SHADER_COUNT; i++) {
+      if (a_k->exists[i] != b_k->exists[i])
+         return false;
+      if (a_k->exists[i] && b_k->exists[i] &&
+          a_k->state[i] != b_k->state[i])
+         return false;
+   }
+   return true;
+}
+
+static uint32_t
+desc_state_hash(const void *key)
+{
+   const struct zink_descriptor_state_key *d_key = (void*)key;
+   uint32_t hash = 0;
+   for (unsigned i = 0; i < ZINK_SHADER_COUNT; i++) {
+      if (d_key->exists[i]) {
+         if (hash)
+            hash = XXH32(&d_key->state[i], sizeof(uint32_t), hash);
+         else
+            hash = d_key->state[i];
+      }
+   }
+   return hash;
+}
+
+static void
+pop_desc_set_ref(struct zink_descriptor_set *zds, struct util_dynarray *refs)
+{
+   size_t size = sizeof(struct zink_descriptor_reference);
+   unsigned num_elements = refs->size / size;
+   for (unsigned i = 0; i < num_elements; i++) {
+      struct zink_descriptor_reference *ref = util_dynarray_element(refs, struct zink_descriptor_reference, i);
+      if (&zds->invalid == ref->invalid) {
+         memcpy(util_dynarray_element(refs, struct zink_descriptor_reference, i),
+                util_dynarray_pop_ptr(refs, struct zink_descriptor_reference), size);
+         break;
+      }
+   }
+}
+
+static void
+descriptor_set_invalidate(struct zink_descriptor_set *zds)
+{
+   zds->invalid = true;
+   for (unsigned i = 0; i < zds->pool->key.layout->num_descriptors; i++) {
+      switch (zds->pool->type) {
+      case ZINK_DESCRIPTOR_TYPE_UBO:
+      case ZINK_DESCRIPTOR_TYPE_SSBO:
+         if (zds->res_objs[i])
+            pop_desc_set_ref(zds, &zds->res_objs[i]->desc_set_refs.refs);
+         zds->res_objs[i] = NULL;
+         break;
+      case ZINK_DESCRIPTOR_TYPE_IMAGE:
+         if (zds->image_views[i])
+            pop_desc_set_ref(zds, &zds->image_views[i]->desc_set_refs.refs);
+         zds->image_views[i] = NULL;
+         break;
+      case ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW:
+         if (zds->sampler_views[i])
+            pop_desc_set_ref(zds, &zds->sampler_views[i]->desc_set_refs.refs);
+         zds->sampler_views[i] = NULL;
+         if (zds->samplers[i])
+            pop_desc_set_ref(zds, &zds->samplers[i]->desc_set_refs.refs);
+         zds->samplers[i] = NULL;
+         break;
+      default:
+         break;
+      }
+   }
+}
+
+static void
+descriptor_pool_clear(struct hash_table *ht)
+{
+ //printf("CLEAR %p\n", pg);
+   hash_table_foreach(ht, entry) {
+      struct zink_descriptor_set *zds = entry->data;
+      descriptor_set_invalidate(zds);
+   }
+   _mesa_hash_table_clear(ht, NULL);
+}
+
+static void
+descriptor_pool_free(struct zink_screen *screen, struct zink_descriptor_pool *pool)
+{
+   if (!pool)
+      return;
+   if (pool->descpool)
+      vkDestroyDescriptorPool(screen->dev, pool->descpool, NULL);
+
+   simple_mtx_lock(&pool->mtx);
+#ifndef NDEBUG
+   if (pool->desc_sets)
+      descriptor_pool_clear(pool->desc_sets);
+   if (pool->free_desc_sets)
+      descriptor_pool_clear(pool->free_desc_sets);
+#endif
+   if (pool->desc_sets)
+      _mesa_hash_table_destroy(pool->desc_sets, NULL);
+   if (pool->free_desc_sets)
+      _mesa_hash_table_destroy(pool->free_desc_sets, NULL);
+
+   simple_mtx_unlock(&pool->mtx);
+   util_dynarray_fini(&pool->alloc_desc_sets);
+   simple_mtx_destroy(&pool->mtx);
+   ralloc_free(pool);
+}
+
+static struct zink_descriptor_pool *
+descriptor_pool_create(struct zink_context *ctx, enum zink_descriptor_type type,
+                       struct zink_descriptor_layout_key *layout_key, VkDescriptorPoolSize *sizes, unsigned num_type_sizes)
+{
+   struct zink_screen *screen = zink_screen(ctx->base.screen);
+   struct zink_descriptor_pool *pool = rzalloc(NULL, struct zink_descriptor_pool);
+   if (!pool)
+      return NULL;
+   pipe_reference_init(&pool->reference, 1);
+   pool->type = type;
+   pool->key.layout = layout_key;
+   pool->key.num_type_sizes = num_type_sizes;
+   size_t types_size = num_type_sizes * sizeof(VkDescriptorPoolSize);
+   pool->key.sizes = ralloc_size(pool, types_size);
+   if (!pool->key.sizes) {
+      ralloc_free(pool);
+      return NULL;
+   }
+   memcpy(pool->key.sizes, sizes, types_size);
+   simple_mtx_init(&pool->mtx, mtx_plain);
+   for (unsigned i = 0; i < layout_key->num_descriptors; i++) {
+       pool->num_resources += layout_key->bindings[i].descriptorCount;
+   }
+   pool->desc_sets = _mesa_hash_table_create(NULL, desc_state_hash, desc_state_equal);
+   if (!pool->desc_sets)
+      goto fail;
+
+   pool->free_desc_sets = _mesa_hash_table_create(NULL, desc_state_hash, desc_state_equal);
+   if (!pool->free_desc_sets)
+      goto fail;
+
+   util_dynarray_init(&pool->alloc_desc_sets, NULL);
+
+   VkDescriptorPoolCreateInfo dpci = {};
+   dpci.sType = VK_STRUCTURE_TYPE_DESCRIPTOR_POOL_CREATE_INFO;
+   dpci.pPoolSizes = sizes;
+   dpci.poolSizeCount = num_type_sizes;
+   dpci.flags = 0;
+   dpci.maxSets = ZINK_DEFAULT_MAX_DESCS;
+   if (vkCreateDescriptorPool(screen->dev, &dpci, 0, &pool->descpool) != VK_SUCCESS) {
+      debug_printf("vkCreateDescriptorPool failed\n");
+      goto fail;
+   }
+
+   return pool;
+fail:
+   descriptor_pool_free(screen, pool);
+   return NULL;
+}
+
+static VkDescriptorSetLayout
+descriptor_layout_create(struct zink_screen *screen, unsigned t, VkDescriptorSetLayoutBinding *bindings, unsigned num_bindings)
+{
+   VkDescriptorSetLayout dsl;
+   VkDescriptorSetLayoutCreateInfo dcslci = {};
+   dcslci.sType = VK_STRUCTURE_TYPE_DESCRIPTOR_SET_LAYOUT_CREATE_INFO;
+   dcslci.pNext = NULL;
+   VkDescriptorSetLayoutBindingFlagsCreateInfo fci = {};
+   VkDescriptorBindingFlags flags[num_bindings];
+   if (screen->lazy_descriptors) {
+      /* FIXME */
+      dcslci.pNext = &fci;
+      if (t == 1)
+         dcslci.flags = VK_DESCRIPTOR_SET_LAYOUT_CREATE_PUSH_DESCRIPTOR_BIT_KHR;
+      else if (!t)
+         dcslci.flags = VK_DESCRIPTOR_SET_LAYOUT_CREATE_UPDATE_AFTER_BIND_POOL_BIT;
+      fci.sType = VK_STRUCTURE_TYPE_DESCRIPTOR_SET_LAYOUT_BINDING_FLAGS_CREATE_INFO;
+      fci.bindingCount = num_bindings;
+      fci.pBindingFlags = flags;
+      for (unsigned i = 0; i < num_bindings; i++) {
+         if (!t)
+            flags[i] = VK_DESCRIPTOR_BINDING_UPDATE_AFTER_BIND_BIT;
+         else
+            flags[i] = 0;
+      }
+   }
+   dcslci.bindingCount = num_bindings;
+   dcslci.pBindings = bindings;
+   VkDescriptorSetLayoutSupport supp;
+   supp.sType = VK_STRUCTURE_TYPE_DESCRIPTOR_SET_LAYOUT_SUPPORT;
+   supp.pNext = NULL;
+   supp.supported = VK_FALSE;
+   vkGetDescriptorSetLayoutSupport(screen->dev, &dcslci, &supp);
+   if (supp.supported == VK_FALSE) {
+      debug_printf("vkGetDescriptorSetLayoutSupport claims layout is unsupported\n");
+      return VK_NULL_HANDLE;
+   }
+   if (vkCreateDescriptorSetLayout(screen->dev, &dcslci, 0, &dsl) != VK_SUCCESS)
+      debug_printf("vkCreateDescriptorSetLayout failed\n");
+   return dsl;
+}
+
+static uint32_t
+hash_descriptor_layout(const void *key)
+{
+   uint32_t hash = 0;
+   const struct zink_descriptor_layout_key *k = key;
+   hash = XXH32(&k->num_descriptors, sizeof(unsigned), hash);
+   hash = XXH32(k->bindings, k->num_descriptors * sizeof(VkDescriptorSetLayoutBinding), hash);
+
+   return hash;
+}
+
+static bool
+equals_descriptor_layout(const void *a, const void *b)
+{
+   const struct zink_descriptor_layout_key *a_k = a;
+   const struct zink_descriptor_layout_key *b_k = b;
+   return a_k->num_descriptors == b_k->num_descriptors &&
+          !memcmp(a_k->bindings, b_k->bindings, a_k->num_descriptors * sizeof(VkDescriptorSetLayoutBinding));
+}
+
+VkDescriptorSetLayout
+zink_descriptor_util_layout_get(struct zink_context *ctx, enum zink_descriptor_type type,
+                      VkDescriptorSetLayoutBinding *bindings, unsigned num_bindings,
+                      struct zink_descriptor_layout_key **layout_key)
+{
+   struct zink_screen *screen = zink_screen(ctx->base.screen);
+   uint32_t hash = 0;
+   struct zink_descriptor_layout_key key = {
+      .num_descriptors = num_bindings,
+      .bindings = bindings,
+   };
+
+   hash = hash_descriptor_layout(&key);
+   struct hash_entry *he = _mesa_hash_table_search_pre_hashed(&ctx->desc_set_layouts[type], hash, &key);
+   if (he) {
+      *layout_key = (void*)he->key;
+      return (void*)he->data;
+   }
+
+   VkDescriptorSetLayout dsl = descriptor_layout_create(screen, type, bindings, MAX2(num_bindings, 1));
+   if (!dsl)
+      return VK_NULL_HANDLE;
+
+   struct zink_descriptor_layout_key *k = ralloc(ctx, struct zink_descriptor_layout_key);
+   k->num_descriptors = num_bindings;
+   size_t bindings_size = MAX2(num_bindings, 1) * sizeof(VkDescriptorSetLayoutBinding);
+   k->bindings = ralloc_size(k, bindings_size);
+   if (!k->bindings) {
+      ralloc_free(k);
+      vkDestroyDescriptorSetLayout(screen->dev, dsl, NULL);
+      return VK_NULL_HANDLE;
+   }
+   memcpy(k->bindings, bindings, bindings_size);
+   _mesa_hash_table_insert_pre_hashed(&ctx->desc_set_layouts[type], hash, k, dsl);
+   *layout_key = k;
+   return dsl;
+}
+
+static uint32_t
+hash_descriptor_pool(const void *key)
+{
+   uint32_t hash = 0;
+   const struct zink_descriptor_pool_key *k = key;
+   hash = XXH32(&k->num_type_sizes, sizeof(unsigned), hash);
+   hash = XXH32(&k->layout, sizeof(k->layout), hash);
+   hash = XXH32(k->sizes, k->num_type_sizes * sizeof(VkDescriptorPoolSize), hash);
+
+   return hash;
+}
+
+static bool
+equals_descriptor_pool(const void *a, const void *b)
+{
+   const struct zink_descriptor_pool_key *a_k = a;
+   const struct zink_descriptor_pool_key *b_k = b;
+   return a_k->num_type_sizes == b_k->num_type_sizes &&
+          a_k->layout == b_k->layout &&
+          !memcmp(a_k->sizes, b_k->sizes, a_k->num_type_sizes * sizeof(VkDescriptorPoolSize));
+}
+
+static struct zink_descriptor_pool *
+descriptor_pool_get(struct zink_context *ctx, enum zink_descriptor_type type,
+                    struct zink_descriptor_layout_key *layout_key, VkDescriptorPoolSize *sizes, unsigned num_type_sizes)
+{
+   uint32_t hash = 0;
+   struct zink_descriptor_pool_key key = {
+      .layout = layout_key,
+      .num_type_sizes = num_type_sizes,
+      .sizes = sizes,
+   };
+
+   hash = hash_descriptor_pool(&key);
+   struct hash_entry *he = _mesa_hash_table_search_pre_hashed(ctx->descriptor_data->descriptor_pools[type], hash, &key);
+   if (he)
+      return (void*)he->data;
+   struct zink_descriptor_pool *pool = descriptor_pool_create(ctx, type, layout_key, sizes, num_type_sizes);
+   _mesa_hash_table_insert_pre_hashed(ctx->descriptor_data->descriptor_pools[type], hash, &pool->key, pool);
+   return pool;
+}
+
+static bool
+get_invalidated_desc_set(struct zink_descriptor_set *zds)
+{
+   if (!zds->invalid)
+      return false;
+   return p_atomic_read(&zds->reference.count) == 1;
+}
+
+bool
+zink_descriptor_util_alloc_sets(struct zink_screen *screen, VkDescriptorSetLayout dsl, VkDescriptorPool pool, VkDescriptorSet *sets, unsigned num_sets)
+{
+   VkDescriptorSetAllocateInfo dsai;
+   VkDescriptorSetLayout layouts[num_sets];
+   memset((void *)&dsai, 0, sizeof(dsai));
+   dsai.sType = VK_STRUCTURE_TYPE_DESCRIPTOR_SET_ALLOCATE_INFO;
+   dsai.pNext = NULL;
+   dsai.descriptorPool = pool;
+   dsai.descriptorSetCount = num_sets;
+   for (unsigned i = 0; i < num_sets; i ++)
+      layouts[i] = dsl;
+   dsai.pSetLayouts = layouts;
+
+   if (vkAllocateDescriptorSets(screen->dev, &dsai, sets) != VK_SUCCESS) {
+      debug_printf("ZINK: %p failed to allocate descriptor set :/\n", dsl);
+      return false;
+   }
+   return true;
+}
+
+static struct zink_descriptor_set *
+allocate_desc_set(struct zink_screen *screen, struct zink_program *pg, enum zink_descriptor_type type, unsigned descs_used, bool is_compute)
+{
+   struct zink_descriptor_pool *pool = pg->descriptor_data->pool[type];
+#define DESC_BUCKET_FACTOR 10
+   unsigned bucket_size = pool->key.layout->num_descriptors ? DESC_BUCKET_FACTOR : 1;
+   unsigned desc_factor = DESC_BUCKET_FACTOR;
+   if (pool->key.layout->num_descriptors) {
+      do {
+         if (descs_used >= desc_factor)
+            bucket_size = desc_factor;
+         else
+            break;
+         desc_factor *= DESC_BUCKET_FACTOR;
+      } while (1);
+   }
+   VkDescriptorSet desc_set[bucket_size];
+   if (!zink_descriptor_util_alloc_sets(screen, pg->dsl[type], pool->descpool, desc_set, bucket_size))
+      return VK_NULL_HANDLE;
+
+   struct zink_descriptor_set *alloc = ralloc_array(pool, struct zink_descriptor_set, bucket_size);
+   assert(alloc);
+   unsigned num_resources = pool->num_resources;
+   struct zink_resource_object **res_objs = rzalloc_array(pool, struct zink_resource_object*, num_resources * bucket_size);
+   assert(res_objs);
+   void **samplers = NULL;
+   if (type == ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW) {
+      samplers = rzalloc_array(pool, void*, num_resources * bucket_size);
+      assert(samplers);
+   }
+   for (unsigned i = 0; i < bucket_size; i ++) {
+      struct zink_descriptor_set *zds = &alloc[i];
+      pipe_reference_init(&zds->reference, 1);
+      zds->pool = pool;
+      zds->hash = 0;
+      zds->batch_uses.usage = 0;
+      zds->invalid = true;
+      zds->punted = zds->recycled = false;
+      if (num_resources) {
+         util_dynarray_init(&zds->barriers, alloc);
+         if (!util_dynarray_grow(&zds->barriers, struct zink_descriptor_barrier, num_resources)) {
+            debug_printf("ZINK: %p failed to allocate descriptor set barriers :/\n", pg);
+            return NULL;
+         }
+      }
+#ifndef NDEBUG
+      zds->num_resources = num_resources;
+#endif
+      if (type == ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW) {
+         zds->sampler_views = (struct zink_sampler_view**)&res_objs[i * pool->key.layout->num_descriptors];
+         zds->samplers = (struct zink_sampler**)&samplers[i * pool->key.layout->num_descriptors];
+      } else
+         zds->res_objs = (struct zink_resource_object**)&res_objs[i * pool->key.layout->num_descriptors];
+      zds->desc_set = desc_set[i];
+      if (i > 0)
+         util_dynarray_append(&pool->alloc_desc_sets, struct zink_descriptor_set *, zds);
+   }
+   pool->num_sets_allocated += bucket_size;
+   return alloc;
+}
+
+static void
+populate_zds_key(struct zink_context *ctx, enum zink_descriptor_type type, bool is_compute,
+                 struct zink_descriptor_state_key *key) {
+   if (is_compute) {
+      for (unsigned i = 1; i < ZINK_SHADER_COUNT; i++)
+         key->exists[i] = false;
+      key->exists[0] = true;
+      key->state[0] = ctx->descriptor_data->descriptor_states[is_compute].state[type];
+   } else {
+      for (unsigned i = 0; i < ZINK_SHADER_COUNT; i++) {
+         key->exists[i] = ctx->descriptor_data->gfx_descriptor_states[i].valid[type];
+         key->state[i] = ctx->descriptor_data->gfx_descriptor_states[i].state[type];
+      }
+   }
+}
+
+static void
+punt_invalid_set(struct zink_descriptor_set *zds, struct hash_entry *he)
+{
+   /* this is no longer usable, so we punt it for now until it gets recycled */
+   assert(!zds->recycled);
+   if (!he)
+      he = _mesa_hash_table_search_pre_hashed(zds->pool->desc_sets, zds->hash, &zds->key);
+   _mesa_hash_table_remove(zds->pool->desc_sets, he);
+   //printf("%u PUNT %u\n", zds->pool->type, zds->hash);
+   zds->punted = true;
+}
+
+static struct zink_descriptor_set *
+zink_descriptor_set_get(struct zink_context *ctx,
+                               enum zink_descriptor_type type,
+                               bool is_compute,
+                               bool *cache_hit,
+                               bool *need_resource_refs)
+{
+   *cache_hit = false;
+   struct zink_descriptor_set *zds;
+   struct zink_screen *screen = zink_screen(ctx->base.screen);
+   struct zink_program *pg = is_compute ? (struct zink_program *)ctx->curr_compute : (struct zink_program *)ctx->curr_program;
+   struct zink_batch *batch = &ctx->batch;
+   struct zink_descriptor_pool *pool = pg->descriptor_data->pool[type];
+   unsigned descs_used = 1;
+   assert(type < ZINK_DESCRIPTOR_TYPES);
+   uint32_t hash = pool->key.layout->num_descriptors ? ctx->descriptor_data->descriptor_states[is_compute].state[type] : 0;
+   struct zink_descriptor_state_key key;
+   populate_zds_key(ctx, type, is_compute, &key);
+
+   simple_mtx_lock(&pool->mtx);
+   if (pg->descriptor_data->last_set[type] && pg->descriptor_data->last_set[type]->hash == hash &&
+       desc_state_equal(&pg->descriptor_data->last_set[type]->key, &key)) {
+      zds = pg->descriptor_data->last_set[type];
+      *cache_hit = !zds->invalid;
+      if (pool->key.layout->num_descriptors) {
+         if (zds->recycled) {
+            struct hash_entry *he = _mesa_hash_table_search_pre_hashed(pool->free_desc_sets, hash, &key);
+            if (he)
+               _mesa_hash_table_remove(pool->free_desc_sets, he);
+            zds->recycled = false;
+         }
+         if (zds->invalid && zink_batch_usage_exists(&zds->batch_uses)) {
+             punt_invalid_set(zds, NULL);
+             zds = NULL;
+         }
+      }
+      if (zds) {
+         //printf("%u LAST %s %u %p %u\n", type, zds->invalid ? "MISS" : "HIT", batch->state->fence.batch_id, pg, hash);
+         goto out;
+      }
+   }
+
+   if (pool->key.layout->num_descriptors) {
+      struct hash_entry *he = _mesa_hash_table_search_pre_hashed(pool->desc_sets, hash, &key);
+      bool recycled = false, punted = false;
+      if (he) {
+          zds = (void*)he->data;
+          if (zds->invalid && zink_batch_usage_exists(&zds->batch_uses)) {
+             punt_invalid_set(zds, he);
+             zds = NULL;
+             punted = true;
+          }
+      }
+      if (!he) {
+         he = _mesa_hash_table_search_pre_hashed(pool->free_desc_sets, hash, &key);
+         recycled = true;
+      }
+      if (he && !punted) {
+         zds = (void*)he->data;
+         *cache_hit = !zds->invalid;
+         //printf("%u CACHE %s%u %p %u -> %u %s\n", type, zds->invalid ? "MISS" : "HIT", batch->state->fence.batch_id, pg, he->hash, hash, recycled ? "RECYCLED" : "VALID");
+         if (recycled) {
+            /* need to migrate this entry back to the in-use hash */
+            _mesa_hash_table_remove(pool->free_desc_sets, he);
+            goto out;
+         }
+         goto quick_out;
+      }
+
+      if (util_dynarray_num_elements(&pool->alloc_desc_sets, struct zink_descriptor_set *)) {
+         /* grab one off the allocated array */
+         zds = util_dynarray_pop(&pool->alloc_desc_sets, struct zink_descriptor_set *);
+         //printf("%u POP%u %p %u (%lu left)\n", type, batch->state->fence.batch_id, pg, hash,
+                //util_dynarray_num_elements(&pool->alloc_desc_sets, struct zink_descriptor_set *));
+         goto out;
+      }
+
+      if (_mesa_hash_table_num_entries(pool->free_desc_sets)) {
+         /* try for an invalidated set first */
+         hash_table_foreach(pool->free_desc_sets, he) {
+            if (get_invalidated_desc_set(he->data)) {
+               zds = (void*)he->data;
+               assert(p_atomic_read(&zds->reference.count) == 1);
+               descriptor_set_invalidate(zds);
+               _mesa_hash_table_remove(pool->free_desc_sets, he);
+               //printf("%u REUSE%u %p %u (%u total - %u / %u)\n", type, batch->state->fence.batch_id, pg, hash,
+                      //_mesa_hash_table_num_entries(pool->desc_sets) + _mesa_hash_table_num_entries(pool->free_desc_sets) + 1,
+                      //_mesa_hash_table_num_entries(pool->desc_sets) + 1, _mesa_hash_table_num_entries(pool->free_desc_sets));
+               goto out;
+            }
+         }
+      }
+
+      if (pool->num_sets_allocated + pool->key.layout->num_descriptors > ZINK_DEFAULT_MAX_DESCS) {
+         simple_mtx_unlock(&pool->mtx);
+         zink_fence_wait(&ctx->base);
+         zink_batch_reference_program(batch, pg);
+         return zink_descriptor_set_get(ctx, type, is_compute, cache_hit, need_resource_refs);
+      }
+   } else {
+      if (pg->descriptor_data->last_set[type] && !pg->descriptor_data->last_set[type]->hash) {
+         zds = pg->descriptor_data->last_set[type];
+         *cache_hit = true;
+         goto quick_out;
+      }
+   }
+
+   zds = allocate_desc_set(screen, pg, type, descs_used, is_compute);
+   //if (pool->key.layout->num_descriptors)
+      //printf("%u NEW%u %p %u (%u total - %u / %u)\n", type, batch->state->fence.batch_id, pg, hash,
+             //_mesa_hash_table_num_entries(pool->desc_sets) + _mesa_hash_table_num_entries(pool->free_desc_sets),
+             //_mesa_hash_table_num_entries(pool->desc_sets), _mesa_hash_table_num_entries(pool->free_desc_sets));
+out:
+   zds->hash = hash;
+   populate_zds_key(ctx, type, is_compute, &zds->key);
+   zds->recycled = false;
+   if (pool->key.layout->num_descriptors)
+      _mesa_hash_table_insert_pre_hashed(pool->desc_sets, hash, &zds->key, zds);
+   else {
+      /* we can safely apply the null set to all the slots which will need it here */
+      for (unsigned i = 0; i < ZINK_DESCRIPTOR_TYPES; i++) {
+         if (pg->descriptor_data->pool[i] && !pg->descriptor_data->pool[i]->key.layout->num_descriptors)
+            pg->descriptor_data->last_set[i] = zds;
+      }
+   }
+quick_out:
+   if (pool->key.layout->num_descriptors && !*cache_hit)
+      util_dynarray_clear(&zds->barriers);
+   zds->punted = zds->invalid = false;
+   *need_resource_refs = false;
+   if (batch_add_desc_set(batch, zds)) {
+      batch->state->descs_used += pool->key.layout->num_descriptors;
+      *need_resource_refs = true;
+   }
+   pg->descriptor_data->last_set[type] = zds;
+   simple_mtx_unlock(&pool->mtx);
+
+   return zds;
+}
+
+void
+zink_descriptor_set_recycle(struct zink_descriptor_set *zds)
+{
+   struct zink_descriptor_pool *pool = zds->pool;
+   /* if desc set is still in use by a batch, don't recache */
+   uint32_t refcount = p_atomic_read(&zds->reference.count);
+   if (refcount != 1)
+      return;
+   /* this is a null set */
+   if (!pool->key.layout->num_descriptors)
+      return;
+   simple_mtx_lock(&pool->mtx);
+   if (zds->punted)
+      zds->invalid = true;
+   else {
+      /* if we've previously punted this set, then it won't have a hash or be in either of the tables */
+      struct hash_entry *he = _mesa_hash_table_search_pre_hashed(pool->desc_sets, zds->hash, &zds->key);
+      if (!he) {
+         /* desc sets can be used multiple times in the same batch */
+         simple_mtx_unlock(&pool->mtx);
+         return;
+      }
+      _mesa_hash_table_remove(pool->desc_sets, he);
+   }
+
+   if (zds->invalid) {
+      descriptor_set_invalidate(zds);
+      util_dynarray_append(&pool->alloc_desc_sets, struct zink_descriptor_set *, zds);
+   } else {
+      zds->recycled = true;
+      _mesa_hash_table_insert_pre_hashed(pool->free_desc_sets, zds->hash, &zds->key, zds);
+   }
+   simple_mtx_unlock(&pool->mtx);
+}
+
+
+static void
+desc_set_ref_add(struct zink_descriptor_set *zds, struct zink_descriptor_refs *refs, void **ref_ptr, void *ptr)
+{
+   struct zink_descriptor_reference ref = {ref_ptr, &zds->invalid};
+   *ref_ptr = ptr;
+   if (ptr)
+      util_dynarray_append(&refs->refs, struct zink_descriptor_reference, ref);
+}
+
+static void
+zink_image_view_desc_set_add(struct zink_image_view *image_view, struct zink_descriptor_set *zds, unsigned idx)
+{
+   desc_set_ref_add(zds, &image_view->desc_set_refs, (void**)&zds->image_views[idx], image_view);
+}
+
+static void
+zink_sampler_desc_set_add(struct zink_sampler *sampler, struct zink_descriptor_set *zds, unsigned idx)
+{
+   desc_set_ref_add(zds, &sampler->desc_set_refs, (void**)&zds->samplers[idx], sampler);
+}
+
+static void
+zink_sampler_view_desc_set_add(struct zink_sampler_view *sampler_view, struct zink_descriptor_set *zds, unsigned idx)
+{
+   desc_set_ref_add(zds, &sampler_view->desc_set_refs, (void**)&zds->sampler_views[idx], sampler_view);
+}
+
+static void
+zink_resource_desc_set_add(struct zink_resource *res, struct zink_descriptor_set *zds, unsigned idx)
+{
+   desc_set_ref_add(zds, res ? &res->obj->desc_set_refs : NULL, (void**)&zds->res_objs[idx], res ? res->obj : NULL);
+}
+
+void
+zink_descriptor_set_refs_clear(struct zink_descriptor_refs *refs, void *ptr)
+{
+   util_dynarray_foreach(&refs->refs, struct zink_descriptor_reference, ref) {
+      if (*ref->ref == ptr) {
+         *ref->invalid = true;
+         *ref->ref = NULL;
+      }
+   }
+   util_dynarray_fini(&refs->refs);
+}
+
+static inline void
+zink_descriptor_pool_reference(struct zink_screen *screen,
+                               struct zink_descriptor_pool **dst,
+                               struct zink_descriptor_pool *src)
+{
+   struct zink_descriptor_pool *old_dst = dst ? *dst : NULL;
+
+   if (pipe_reference_described(old_dst ? &old_dst->reference : NULL, &src->reference,
+                                (debug_reference_descriptor)debug_describe_zink_descriptor_pool))
+      descriptor_pool_free(screen, old_dst);
+   if (dst) *dst = src;
+}
+
+bool
+zink_descriptor_program_init(struct zink_context *ctx, struct zink_program *pg)
+{
+   VkDescriptorSetLayoutBinding bindings[ZINK_DESCRIPTOR_TYPES][PIPE_SHADER_TYPES * 32];
+   unsigned num_bindings[ZINK_DESCRIPTOR_TYPES] = {};
+
+   VkDescriptorPoolSize sizes[6] = {};
+   int type_map[12];
+   unsigned num_types = 0;
+   memset(type_map, -1, sizeof(type_map));
+
+   struct zink_shader **stages;
+   if (pg->is_compute)
+      stages = &((struct zink_compute_program*)pg)->shader;
+   else
+      stages = ((struct zink_gfx_program*)pg)->shaders;
+
+   for (int i = 0; i < (pg->is_compute ? 1 : ZINK_SHADER_COUNT); i++) {
+      struct zink_shader *shader = stages[i];
+      if (!shader)
+         continue;
+
+      VkShaderStageFlagBits stage_flags = zink_shader_stage(pipe_shader_type_from_mesa(shader->nir->info.stage));
+      for (int j = 0; j < ZINK_DESCRIPTOR_TYPES; j++) {
+         for (int k = 0; k < shader->num_bindings[j]; k++) {
+            assert(num_bindings[j] < ARRAY_SIZE(bindings[j]));
+            bindings[j][num_bindings[j]].binding = shader->bindings[j][k].binding;
+            bindings[j][num_bindings[j]].descriptorType = shader->bindings[j][k].type;
+            bindings[j][num_bindings[j]].descriptorCount = shader->bindings[j][k].size;
+            bindings[j][num_bindings[j]].stageFlags = stage_flags;
+            bindings[j][num_bindings[j]].pImmutableSamplers = NULL;
+            if (type_map[shader->bindings[j][k].type] == -1) {
+               type_map[shader->bindings[j][k].type] = num_types++;
+               sizes[type_map[shader->bindings[j][k].type]].type = shader->bindings[j][k].type;
+            }
+            sizes[type_map[shader->bindings[j][k].type]].descriptorCount += shader->bindings[j][k].size;
+            ++num_bindings[j];
+         }
+      }
+   }
+
+   unsigned total_descs = 0;
+   for (unsigned i = 0; i < ZINK_DESCRIPTOR_TYPES; i++) {
+      total_descs += num_bindings[i];;
+   }
+   if (!total_descs) {
+      pg->layout = zink_pipeline_layout_create(zink_screen(ctx->base.screen), pg, pg->is_compute);
+      return !!pg->layout;
+   }
+
+   if (!pg->descriptor_data)
+      pg->descriptor_data = rzalloc(pg, struct zink_program_descriptor_data);
+   if (!pg->descriptor_data)
+      return false;
+
+   for (int i = 0; i < num_types; i++)
+      sizes[i].descriptorCount *= ZINK_DEFAULT_MAX_DESCS;
+
+   bool found_descriptors = false;
+   struct zink_descriptor_layout_key *layout_key[ZINK_DESCRIPTOR_TYPES] = {};
+   for (unsigned i = ZINK_DESCRIPTOR_TYPES - 1; i < ZINK_DESCRIPTOR_TYPES; i--) {
+      if (!num_bindings[i]) {
+         if (!found_descriptors)
+            continue;
+         VkDescriptorSetLayoutBinding null_binding;
+         null_binding.binding = 1;
+         null_binding.descriptorType = VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER;
+         null_binding.descriptorCount = 1;
+         null_binding.pImmutableSamplers = NULL;
+         null_binding.stageFlags = VK_SHADER_STAGE_VERTEX_BIT | VK_SHADER_STAGE_FRAGMENT_BIT |
+                                   VK_SHADER_STAGE_GEOMETRY_BIT | VK_SHADER_STAGE_TESSELLATION_CONTROL_BIT |
+                                   VK_SHADER_STAGE_TESSELLATION_EVALUATION_BIT | VK_SHADER_STAGE_COMPUTE_BIT;
+         pg->dsl[i] = zink_descriptor_util_layout_get(ctx, i, &null_binding, 0, &layout_key[i]);
+         if (!pg->dsl[i])
+            return false;
+         VkDescriptorPoolSize null_size = {VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER, ZINK_DEFAULT_MAX_DESCS};
+         struct zink_descriptor_pool *pool = descriptor_pool_get(ctx, i, layout_key[i], &null_size, 1);
+         if (!pool)
+            return false;
+         zink_descriptor_pool_reference(zink_screen(ctx->base.screen), &pg->descriptor_data->pool[i], pool);
+         pg->num_dsl++;
+         continue;
+      }
+      found_descriptors = true;
+
+      VkDescriptorPoolSize type_sizes[2] = {};
+      int num_type_sizes = 0;
+      switch (i) {
+      case ZINK_DESCRIPTOR_TYPE_UBO:
+         if (type_map[VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER] != -1) {
+            type_sizes[num_type_sizes] = sizes[type_map[VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER]];
+            num_type_sizes++;
+         }
+         if (type_map[VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER_DYNAMIC] != -1) {
+            type_sizes[num_type_sizes] = sizes[type_map[VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER_DYNAMIC]];
+            num_type_sizes++;
+         }
+         break;
+      case ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW:
+         if (type_map[VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER] != -1) {
+            type_sizes[num_type_sizes] = sizes[type_map[VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER]];
+            num_type_sizes++;
+         }
+         if (type_map[VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER] != -1) {
+            type_sizes[num_type_sizes] = sizes[type_map[VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER]];
+            num_type_sizes++;
+         }
+         break;
+      case ZINK_DESCRIPTOR_TYPE_SSBO:
+         if (type_map[VK_DESCRIPTOR_TYPE_STORAGE_BUFFER] != -1) {
+            num_type_sizes = 1;
+            type_sizes[0] = sizes[type_map[VK_DESCRIPTOR_TYPE_STORAGE_BUFFER]];
+         }
+         break;
+      case ZINK_DESCRIPTOR_TYPE_IMAGE:
+         if (type_map[VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER] != -1) {
+            type_sizes[num_type_sizes] = sizes[type_map[VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER]];
+            num_type_sizes++;
+         }
+         if (type_map[VK_DESCRIPTOR_TYPE_STORAGE_IMAGE] != -1) {
+            type_sizes[num_type_sizes] = sizes[type_map[VK_DESCRIPTOR_TYPE_STORAGE_IMAGE]];
+            num_type_sizes++;
+         }
+         break;
+      }
+      pg->dsl[i] = zink_descriptor_util_layout_get(ctx, i, bindings[i], num_bindings[i], &layout_key[i]);
+      if (!pg->dsl[i])
+         return false;
+      struct zink_descriptor_pool *pool = descriptor_pool_get(ctx, i, layout_key[i], type_sizes, num_type_sizes);
+      if (!pool)
+         return false;
+      zink_descriptor_pool_reference(zink_screen(ctx->base.screen), &pg->descriptor_data->pool[i], pool);
+      pg->num_dsl++;
+   }
+
+   pg->layout = zink_pipeline_layout_create(zink_screen(ctx->base.screen), pg, pg->is_compute);
+   return !!pg->layout;
+}
+
+void
+zink_descriptor_program_deinit(struct zink_screen *screen, struct zink_program *pg)
+{
+   for (unsigned i = 0; i < ZINK_DESCRIPTOR_TYPES; i++)
+      zink_descriptor_pool_reference(screen, &pg->descriptor_data->pool[i], NULL);
+}
+
+static void
+zink_descriptor_pool_deinit(struct zink_context *ctx)
+{
+   for (unsigned i = 0; i < ZINK_DESCRIPTOR_TYPES; i++) {
+      hash_table_foreach(ctx->descriptor_data->descriptor_pools[i], entry) {
+         struct zink_descriptor_pool *pool = (void*)entry->data;
+         zink_descriptor_pool_reference(zink_screen(ctx->base.screen), &pool, NULL);
+      }
+      _mesa_hash_table_destroy(ctx->descriptor_data->descriptor_pools[i], NULL);
+   }
+}
+
+static bool
+zink_descriptor_pool_init(struct zink_context *ctx)
+{
+   for (unsigned i = 0; i < ZINK_DESCRIPTOR_TYPES; i++) {
+      ctx->descriptor_data->descriptor_pools[i] = _mesa_hash_table_create(ctx, hash_descriptor_pool, equals_descriptor_pool);
+      if (!ctx->descriptor_data->descriptor_pools[i])
+         return false;
+   }
+   return true;
+}
+
+
+static void
+desc_set_res_add(struct zink_descriptor_set *zds, struct zink_resource *res, unsigned int i, bool cache_hit)
+{
+   /* if we got a cache hit, we have to verify that the cached set is still valid;
+    * we store the vk resource to the set here to avoid a more complex and costly mechanism of maintaining a
+    * hash table on every resource with the associated descriptor sets that then needs to be iterated through
+    * whenever a resource is destroyed
+    */
+   assert(!cache_hit || zds->res_objs[i] == (res ? res->obj : NULL));
+   if (!cache_hit)
+      zink_resource_desc_set_add(res, zds, i);
+}
+
+static void
+desc_set_sampler_add(struct zink_context *ctx, struct zink_descriptor_set *zds, struct zink_sampler_view *sv,
+                     struct zink_sampler *sampler, unsigned int i, bool is_buffer, bool cache_hit)
+{
+   /* if we got a cache hit, we have to verify that the cached set is still valid;
+    * we store the vk resource to the set here to avoid a more complex and costly mechanism of maintaining a
+    * hash table on every resource with the associated descriptor sets that then needs to be iterated through
+    * whenever a resource is destroyed
+    */
+#ifndef NDEBUG
+   uint32_t cur_hash = zink_get_sampler_view_hash(ctx, zds->sampler_views[i], is_buffer);
+   uint32_t new_hash = zink_get_sampler_view_hash(ctx, sv, is_buffer);
+#endif
+   assert(!cache_hit || cur_hash == new_hash);
+   assert(!cache_hit || zds->samplers[i] == sampler);
+   if (!cache_hit) {
+      zink_sampler_view_desc_set_add(sv, zds, i);
+      zink_sampler_desc_set_add(sampler, zds, i);
+   }
+}
+
+static void
+desc_set_image_add(struct zink_context *ctx, struct zink_descriptor_set *zds, struct zink_image_view *image_view,
+                   unsigned int i, bool is_buffer, bool cache_hit)
+{
+   /* if we got a cache hit, we have to verify that the cached set is still valid;
+    * we store the vk resource to the set here to avoid a more complex and costly mechanism of maintaining a
+    * hash table on every resource with the associated descriptor sets that then needs to be iterated through
+    * whenever a resource is destroyed
+    */
+#ifndef NDEBUG
+   uint32_t cur_hash = zink_get_image_view_hash(ctx, zds->image_views[i], is_buffer);
+   uint32_t new_hash = zink_get_image_view_hash(ctx, image_view, is_buffer);
+#endif
+   assert(!cache_hit || cur_hash == new_hash);
+   if (!cache_hit)
+      zink_image_view_desc_set_add(image_view, zds, i);
+}
+
+static bool
+barrier_equals(const void *a, const void *b)
+{
+   const struct zink_descriptor_barrier *t1 = a, *t2 = b;
+   if (t1->res != t2->res)
+      return false;
+   if ((t1->access & t2->access) != t2->access)
+      return false;
+   if (t1->layout != t2->layout)
+      return false;
+   return true;
+}
+
+static uint32_t
+barrier_hash(const void *key)
+{
+   return _mesa_hash_data(key, offsetof(struct zink_descriptor_barrier, stage));
+}
+
+static inline void
+add_barrier(struct zink_resource *res, VkImageLayout layout, VkAccessFlags flags, enum pipe_shader_type stage, struct util_dynarray *barriers, struct set *ht)
+{
+   VkPipelineStageFlags pipeline = zink_pipeline_flags_from_stage(zink_shader_stage(stage));
+   struct zink_descriptor_barrier key = {res, layout, flags, 0}, *t;
+
+   uint32_t hash = barrier_hash(&key);
+   struct set_entry *entry = _mesa_set_search_pre_hashed(ht, hash, &key);
+   if (entry)
+      t = (struct zink_descriptor_barrier*)entry->key;
+   else {
+      util_dynarray_append(barriers, struct zink_descriptor_barrier, key);
+      t = util_dynarray_element(barriers, struct zink_descriptor_barrier,
+                                util_dynarray_num_elements(barriers, struct zink_descriptor_barrier) - 1);
+      t->stage = 0;
+      t->layout = layout;
+      t->res = res;
+      t->access = flags;
+      _mesa_set_add_pre_hashed(ht, hash, t);
+   }
+   t->stage |= pipeline;
+}
+
+static int
+cmp_dynamic_offset_binding(const void *a, const void *b)
+{
+   const uint32_t *binding_a = a, *binding_b = b;
+   return *binding_a - *binding_b;
+}
+
+static void
+write_descriptors(struct zink_context *ctx, unsigned num_wds, VkWriteDescriptorSet *wds, bool cache_hit)
+{
+   struct zink_screen *screen = zink_screen(ctx->base.screen);
+
+   if (!cache_hit && num_wds)
+      vkUpdateDescriptorSets(screen->dev, num_wds, wds, 0, NULL);
+}
+
+static unsigned
+init_write_descriptor(struct zink_shader *shader, struct zink_descriptor_set *zds, enum zink_descriptor_type type, int idx, VkWriteDescriptorSet *wd, unsigned num_wds)
+{
+    wd->sType = VK_STRUCTURE_TYPE_WRITE_DESCRIPTOR_SET;
+    wd->pNext = NULL;
+    wd->dstBinding = shader->bindings[type][idx].binding;
+    wd->dstArrayElement = 0;
+    wd->descriptorCount = shader->bindings[type][idx].size;
+    wd->descriptorType = shader->bindings[type][idx].type;
+    wd->dstSet = zds->desc_set;
+    return num_wds + 1;
+}
+
+static void
+update_ubo_descriptors(struct zink_context *ctx, struct zink_descriptor_set *zds,
+                       bool is_compute, bool cache_hit, bool need_resource_refs,
+                       uint32_t *dynamic_offsets, unsigned *dynamic_offset_idx)
+{
+   struct zink_program *pg = is_compute ? (struct zink_program *)ctx->curr_compute : (struct zink_program *)ctx->curr_program;
+   struct zink_screen *screen = zink_screen(ctx->base.screen);
+   unsigned num_descriptors = pg->descriptor_data->pool[ZINK_DESCRIPTOR_TYPE_UBO]->key.layout->num_descriptors;
+   unsigned num_bindings = zds->pool->num_resources;
+   VkWriteDescriptorSet wds[num_descriptors];
+   VkDescriptorBufferInfo buffer_infos[num_bindings];
+   unsigned num_wds = 0;
+   unsigned num_buffer_info = 0;
+   unsigned num_resources = 0;
+   struct zink_shader **stages;
+   struct {
+      uint32_t binding;
+      uint32_t offset;
+   } dynamic_buffers[PIPE_MAX_CONSTANT_BUFFERS];
+   unsigned dynamic_offset_count = 0;
+   struct set *ht = NULL;
+   if (!cache_hit) {
+      ht = _mesa_set_create(NULL, barrier_hash, barrier_equals);
+      _mesa_set_resize(ht, num_bindings);
+   }
+
+   unsigned num_stages = is_compute ? 1 : ZINK_SHADER_COUNT;
+   if (is_compute)
+      stages = &ctx->curr_compute->shader;
+   else
+      stages = &ctx->gfx_stages[0];
+
+unsigned loop_count = 0;
+   for (int i = 0; i < num_stages; i++) {
+      struct zink_shader *shader = stages[i];
+      if (!shader)
+         continue;
+      enum pipe_shader_type stage = pipe_shader_type_from_mesa(shader->nir->info.stage);
+
+      for (int j = 0; j < shader->num_bindings[ZINK_DESCRIPTOR_TYPE_UBO]; j++) {
+       loop_count++;
+         int index = shader->bindings[ZINK_DESCRIPTOR_TYPE_UBO][j].index;
+         assert(shader->bindings[ZINK_DESCRIPTOR_TYPE_UBO][j].type == VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER ||
+             shader->bindings[ZINK_DESCRIPTOR_TYPE_UBO][j].type == VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER_DYNAMIC);
+         assert(ctx->ubos[stage][index].buffer_size <= screen->info.props.limits.maxUniformBufferRange);
+         struct zink_resource *res = zink_resource(ctx->ubos[stage][index].buffer);
+         assert(num_resources < num_bindings);
+         assert(!res || ctx->ubos[stage][index].buffer_size > 0);
+         assert(!res || ctx->ubos[stage][index].buffer);
+         desc_set_res_add(zds, res, num_resources++, cache_hit);
+         assert(num_buffer_info < num_bindings);
+         buffer_infos[num_buffer_info].buffer = res ? res->obj->buffer :
+                                                (screen->info.rb2_feats.nullDescriptor ?
+                                                 VK_NULL_HANDLE :
+                                                 zink_resource(ctx->dummy_vertex_buffer)->obj->buffer);
+         if (shader->bindings[ZINK_DESCRIPTOR_TYPE_UBO][j].type == VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER_DYNAMIC) {
+            buffer_infos[num_buffer_info].offset = 0;
+            /* we're storing this to qsort later */
+            dynamic_buffers[dynamic_offset_count].binding = shader->bindings[ZINK_DESCRIPTOR_TYPE_UBO][j].binding;
+            dynamic_buffers[dynamic_offset_count++].offset = res ? ctx->ubos[stage][index].buffer_offset : 0;
+         } else
+            buffer_infos[num_buffer_info].offset = res ? ctx->ubos[stage][index].buffer_offset : 0;
+         buffer_infos[num_buffer_info].range = res ? ctx->ubos[stage][index].buffer_size : VK_WHOLE_SIZE;
+         if (!cache_hit && res)
+            add_barrier(res, 0, VK_ACCESS_UNIFORM_READ_BIT, stage, &zds->barriers, ht);
+         wds[num_wds].pBufferInfo = buffer_infos + num_buffer_info;
+         ++num_buffer_info;
+
+         num_wds = init_write_descriptor(shader, zds, ZINK_DESCRIPTOR_TYPE_UBO, j, &wds[num_wds], num_wds);
+      }
+   }
+   //printf("UBO LOOPS %u ", loop_count);
+   _mesa_set_destroy(ht, NULL);
+   /* Values are taken from pDynamicOffsets in an order such that all entries for set N come before set N+1;
+    * within a set, entries are ordered by the binding numbers in the descriptor set layouts
+    * - vkCmdBindDescriptorSets spec
+    *
+    * because of this, we have to sort all the dynamic offsets by their associated binding to ensure they
+    * match what the driver expects
+    */
+   if (dynamic_offset_count > 1)
+      qsort(dynamic_buffers, dynamic_offset_count, sizeof(uint32_t) * 2, cmp_dynamic_offset_binding);
+   for (int i = 0; i < dynamic_offset_count; i++)
+      dynamic_offsets[i] = dynamic_buffers[i].offset;
+   *dynamic_offset_idx = dynamic_offset_count;
+
+   write_descriptors(ctx, num_wds, wds, cache_hit);
+}
+
+static void
+update_ssbo_descriptors(struct zink_context *ctx, struct zink_descriptor_set *zds,
+                        bool is_compute, bool cache_hit, bool need_resource_refs)
+{
+   struct zink_program *pg = is_compute ? (struct zink_program *)ctx->curr_compute : (struct zink_program *)ctx->curr_program;
+   struct zink_screen *screen = zink_screen(ctx->base.screen);
+   unsigned num_descriptors = pg->descriptor_data->pool[ZINK_DESCRIPTOR_TYPE_SSBO]->key.layout->num_descriptors;
+   unsigned num_bindings = zds->pool->num_resources;
+   VkWriteDescriptorSet wds[num_descriptors];
+   VkDescriptorBufferInfo buffer_infos[num_bindings];
+   unsigned num_wds = 0;
+   unsigned num_buffer_info = 0;
+   unsigned num_resources = 0;
+   struct zink_shader **stages;
+   struct set *ht = NULL;
+   if (!cache_hit) {
+      ht = _mesa_set_create(NULL, barrier_hash, barrier_equals);
+      _mesa_set_resize(ht, num_bindings);
+   }
+
+   unsigned num_stages = is_compute ? 1 : ZINK_SHADER_COUNT;
+   if (is_compute)
+      stages = &ctx->curr_compute->shader;
+   else
+      stages = &ctx->gfx_stages[0];
+unsigned loop_count = 0;
+   for (int i = 0; (!cache_hit || need_resource_refs) && i < num_stages; i++) {
+      struct zink_shader *shader = stages[i];
+      if (!shader)
+         continue;
+      enum pipe_shader_type stage = pipe_shader_type_from_mesa(shader->nir->info.stage);
+
+      for (int j = 0; j < shader->num_bindings[ZINK_DESCRIPTOR_TYPE_SSBO]; j++) {
+       loop_count++;
+         int index = shader->bindings[ZINK_DESCRIPTOR_TYPE_SSBO][j].index;
+         assert(shader->bindings[ZINK_DESCRIPTOR_TYPE_SSBO][j].type == VK_DESCRIPTOR_TYPE_STORAGE_BUFFER);
+         assert(num_resources < num_bindings);
+         struct zink_resource *res = zink_resource(ctx->ssbos[stage][index].buffer);
+         desc_set_res_add(zds, res, num_resources++, cache_hit);
+         if (res) {
+            assert(ctx->ssbos[stage][index].buffer_size > 0);
+            assert(ctx->ssbos[stage][index].buffer_size <= screen->info.props.limits.maxStorageBufferRange);
+            assert(num_buffer_info < num_bindings);
+            unsigned flag = VK_ACCESS_SHADER_READ_BIT;
+            if (ctx->writable_ssbos[stage] & (1 << index))
+               flag |= VK_ACCESS_SHADER_WRITE_BIT;
+            if (!cache_hit)
+               add_barrier(res, 0, flag, stage, &zds->barriers, ht);
+            buffer_infos[num_buffer_info].buffer = res->obj->buffer;
+            buffer_infos[num_buffer_info].offset = ctx->ssbos[stage][index].buffer_offset;
+            buffer_infos[num_buffer_info].range  = ctx->ssbos[stage][index].buffer_size;
+         } else {
+            assert(screen->info.rb2_feats.nullDescriptor);
+            buffer_infos[num_buffer_info].buffer = VK_NULL_HANDLE;
+            buffer_infos[num_buffer_info].offset = 0;
+            buffer_infos[num_buffer_info].range  = VK_WHOLE_SIZE;
+         }
+         wds[num_wds].pBufferInfo = buffer_infos + num_buffer_info;
+         ++num_buffer_info;
+
+         num_wds = init_write_descriptor(shader, zds, ZINK_DESCRIPTOR_TYPE_SSBO, j, &wds[num_wds], num_wds);
+      }
+   }
+   //printf("SSBO LOOPS %u ", loop_count);
+   _mesa_set_destroy(ht, NULL);
+   write_descriptors(ctx, num_wds, wds, cache_hit);
+}
+
+static void
+handle_image_descriptor(struct zink_screen *screen, struct zink_resource *res, enum zink_descriptor_type type, VkDescriptorType vktype, VkWriteDescriptorSet *wd,
+                        VkImageLayout layout, unsigned *num_image_info, VkDescriptorImageInfo *image_info,
+                        unsigned *num_buffer_info, VkBufferView *buffer_info,
+                        struct zink_sampler *sampler,
+                        VkImageView imageview, VkBufferView bufferview, bool do_set)
+{
+    if (!res) {
+        /* if we're hitting this assert often, we can probably just throw a junk buffer in since
+         * the results of this codepath are undefined in ARB_texture_buffer_object spec
+         */
+        assert(screen->info.rb2_feats.nullDescriptor);
+        
+        switch (vktype) {
+        case VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER:
+        case VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER:
+           *buffer_info = VK_NULL_HANDLE;
+           if (do_set)
+              wd->pTexelBufferView = buffer_info;
+           ++(*num_buffer_info);
+           break;
+        case VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER:
+        case VK_DESCRIPTOR_TYPE_STORAGE_IMAGE:
+           image_info->imageLayout = VK_IMAGE_LAYOUT_UNDEFINED;
+           image_info->imageView = VK_NULL_HANDLE;
+           if (sampler)
+              image_info->sampler = sampler->sampler;
+           if (do_set)
+              wd->pImageInfo = image_info;
+           ++(*num_image_info);
+           break;
+        default:
+           unreachable("unknown descriptor type");
+        }
+     } else if (res->base.b.target != PIPE_BUFFER) {
+        assert(layout != VK_IMAGE_LAYOUT_UNDEFINED);
+        image_info->imageLayout = layout;
+        image_info->imageView = imageview;
+        if (sampler)
+           image_info->sampler = sampler->sampler;
+        if (do_set)
+           wd->pImageInfo = image_info;
+        ++(*num_image_info);
+     } else {
+        if (do_set)
+           wd->pTexelBufferView = buffer_info;
+        *buffer_info = bufferview;
+        ++(*num_buffer_info);
+     }
+}
+
+static void
+update_sampler_descriptors(struct zink_context *ctx, struct zink_descriptor_set *zds,
+                           bool is_compute, bool cache_hit, bool need_resource_refs)
+{
+   struct zink_program *pg = is_compute ? (struct zink_program *)ctx->curr_compute : (struct zink_program *)ctx->curr_program;
+   struct zink_screen *screen = zink_screen(ctx->base.screen);
+   unsigned num_descriptors = pg->descriptor_data->pool[ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW]->key.layout->num_descriptors;
+   unsigned num_bindings = zds->pool->num_resources;
+   VkWriteDescriptorSet wds[num_descriptors];
+   VkDescriptorImageInfo image_infos[num_bindings];
+   VkBufferView buffer_views[num_bindings];
+   unsigned num_wds = 0;
+   unsigned num_image_info = 0;
+   unsigned num_buffer_info = 0;
+   unsigned num_resources = 0;
+   struct zink_shader **stages;
+   struct set *ht = NULL;
+   if (!cache_hit) {
+      ht = _mesa_set_create(NULL, barrier_hash, barrier_equals);
+      _mesa_set_resize(ht, num_bindings);
+   }
+
+   unsigned num_stages = is_compute ? 1 : ZINK_SHADER_COUNT;
+   if (is_compute)
+      stages = &ctx->curr_compute->shader;
+   else
+      stages = &ctx->gfx_stages[0];
+unsigned loop_count = 0;
+   for (int i = 0; (!cache_hit || need_resource_refs) && i < num_stages; i++) {
+      struct zink_shader *shader = stages[i];
+      if (!shader)
+         continue;
+      enum pipe_shader_type stage = pipe_shader_type_from_mesa(shader->nir->info.stage);
+
+      for (int j = 0; j < shader->num_bindings[ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW]; j++) {
+         int index = shader->bindings[ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW][j].index;
+         assert(shader->bindings[ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW][j].type == VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER ||
+                shader->bindings[ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW][j].type == VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER);
+
+         for (unsigned k = 0; k < shader->bindings[ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW][j].size; k++) {
+          loop_count++;
+            VkImageView imageview = VK_NULL_HANDLE;
+            VkBufferView bufferview = VK_NULL_HANDLE;
+            struct zink_resource *res = NULL;
+            VkImageLayout layout = VK_IMAGE_LAYOUT_UNDEFINED;
+            struct zink_sampler *sampler = NULL;
+
+            struct pipe_sampler_view *psampler_view = ctx->sampler_views[stage][index + k];
+            struct zink_sampler_view *sampler_view = zink_sampler_view(psampler_view);
+            res = psampler_view ? zink_resource(psampler_view->texture) : NULL;
+            if (res && res->base.b.target == PIPE_BUFFER) {
+               bufferview = sampler_view->buffer_view->buffer_view;
+            } else if (res) {
+               imageview = sampler_view->image_view->image_view;
+               layout = (res->bind_history & BITFIELD64_BIT(ZINK_DESCRIPTOR_TYPE_IMAGE)) ?
+                        VK_IMAGE_LAYOUT_GENERAL : VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL;
+               sampler = ctx->samplers[stage][index + k];
+            }
+            assert(num_resources < num_bindings);
+            if (res) {
+               if (!cache_hit)
+                  add_barrier(res, layout, VK_ACCESS_SHADER_READ_BIT, stage, &zds->barriers, ht);
+            }
+            assert(num_image_info < num_bindings);
+            handle_image_descriptor(screen, res, ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW, shader->bindings[ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW][j].type,
+                                    &wds[num_wds], layout, &num_image_info, &image_infos[num_image_info],
+                                    &num_buffer_info, &buffer_views[num_buffer_info],
+                                    sampler, imageview, bufferview, !k);
+            desc_set_sampler_add(ctx, zds, sampler_view, sampler, num_resources++,
+                                 shader->bindings[ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW][j].type == VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER,
+                                 cache_hit);
+            struct zink_batch *batch = &ctx->batch;
+            if (sampler_view)
+               zink_batch_reference_sampler_view(batch, sampler_view);
+            if (sampler)
+              zink_batch_reference_sampler(batch, sampler);
+              //printf("SAMPLER LOOPS %u ", loop_count);
+         }
+         assert(num_wds < num_descriptors);
+
+         num_wds = init_write_descriptor(shader, zds, ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW, j, &wds[num_wds], num_wds);
+      }
+   }
+   _mesa_set_destroy(ht, NULL);
+   write_descriptors(ctx, num_wds, wds, cache_hit);
+}
+
+static void
+update_image_descriptors(struct zink_context *ctx, struct zink_descriptor_set *zds,
+                         bool is_compute, bool cache_hit, bool need_resource_refs)
+{
+   struct zink_program *pg = is_compute ? (struct zink_program *)ctx->curr_compute : (struct zink_program *)ctx->curr_program;
+   struct zink_screen *screen = zink_screen(ctx->base.screen);
+   unsigned num_descriptors = pg->descriptor_data->pool[ZINK_DESCRIPTOR_TYPE_IMAGE]->key.layout->num_descriptors;
+   unsigned num_bindings = zds->pool->num_resources;
+   VkWriteDescriptorSet wds[num_descriptors];
+   VkDescriptorImageInfo image_infos[num_bindings];
+   VkBufferView buffer_views[num_bindings];
+   unsigned num_wds = 0;
+   unsigned num_image_info = 0;
+   unsigned num_buffer_info = 0;
+   unsigned num_resources = 0;
+   struct zink_shader **stages;
+   struct set *ht = NULL;
+   if (!cache_hit) {
+      ht = _mesa_set_create(NULL, barrier_hash, barrier_equals);
+      _mesa_set_resize(ht, num_bindings);
+   }
+
+   unsigned num_stages = is_compute ? 1 : ZINK_SHADER_COUNT;
+   if (is_compute)
+      stages = &ctx->curr_compute->shader;
+   else
+      stages = &ctx->gfx_stages[0];
+unsigned loop_count = 0;
+   for (int i = 0; (!cache_hit || need_resource_refs) && i < num_stages; i++) {
+      struct zink_shader *shader = stages[i];
+      if (!shader)
+         continue;
+      enum pipe_shader_type stage = pipe_shader_type_from_mesa(shader->nir->info.stage);
+
+      for (int j = 0; j < shader->num_bindings[ZINK_DESCRIPTOR_TYPE_IMAGE]; j++) {
+         int index = shader->bindings[ZINK_DESCRIPTOR_TYPE_IMAGE][j].index;
+         assert(shader->bindings[ZINK_DESCRIPTOR_TYPE_IMAGE][j].type == VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER ||
+                shader->bindings[ZINK_DESCRIPTOR_TYPE_IMAGE][j].type == VK_DESCRIPTOR_TYPE_STORAGE_IMAGE);
+
+         for (unsigned k = 0; k < shader->bindings[ZINK_DESCRIPTOR_TYPE_IMAGE][j].size; k++) {
+          loop_count++;
+            VkImageView imageview = VK_NULL_HANDLE;
+            VkBufferView bufferview = VK_NULL_HANDLE;
+            struct zink_resource *res = NULL;
+            VkImageLayout layout = VK_IMAGE_LAYOUT_UNDEFINED;
+            struct zink_image_view *image_view = &ctx->image_views[stage][index + k];
+            assert(image_view);
+            res = zink_resource(image_view->base.resource);
+
+            if (res && image_view->base.resource->target == PIPE_BUFFER) {
+               bufferview = image_view->buffer_view->buffer_view;
+            } else if (res) {
+               imageview = image_view->surface->image_view;
+               layout = VK_IMAGE_LAYOUT_GENERAL;
+            }
+            assert(num_resources < num_bindings);
+            desc_set_image_add(ctx, zds, image_view, num_resources++,
+                               shader->bindings[ZINK_DESCRIPTOR_TYPE_IMAGE][j].type == VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER,
+                               cache_hit);
+            if (res) {
+               VkAccessFlags flags = 0;
+               if (image_view->base.access & PIPE_IMAGE_ACCESS_READ)
+                  flags |= VK_ACCESS_SHADER_READ_BIT;
+               if (image_view->base.access & PIPE_IMAGE_ACCESS_WRITE)
+                  flags |= VK_ACCESS_SHADER_WRITE_BIT;
+               if (!cache_hit)
+                  add_barrier(res, layout, flags, stage, &zds->barriers, ht);
+            }
+
+            assert(num_image_info < num_bindings);
+            handle_image_descriptor(screen, res, ZINK_DESCRIPTOR_TYPE_IMAGE, shader->bindings[ZINK_DESCRIPTOR_TYPE_IMAGE][j].type,
+                                    &wds[num_wds], layout, &num_image_info, &image_infos[num_image_info],
+                                    &num_buffer_info, &buffer_views[num_buffer_info],
+                                    NULL, imageview, bufferview, !k);
+
+            struct zink_batch *batch = &ctx->batch;
+            if (res)
+               zink_batch_reference_image_view(batch, image_view);
+               //printf("IMAGE LOOPS %u ", loop_count);
+         }
+         assert(num_wds < num_descriptors);
+
+         num_wds = init_write_descriptor(shader, zds, ZINK_DESCRIPTOR_TYPE_IMAGE, j, &wds[num_wds], num_wds);
+      }
+   }
+   _mesa_set_destroy(ht, NULL);
+   write_descriptors(ctx, num_wds, wds, cache_hit);
+}
+
+static void
+zink_context_update_descriptor_states(struct zink_context *ctx, bool is_compute);
+
+struct set *
+zink_descriptors_update(struct zink_context *ctx, bool is_compute)
+{
+   struct zink_program *pg = is_compute ? (struct zink_program *)ctx->curr_compute : (struct zink_program *)ctx->curr_program;
+//TIMING_START(update);
+   zink_context_update_descriptor_states(ctx, is_compute);
+   bool cache_hit[ZINK_DESCRIPTOR_TYPES];
+   bool need_resource_refs[ZINK_DESCRIPTOR_TYPES];
+   struct zink_descriptor_set *zds[ZINK_DESCRIPTOR_TYPES];
+   for (int h = 0; h < ZINK_DESCRIPTOR_TYPES; h++) {
+      if (pg->descriptor_data->pool[h])
+         zds[h] = zink_descriptor_set_get(ctx, h, is_compute, &cache_hit[h], &need_resource_refs[h]);
+      else
+         zds[h] = NULL;
+   }
+   struct zink_batch *batch = &ctx->batch;
+   zink_batch_reference_program(batch, pg);
+
+   struct set *persistent = NULL;
+   if (ctx->num_persistent_maps)
+      persistent = _mesa_pointer_set_create(NULL);
+
+   uint32_t dynamic_offsets[PIPE_MAX_CONSTANT_BUFFERS];
+   unsigned dynamic_offset_idx = 0;
+
+   if (zds[ZINK_DESCRIPTOR_TYPE_UBO])
+      update_ubo_descriptors(ctx, zds[ZINK_DESCRIPTOR_TYPE_UBO],
+                                           is_compute, cache_hit[ZINK_DESCRIPTOR_TYPE_UBO],
+                                           need_resource_refs[ZINK_DESCRIPTOR_TYPE_UBO], dynamic_offsets, &dynamic_offset_idx);
+   if (zds[ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW])
+      update_sampler_descriptors(ctx, zds[ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW],
+                                               is_compute, cache_hit[ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW],
+                                               need_resource_refs[ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW]);
+   if (zds[ZINK_DESCRIPTOR_TYPE_SSBO])
+      update_ssbo_descriptors(ctx, zds[ZINK_DESCRIPTOR_TYPE_SSBO],
+                                               is_compute, cache_hit[ZINK_DESCRIPTOR_TYPE_SSBO],
+                                               need_resource_refs[ZINK_DESCRIPTOR_TYPE_SSBO]);
+   if (zds[ZINK_DESCRIPTOR_TYPE_IMAGE])
+      update_image_descriptors(ctx, zds[ZINK_DESCRIPTOR_TYPE_IMAGE],
+                                               is_compute, cache_hit[ZINK_DESCRIPTOR_TYPE_IMAGE],
+                                               need_resource_refs[ZINK_DESCRIPTOR_TYPE_IMAGE]);
+
+   for (int h = 0; zds[h] && h < ZINK_DESCRIPTOR_TYPES; h++) {
+      /* skip null descriptor sets since they have no resources */
+      if (!zds[h]->hash)
+         continue;
+      assert(zds[h]->desc_set);
+      util_dynarray_foreach(&zds[h]->barriers, struct zink_descriptor_barrier, barrier) {
+         if (barrier->res->persistent_maps)
+            _mesa_set_add(persistent, barrier->res);
+         if (need_resource_refs[h])
+            zink_batch_reference_resource_rw(batch, barrier->res, zink_resource_access_is_write(barrier->access));
+         zink_resource_barrier(ctx, NULL, barrier->res,
+                               barrier->layout, barrier->access, barrier->stage);
+      }
+   }
+
+   if (!is_compute)
+      batch = zink_batch_rp(ctx);
+
+   for (unsigned h = 0; h < ZINK_DESCRIPTOR_TYPES; h++) {
+      if (zds[h]) {
+         vkCmdBindDescriptorSets(batch->state->cmdbuf, is_compute ? VK_PIPELINE_BIND_POINT_COMPUTE : VK_PIPELINE_BIND_POINT_GRAPHICS,
+                                 pg->layout, zds[h]->pool->type, 1, &zds[h]->desc_set,
+                                 zds[h]->pool->type == ZINK_DESCRIPTOR_TYPE_UBO ? dynamic_offset_idx : 0, dynamic_offsets);
+      }
+   }
+//TIMING_END(update);
+//TIMING_PRINT(update, "update");
+   return persistent;
+}
+
+void
+zink_batch_descriptor_deinit(struct zink_screen *screen, struct zink_batch_state *bs)
+{
+   if (!bs->dd)
+      return;
+   _mesa_set_destroy(bs->dd->desc_sets, NULL);
+   ralloc_free(bs->dd);
+}
+
+void
+zink_batch_descriptor_reset(struct zink_screen *screen, struct zink_batch_state *bs)
+{
+   set_foreach(bs->dd->desc_sets, entry) {
+      struct zink_descriptor_set *zds = (void*)entry->key;
+      zink_batch_usage_unset(&zds->batch_uses, bs->fence.batch_id);
+      /* reset descriptor pools when no bs is using this program to avoid
+       * having some inactive program hogging a billion descriptors
+       */
+      pipe_reference(&zds->reference, NULL);
+      zink_descriptor_set_recycle(zds);
+      _mesa_set_remove(bs->dd->desc_sets, entry);
+   }
+}
+
+bool
+zink_batch_descriptor_init(struct zink_batch_state *bs)
+{
+   bs->dd = rzalloc(bs, struct zink_batch_descriptor_data);
+   if (!bs->dd)
+      return false;
+   bs->dd->desc_sets = _mesa_pointer_set_create(bs);
+   return !!bs->dd->desc_sets;
+}
+
+static inline uint32_t
+maybe_hash_u32(uint32_t val, uint32_t hash)
+{
+   if (!hash)
+      return val;
+   return XXH32(&val, sizeof(uint32_t), hash);
+}
+
+struct zink_resource *
+zink_get_resource_for_descriptor(struct zink_context *ctx, enum zink_descriptor_type type, enum pipe_shader_type shader, int idx)
+{
+   switch (type) {
+   case ZINK_DESCRIPTOR_TYPE_UBO:
+      return zink_resource(ctx->ubos[shader][idx].buffer);
+   case ZINK_DESCRIPTOR_TYPE_SSBO:
+      return zink_resource(ctx->ssbos[shader][idx].buffer);
+   case ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW:
+      return ctx->sampler_views[shader][idx] ? zink_resource(ctx->sampler_views[shader][idx]->texture) : NULL;
+   case ZINK_DESCRIPTOR_TYPE_IMAGE:
+      return zink_resource(ctx->image_views[shader][idx].base.resource);
+   default:
+      break;
+   }
+   unreachable("unknown descriptor type!");
+   return NULL;
+}
+
+static uint32_t
+calc_descriptor_state_hash_ubo(struct zink_context *ctx, struct zink_shader *zs, enum pipe_shader_type shader, int i, int idx, uint32_t hash)
+{
+   struct zink_resource *res = zink_get_resource_for_descriptor(ctx, ZINK_DESCRIPTOR_TYPE_UBO, shader, idx);
+   struct zink_resource_object *obj = res ? res->obj : NULL;
+   hash = XXH32(&obj, sizeof(void*), hash);
+   void *hash_data = &ctx->ubos[shader][idx].buffer_size;
+   size_t data_size = sizeof(unsigned);
+   hash = XXH32(hash_data, data_size, hash);
+   if (zs->bindings[ZINK_DESCRIPTOR_TYPE_UBO][i].type == VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER)
+      hash = XXH32(&ctx->ubos[shader][idx].buffer_offset, sizeof(unsigned), hash);
+   return hash;
+}
+
+static uint32_t
+calc_descriptor_state_hash_ssbo(struct zink_context *ctx, struct zink_shader *zs, enum pipe_shader_type shader, int i, int idx, uint32_t hash)
+{
+   struct zink_resource *res = zink_get_resource_for_descriptor(ctx, ZINK_DESCRIPTOR_TYPE_SSBO, shader, idx);
+   struct zink_resource_object *obj = res ? res->obj : NULL;
+   hash = XXH32(&obj, sizeof(void*), hash);
+   if (obj) {
+      struct pipe_shader_buffer *ssbo = &ctx->ssbos[shader][idx];
+      hash = XXH32(&ssbo->buffer_offset, sizeof(ssbo->buffer_offset), hash);
+      hash = XXH32(&ssbo->buffer_size, sizeof(ssbo->buffer_size), hash);
+   }
+   return hash;
+}
+
+static inline uint32_t
+get_sampler_view_hash(const struct zink_sampler_view *sampler_view)
+{
+   if (!sampler_view)
+      return 0;
+   return sampler_view->base.target == PIPE_BUFFER ?
+          sampler_view->buffer_view->hash : sampler_view->image_view->hash;
+}
+
+static inline uint32_t
+get_image_view_hash(const struct zink_image_view *image_view)
+{
+   if (!image_view || !image_view->base.resource)
+      return 0;
+   return image_view->base.resource->target == PIPE_BUFFER ?
+          image_view->buffer_view->hash : image_view->surface->hash;
+}
+
+uint32_t
+zink_get_sampler_view_hash(struct zink_context *ctx, struct zink_sampler_view *sampler_view, bool is_buffer)
+{
+   return get_sampler_view_hash(sampler_view) ? get_sampler_view_hash(sampler_view) :
+          (is_buffer ? zink_screen(ctx->base.screen)->null_descriptor_hashes.buffer_view :
+                       zink_screen(ctx->base.screen)->null_descriptor_hashes.image_view);
+}
+
+uint32_t
+zink_get_image_view_hash(struct zink_context *ctx, struct zink_image_view *image_view, bool is_buffer)
+{
+   return get_image_view_hash(image_view) ? get_image_view_hash(image_view) :
+          (is_buffer ? zink_screen(ctx->base.screen)->null_descriptor_hashes.buffer_view :
+                       zink_screen(ctx->base.screen)->null_descriptor_hashes.image_view);
+}
+
+static uint32_t
+calc_descriptor_state_hash_sampler(struct zink_context *ctx, struct zink_shader *zs, enum pipe_shader_type shader, int i, int idx, uint32_t hash)
+{
+   for (unsigned k = 0; k < zs->bindings[ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW][i].size; k++) {
+      struct zink_sampler_view *sampler_view = zink_sampler_view(ctx->sampler_views[shader][idx + k]);
+      bool is_buffer = zink_shader_descriptor_is_buffer(zs, ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW, i);
+      hash = maybe_hash_u32(zink_get_sampler_view_hash(ctx, sampler_view, is_buffer), hash);
+      if (is_buffer)
+         continue;
+
+      struct zink_sampler *sampler = ctx->samplers[shader][idx + k];
+
+      if (sampler) {
+         hash = XXH32(&sampler, sizeof(void*), hash);
+      }
+   }
+   return hash;
+}
+
+static uint32_t
+calc_descriptor_state_hash_image(struct zink_context *ctx, struct zink_shader *zs, enum pipe_shader_type shader, int i, int idx, uint32_t hash)
+{
+   for (unsigned k = 0; k < zs->bindings[ZINK_DESCRIPTOR_TYPE_IMAGE][i].size; k++) {
+      hash = maybe_hash_u32(zink_get_image_view_hash(ctx, &ctx->image_views[shader][idx + k],
+                                                     zink_shader_descriptor_is_buffer(zs, ZINK_DESCRIPTOR_TYPE_IMAGE, i)),
+                            hash);
+   }
+   return hash;
+}
+
+static uint32_t
+update_descriptor_stage_state(struct zink_context *ctx, enum pipe_shader_type shader, enum zink_descriptor_type type)
+{
+   struct zink_shader *zs = shader == PIPE_SHADER_COMPUTE ? ctx->compute_stage : ctx->gfx_stages[shader];
+
+   if (!zink_program_get_descriptor_usage(ctx, shader, type))
+      return 0;
+
+   uint32_t hash = 0;
+   for (int i = 0; i < zs->num_bindings[type]; i++) {
+      int idx = zs->bindings[type][i].index;
+      switch (type) {
+      case ZINK_DESCRIPTOR_TYPE_UBO:
+         hash = calc_descriptor_state_hash_ubo(ctx, zs, shader, i, idx, hash);
+         break;
+      case ZINK_DESCRIPTOR_TYPE_SSBO:
+         hash = calc_descriptor_state_hash_ssbo(ctx, zs, shader, i, idx, hash);
+         break;
+      case ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW:
+         hash = calc_descriptor_state_hash_sampler(ctx, zs, shader, i, idx, hash);
+         break;
+      case ZINK_DESCRIPTOR_TYPE_IMAGE:
+         hash = calc_descriptor_state_hash_image(ctx, zs, shader, i, idx, hash);
+         break;
+      default:
+         unreachable("unknown descriptor type");
+      }
+   }
+   return hash;
+}
+
+static void
+update_descriptor_state(struct zink_context *ctx, enum zink_descriptor_type type, bool is_compute)
+{
+   /* we shouldn't be calling this if we don't have to */
+   assert(!ctx->descriptor_data->descriptor_states[is_compute].valid[type]);
+
+   if (is_compute)
+      /* just update compute state */
+      ctx->descriptor_data->descriptor_states[is_compute].state[type] = update_descriptor_stage_state(ctx, PIPE_SHADER_COMPUTE, type);
+   else {
+      /* update all gfx states */
+      for (unsigned i = 0; i < ZINK_SHADER_COUNT; i++) {
+         /* this is the incremental update for the shader stage */
+         if (!ctx->descriptor_data->gfx_descriptor_states[i].valid[type] && ctx->gfx_stages[i]) {
+            ctx->descriptor_data->gfx_descriptor_states[i].state[type] = update_descriptor_stage_state(ctx, i, type);
+            ctx->descriptor_data->gfx_descriptor_states[i].valid[type] = true;
+         }
+         if (ctx->descriptor_data->gfx_descriptor_states[i].valid[type]) {
+            /* this is the overall state update for the descriptor set hash */
+            ctx->descriptor_data->descriptor_states[is_compute].state[type] = maybe_hash_u32(ctx->descriptor_data->gfx_descriptor_states[i].state[type],
+                                                                            ctx->descriptor_data->descriptor_states[is_compute].state[type]);
+         }
+      }
+   }
+   ctx->descriptor_data->descriptor_states[is_compute].valid[type] = true;
+}
+
+static void
+zink_context_update_descriptor_states(struct zink_context *ctx, bool is_compute)
+{
+   for (unsigned i = 0; i < ZINK_DESCRIPTOR_TYPES; i++) {
+      if (!ctx->descriptor_data->descriptor_states[is_compute].valid[i])
+         update_descriptor_state(ctx, i, is_compute);
+   }
+}
+
+void
+zink_context_invalidate_descriptor_state(struct zink_context *ctx, enum pipe_shader_type shader, enum zink_descriptor_type type, unsigned start, unsigned count)
+{
+   if (shader != PIPE_SHADER_COMPUTE) {
+      ctx->descriptor_data->gfx_descriptor_states[shader].valid[type] = false;
+      ctx->descriptor_data->gfx_descriptor_states[shader].state[type] = 0;
+   }
+   ctx->descriptor_data->descriptor_states[shader == PIPE_SHADER_COMPUTE].valid[type] = false;
+   ctx->descriptor_data->descriptor_states[shader == PIPE_SHADER_COMPUTE].state[type] = 0;
+}
+
+enum zink_descriptor_barrier_type
+zink_vktype_to_descriptor_barrier_type(VkImageLayout layout, VkAccessFlags access)
+{
+   if (layout == VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL)
+      return ZINK_DESCRIPTOR_BARRIER_SAMPLERVIEW;
+   else if (layout == VK_IMAGE_LAYOUT_GENERAL) {
+      if (zink_resource_access_is_write(access)) {
+         if (access & VK_ACCESS_SHADER_READ_BIT)
+            return ZINK_DESCRIPTOR_BARRIER_IMAGE_RW;
+         return ZINK_DESCRIPTOR_BARRIER_IMAGE_WRITE;
+      }
+      return ZINK_DESCRIPTOR_BARRIER_IMAGE_READ;
+   }
+   if (zink_resource_access_is_write(access)) {
+      if (access & VK_ACCESS_SHADER_READ_BIT)
+         return ZINK_DESCRIPTOR_BARRIER_BUFFER_RW;
+      return ZINK_DESCRIPTOR_BARRIER_BUFFER_WRITE;
+   }
+   return ZINK_DESCRIPTOR_BARRIER_BUFFER_READ;
+}
+
+bool
+zink_descriptors_init(struct zink_context *ctx)
+{
+   ctx->descriptor_data = rzalloc(ctx, struct zink_descriptor_data);
+   if (!ctx->descriptor_data)
+      return false;
+   return zink_descriptor_pool_init(ctx);
+}
+
+void
+zink_descriptors_deinit(struct zink_context *ctx)
+{
+   zink_descriptor_pool_deinit(ctx);
+}
+
+bool
+zink_descriptor_layouts_init(struct zink_context *ctx)
+{
+   for (unsigned i = 0; i < ZINK_DESCRIPTOR_TYPES; i++)
+      if (!_mesa_hash_table_init(&ctx->desc_set_layouts[i], ctx, hash_descriptor_layout, equals_descriptor_layout))
+         return false;
+   return true;
+}
+
+void
+zink_descriptor_layouts_deinit(struct zink_context *ctx)
+{
+   struct zink_screen *screen = zink_screen(ctx->base.screen);
+   for (unsigned i = 0; i < ZINK_DESCRIPTOR_TYPES; i++) {
+      hash_table_foreach(&ctx->desc_set_layouts[i], he) {
+         vkDestroyDescriptorSetLayout(screen->dev, (VkDescriptorSetLayout)he->data, NULL);
+         _mesa_hash_table_remove(&ctx->desc_set_layouts[i], he);
+      }
+   }
+}
diff --git a/src/gallium/drivers/zink/zink_descriptors.h b/src/gallium/drivers/zink/zink_descriptors.h
new file mode 100644
index 00000000000..dfc20e35858
--- /dev/null
+++ b/src/gallium/drivers/zink/zink_descriptors.h
@@ -0,0 +1,207 @@
+/*
+ * Copyright  2020 Mike Blumenkrantz
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ * 
+ * Authors:
+ *    Mike Blumenkrantz <michael.blumenkrantz@gmail.com>
+ */
+
+#ifndef ZINK_DESCRIPTOR_H
+# define  ZINK_DESCRIPTOR_H
+#include <vulkan/vulkan.h>
+#include "util/u_dynarray.h"
+#include "util/u_inlines.h"
+#include "util/simple_mtx.h"
+
+#include "zink_batch.h"
+
+#ifndef ZINK_SHADER_COUNT
+#define ZINK_SHADER_COUNT (PIPE_SHADER_TYPES - 1)
+#endif
+
+enum zink_descriptor_type {
+   ZINK_DESCRIPTOR_TYPE_UBO,
+   ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW,
+   ZINK_DESCRIPTOR_TYPE_SSBO,
+   ZINK_DESCRIPTOR_TYPE_IMAGE,
+   ZINK_DESCRIPTOR_TYPES,
+};
+
+
+struct zink_descriptor_refs {
+   struct util_dynarray refs;
+};
+
+
+/* hashes of all the named types in a given state */
+struct zink_descriptor_state {
+   bool valid[ZINK_DESCRIPTOR_TYPES];
+   uint32_t state[ZINK_DESCRIPTOR_TYPES];
+};
+
+struct hash_table;
+
+struct zink_context;
+struct zink_image_view;
+struct zink_program;
+struct zink_resource;
+struct zink_sampler;
+struct zink_sampler_view;
+struct zink_shader;
+struct zink_screen;
+
+
+struct zink_descriptor_state_key {
+   bool exists[ZINK_SHADER_COUNT];
+   uint32_t state[ZINK_SHADER_COUNT];
+};
+
+enum zink_descriptor_barrier_type {
+   ZINK_DESCRIPTOR_BARRIER_BUFFER_READ,
+   ZINK_DESCRIPTOR_BARRIER_BUFFER_WRITE,
+   ZINK_DESCRIPTOR_BARRIER_BUFFER_RW,
+   ZINK_DESCRIPTOR_BARRIER_IMAGE_READ,
+   ZINK_DESCRIPTOR_BARRIER_IMAGE_WRITE,
+   ZINK_DESCRIPTOR_BARRIER_IMAGE_RW,
+   ZINK_DESCRIPTOR_BARRIER_SAMPLERVIEW,
+   ZINK_DESCRIPTOR_BARRIER_TYPES,
+};
+
+struct zink_descriptor_barrier {
+   struct zink_resource *res;
+   VkImageLayout layout;
+   VkAccessFlags access;
+   VkPipelineStageFlagBits stage;
+};
+
+struct zink_descriptor_layout_key {
+   unsigned num_descriptors;
+   VkDescriptorSetLayoutBinding *bindings;
+};
+
+struct zink_descriptor_pool_key {
+   struct zink_descriptor_layout_key *layout;
+   unsigned num_type_sizes;
+   VkDescriptorPoolSize *sizes;
+};
+
+struct zink_descriptor_reference {
+   void **ref;
+   bool *invalid;
+};
+
+enum zink_descriptor_barrier_type
+zink_vktype_to_descriptor_barrier_type(VkImageLayout layout, VkAccessFlags access);
+
+bool
+zink_descriptor_layouts_init(struct zink_context *ctx);
+
+void
+zink_descriptor_layouts_deinit(struct zink_context *ctx);
+
+uint32_t
+zink_get_sampler_view_hash(struct zink_context *ctx, struct zink_sampler_view *sampler_view, bool is_buffer);
+uint32_t
+zink_get_image_view_hash(struct zink_context *ctx, struct zink_image_view *image_view, bool is_buffer);
+bool
+zink_descriptor_util_alloc_sets(struct zink_screen *screen, VkDescriptorSetLayout dsl, VkDescriptorPool pool, VkDescriptorSet *sets, unsigned num_sets);
+bool
+zink_descriptors_util_program_init(struct zink_context *ctx,
+                                   struct zink_shader *stages[ZINK_SHADER_COUNT],
+                                   struct zink_program *pg);
+VkDescriptorSetLayout
+zink_descriptor_util_layout_get(struct zink_context *ctx, enum zink_descriptor_type type,
+                      VkDescriptorSetLayoutBinding *bindings, unsigned num_bindings,
+                      struct zink_descriptor_layout_key **layout_key);
+struct zink_resource *
+zink_get_resource_for_descriptor(struct zink_context *ctx, enum zink_descriptor_type type, enum pipe_shader_type shader, int idx);
+
+/* these two can't be called in lazy mode */
+void
+zink_descriptor_set_refs_clear(struct zink_descriptor_refs *refs, void *ptr);
+void
+zink_descriptor_set_recycle(struct zink_descriptor_set *zds);
+
+
+
+
+
+bool
+zink_descriptor_program_init(struct zink_context *ctx, struct zink_program *pg);
+
+void
+zink_descriptor_program_deinit(struct zink_screen *screen, struct zink_program *pg);
+
+struct set *
+zink_descriptors_update(struct zink_context *ctx, bool is_compute);
+
+
+void
+zink_context_invalidate_descriptor_state(struct zink_context *ctx, enum pipe_shader_type shader, enum zink_descriptor_type type, unsigned, unsigned);
+
+uint32_t
+zink_get_sampler_view_hash(struct zink_context *ctx, struct zink_sampler_view *sampler_view, bool is_buffer);
+uint32_t
+zink_get_image_view_hash(struct zink_context *ctx, struct zink_image_view *image_view, bool is_buffer);
+struct zink_resource *
+zink_get_resource_for_descriptor(struct zink_context *ctx, enum zink_descriptor_type type, enum pipe_shader_type shader, int idx);
+
+void
+zink_batch_descriptor_deinit(struct zink_screen *screen, struct zink_batch_state *bs);
+void
+zink_batch_descriptor_reset(struct zink_screen *screen, struct zink_batch_state *bs);
+bool
+zink_batch_descriptor_init(struct zink_batch_state *bs);
+
+bool
+zink_descriptors_init(struct zink_context *ctx);
+
+void
+zink_descriptors_deinit(struct zink_context *ctx);
+
+//LAZY
+bool
+zink_descriptor_program_init_lazy(struct zink_context *ctx, struct zink_program *pg);
+
+void
+zink_descriptor_program_deinit_lazy(struct zink_screen *screen, struct zink_program *pg);
+
+struct set *
+zink_descriptors_update_lazy(struct zink_context *ctx, bool is_compute);
+
+
+void
+zink_context_invalidate_descriptor_state_lazy(struct zink_context *ctx, enum pipe_shader_type shader, enum zink_descriptor_type type, unsigned, unsigned);
+
+void
+zink_batch_descriptor_deinit_lazy(struct zink_screen *screen, struct zink_batch_state *bs);
+void
+zink_batch_descriptor_reset_lazy(struct zink_screen *screen, struct zink_batch_state *bs);
+bool
+zink_batch_descriptor_init_lazy(struct zink_batch_state *bs);
+
+bool
+zink_descriptors_init_lazy(struct zink_context *ctx);
+
+void
+zink_descriptors_deinit_lazy(struct zink_context *ctx);
+
+#endif
diff --git a/src/gallium/drivers/zink/zink_descriptors_lazy.c b/src/gallium/drivers/zink/zink_descriptors_lazy.c
new file mode 100644
index 00000000000..9d2a42ac3ca
--- /dev/null
+++ b/src/gallium/drivers/zink/zink_descriptors_lazy.c
@@ -0,0 +1,961 @@
+/*
+ * Copyright  2021 Mike Blumenkrantz
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ * 
+ * Authors:
+ *    Mike Blumenkrantz <michael.blumenkrantz@gmail.com>
+ */
+#include "tgsi/tgsi_from_mesa.h"
+
+
+
+#include "zink_context.h"
+#include "zink_compiler.h"
+#include "zink_descriptors.h"
+#include "zink_program.h"
+#include "zink_resource.h"
+#include "zink_screen.h"
+
+#define MAX_DESCRIPTOR_RESOURCES PIPE_SHADER_TYPES * (PIPE_MAX_CONSTANT_BUFFERS + PIPE_MAX_SHADER_BUFFERS + PIPE_MAX_SAMPLERS + PIPE_MAX_SHADER_IMAGES)
+
+struct zink_descriptor_data {
+   VkDescriptorSetLayout push_dsl[2]; //gfx, compute
+   VkDescriptorSetLayout dummy_dsl;
+   VkDescriptorPool dummy_pool;
+   VkDescriptorSet dummy_set;
+   VkDescriptorUpdateTemplateEntry push_entries[PIPE_SHADER_TYPES];
+};
+
+struct zink_lazy_barrier {
+   struct zink_resource **res;
+   VkAccessFlags access;
+   VkImageLayout layout;
+   VkPipelineStageFlagBits stage;
+   union zink_descriptor_surface *surface;
+   unsigned slot;
+   enum zink_descriptor_type type;
+};
+
+struct zink_program_descriptor_data {
+   unsigned num_resources;
+   unsigned num_type_sizes;
+   VkDescriptorPoolSize sizes[6];
+   unsigned has_descriptors_mask[ZINK_SHADER_COUNT];
+   struct zink_descriptor_layout_key *layout_key;
+   unsigned push_usage;
+   VkDescriptorUpdateTemplateKHR templates[2];
+   unsigned barrier_count_type[ZINK_DESCRIPTOR_BARRIER_TYPES];
+   unsigned barrier_count[2][ZINK_SHADER_COUNT]; //[buffer, image][stage]
+   unsigned total_barrier_count[2]; //[buffer, image]
+   struct zink_lazy_barrier *type_barriers[2][ZINK_SHADER_COUNT]; //[buffer, image][stage]
+};
+
+struct zink_batch_descriptor_data {
+   struct hash_table pools;
+};
+
+struct zink_resolve_barrier {
+   struct zink_resource *res;
+   enum zink_descriptor_barrier_type type[ZINK_SHADER_COUNT]; //MESA_SHADER_ for ordering
+   struct zink_lazy_barrier *barriers[ZINK_SHADER_COUNT];
+   unsigned valid_stages;
+   VkImageLayout final_layout;
+};
+
+struct zink_descriptor_resolve_data {
+   struct set barrier_sort[2];
+   struct zink_resolve_barrier *sort_barriers[2];
+};
+
+struct zink_descriptor_pool {
+   VkDescriptorPool pool;
+   VkDescriptorSet sets[ZINK_DEFAULT_MAX_DESCS];
+   unsigned set_idx;
+   unsigned sets_alloc;
+   struct util_dynarray resolves;
+   unsigned cur_resolve;
+};
+
+static uint32_t
+barrier_hash_lazy(const void *key)
+{
+   const struct zink_resolve_barrier *rb = key;
+   return _mesa_hash_pointer(rb->res);
+}
+
+static void
+init_template_entry(struct zink_shader *shader, enum zink_descriptor_type type,
+                    unsigned idx, unsigned offset, VkDescriptorUpdateTemplateEntry *entry, unsigned *entry_idx)
+{
+    int index = shader->bindings[type][idx].index;
+    enum pipe_shader_type stage = pipe_shader_type_from_mesa(shader->nir->info.stage);
+    entry->dstArrayElement = 0;
+    entry->dstBinding = shader->bindings[type][idx].binding;
+    if (shader->bindings[type][idx].type == VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER_DYNAMIC)
+       /* filter out DYNAMIC type here */
+       entry->descriptorType = VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER;
+    else
+       entry->descriptorType = shader->bindings[type][idx].type;
+    switch (shader->bindings[type][idx].type) {
+    case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER:
+    case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER_DYNAMIC:
+       entry->descriptorCount = 1;
+       entry->offset = offsetof(struct zink_context, di.ubos[stage][index + offset]);
+       entry->stride = sizeof(VkDescriptorBufferInfo);
+       break;
+    case VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER:
+       entry->descriptorCount = shader->bindings[type][idx].size;
+       entry->offset = offsetof(struct zink_context, di.textures[stage][index + offset]);
+       entry->stride = sizeof(VkDescriptorImageInfo);
+       break;
+    case VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER:
+       entry->descriptorCount = shader->bindings[type][idx].size;
+       entry->offset = offsetof(struct zink_context, di.tbos[stage][index + offset]);
+       entry->stride = sizeof(VkBufferView);
+       break;
+    case VK_DESCRIPTOR_TYPE_STORAGE_BUFFER:
+       entry->descriptorCount = 1;
+       entry->offset = offsetof(struct zink_context, di.ssbos[stage][index + offset]);
+       entry->stride = sizeof(VkDescriptorBufferInfo);
+       break;
+    case VK_DESCRIPTOR_TYPE_STORAGE_IMAGE:
+       entry->descriptorCount = shader->bindings[type][idx].size;
+       entry->offset = offsetof(struct zink_context, di.images[stage][index + offset]);
+       entry->stride = sizeof(VkDescriptorImageInfo);
+       break;
+    case VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER:
+       entry->descriptorCount = shader->bindings[type][idx].size;
+       entry->offset = offsetof(struct zink_context, di.texel_images[stage][index + offset]);
+       entry->stride = sizeof(VkBufferView);
+       break;
+    default:
+       unreachable("unknown type");
+    }
+    (*entry_idx)++;
+}
+
+static VkAccessFlags
+get_access_flags_for_binding(struct zink_context *ctx, enum pipe_shader_type stage, enum zink_descriptor_type type, unsigned idx)
+{
+   VkAccessFlags flags = 0;
+   switch (type) {
+   case ZINK_DESCRIPTOR_TYPE_UBO:
+      return VK_ACCESS_UNIFORM_READ_BIT;
+   case ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW:
+      return VK_ACCESS_SHADER_READ_BIT;
+   case ZINK_DESCRIPTOR_TYPE_SSBO: {
+      flags = VK_ACCESS_SHADER_READ_BIT;
+      if (ctx->writable_ssbos[stage] & (1 << idx))
+         flags |= VK_ACCESS_SHADER_WRITE_BIT;
+      return flags;
+   }
+   case ZINK_DESCRIPTOR_TYPE_IMAGE: {
+      struct zink_image_view *image_view = &ctx->image_views[stage][idx];
+      if (image_view->base.access & PIPE_IMAGE_ACCESS_READ)
+         flags |= VK_ACCESS_SHADER_READ_BIT;
+      if (image_view->base.access & PIPE_IMAGE_ACCESS_WRITE)
+         flags |= VK_ACCESS_SHADER_WRITE_BIT;
+      return flags;
+   }
+   default:
+      break;
+   }
+   unreachable("ACK");
+   return 0;
+}
+
+bool
+zink_descriptor_program_init_lazy(struct zink_context *ctx, struct zink_program *pg)
+{
+   struct zink_screen *screen = zink_screen(ctx->base.screen);
+   VkDescriptorSetLayoutBinding bindings[ZINK_DESCRIPTOR_TYPES * PIPE_SHADER_TYPES * 32];
+   VkDescriptorUpdateTemplateEntry entries[ZINK_DESCRIPTOR_TYPES * PIPE_SHADER_TYPES * 32];
+   unsigned num_bindings = 0;
+
+   int type_map[12];
+   unsigned num_types = 0;
+   memset(type_map, -1, sizeof(type_map));
+
+   struct zink_shader **stages;
+   if (pg->is_compute)
+      stages = &((struct zink_compute_program*)pg)->shader;
+   else
+      stages = ((struct zink_gfx_program*)pg)->shaders;
+
+
+   if (!pg->descriptor_data)
+      pg->descriptor_data = rzalloc(pg, struct zink_program_descriptor_data);
+   if (!pg->descriptor_data)
+      return false;
+
+   unsigned push_count = 0;
+   unsigned entry_idx = 0;
+
+   unsigned num_shaders = pg->is_compute ? 1 : ZINK_SHADER_COUNT;
+   for (int i = 0; i < num_shaders; i++) {
+      struct zink_shader *shader = stages[i];
+      if (!shader)
+         continue;
+
+      enum pipe_shader_type stage = pipe_shader_type_from_mesa(shader->nir->info.stage);
+      unsigned stage_idx = pg->is_compute ? 0 : stage;
+      VkShaderStageFlagBits stage_flags = zink_shader_stage(stage);
+      for (int j = 0; j < ZINK_DESCRIPTOR_TYPES; j++) {
+         for (int k = 0; k < shader->num_bindings[j]; k++) {
+            pg->descriptor_data->has_descriptors_mask[stage] |= BITFIELD64_BIT(j);
+            /* dynamic ubos handled in push */
+            if (shader->bindings[j][k].type == VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER_DYNAMIC) {
+               pg->descriptor_data->push_usage |= BITFIELD64_BIT(stage);
+
+               push_count++;
+               continue;
+            }
+
+            assert(num_bindings < ARRAY_SIZE(bindings));
+            bindings[num_bindings].binding = shader->bindings[j][k].binding;
+            bindings[num_bindings].descriptorType = shader->bindings[j][k].type;
+            bindings[num_bindings].descriptorCount = shader->bindings[j][k].size;
+            pg->descriptor_data->num_resources += shader->bindings[j][k].size;
+            bindings[num_bindings].stageFlags = stage_flags;
+            bindings[num_bindings].pImmutableSamplers = NULL;
+            if (type_map[shader->bindings[j][k].type] == -1) {
+               type_map[shader->bindings[j][k].type] = num_types++;
+               pg->descriptor_data->sizes[type_map[shader->bindings[j][k].type]].type = shader->bindings[j][k].type;
+            }
+            pg->descriptor_data->sizes[type_map[shader->bindings[j][k].type]].descriptorCount += shader->bindings[j][k].size;
+            switch (shader->bindings[j][k].type) {
+            case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER:
+               pg->descriptor_data->barrier_count[0][stage_idx] += shader->bindings[j][k].size;
+               pg->descriptor_data->barrier_count_type[ZINK_DESCRIPTOR_BARRIER_BUFFER_READ] += shader->bindings[j][k].size;
+               break;
+            case VK_DESCRIPTOR_TYPE_STORAGE_BUFFER:
+               pg->descriptor_data->barrier_count[0][stage_idx] += shader->bindings[j][k].size;
+               pg->descriptor_data->barrier_count_type[ZINK_DESCRIPTOR_BARRIER_BUFFER_READ] += shader->bindings[j][k].size;
+               pg->descriptor_data->barrier_count_type[ZINK_DESCRIPTOR_BARRIER_BUFFER_RW] += shader->bindings[j][k].size;
+               break;
+            case VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER:
+               pg->descriptor_data->barrier_count[1][stage_idx] += shader->bindings[j][k].size;
+               pg->descriptor_data->barrier_count_type[ZINK_DESCRIPTOR_BARRIER_SAMPLERVIEW] += shader->bindings[j][k].size;
+               init_template_entry(shader, j, k, 0, &entries[entry_idx], &entry_idx);
+               break;
+            case VK_DESCRIPTOR_TYPE_STORAGE_IMAGE:
+               pg->descriptor_data->barrier_count[1][stage_idx] += shader->bindings[j][k].size;
+               pg->descriptor_data->barrier_count_type[ZINK_DESCRIPTOR_BARRIER_IMAGE_READ] += shader->bindings[j][k].size;
+               pg->descriptor_data->barrier_count_type[ZINK_DESCRIPTOR_BARRIER_IMAGE_WRITE] += shader->bindings[j][k].size;
+               pg->descriptor_data->barrier_count_type[ZINK_DESCRIPTOR_BARRIER_IMAGE_RW] += shader->bindings[j][k].size;
+               init_template_entry(shader, j, k, 0, &entries[entry_idx], &entry_idx);
+               break;
+            case VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER:
+               pg->descriptor_data->barrier_count[0][stage_idx] += shader->bindings[j][k].size;
+               pg->descriptor_data->barrier_count_type[ZINK_DESCRIPTOR_BARRIER_BUFFER_READ] += shader->bindings[j][k].size;
+               init_template_entry(shader, j, k, 0, &entries[entry_idx], &entry_idx);
+               break;
+            case VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER:
+               pg->descriptor_data->barrier_count[0][stage_idx] += shader->bindings[j][k].size;
+               pg->descriptor_data->barrier_count_type[ZINK_DESCRIPTOR_BARRIER_BUFFER_READ] += shader->bindings[j][k].size;
+               pg->descriptor_data->barrier_count_type[ZINK_DESCRIPTOR_BARRIER_BUFFER_WRITE] += shader->bindings[j][k].size;
+               pg->descriptor_data->barrier_count_type[ZINK_DESCRIPTOR_BARRIER_BUFFER_RW] += shader->bindings[j][k].size;
+               init_template_entry(shader, j, k, 0, &entries[entry_idx], &entry_idx);
+               break;
+            default:
+               unreachable("unknown type");
+               break;
+            }
+            ++num_bindings;
+         }
+      }
+   }
+
+   if (!num_bindings && !push_count) {
+      ralloc_free(pg->descriptor_data);
+      pg->descriptor_data = NULL;
+
+      pg->layout = zink_pipeline_layout_create(screen, pg, pg->is_compute);
+      return !!pg->layout;
+   }
+
+   /* grow this for barrier table size */
+   pg->descriptor_data->num_resources += push_count;
+
+   pg->num_dsl = 1;
+   if (num_bindings) {
+      pg->dsl[0] = zink_descriptor_util_layout_get(ctx, 0, bindings, num_bindings, &pg->descriptor_data->layout_key);
+      pg->descriptor_data->num_type_sizes = num_types;
+      for (unsigned i = 0; i < num_types; i++)
+         pg->descriptor_data->sizes[i].descriptorCount *= ZINK_DEFAULT_MAX_DESCS;
+   } else
+      pg->dsl[0] = ctx->descriptor_data->dummy_dsl;
+
+   if (push_count) {
+      pg->dsl[1] = ctx->descriptor_data->push_dsl[pg->is_compute];
+      pg->num_dsl++;
+   }
+
+   pg->layout = zink_pipeline_layout_create(screen, pg, pg->is_compute);
+   if (!pg->layout)
+      return false;
+
+   if (!num_bindings && !push_count)
+      return true;
+
+   for (unsigned i = 0; num_bindings && i < num_shaders; i++) {
+      if (pg->descriptor_data->barrier_count[0][i]) {
+         pg->descriptor_data->type_barriers[0][i] = rzalloc_array(pg->descriptor_data, struct zink_lazy_barrier, pg->descriptor_data->barrier_count[0][i]);
+         pg->descriptor_data->total_barrier_count[0] += pg->descriptor_data->barrier_count[0][i];
+      }
+      if (pg->descriptor_data->barrier_count[1][i]) {
+         pg->descriptor_data->type_barriers[1][i] = rzalloc_array(pg->descriptor_data, struct zink_lazy_barrier, pg->descriptor_data->barrier_count[1][i]);
+         pg->descriptor_data->total_barrier_count[1] += pg->descriptor_data->barrier_count[1][i];
+      }
+   }
+   for (int i = 0; num_bindings && i < num_shaders; i++) {
+      struct zink_shader *shader = stages[i];
+      if (!shader)
+         continue;
+
+      enum pipe_shader_type stage = pipe_shader_type_from_mesa(shader->nir->info.stage);
+      unsigned buffer_barrier_idx = 0;
+      unsigned image_barrier_idx = 0;
+
+      for (int j = 0; j < ZINK_DESCRIPTOR_TYPES; j++) {
+         for (int k = 0; k < shader->num_bindings[j]; k++) {
+            /* dynamic ubos handled in push */
+            if (shader->bindings[j][k].type == VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER_DYNAMIC)
+               continue;
+            int index = shader->bindings[j][k].index;
+            unsigned stage_idx = pg->is_compute ? 0 : stage;
+            VkAccessFlags flags = get_access_flags_for_binding(ctx, stage, j, index);
+            VkImageLayout layout = 0;
+            bool is_image = false;
+            switch (shader->bindings[j][k].type) {
+            case VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER:
+               layout = VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL;
+               is_image = true;
+               break;
+            case VK_DESCRIPTOR_TYPE_STORAGE_IMAGE:
+               layout = VK_IMAGE_LAYOUT_GENERAL;
+               is_image = true;
+               break;
+            default:
+               break;
+            }
+            for (unsigned l = 0; l < shader->bindings[j][k].size; l++) {
+               unsigned barrier_idx = is_image ? image_barrier_idx++ : buffer_barrier_idx++;
+               struct zink_lazy_barrier *type_barrier = &pg->descriptor_data->type_barriers[is_image][stage_idx][barrier_idx];
+               type_barrier->res = &ctx->di.descriptor_res[j][stage][index + l];
+               type_barrier->slot = index + l;
+               type_barrier->type = j;
+               switch (shader->bindings[j][k].type) {
+               case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER:
+               case VK_DESCRIPTOR_TYPE_STORAGE_BUFFER:
+                  init_template_entry(shader, j, k, l, &entries[entry_idx], &entry_idx);
+                  type_barrier->surface = NULL;
+                  break;
+               case VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER:
+                  type_barrier->surface = &ctx->di.sampler_surfaces[stage][index + l];
+                  break;
+               case VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER:
+                  type_barrier->surface = &ctx->di.sampler_surfaces[stage][index + l];
+                  break;
+               case VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER:
+                  type_barrier->surface = &ctx->di.sampler_surfaces[stage][index + l];
+                  break;
+               case VK_DESCRIPTOR_TYPE_STORAGE_IMAGE:
+                  type_barrier->surface = &ctx->di.image_surfaces[stage][index + l];
+                  break;
+               default:
+                  break;
+               }
+               type_barrier->layout = layout;
+               type_barrier->access = flags;
+               type_barrier->stage = zink_pipeline_flags_from_stage(zink_shader_stage(stage));
+            }
+         }
+      }
+   }
+
+   VkDescriptorUpdateTemplateCreateInfo template[2] = {};
+   VkDescriptorUpdateTemplateType types[2] = {
+      VK_DESCRIPTOR_UPDATE_TEMPLATE_TYPE_DESCRIPTOR_SET,
+      VK_DESCRIPTOR_UPDATE_TEMPLATE_TYPE_PUSH_DESCRIPTORS_KHR
+   };
+   unsigned wd_count[2] = {
+      pg->descriptor_data->layout_key ? pg->descriptor_data->layout_key->num_descriptors : 0,
+      pg->is_compute ? 1 : ZINK_SHADER_COUNT
+   };
+   VkDescriptorUpdateTemplateEntry *push_entries[2] = {
+      ctx->descriptor_data->push_entries,
+      &ctx->descriptor_data->push_entries[PIPE_SHADER_COMPUTE],
+   };
+   for (unsigned i = !num_bindings; i < 1 + !!push_count; i++) {
+      template[i].sType = VK_STRUCTURE_TYPE_DESCRIPTOR_UPDATE_TEMPLATE_CREATE_INFO;
+      template[i].descriptorUpdateEntryCount = wd_count[i];
+      template[i].pDescriptorUpdateEntries = i ? push_entries[pg->is_compute] : entries;
+      template[i].templateType = types[i];
+      template[i].descriptorSetLayout = pg->dsl[i];
+      template[i].pipelineBindPoint = pg->is_compute ? VK_PIPELINE_BIND_POINT_COMPUTE : VK_PIPELINE_BIND_POINT_GRAPHICS;
+      template[i].pipelineLayout = pg->layout;
+      template[i].set = i;
+      if (screen->vk_CreateDescriptorUpdateTemplate(screen->dev, &template[i], NULL, &pg->descriptor_data->templates[i]) != VK_SUCCESS)
+         return false;
+   }
+   return true;
+}
+
+void
+zink_descriptor_program_deinit_lazy(struct zink_screen *screen, struct zink_program *pg)
+{
+   if (!pg->descriptor_data)
+      return;
+   for (unsigned i = 0; i < 1 + !!pg->descriptor_data->push_usage; i++) {
+      if (pg->descriptor_data->templates[i])
+         screen->vk_DestroyDescriptorUpdateTemplate(screen->dev, pg->descriptor_data->templates[i], NULL);
+   }
+   ralloc_free(pg->descriptor_data);
+}
+
+static VkDescriptorPool
+create_pool(struct zink_screen *screen, unsigned num_type_sizes, VkDescriptorPoolSize *sizes, unsigned flags)
+{
+   VkDescriptorPool pool;
+   VkDescriptorPoolCreateInfo dpci = {};
+   dpci.sType = VK_STRUCTURE_TYPE_DESCRIPTOR_POOL_CREATE_INFO;
+   dpci.pPoolSizes = sizes;
+   dpci.poolSizeCount = num_type_sizes;
+   dpci.flags = flags;
+   dpci.maxSets = ZINK_DEFAULT_MAX_DESCS;
+   if (vkCreateDescriptorPool(screen->dev, &dpci, 0, &pool) != VK_SUCCESS) {
+      debug_printf("vkCreateDescriptorPool failed\n");
+      return VK_NULL_HANDLE;
+   }
+   return pool;
+}
+
+static struct zink_descriptor_pool *
+get_descriptor_pool_lazy(struct zink_context *ctx, struct zink_program *pg, struct zink_batch_state *bs)
+{
+   struct zink_screen *screen = zink_screen(ctx->base.screen);
+   struct hash_entry *he = _mesa_hash_table_search(&bs->dd->pools, pg->descriptor_data->layout_key);
+   if (he)
+      return he->data;
+   struct zink_descriptor_pool *pool = rzalloc(bs, struct zink_descriptor_pool);
+   if (!pool)
+      return NULL;
+
+   pool->pool = create_pool(screen, pg->descriptor_data->num_type_sizes, pg->descriptor_data->sizes, VK_DESCRIPTOR_POOL_CREATE_UPDATE_AFTER_BIND_BIT);
+   if (!pool->pool) {
+      ralloc_free(pool);
+      return NULL;
+   }
+   util_dynarray_init(&pool->resolves, pool);
+   _mesa_hash_table_insert(&bs->dd->pools, pg->descriptor_data->layout_key, pool);
+   return pool;
+}
+
+static VkDescriptorSet
+get_descriptor_set_lazy(struct zink_context *ctx, struct zink_program *pg, struct zink_descriptor_pool *pool)
+{
+   struct zink_screen *screen = zink_screen(ctx->base.screen);
+   if (!pool)
+      return VK_NULL_HANDLE;
+
+   if (pool->set_idx < pool->sets_alloc)
+      return pool->sets[pool->set_idx++];
+
+   /* allocate up to $current * 10, e.g., 10 -> 100 or 100 -> 1000 */
+   unsigned sets_to_alloc = MIN2(MAX2(pool->sets_alloc * 10, 10), ZINK_DEFAULT_MAX_DESCS) - pool->sets_alloc;
+   if (!sets_to_alloc) {//pool full
+      zink_fence_wait(&ctx->base);
+      return get_descriptor_set_lazy(ctx, pg, pool);
+   }
+   if (!zink_descriptor_util_alloc_sets(screen, pg->dsl[0], pool->pool, &pool->sets[pool->sets_alloc], sets_to_alloc))
+      return VK_NULL_HANDLE;
+   pool->sets_alloc += sets_to_alloc;
+   return pool->sets[pool->set_idx++];
+}
+
+static struct zink_descriptor_resolve_data *
+get_resolve(struct zink_descriptor_pool *pool, struct zink_program *pg)
+{
+   unsigned count = util_dynarray_num_elements(&pool->resolves, struct zink_descriptor_resolve_data*);
+   unsigned grow = MIN2(MAX2(count * 10, 10), 100);
+
+   if (pool->cur_resolve == count) {
+      struct zink_descriptor_resolve_data **rd_ptrs = util_dynarray_grow(&pool->resolves, struct zink_descriptor_resolve_data*, grow);
+      struct zink_descriptor_resolve_data *rds = ralloc_array(pool, struct zink_descriptor_resolve_data, grow);
+      for (unsigned i = 0; i < grow; i++) {
+         struct zink_descriptor_resolve_data *rd = &rds[i];
+         rd_ptrs[i] = rd;
+
+         for (unsigned j = 0; j < 2; j++) {
+            if (pg->descriptor_data->total_barrier_count[j]) {
+               _mesa_set_init(&rd->barrier_sort[j], pool, barrier_hash_lazy, _mesa_key_pointer_equal);
+               _mesa_set_resize(&rd->barrier_sort[j], pg->descriptor_data->total_barrier_count[j]);
+               rd->sort_barriers[j] = rzalloc_array(pool, struct zink_resolve_barrier, pg->descriptor_data->total_barrier_count[j]);
+            }
+         }
+      }
+   }
+   struct zink_descriptor_resolve_data *rd = *util_dynarray_element(&pool->resolves, struct zink_descriptor_resolve_data*, pool->cur_resolve++);
+   return rd;
+}
+
+static inline void
+add_surface_ref(struct zink_context *ctx, struct zink_resource *res, union zink_descriptor_surface *surface)
+{
+   if (!surface)
+      return;
+   if (res->obj->is_buffer) {
+      if (surface->bufferview)
+         zink_batch_reference_bufferview(&ctx->batch, surface->bufferview);
+   } else if (surface->surface)
+         zink_batch_reference_surface(&ctx->batch, surface->surface);
+}
+
+static void
+emit_buffer_barriers_hashed(struct zink_context *ctx, struct zink_program *pg, struct zink_descriptor_resolve_data *rd)
+{
+   unsigned num_shaders = pg->is_compute ? 1 : ZINK_SHADER_COUNT;
+   for (unsigned j = 0; j < num_shaders; j++) {
+      for (unsigned k = 0; k < pg->descriptor_data->barrier_count[0][j]; k++) {
+         enum pipe_shader_type pstage = pg->is_compute ? PIPE_SHADER_COMPUTE : j;
+         struct zink_lazy_barrier *barrier = &pg->descriptor_data->type_barriers[0][j][k];
+         struct zink_resource *res = *barrier->res;
+         if (!res)
+            continue;
+         add_surface_ref(ctx, res, barrier->surface);
+         if (barrier->type == ZINK_DESCRIPTOR_TYPE_SSBO || barrier->type == ZINK_DESCRIPTOR_TYPE_IMAGE)
+            /* this isn't known at program init */
+            barrier->access = get_access_flags_for_binding(ctx, pstage, barrier->type, barrier->slot);
+         /* fastpath for single bind or read-only */
+         if (!res->write_bind_count[pg->is_compute] || res->bind_count[pg->is_compute] == 1) {
+            zink_resource_buffer_barrier(ctx, NULL, res, barrier->access, barrier->stage);
+            zink_batch_reference_resource_rw(&ctx->batch, res, zink_resource_access_is_write(barrier->access));
+            continue;
+         }
+
+         bool found = false;
+         struct zink_resolve_barrier *rb = &rd->sort_barriers[0][rd->barrier_sort[0].entries];
+         rb->res = res;
+         struct set_entry *entry = _mesa_set_search_or_add(&rd->barrier_sort[0], rb, &found);
+         rb = (void*)entry->key;
+         if (!found)
+            rb->valid_stages = 0;
+         unsigned stage = tgsi_processor_to_shader_stage(pstage);
+         rb->valid_stages |= BITFIELD64_BIT(stage);
+         rb->type[stage] = zink_vktype_to_descriptor_barrier_type(barrier->layout, barrier->access);
+         rb->barriers[stage] = barrier;
+      }
+   }
+   if (!rd->barrier_sort[0].entries)
+      return;
+   set_foreach_remove(&rd->barrier_sort[0], entry) {
+      struct zink_resolve_barrier *rb = (void*)entry->key;
+      struct zink_resource *res = rb->res;
+      VkPipelineStageFlagBits src[PIPE_SHADER_TYPES];
+      VkPipelineStageFlagBits dst[PIPE_SHADER_TYPES] = {};
+      bool has_write = false;
+      bool batch_write = false;
+      VkMemoryBarrier b[PIPE_SHADER_TYPES];
+      if (zink_resource_access_is_write(res->access)) {
+         src[0] = res->access_stage;
+         b[0].srcAccessMask = res->access;
+         has_write = true;
+      } else {
+         src[0] = VK_PIPELINE_STAGE_TOP_OF_PIPE_BIT;
+         b[0].srcAccessMask = 0;
+      }
+      unsigned num_barriers = 0;
+      while (rb->valid_stages) {
+         unsigned stage = u_bit_scan(&rb->valid_stages);
+         struct zink_lazy_barrier *barrier = rb->barriers[stage];
+
+         switch (rb->type[stage]) {
+         case ZINK_DESCRIPTOR_BARRIER_BUFFER_READ:
+            if (has_write) {
+               has_write = false;
+               b[num_barriers].dstAccessMask = barrier->access;
+               dst[num_barriers] = barrier->stage;
+               num_barriers++;
+               src[num_barriers] = VK_PIPELINE_STAGE_TOP_OF_PIPE_BIT;
+               b[num_barriers].srcAccessMask = 0;
+            }
+            break;
+         case ZINK_DESCRIPTOR_BARRIER_BUFFER_WRITE:
+            has_write = batch_write = true;
+            b[num_barriers].srcAccessMask = barrier->access;
+            src[num_barriers] = barrier->stage;
+            break;
+         case ZINK_DESCRIPTOR_BARRIER_BUFFER_RW:
+            has_write = batch_write = true;
+            if (has_write) {
+               b[num_barriers].dstAccessMask = barrier->access;
+               dst[num_barriers] = barrier->stage;
+               num_barriers++;
+               src[num_barriers] = barrier->stage;
+               b[num_barriers].srcAccessMask = barrier->access;
+            } else {
+               b[num_barriers].srcAccessMask = barrier->access;
+               src[num_barriers] = barrier->stage;
+            }
+            break;
+         default:
+            unreachable("impossible barrier type");
+         }
+
+         if (!rb->valid_stages) {
+            res->access = barrier->access;
+            res->access_stage = barrier->stage;
+         }
+      }
+      zink_batch_reference_resource_rw(&ctx->batch, res, has_write);
+      for (unsigned i = 0; i < num_barriers; i++) {
+         zink_batch_no_rp(ctx);
+         vkCmdPipelineBarrier(
+            ctx->batch.state->cmdbuf,
+            src[i],
+            dst[i],
+            0,
+            1, &b[i],
+            0, NULL,
+            0, NULL
+         );
+      }
+   }
+}
+
+static void
+init_image_barrier(struct zink_resource *res, VkImageMemoryBarrier *ib)
+{
+    ib->sType = VK_STRUCTURE_TYPE_IMAGE_MEMORY_BARRIER;
+    ib->pNext = NULL;
+    ib->srcQueueFamilyIndex = VK_QUEUE_FAMILY_IGNORED;
+    ib->dstQueueFamilyIndex = VK_QUEUE_FAMILY_IGNORED;
+    ib->image = res->obj->image;
+    ib->subresourceRange.aspectMask = res->aspect;
+    ib->subresourceRange.baseArrayLayer = 0;
+    ib->subresourceRange.layerCount = VK_REMAINING_ARRAY_LAYERS;
+    ib->subresourceRange.baseMipLevel = 0;
+    ib->subresourceRange.levelCount = VK_REMAINING_MIP_LEVELS;
+}
+
+static void
+emit_image_barriers_hashed(struct zink_context *ctx, struct zink_program *pg, struct zink_descriptor_resolve_data *rd)
+{
+   unsigned num_shaders = pg->is_compute ? 1 : ZINK_SHADER_COUNT;
+   for (unsigned j = 0; j < num_shaders; j++) {
+      for (unsigned k = 0; k < pg->descriptor_data->barrier_count[1][j]; k++) {
+         enum pipe_shader_type pstage = pg->is_compute ? PIPE_SHADER_COMPUTE : j;
+         struct zink_lazy_barrier *barrier = &pg->descriptor_data->type_barriers[1][j][k];
+         struct zink_resource *res = *barrier->res;
+         if (!res)
+            continue;
+         add_surface_ref(ctx, res, barrier->surface);
+         if (barrier->type == ZINK_DESCRIPTOR_TYPE_IMAGE)
+            /* this isn't known at program init */
+            barrier->access = get_access_flags_for_binding(ctx, pstage, barrier->type, barrier->slot);
+         /* fastpath for single bind or read-only */
+         if (!res->write_bind_count[pg->is_compute] || res->bind_count[pg->is_compute] == 1) {
+            VkImageLayout layout = res->image_bind_count[pg->is_compute] ? VK_IMAGE_LAYOUT_GENERAL : barrier->layout;
+            zink_resource_image_barrier(ctx, NULL, res, layout, barrier->access, barrier->stage);
+            zink_batch_reference_resource_rw(&ctx->batch, res, res->image_bind_count ?
+                                             zink_resource_access_is_write(barrier->access) :
+                                             false);
+            continue;
+         }
+
+         bool found = false;
+         struct zink_resolve_barrier *rb = &rd->sort_barriers[1][rd->barrier_sort[1].entries];
+         rb->res = res;
+         struct set_entry *entry = _mesa_set_search_or_add(&rd->barrier_sort[1], rb, &found);
+         rb = (void*)entry->key;
+         if (!found) {
+            rb->valid_stages = 0;
+            rb->final_layout = 0;
+         }
+         unsigned stage = tgsi_processor_to_shader_stage(pstage);
+         rb->valid_stages |= BITFIELD64_BIT(stage);
+         rb->type[stage] = zink_vktype_to_descriptor_barrier_type(barrier->layout, barrier->access);
+         rb->barriers[stage] = barrier;
+         /* clamp to avoid doing ZINK_SHADER_COUNT number of transitions */
+         rb->final_layout = rb->final_layout == VK_IMAGE_LAYOUT_GENERAL ? rb->final_layout : barrier->layout;
+      }
+   }
+   if (!rd->barrier_sort[1].entries)
+      return;
+   set_foreach_remove(&rd->barrier_sort[1], entry) {
+      struct zink_resolve_barrier *rb = (void*)entry->key;
+      struct zink_resource *res = rb->res;
+      VkPipelineStageFlagBits src[PIPE_SHADER_TYPES];
+      VkPipelineStageFlagBits dst[PIPE_SHADER_TYPES] = {};
+      src[0] = res->access_stage;
+      VkImageLayout layout = res->layout;
+      bool has_write = false;
+      bool batch_write = false;
+      unsigned num_barriers = 0;
+      VkImageMemoryBarrier b[PIPE_SHADER_TYPES];
+      if (zink_resource_access_is_write(res->access)) {
+         src[0] = res->access_stage;
+         b[0].srcAccessMask = res->access;
+         has_write = true;
+      } else {
+         src[0] = VK_PIPELINE_STAGE_TOP_OF_PIPE_BIT;
+         b[0].srcAccessMask = 0;
+      }
+      while (rb->valid_stages) {
+         unsigned stage = u_bit_scan(&rb->valid_stages);
+         struct zink_lazy_barrier *barrier = rb->barriers[stage];
+
+         switch (rb->type[stage]) {
+         case ZINK_DESCRIPTOR_BARRIER_IMAGE_READ:
+         case ZINK_DESCRIPTOR_BARRIER_SAMPLERVIEW:
+            if (has_write) {
+               has_write = false;
+               init_image_barrier(res, &b[num_barriers]);
+               b[num_barriers].dstAccessMask = barrier->access;
+               b[num_barriers].oldLayout = layout;
+               b[num_barriers].newLayout = rb->final_layout;
+               layout = rb->final_layout;
+               dst[num_barriers] = barrier->stage;
+               num_barriers++;
+               src[num_barriers] = VK_PIPELINE_STAGE_TOP_OF_PIPE_BIT;
+               b[num_barriers].srcAccessMask = 0;
+            }
+            break;
+         case ZINK_DESCRIPTOR_BARRIER_IMAGE_WRITE:
+            has_write = batch_write = true;
+            b[num_barriers].srcAccessMask = barrier->access;
+            src[num_barriers] = barrier->stage;
+            break;
+         case ZINK_DESCRIPTOR_BARRIER_IMAGE_RW:
+            has_write = batch_write = true;
+            if (has_write) {
+               init_image_barrier(res, &b[num_barriers]);
+               b[num_barriers].dstAccessMask = barrier->access;
+               b[num_barriers].oldLayout = layout;
+               b[num_barriers].newLayout = rb->final_layout;
+               layout = rb->final_layout;
+               dst[num_barriers] = barrier->stage;
+               num_barriers++;
+               src[num_barriers] = barrier->stage;
+               b[num_barriers].srcAccessMask = barrier->access;
+            } else {
+               b[num_barriers].srcAccessMask = barrier->access;
+               src[num_barriers] = barrier->stage;
+            }
+            break;
+         default:
+            unreachable("impossible barrier type");
+         }
+
+         if (!rb->valid_stages) {
+            res->access = barrier->access;
+            res->access_stage = barrier->stage;
+            res->layout = barrier->layout;
+         }
+      }
+      zink_batch_reference_resource_rw(&ctx->batch, res, has_write);
+      for (unsigned i = 0; i < num_barriers; i++) {
+         zink_batch_no_rp(ctx);
+         vkCmdPipelineBarrier(
+            ctx->batch.state->cmdbuf,
+            src[i],
+            dst[i],
+            0,
+            0, NULL,
+            0, NULL,
+            1, &b[i]
+         );
+      }
+   }
+}
+
+struct set *
+zink_descriptors_update_lazy(struct zink_context *ctx, bool is_compute)
+{
+   struct zink_screen *screen = zink_screen(ctx->base.screen);
+   struct zink_batch *batch = &ctx->batch;
+   struct zink_batch_state *bs = ctx->batch.state;
+   struct zink_program *pg = is_compute ? zink_program(ctx->curr_compute) : zink_program(ctx->curr_program);
+   struct zink_descriptor_pool *pool = NULL;
+   VkDescriptorSet desc_set = VK_NULL_HANDLE;
+   //TIMING_START(update_lazy);
+
+   if (pg->descriptor_data->layout_key) {
+      //TIMING_START(update_lazy_init);
+      pool = get_descriptor_pool_lazy(ctx, pg, bs);
+      desc_set = get_descriptor_set_lazy(ctx, pg, pool);
+
+      assert(pg->descriptor_data->layout_key->num_descriptors);
+      screen->vk_UpdateDescriptorSetWithTemplate(screen->dev, desc_set, pg->descriptor_data->templates[0], ctx);
+      //TIMING_END(update_lazy_init);
+
+      struct zink_descriptor_resolve_data *rd = get_resolve(pool, pg);
+
+      //TIMING_START(type_barriers);
+         if (pg->descriptor_data->total_barrier_count[0])
+            emit_buffer_barriers_hashed(ctx, pg, rd);
+         if (pg->descriptor_data->total_barrier_count[1])
+            emit_image_barriers_hashed(ctx, pg, rd);
+   }
+   //TIMING_END(type_barriers);
+
+   if (!is_compute)
+      batch = zink_batch_rp(ctx);
+   zink_batch_reference_program(batch, pg);
+
+   if (pg->descriptor_data->layout_key)
+      vkCmdBindDescriptorSets(batch->state->cmdbuf,
+                              is_compute ? VK_PIPELINE_BIND_POINT_COMPUTE : VK_PIPELINE_BIND_POINT_GRAPHICS,
+                              pg->layout, 0, 1, &desc_set,
+                              0, NULL);
+
+//TIMING_START(push);
+   if (pg->descriptor_data->push_usage) {
+      unsigned max_shader = pg->is_compute ? PIPE_SHADER_TYPES : ZINK_SHADER_COUNT;
+      for (unsigned i = pg->is_compute ? PIPE_SHADER_COMPUTE : 0; i < max_shader; i++) {
+         if (pg->descriptor_data->push_usage & ctx->di.push_valid & BITFIELD64_BIT(i))
+            zink_batch_reference_resource_rw(&ctx->batch, ctx->di.descriptor_res[0][i][0], false);
+      }
+      if (!pg->descriptor_data->layout_key) {
+         vkCmdBindDescriptorSets(batch->state->cmdbuf,
+                                 is_compute ? VK_PIPELINE_BIND_POINT_COMPUTE : VK_PIPELINE_BIND_POINT_GRAPHICS,
+                                 pg->layout, 0, 1, &ctx->descriptor_data->dummy_set,
+                                 0, NULL);
+      }
+      screen->vk_CmdPushDescriptorSetWithTemplateKHR(batch->state->cmdbuf, pg->descriptor_data->templates[1],
+                                                  pg->layout, 1, ctx);
+      //TIMING_END(push);
+   }
+
+//TIMING_END(update_lazy);
+//TIMING_PRINT(update_lazy_init, "update_lazy_init");
+//if (rd)
+//TIMING_PRINT(thread, "thread");
+//if (pg->descriptor_data->push_usage)
+//TIMING_PRINT(push, "push");
+//TIMING_PRINT(type_barriers, "type_barriers");
+//TIMING_PRINT(update_lazy, "update_lazy");
+   return NULL;
+}
+
+void
+zink_context_invalidate_descriptor_state_lazy(struct zink_context *ctx, enum pipe_shader_type shader, enum zink_descriptor_type type, unsigned start, unsigned count)
+{
+}
+
+void
+zink_batch_descriptor_deinit_lazy(struct zink_screen *screen, struct zink_batch_state *bs)
+{
+   if (!bs->dd)
+      return;
+   hash_table_foreach(&bs->dd->pools, entry) {
+      struct zink_descriptor_pool *pool = (void*)entry->data;
+      vkDestroyDescriptorPool(screen->dev, pool->pool, NULL);
+   }
+   ralloc_free(bs->dd);
+}
+
+void
+zink_batch_descriptor_reset_lazy(struct zink_screen *screen, struct zink_batch_state *bs)
+{
+   hash_table_foreach(&bs->dd->pools, entry) {
+      struct zink_descriptor_pool *pool = (void*)entry->data;
+      pool->set_idx = 0;
+      pool->cur_resolve = 0;
+   }
+}
+
+bool
+zink_batch_descriptor_init_lazy(struct zink_batch_state *bs)
+{
+   bs->dd = rzalloc(bs, struct zink_batch_descriptor_data);
+   if (!bs->dd)
+      return false;
+   for (unsigned i = 0; i < ZINK_DESCRIPTOR_TYPES; i++) {
+      if (!_mesa_hash_table_init(&bs->dd->pools, bs->dd, _mesa_hash_pointer, _mesa_key_pointer_equal))
+         return false;
+   }
+   return true;
+}
+
+bool
+zink_descriptors_init_lazy(struct zink_context *ctx)
+{
+   ctx->descriptor_data = rzalloc(ctx, struct zink_descriptor_data);
+   if (!ctx->descriptor_data)
+      return false;
+
+   VkDescriptorSetLayoutBinding bindings[PIPE_SHADER_TYPES];
+   for (unsigned i = 0; i < PIPE_SHADER_TYPES; i++) {
+      zink_context_invalidate_descriptor_state_lazy(ctx, i, ZINK_DESCRIPTOR_TYPE_UBO, 0, PIPE_MAX_CONSTANT_BUFFERS);
+      zink_context_invalidate_descriptor_state_lazy(ctx, i, ZINK_DESCRIPTOR_TYPE_SSBO, 0, PIPE_MAX_CONSTANT_BUFFERS);
+
+      VkDescriptorUpdateTemplateEntry *entry = &ctx->descriptor_data->push_entries[i];
+      entry->dstBinding = tgsi_processor_to_shader_stage(i);
+      entry->descriptorCount = 1;
+      entry->descriptorType = VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER;
+      entry->offset = offsetof(struct zink_context, di.ubos[i][0]);
+      entry->stride = sizeof(VkDescriptorBufferInfo);
+
+      bindings[i].binding = tgsi_processor_to_shader_stage(i);
+      bindings[i].descriptorType = VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER;
+      bindings[i].descriptorCount = 1;
+      bindings[i].stageFlags = zink_shader_stage(i);
+      bindings[i].pImmutableSamplers = NULL;
+   }
+   struct zink_descriptor_layout_key *layout_key;
+   ctx->descriptor_data->push_dsl[0] = zink_descriptor_util_layout_get(ctx, 1, bindings, ZINK_SHADER_COUNT, &layout_key);
+   ctx->descriptor_data->push_dsl[1] = zink_descriptor_util_layout_get(ctx, 1, &bindings[PIPE_SHADER_COMPUTE], 1, &layout_key);
+   if (!ctx->descriptor_data->push_dsl[0] || !ctx->descriptor_data->push_dsl[1])
+      return false;
+
+   ctx->descriptor_data->dummy_dsl = zink_descriptor_util_layout_get(ctx, 2, bindings, 1, &layout_key);
+   VkDescriptorPoolSize null_size = {VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER, 1};
+   struct zink_screen *screen = zink_screen(ctx->base.screen);
+   ctx->descriptor_data->dummy_pool = create_pool(screen, 1, &null_size, 0);
+   zink_descriptor_util_alloc_sets(screen, ctx->descriptor_data->dummy_dsl,
+                                   ctx->descriptor_data->dummy_pool, &ctx->descriptor_data->dummy_set, 1);
+   VkDescriptorBufferInfo push_info;
+   VkWriteDescriptorSet push_wd;
+   push_wd.sType = VK_STRUCTURE_TYPE_WRITE_DESCRIPTOR_SET;
+   push_wd.pNext = NULL;
+   push_wd.dstBinding = 0;
+   push_wd.dstArrayElement = 0;
+   push_wd.descriptorCount = 1;
+   push_wd.descriptorType = VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER;
+   push_wd.dstSet = ctx->descriptor_data->dummy_set;
+   push_wd.pBufferInfo = &push_info;
+   push_info.buffer = screen->info.rb2_feats.nullDescriptor ?
+                      VK_NULL_HANDLE :
+                      zink_resource(ctx->dummy_vertex_buffer)->obj->buffer;
+   push_info.offset = 0;
+   push_info.range = VK_WHOLE_SIZE;
+   vkUpdateDescriptorSets(screen->dev, 1, &push_wd, 0, NULL);
+ printf("ZINK: USING EXPERIMENTAL LAZY DESCRIPTORS\n");
+   return !!ctx->descriptor_data->dummy_dsl;
+}
+
+void
+zink_descriptors_deinit_lazy(struct zink_context *ctx)
+{
+   ralloc_free(ctx->descriptor_data);
+}
diff --git a/src/gallium/drivers/zink/zink_device_info.py b/src/gallium/drivers/zink/zink_device_info.py
index 60bcfdc2eee..0efcf83f612 100644
--- a/src/gallium/drivers/zink/zink_device_info.py
+++ b/src/gallium/drivers/zink/zink_device_info.py
@@ -30,7 +30,7 @@ from zink_extensions import Extension,Version
 import sys
 
 # constructor: 
-#     Extension(name, alias="", required=False, properties=False, features=False, conditions=None, guard=False)
+#     Extensions(name, alias="", required=False, properties=False, features=False, conditions=None, guard=False)
 # The attributes:
 #  - required: the generated code debug_prints "ZINK: {name} required!" and
 #              returns NULL if the extension is unavailable.
@@ -64,9 +64,6 @@ EXTENSIONS = [
     Extension("VK_KHR_external_memory"),
     Extension("VK_KHR_external_memory_fd"),
     Extension("VK_KHR_vulkan_memory_model"),
-    Extension("VK_KHR_driver_properties",
-        alias="driver",
-        properties=True),
     Extension("VK_EXT_conditional_rendering",
         alias="cond_render", 
         features=True, 
@@ -119,6 +116,16 @@ EXTENSIONS = [
     Extension("VK_EXT_4444_formats",
         alias="format_4444",
         features=True),
+    Extension("VK_EXT_descriptor_indexing",
+        alias="desc_indexing",
+        features=True,
+        properties=True,
+        conditions=["$feats.descriptorBindingPartiallyBound"]),
+    Extension("VK_KHR_push_descriptor",
+        alias="push",
+        properties=True),
+    Extension("VK_KHR_descriptor_update_template",
+        alias="template"),
 ]
 
 # constructor: Versions(device_version(major, minor, patch), struct_version(major, minor))
@@ -137,9 +144,7 @@ VERSIONS = [
 # There exists some inconsistencies regarding the enum constants, fix them.
 # This is basically generated_code.replace(key, value).
 REPLACEMENTS = {
-    "ROBUSTNESS2": "ROBUSTNESS_2",
-    "PropertiesProperties": "Properties",
-    "PROPERTIES_PROPERTIES": "PROPERTIES",
+    "ROBUSTNESS2": "ROBUSTNESS_2"
 }
 
 
diff --git a/src/gallium/drivers/zink/zink_draw.c b/src/gallium/drivers/zink/zink_draw.c
index b3fcbde4f82..4372b282448 100644
--- a/src/gallium/drivers/zink/zink_draw.c
+++ b/src/gallium/drivers/zink/zink_draw.c
@@ -5,8 +5,10 @@
 #include "zink_resource.h"
 #include "zink_screen.h"
 #include "zink_state.h"
+#include "zink_surface.h"
 
 #include "indices/u_primconvert.h"
+#include "tgsi/tgsi_from_mesa.h"
 #include "util/hash_table.h"
 #include "util/u_debug.h"
 #include "util/u_helpers.h"
@@ -14,29 +16,6 @@
 #include "util/u_prim.h"
 #include "util/u_prim_restart.h"
 
-static VkDescriptorSet
-allocate_descriptor_set(struct zink_screen *screen,
-                        struct zink_batch *batch,
-                        struct zink_gfx_program *prog)
-{
-   assert(batch->descs_left >= prog->num_descriptors);
-   VkDescriptorSetAllocateInfo dsai;
-   memset((void *)&dsai, 0, sizeof(dsai));
-   dsai.sType = VK_STRUCTURE_TYPE_DESCRIPTOR_SET_ALLOCATE_INFO;
-   dsai.pNext = NULL;
-   dsai.descriptorPool = batch->descpool;
-   dsai.descriptorSetCount = 1;
-   dsai.pSetLayouts = &prog->dsl;
-
-   VkDescriptorSet desc_set;
-   if (vkAllocateDescriptorSets(screen->dev, &dsai, &desc_set) != VK_SUCCESS) {
-      debug_printf("ZINK: failed to allocate descriptor set :/");
-      return VK_NULL_HANDLE;
-   }
-
-   batch->descs_left -= prog->num_descriptors;
-   return desc_set;
-}
 
 static void
 zink_emit_xfb_counter_barrier(struct zink_context *ctx)
@@ -49,29 +28,18 @@ zink_emit_xfb_counter_barrier(struct zink_context *ctx)
     *
     * - from VK_EXT_transform_feedback spec
     */
-   VkBufferMemoryBarrier barriers[PIPE_MAX_SO_OUTPUTS] = {};
-   unsigned barrier_count = 0;
-
    for (unsigned i = 0; i < ctx->num_so_targets; i++) {
       struct zink_so_target *t = zink_so_target(ctx->so_targets[i]);
-      if (t->counter_buffer_valid) {
-          barriers[i].sType = VK_STRUCTURE_TYPE_BUFFER_MEMORY_BARRIER;
-          barriers[i].srcAccessMask = VK_ACCESS_TRANSFORM_FEEDBACK_COUNTER_WRITE_BIT_EXT;
-          barriers[i].dstAccessMask = VK_ACCESS_TRANSFORM_FEEDBACK_COUNTER_READ_BIT_EXT;
-          barriers[i].buffer = zink_resource(t->counter_buffer)->buffer;
-          barriers[i].size = VK_WHOLE_SIZE;
-          barrier_count++;
-      }
+      if (!t)
+         continue;
+      struct zink_resource *res = zink_resource(t->counter_buffer);
+      if (t->counter_buffer_valid)
+          zink_resource_buffer_barrier(ctx, NULL, res, VK_ACCESS_TRANSFORM_FEEDBACK_COUNTER_READ_BIT_EXT,
+                                       VK_PIPELINE_STAGE_DRAW_INDIRECT_BIT);
+      else
+          zink_resource_buffer_barrier(ctx, NULL, res, VK_ACCESS_TRANSFORM_FEEDBACK_COUNTER_WRITE_BIT_EXT,
+                                       VK_PIPELINE_STAGE_TRANSFORM_FEEDBACK_BIT_EXT);
    }
-   struct zink_batch *batch = zink_batch_no_rp(ctx);
-   vkCmdPipelineBarrier(batch->cmdbuf,
-      VK_PIPELINE_STAGE_TRANSFORM_FEEDBACK_BIT_EXT,
-      VK_PIPELINE_STAGE_DRAW_INDIRECT_BIT,
-      0,
-      0, NULL,
-      barrier_count, barriers,
-      0, NULL
-   );
    ctx->xfb_barrier = false;
 }
 
@@ -89,23 +57,8 @@ zink_emit_xfb_vertex_input_barrier(struct zink_context *ctx, struct zink_resourc
     *
     * - 20.3.1. Drawing Transform Feedback
     */
-   VkBufferMemoryBarrier barriers[1] = {};
-   barriers[0].sType = VK_STRUCTURE_TYPE_BUFFER_MEMORY_BARRIER;
-   barriers[0].srcAccessMask = VK_ACCESS_TRANSFORM_FEEDBACK_COUNTER_WRITE_BIT_EXT;
-   barriers[0].dstAccessMask = VK_ACCESS_VERTEX_ATTRIBUTE_READ_BIT;
-   barriers[0].buffer = res->buffer;
-   barriers[0].size = VK_WHOLE_SIZE;
-   struct zink_batch *batch = zink_batch_no_rp(ctx);
-   zink_batch_reference_resource_rw(batch, res, false);
-   vkCmdPipelineBarrier(batch->cmdbuf,
-      VK_PIPELINE_STAGE_TRANSFORM_FEEDBACK_BIT_EXT,
-      VK_PIPELINE_STAGE_VERTEX_INPUT_BIT,
-      0,
-      0, NULL,
-      ARRAY_SIZE(barriers), barriers,
-      0, NULL
-   );
-   res->needs_xfb_barrier = false;
+   zink_resource_buffer_barrier(ctx, NULL, res, VK_ACCESS_VERTEX_ATTRIBUTE_READ_BIT,
+                                VK_PIPELINE_STAGE_VERTEX_INPUT_BIT);
 }
 
 static void
@@ -113,55 +66,143 @@ zink_emit_stream_output_targets(struct pipe_context *pctx)
 {
    struct zink_context *ctx = zink_context(pctx);
    struct zink_screen *screen = zink_screen(pctx->screen);
-   struct zink_batch *batch = zink_curr_batch(ctx);
-   VkBuffer buffers[PIPE_MAX_SO_OUTPUTS];
-   VkDeviceSize buffer_offsets[PIPE_MAX_SO_OUTPUTS];
-   VkDeviceSize buffer_sizes[PIPE_MAX_SO_OUTPUTS];
+   struct zink_batch *batch = &ctx->batch;
+   VkBuffer buffers[PIPE_MAX_SO_OUTPUTS] = {};
+   VkDeviceSize buffer_offsets[PIPE_MAX_SO_OUTPUTS] = {};
+   VkDeviceSize buffer_sizes[PIPE_MAX_SO_OUTPUTS] = {};
 
    for (unsigned i = 0; i < ctx->num_so_targets; i++) {
       struct zink_so_target *t = (struct zink_so_target *)ctx->so_targets[i];
-      buffers[i] = zink_resource(t->base.buffer)->buffer;
+      if (!t) {
+         /* no need to reference this or anything */
+         buffers[i] = zink_resource(ctx->dummy_xfb_buffer)->obj->buffer;
+         buffer_offsets[i] = 0;
+         buffer_sizes[i] = sizeof(uint8_t);
+         continue;
+      }
+      buffers[i] = zink_resource(t->base.buffer)->obj->buffer;
+      zink_resource_buffer_barrier(ctx, NULL, zink_resource(t->base.buffer),
+                                   VK_ACCESS_TRANSFORM_FEEDBACK_WRITE_BIT_EXT, VK_PIPELINE_STAGE_TRANSFORM_FEEDBACK_BIT_EXT);
       zink_batch_reference_resource_rw(batch, zink_resource(t->base.buffer), true);
       buffer_offsets[i] = t->base.buffer_offset;
       buffer_sizes[i] = t->base.buffer_size;
    }
 
-   screen->vk_CmdBindTransformFeedbackBuffersEXT(batch->cmdbuf, 0, ctx->num_so_targets,
+   screen->vk_CmdBindTransformFeedbackBuffersEXT(batch->state->cmdbuf, 0, ctx->num_so_targets,
                                                  buffers, buffer_offsets,
                                                  buffer_sizes);
    ctx->dirty_so_targets = false;
 }
 
+static void
+barrier_vertex_buffers(struct zink_context *ctx)
+{
+   const struct zink_vertex_elements_state *elems = ctx->element_state;
+   for (unsigned i = 0; i < elems->hw_state.num_bindings; i++) {
+      struct pipe_vertex_buffer *vb = ctx->vertex_buffers + ctx->element_state->binding_map[i];
+      assert(vb);
+      if (vb->buffer.resource) {
+         struct zink_resource *res = zink_resource(vb->buffer.resource);
+         zink_resource_buffer_barrier(ctx, NULL, res, VK_ACCESS_VERTEX_ATTRIBUTE_READ_BIT,
+                                      VK_PIPELINE_STAGE_VERTEX_INPUT_BIT);
+      }
+   }
+}
+
+static void
+check_buffer_barrier(struct zink_context *ctx, struct pipe_resource *pres, VkAccessFlags flags, VkPipelineStageFlags pipeline)
+{
+   struct zink_resource *res = zink_resource(pres);
+   zink_resource_buffer_barrier(ctx, NULL, res, flags, pipeline);
+}
+
+static void
+barrier_draw_buffers(struct zink_context *ctx, const struct pipe_draw_info *dinfo,
+                     const struct pipe_draw_indirect_info *dindirect, struct pipe_resource *index_buffer)
+{
+   if (index_buffer)
+      check_buffer_barrier(ctx, index_buffer, VK_ACCESS_INDEX_READ_BIT, VK_PIPELINE_STAGE_VERTEX_INPUT_BIT);
+   if (dindirect && dindirect->buffer) {
+      check_buffer_barrier(ctx, dindirect->buffer,
+                           VK_ACCESS_INDIRECT_COMMAND_READ_BIT, VK_PIPELINE_STAGE_DRAW_INDIRECT_BIT);
+      if (dindirect->indirect_draw_count)
+         check_buffer_barrier(ctx, dindirect->indirect_draw_count,
+                              VK_ACCESS_INDIRECT_COMMAND_READ_BIT, VK_PIPELINE_STAGE_DRAW_INDIRECT_BIT);
+   }
+}
+
 static void
 zink_bind_vertex_buffers(struct zink_batch *batch, struct zink_context *ctx)
 {
    VkBuffer buffers[PIPE_MAX_ATTRIBS];
    VkDeviceSize buffer_offsets[PIPE_MAX_ATTRIBS];
+   VkDeviceSize buffer_strides[PIPE_MAX_ATTRIBS];
    const struct zink_vertex_elements_state *elems = ctx->element_state;
+   struct zink_screen *screen = zink_screen(ctx->base.screen);
+
+   if (!elems->hw_state.num_bindings)
+      return;
+
    for (unsigned i = 0; i < elems->hw_state.num_bindings; i++) {
-      struct pipe_vertex_buffer *vb = ctx->buffers + ctx->element_state->binding_map[i];
+      struct pipe_vertex_buffer *vb = ctx->vertex_buffers + ctx->element_state->binding_map[i];
       assert(vb);
       if (vb->buffer.resource) {
          struct zink_resource *res = zink_resource(vb->buffer.resource);
-         buffers[i] = res->buffer;
+         buffers[i] = res->obj->buffer;
          buffer_offsets[i] = vb->buffer_offset;
+         buffer_strides[i] = vb->stride;
          zink_batch_reference_resource_rw(batch, res, false);
       } else {
-         buffers[i] = zink_resource(ctx->dummy_buffer)->buffer;
+         buffers[i] = zink_resource(ctx->dummy_vertex_buffer)->obj->buffer;
          buffer_offsets[i] = 0;
+         buffer_strides[i] = 0;
       }
    }
 
-   if (elems->hw_state.num_bindings > 0)
-      vkCmdBindVertexBuffers(batch->cmdbuf, 0,
+   if (screen->info.have_EXT_extended_dynamic_state)
+      screen->vk_CmdBindVertexBuffers2EXT(batch->state->cmdbuf, 0,
+                                          elems->hw_state.num_bindings,
+                                          buffers, buffer_offsets, NULL, buffer_strides);
+   else
+      vkCmdBindVertexBuffers(batch->state->cmdbuf, 0,
                              elems->hw_state.num_bindings,
                              buffers, buffer_offsets);
 }
 
+static struct zink_compute_program *
+get_compute_program(struct zink_context *ctx)
+{
+   unsigned bits = 1 << PIPE_SHADER_COMPUTE;
+   ctx->dirty_shader_stages |= ctx->inlinable_uniforms_dirty_mask &
+                               ctx->inlinable_uniforms_valid_mask &
+                               ctx->shader_has_inlinable_uniforms_mask & bits;
+   if (ctx->dirty_shader_stages & bits) {
+      struct hash_entry *entry = _mesa_hash_table_search(ctx->compute_program_cache,
+                                                         &ctx->compute_stage->shader_id);
+      if (!entry) {
+         struct zink_compute_program *comp;
+         comp = zink_create_compute_program(ctx, ctx->compute_stage);
+         entry = _mesa_hash_table_insert(ctx->compute_program_cache, &comp->shader->shader_id, comp);
+         if (!entry)
+            return NULL;
+      }
+      ctx->curr_compute = entry->data;
+      ctx->dirty_shader_stages &= bits;
+      ctx->inlinable_uniforms_dirty_mask &= bits;
+   }
+
+   assert(ctx->curr_compute);
+   return ctx->curr_compute;
+}
+
 static struct zink_gfx_program *
 get_gfx_program(struct zink_context *ctx)
 {
-   if (ctx->dirty_shader_stages) {
+   unsigned bits = u_bit_consecutive(PIPE_SHADER_VERTEX, 5);
+   ctx->dirty_shader_stages |= ctx->inlinable_uniforms_dirty_mask &
+                               ctx->inlinable_uniforms_valid_mask &
+                               ctx->shader_has_inlinable_uniforms_mask & bits;
+   if (ctx->dirty_shader_stages & bits) {
       struct hash_entry *entry = _mesa_hash_table_search(ctx->program_cache,
                                                          ctx->gfx_stages);
       if (entry)
@@ -174,13 +215,32 @@ get_gfx_program(struct zink_context *ctx)
             return NULL;
       }
       ctx->curr_program = entry->data;
-      ctx->dirty_shader_stages = 0;
+      ctx->dirty_shader_stages &= ~bits;
+      ctx->inlinable_uniforms_dirty_mask &= ~bits;
    }
 
    assert(ctx->curr_program);
    return ctx->curr_program;
 }
 
+static void
+flush_persistent_maps(struct zink_screen *screen, struct set *persistent)
+{
+   set_foreach(persistent, entry) {
+      struct zink_resource *res = (struct zink_resource *)entry->key;
+      /* TODO: only flush the actual mapped region */
+      VkMappedMemoryRange range = {
+         VK_STRUCTURE_TYPE_MAPPED_MEMORY_RANGE,
+         NULL,
+         res->obj->mem,
+         res->obj->offset,
+         res->obj->size,
+      };
+      vkFlushMappedMemoryRanges(screen->dev, 1, &range);
+   }
+   _mesa_set_destroy(persistent, NULL);
+}
+
 static bool
 line_width_needed(enum pipe_prim_type reduced_prim,
                   VkPolygonMode polygon_mode)
@@ -206,27 +266,24 @@ restart_supported(enum pipe_prim_type mode)
     return mode == PIPE_PRIM_LINE_STRIP || mode == PIPE_PRIM_TRIANGLE_STRIP || mode == PIPE_PRIM_TRIANGLE_FAN;
 }
 
-void
-zink_draw_vbo(struct pipe_context *pctx,
-              const struct pipe_draw_info *dinfo,
-              const struct pipe_draw_indirect_info *dindirect,
-              const struct pipe_draw_start_count *draws,
-              unsigned num_draws)
+static void
+update_drawid(struct zink_context *ctx, unsigned draw_id)
 {
-   if (num_draws > 1) {
-      struct pipe_draw_info tmp_info = *dinfo;
-
-      for (unsigned i = 0; i < num_draws; i++) {
-         zink_draw_vbo(pctx, &tmp_info, dindirect, &draws[i], 1);
-         if (tmp_info.increment_draw_id)
-            tmp_info.drawid++;
-      }
-      return;
+   struct zink_batch *batch = zink_batch_rp(ctx);
+   if (ctx->gfx_stages[PIPE_SHADER_VERTEX]->nir->info.system_values_read & (1ull << SYSTEM_VALUE_DRAW_ID)) {
+      vkCmdPushConstants(batch->state->cmdbuf, ctx->curr_program->layout, VK_SHADER_STAGE_VERTEX_BIT,
+                         offsetof(struct zink_gfx_push_constant, draw_id), sizeof(unsigned),
+                         &draw_id);
    }
+}
 
-   if (!dindirect && (!draws[0].count || !dinfo->instance_count))
-      return;
-
+void
+zink_draw_vbo(struct pipe_context *pctx,
+                    const struct pipe_draw_info *dinfo,
+                    const struct pipe_draw_indirect_info *dindirect,
+                    const struct pipe_draw_start_count *draws,
+                    unsigned num_draws)
+{
    struct zink_context *ctx = zink_context(pctx);
    struct zink_screen *screen = zink_screen(pctx->screen);
    struct zink_rasterizer_state *rast_state = ctx->rast_state;
@@ -238,6 +295,9 @@ zink_draw_vbo(struct pipe_context *pctx,
    VkDeviceSize counter_buffer_offsets[PIPE_MAX_SO_OUTPUTS] = {};
    bool need_index_buffer_unref = false;
 
+   /* check memory usage and flush/stall as needed to avoid oom */
+   zink_maybe_flush_or_stall(ctx);
+   zink_update_samplers(ctx, false);
 
    if (dinfo->primitive_restart && !restart_supported(dinfo->mode)) {
        util_draw_vbo_without_prim_restart(pctx, dinfo, dindirect, &draws[0]);
@@ -248,15 +308,17 @@ zink_draw_vbo(struct pipe_context *pctx,
        dinfo->mode == PIPE_PRIM_POLYGON ||
        (dinfo->mode == PIPE_PRIM_TRIANGLE_FAN && !screen->have_triangle_fans) ||
        dinfo->mode == PIPE_PRIM_LINE_LOOP) {
-      if (!u_trim_pipe_prim(dinfo->mode, (unsigned *)&draws[0].count))
-         return;
-
       util_primconvert_save_rasterizer_state(ctx->primconvert, &rast_state->base);
-      util_primconvert_draw_vbo(ctx->primconvert, dinfo, &draws[0]);
+      for (unsigned i = 0; i < num_draws; i++) {
+         /* TODO: is there actually a way to correctly handle this? no other driver does... */
+         if (!u_trim_pipe_prim(dinfo->mode, (unsigned *)&draws[i].count))
+            continue;
+         util_primconvert_draw_vbo(ctx->primconvert, dinfo, &draws[i]);
+      }
       return;
    }
    if (ctx->gfx_pipeline_state.vertices_per_patch != dinfo->vertices_per_patch)
-      ctx->gfx_pipeline_state.hash = 0;
+      ctx->gfx_pipeline_state.dirty = true;
    ctx->gfx_pipeline_state.vertices_per_patch = dinfo->vertices_per_patch;
    struct zink_gfx_program *gfx_program = get_gfx_program(ctx);
    if (!gfx_program)
@@ -266,19 +328,6 @@ zink_draw_vbo(struct pipe_context *pctx,
       ctx->gfx_pipeline_state.dirty = true;
    ctx->gfx_pipeline_state.primitive_restart = !!dinfo->primitive_restart;
 
-   for (unsigned i = 0; i < ctx->element_state->hw_state.num_bindings; i++) {
-      unsigned binding = ctx->element_state->binding_map[i];
-      const struct pipe_vertex_buffer *vb = ctx->buffers + binding;
-      if (ctx->gfx_pipeline_state.bindings[i].stride != vb->stride) {
-         ctx->gfx_pipeline_state.bindings[i].stride = vb->stride;
-         ctx->gfx_pipeline_state.dirty = true;
-      }
-   }
-
-   VkPipeline pipeline = zink_get_gfx_pipeline(screen, gfx_program,
-                                               &ctx->gfx_pipeline_state,
-                                               dinfo->mode);
-
    enum pipe_prim_type reduced_prim = u_reduced_prim(dinfo->mode);
 
    bool depth_bias = false;
@@ -317,217 +366,121 @@ zink_draw_vbo(struct pipe_context *pctx,
              index_buffer = dinfo->index.resource;
        }
    }
-
-   VkWriteDescriptorSet wds[PIPE_SHADER_TYPES * (PIPE_MAX_CONSTANT_BUFFERS + PIPE_MAX_SAMPLERS + PIPE_MAX_SHADER_BUFFERS)];
-   struct zink_resource *read_desc_resources[PIPE_SHADER_TYPES * (PIPE_MAX_CONSTANT_BUFFERS + PIPE_MAX_SAMPLERS + PIPE_MAX_SHADER_BUFFERS)] = {};
-   struct zink_resource *write_desc_resources[PIPE_SHADER_TYPES * (PIPE_MAX_CONSTANT_BUFFERS + PIPE_MAX_SAMPLERS + PIPE_MAX_SHADER_BUFFERS)] = {};
-   VkDescriptorBufferInfo buffer_infos[PIPE_SHADER_TYPES * (PIPE_MAX_CONSTANT_BUFFERS + PIPE_MAX_SHADER_BUFFERS)];
-   VkDescriptorImageInfo image_infos[PIPE_SHADER_TYPES * PIPE_MAX_SAMPLERS];
-   VkBufferView buffer_view[] = {VK_NULL_HANDLE};
-   int num_wds = 0, num_buffer_info = 0, num_image_info = 0;
-
-   struct zink_resource *transitions[PIPE_SHADER_TYPES * PIPE_MAX_SAMPLERS];
-   int num_transitions = 0;
-
-   for (int i = 0; i < ARRAY_SIZE(ctx->gfx_stages); i++) {
-      struct zink_shader *shader = ctx->gfx_stages[i];
-      if (!shader)
-         continue;
-
-      if (ctx->num_so_targets &&
-          (i == PIPE_SHADER_GEOMETRY ||
-          (i == PIPE_SHADER_TESS_EVAL && !ctx->gfx_stages[PIPE_SHADER_GEOMETRY]) ||
-          (i == PIPE_SHADER_VERTEX && !ctx->gfx_stages[PIPE_SHADER_GEOMETRY] && !ctx->gfx_stages[PIPE_SHADER_TESS_EVAL]))) {
-         for (unsigned i = 0; i < ctx->num_so_targets; i++) {
-            struct zink_so_target *t = zink_so_target(ctx->so_targets[i]);
-            t->stride = shader->streamout.so_info.stride[i] * sizeof(uint32_t);
-         }
-      }
-
-      for (int j = 0; j < shader->num_bindings; j++) {
-         int index = shader->bindings[j].index;
-         if (shader->bindings[j].type == VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER) {
-            assert(ctx->ubos[i][index].buffer_size <= screen->info.props.limits.maxUniformBufferRange);
-            struct zink_resource *res = zink_resource(ctx->ubos[i][index].buffer);
-            assert(!res || ctx->ubos[i][index].buffer_size > 0);
-            assert(!res || ctx->ubos[i][index].buffer);
-            read_desc_resources[num_wds] = res;
-            buffer_infos[num_buffer_info].buffer = res ? res->buffer :
-                                                   (screen->info.rb2_feats.nullDescriptor ?
-                                                    VK_NULL_HANDLE :
-                                                    zink_resource(ctx->dummy_buffer)->buffer);
-            buffer_infos[num_buffer_info].offset = res ? ctx->ubos[i][index].buffer_offset : 0;
-            buffer_infos[num_buffer_info].range  = res ? ctx->ubos[i][index].buffer_size : VK_WHOLE_SIZE;
-            wds[num_wds].pBufferInfo = buffer_infos + num_buffer_info;
-            ++num_buffer_info;
-         } else if (shader->bindings[j].type == VK_DESCRIPTOR_TYPE_STORAGE_BUFFER) {
-            assert(ctx->ssbos[i][index].buffer_size > 0);
-            assert(ctx->ssbos[i][index].buffer_size <= screen->info.props.limits.maxStorageBufferRange);
-            assert(ctx->ssbos[i][index].buffer);
-            struct zink_resource *res = zink_resource(ctx->ssbos[i][index].buffer);
-            if (ctx->writable_ssbos & (1 << index))
-               write_desc_resources[num_wds] = res;
-            else
-               read_desc_resources[num_wds] = res;
-            buffer_infos[num_buffer_info].buffer = res->buffer;
-            buffer_infos[num_buffer_info].offset = ctx->ssbos[i][index].buffer_offset;
-            buffer_infos[num_buffer_info].range  = ctx->ssbos[i][index].buffer_size;
-            wds[num_wds].pBufferInfo = buffer_infos + num_buffer_info;
-            ++num_buffer_info;
-         } else {
-            for (unsigned k = 0; k < shader->bindings[j].size; k++) {
-               struct pipe_sampler_view *psampler_view = ctx->image_views[i][index + k];
-               struct zink_sampler_view *sampler_view = zink_sampler_view(psampler_view);
-
-               struct zink_resource *res = psampler_view ? zink_resource(psampler_view->texture) : NULL;
-               read_desc_resources[num_wds] = res;
-               if (!res) {
-                  /* if we're hitting this assert often, we can probably just throw a junk buffer in since
-                   * the results of this codepath are undefined in ARB_texture_buffer_object spec
-                   */
-                  assert(screen->info.rb2_feats.nullDescriptor);
-                  if (shader->bindings[j].type == VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER)
-                     wds[num_wds].pTexelBufferView = &buffer_view[0];
-                  else {
-                     image_infos[num_image_info].imageLayout = VK_IMAGE_LAYOUT_UNDEFINED;
-                     image_infos[num_image_info].imageView = VK_NULL_HANDLE;
-                     image_infos[num_image_info].sampler = ctx->samplers[i][index + k];
-                     if (!k)
-                        wds[num_wds].pImageInfo = image_infos + num_image_info;
-                     ++num_image_info;
-                  }
-               } else if (res->base.target == PIPE_BUFFER)
-                  wds[num_wds].pTexelBufferView = &sampler_view->buffer_view;
-               else {
-                  if (res->layout != VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL)
-                     transitions[num_transitions++] = res;
-                  image_infos[num_image_info].imageLayout = VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL;
-                  image_infos[num_image_info].imageView = sampler_view->image_view;
-                  image_infos[num_image_info].sampler = ctx->samplers[i][index + k];
-                  if (!k)
-                     wds[num_wds].pImageInfo = image_infos + num_image_info;
-                  ++num_image_info;
-               }
-            }
-         }
-
-         wds[num_wds].sType = VK_STRUCTURE_TYPE_WRITE_DESCRIPTOR_SET;
-         wds[num_wds].pNext = NULL;
-         wds[num_wds].dstBinding = shader->bindings[j].binding;
-         wds[num_wds].dstArrayElement = 0;
-         wds[num_wds].descriptorCount = shader->bindings[j].size;
-         wds[num_wds].descriptorType = shader->bindings[j].type;
-         ++num_wds;
-      }
-   }
-
-   struct zink_batch *batch;
-   if (num_transitions > 0) {
-      batch = zink_batch_no_rp(ctx);
-
-      for (int i = 0; i < num_transitions; ++i)
-         zink_resource_barrier(batch->cmdbuf, transitions[i],
-                               transitions[i]->aspect,
-                               VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL);
-   }
-
    if (ctx->xfb_barrier)
       zink_emit_xfb_counter_barrier(ctx);
 
    if (ctx->dirty_so_targets && ctx->num_so_targets)
       zink_emit_stream_output_targets(pctx);
 
-   if (so_target && zink_resource(so_target->base.buffer)->needs_xfb_barrier)
+   if (so_target)
       zink_emit_xfb_vertex_input_barrier(ctx, zink_resource(so_target->base.buffer));
 
+   barrier_vertex_buffers(ctx);
+   barrier_draw_buffers(ctx, dinfo, dindirect, index_buffer);
 
-   batch = zink_batch_rp(ctx);
-
-   if (batch->descs_left < gfx_program->num_descriptors) {
-      ctx->base.flush(&ctx->base, NULL, 0);
-      batch = zink_batch_rp(ctx);
-      assert(batch->descs_left >= gfx_program->num_descriptors);
-   }
-   zink_batch_reference_program(batch, ctx->curr_program);
-
-   VkDescriptorSet desc_set = allocate_descriptor_set(screen, batch,
-                                                      gfx_program);
-   assert(desc_set != VK_NULL_HANDLE);
-
-   for (int i = 0; i < ARRAY_SIZE(ctx->gfx_stages); i++) {
+   for (int i = 0; i < ZINK_SHADER_COUNT; i++) {
       struct zink_shader *shader = ctx->gfx_stages[i];
       if (!shader)
          continue;
-
-      for (int j = 0; j < shader->num_bindings; j++) {
-         int index = shader->bindings[j].index;
-         if (shader->bindings[j].type != VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER) {
-            struct zink_sampler_view *sampler_view = zink_sampler_view(ctx->image_views[i][index]);
-            if (sampler_view)
-               zink_batch_reference_sampler_view(batch, sampler_view);
+      enum pipe_shader_type stage = pipe_shader_type_from_mesa(shader->nir->info.stage);
+      if (ctx->num_so_targets &&
+          (stage == PIPE_SHADER_GEOMETRY ||
+          (stage == PIPE_SHADER_TESS_EVAL && !ctx->gfx_stages[PIPE_SHADER_GEOMETRY]) ||
+          (stage == PIPE_SHADER_VERTEX && !ctx->gfx_stages[PIPE_SHADER_GEOMETRY] && !ctx->gfx_stages[PIPE_SHADER_TESS_EVAL]))) {
+         for (unsigned j = 0; j < ctx->num_so_targets; j++) {
+            struct zink_so_target *t = zink_so_target(ctx->so_targets[j]);
+            if (t)
+               t->stride = shader->streamout.so_info.stride[j] * sizeof(uint32_t);
          }
       }
    }
 
-   vkCmdSetViewport(batch->cmdbuf, 0, ctx->gfx_pipeline_state.num_viewports, ctx->viewports);
-   if (ctx->rast_state->base.scissor)
-      vkCmdSetScissor(batch->cmdbuf, 0, ctx->gfx_pipeline_state.num_viewports, ctx->scissors);
-   else if (ctx->fb_state.width && ctx->fb_state.height) {
-      VkRect2D fb_scissor[ctx->gfx_pipeline_state.num_viewports];
-      for (unsigned i = 0; i < ctx->gfx_pipeline_state.num_viewports; i++) {
-         fb_scissor[i].offset.x = fb_scissor[i].offset.y = 0;
-         fb_scissor[i].extent.width = ctx->fb_state.width;
-         fb_scissor[i].extent.height = ctx->fb_state.height;
+   struct set *persistent = NULL;
+   if (zink_program_has_descriptors(zink_program(gfx_program)))
+      persistent = screen->descriptors_update(ctx, false);
+
+   struct zink_batch *batch = zink_batch_rp(ctx);
+   VkViewport viewports[PIPE_MAX_VIEWPORTS] = {};
+   for (unsigned i = 0; i < ctx->vp_state.num_viewports; i++) {
+      VkViewport viewport = {
+         ctx->vp_state.viewport_states[i].translate[0] - ctx->vp_state.viewport_states[i].scale[0],
+         ctx->vp_state.viewport_states[i].translate[1] - ctx->vp_state.viewport_states[i].scale[1],
+         ctx->vp_state.viewport_states[i].scale[0] * 2,
+         ctx->vp_state.viewport_states[i].scale[1] * 2,
+         ctx->rast_state->base.clip_halfz ?
+            ctx->vp_state.viewport_states[i].translate[2] :
+            ctx->vp_state.viewport_states[i].translate[2] - ctx->vp_state.viewport_states[i].scale[2],
+         ctx->vp_state.viewport_states[i].translate[2] + ctx->vp_state.viewport_states[i].scale[2]
+      };
+      viewports[i] = viewport;
+   }
+   if (screen->info.have_EXT_extended_dynamic_state)
+      screen->vk_CmdSetViewportWithCountEXT(batch->state->cmdbuf, ctx->vp_state.num_viewports, viewports);
+   else
+      vkCmdSetViewport(batch->state->cmdbuf, 0, ctx->vp_state.num_viewports, viewports);
+   VkRect2D scissors[PIPE_MAX_VIEWPORTS] = {};
+   if (ctx->rast_state->base.scissor) {
+      for (unsigned i = 0; i < ctx->vp_state.num_viewports; i++) {
+         scissors[i].offset.x = ctx->vp_state.scissor_states[i].minx;
+         scissors[i].offset.y = ctx->vp_state.scissor_states[i].miny;
+         scissors[i].extent.width = ctx->vp_state.scissor_states[i].maxx - ctx->vp_state.scissor_states[i].minx;
+         scissors[i].extent.height = ctx->vp_state.scissor_states[i].maxy - ctx->vp_state.scissor_states[i].miny;
+      }
+   } else if (ctx->fb_state.width && ctx->fb_state.height) {
+      for (unsigned i = 0; i < ctx->vp_state.num_viewports; i++) {
+         scissors[i].extent.width = ctx->fb_state.width;
+         scissors[i].extent.height = ctx->fb_state.height;
       }
-      vkCmdSetScissor(batch->cmdbuf, 0, ctx->gfx_pipeline_state.num_viewports, fb_scissor);
    }
+   if (screen->info.have_EXT_extended_dynamic_state)
+      screen->vk_CmdSetScissorWithCountEXT(batch->state->cmdbuf, ctx->vp_state.num_viewports, scissors);
+   else
+      vkCmdSetScissor(batch->state->cmdbuf, 0, ctx->vp_state.num_viewports, scissors);
 
    if (line_width_needed(reduced_prim, rast_state->hw_state.polygon_mode)) {
       if (screen->info.feats.features.wideLines || ctx->line_width == 1.0f)
-         vkCmdSetLineWidth(batch->cmdbuf, ctx->line_width);
+         vkCmdSetLineWidth(batch->state->cmdbuf, ctx->line_width);
       else
          debug_printf("BUG: wide lines not supported, needs fallback!");
    }
 
    if (dsa_state->base.stencil[0].enabled) {
       if (dsa_state->base.stencil[1].enabled) {
-         vkCmdSetStencilReference(batch->cmdbuf, VK_STENCIL_FACE_FRONT_BIT,
+         vkCmdSetStencilReference(batch->state->cmdbuf, VK_STENCIL_FACE_FRONT_BIT,
                                   ctx->stencil_ref.ref_value[0]);
-         vkCmdSetStencilReference(batch->cmdbuf, VK_STENCIL_FACE_BACK_BIT,
+         vkCmdSetStencilReference(batch->state->cmdbuf, VK_STENCIL_FACE_BACK_BIT,
                                   ctx->stencil_ref.ref_value[1]);
       } else
-         vkCmdSetStencilReference(batch->cmdbuf,
+         vkCmdSetStencilReference(batch->state->cmdbuf,
                                   VK_STENCIL_FACE_FRONT_AND_BACK,
                                   ctx->stencil_ref.ref_value[0]);
    }
 
    if (depth_bias)
-      vkCmdSetDepthBias(batch->cmdbuf, rast_state->offset_units, rast_state->offset_clamp, rast_state->offset_scale);
+      vkCmdSetDepthBias(batch->state->cmdbuf, rast_state->offset_units, rast_state->offset_clamp, rast_state->offset_scale);
    else
-      vkCmdSetDepthBias(batch->cmdbuf, 0.0f, 0.0f, 0.0f);
+      vkCmdSetDepthBias(batch->state->cmdbuf, 0.0f, 0.0f, 0.0f);
 
    if (ctx->gfx_pipeline_state.blend_state->need_blend_constants)
-      vkCmdSetBlendConstants(batch->cmdbuf, ctx->blend_constants);
-
-   if (num_wds > 0) {
-      for (int i = 0; i < num_wds; ++i) {
-         wds[i].dstSet = desc_set;
-         if (read_desc_resources[i])
-            zink_batch_reference_resource_rw(batch, read_desc_resources[i], false);
-         else if (write_desc_resources[i])
-            zink_batch_reference_resource_rw(batch, write_desc_resources[i], true);
-      }
-      vkUpdateDescriptorSets(screen->dev, num_wds, wds, 0, NULL);
-   }
+      vkCmdSetBlendConstants(batch->state->cmdbuf, ctx->blend_constants);
+
+
+   VkPipeline pipeline = zink_get_gfx_pipeline(screen, gfx_program,
+                                               &ctx->gfx_pipeline_state,
+                                               dinfo->mode);
+   vkCmdBindPipeline(batch->state->cmdbuf, VK_PIPELINE_BIND_POINT_GRAPHICS, pipeline);
 
-   vkCmdBindPipeline(batch->cmdbuf, VK_PIPELINE_BIND_POINT_GRAPHICS, pipeline);
-   vkCmdBindDescriptorSets(batch->cmdbuf, VK_PIPELINE_BIND_POINT_GRAPHICS,
-                           gfx_program->layout, 0, 1, &desc_set, 0, NULL);
    zink_bind_vertex_buffers(batch, ctx);
 
+   if (ctx->gfx_stages[PIPE_SHADER_VERTEX]->nir->info.system_values_read & (1ull << SYSTEM_VALUE_BASE_VERTEX)) {
+      unsigned draw_mode_is_indexed = dinfo->index_size > 0;
+      vkCmdPushConstants(batch->state->cmdbuf, gfx_program->layout, VK_SHADER_STAGE_VERTEX_BIT,
+                         offsetof(struct zink_gfx_push_constant, draw_mode_is_indexed), sizeof(unsigned),
+                         &draw_mode_is_indexed);
+   }
    if (gfx_program->shaders[PIPE_SHADER_TESS_CTRL] && gfx_program->shaders[PIPE_SHADER_TESS_CTRL]->is_generated)
-      vkCmdPushConstants(batch->cmdbuf, gfx_program->layout, VK_SHADER_STAGE_TESSELLATION_CONTROL_BIT,
-                         0, sizeof(float) * 6,
+      vkCmdPushConstants(batch->state->cmdbuf, gfx_program->layout, VK_SHADER_STAGE_TESSELLATION_CONTROL_BIT,
+                         offsetof(struct zink_gfx_push_constant, default_inner_level), sizeof(float) * 6,
                          &ctx->tess_levels[0]);
 
    zink_query_update_gs_states(ctx);
@@ -535,17 +488,23 @@ zink_draw_vbo(struct pipe_context *pctx,
    if (ctx->num_so_targets) {
       for (unsigned i = 0; i < ctx->num_so_targets; i++) {
          struct zink_so_target *t = zink_so_target(ctx->so_targets[i]);
-         struct zink_resource *res = zink_resource(t->counter_buffer);
-         if (t->counter_buffer_valid) {
+         counter_buffers[i] = VK_NULL_HANDLE;
+         if (t) {
+            struct zink_resource *res = zink_resource(t->counter_buffer);
             zink_batch_reference_resource_rw(batch, res, true);
-            counter_buffers[i] = res->buffer;
-            counter_buffer_offsets[i] = t->counter_buffer_offset;
-         } else
-            counter_buffers[i] = VK_NULL_HANDLE;
+            if (t->counter_buffer_valid) {
+               counter_buffers[i] = res->obj->buffer;
+               counter_buffer_offsets[i] = t->counter_buffer_offset;
+            }
+         }
       }
-      screen->vk_CmdBeginTransformFeedbackEXT(batch->cmdbuf, 0, ctx->num_so_targets, counter_buffers, counter_buffer_offsets);
+      screen->vk_CmdBeginTransformFeedbackEXT(batch->state->cmdbuf, 0, ctx->num_so_targets, counter_buffers, counter_buffer_offsets);
    }
 
+   if (persistent)
+      flush_persistent_maps(screen, persistent);
+
+   unsigned draw_id = dinfo->drawid;
    if (dinfo->index_size > 0) {
       VkIndexType index_type;
       unsigned index_size = dinfo->index_size;
@@ -567,42 +526,60 @@ zink_draw_vbo(struct pipe_context *pctx,
          unreachable("unknown index size!");
       }
       struct zink_resource *res = zink_resource(index_buffer);
-      vkCmdBindIndexBuffer(batch->cmdbuf, res->buffer, index_offset, index_type);
+      vkCmdBindIndexBuffer(batch->state->cmdbuf, res->obj->buffer, index_offset, index_type);
       zink_batch_reference_resource_rw(batch, res, false);
       if (dindirect && dindirect->buffer) {
+         assert(num_draws == 1);
+         update_drawid(ctx, draw_id);
          struct zink_resource *indirect = zink_resource(dindirect->buffer);
          zink_batch_reference_resource_rw(batch, indirect, false);
          if (dindirect->indirect_draw_count) {
              struct zink_resource *indirect_draw_count = zink_resource(dindirect->indirect_draw_count);
              zink_batch_reference_resource_rw(batch, indirect_draw_count, false);
-             screen->vk_CmdDrawIndexedIndirectCount(batch->cmdbuf, indirect->buffer, dindirect->offset,
-                                           indirect_draw_count->buffer, dindirect->indirect_draw_count_offset,
+             screen->vk_CmdDrawIndexedIndirectCount(batch->state->cmdbuf, indirect->obj->buffer, dindirect->offset,
+                                           indirect_draw_count->obj->buffer, dindirect->indirect_draw_count_offset,
                                            dindirect->draw_count, dindirect->stride);
          } else
-            vkCmdDrawIndexedIndirect(batch->cmdbuf, indirect->buffer, dindirect->offset, dindirect->draw_count, dindirect->stride);
-      } else
-         vkCmdDrawIndexed(batch->cmdbuf,
-            draws[0].count, dinfo->instance_count,
-            need_index_buffer_unref ? 0 : draws[0].start, dinfo->index_bias, dinfo->start_instance);
+            vkCmdDrawIndexedIndirect(batch->state->cmdbuf, indirect->obj->buffer, dindirect->offset, dindirect->draw_count, dindirect->stride);
+      } else {
+         for (unsigned i = 0; i < num_draws; i++) {
+            update_drawid(ctx, draw_id);
+            vkCmdDrawIndexed(batch->state->cmdbuf,
+               draws[i].count, dinfo->instance_count,
+               need_index_buffer_unref ? 0 : draws[i].start, dinfo->index_bias, dinfo->start_instance);
+            if (dinfo->increment_draw_id)
+               draw_id++;
+        }
+      }
    } else {
       if (so_target && screen->info.tf_props.transformFeedbackDraw) {
+         update_drawid(ctx, draw_id);
+         zink_batch_reference_resource_rw(batch, zink_resource(so_target->base.buffer), false);
          zink_batch_reference_resource_rw(batch, zink_resource(so_target->counter_buffer), true);
-         screen->vk_CmdDrawIndirectByteCountEXT(batch->cmdbuf, dinfo->instance_count, dinfo->start_instance,
-                                       zink_resource(so_target->counter_buffer)->buffer, so_target->counter_buffer_offset, 0,
+         screen->vk_CmdDrawIndirectByteCountEXT(batch->state->cmdbuf, dinfo->instance_count, dinfo->start_instance,
+                                       zink_resource(so_target->counter_buffer)->obj->buffer, so_target->counter_buffer_offset, 0,
                                        MIN2(so_target->stride, screen->info.tf_props.maxTransformFeedbackBufferDataStride));
       } else if (dindirect && dindirect->buffer) {
+         assert(num_draws == 1);
+         update_drawid(ctx, draw_id);
          struct zink_resource *indirect = zink_resource(dindirect->buffer);
          zink_batch_reference_resource_rw(batch, indirect, false);
          if (dindirect->indirect_draw_count) {
              struct zink_resource *indirect_draw_count = zink_resource(dindirect->indirect_draw_count);
              zink_batch_reference_resource_rw(batch, indirect_draw_count, false);
-             screen->vk_CmdDrawIndirectCount(batch->cmdbuf, indirect->buffer, dindirect->offset,
-                                           indirect_draw_count->buffer, dindirect->indirect_draw_count_offset,
+             screen->vk_CmdDrawIndirectCount(batch->state->cmdbuf, indirect->obj->buffer, dindirect->offset,
+                                           indirect_draw_count->obj->buffer, dindirect->indirect_draw_count_offset,
                                            dindirect->draw_count, dindirect->stride);
          } else
-            vkCmdDrawIndirect(batch->cmdbuf, indirect->buffer, dindirect->offset, dindirect->draw_count, dindirect->stride);
-      } else
-         vkCmdDraw(batch->cmdbuf, draws[0].count, dinfo->instance_count, draws[0].start, dinfo->start_instance);
+            vkCmdDrawIndirect(batch->state->cmdbuf, indirect->obj->buffer, dindirect->offset, dindirect->draw_count, dindirect->stride);
+      } else {
+         for (unsigned i = 0; i < num_draws; i++) {
+            update_drawid(ctx, draw_id);
+            vkCmdDraw(batch->state->cmdbuf, draws[i].count, dinfo->instance_count, draws[i].start, dinfo->start_instance);
+            if (dinfo->increment_draw_id)
+               draw_id++;
+         }
+      }
    }
 
    if (dinfo->index_size > 0 && (dinfo->has_user_indices || need_index_buffer_unref))
@@ -611,11 +588,55 @@ zink_draw_vbo(struct pipe_context *pctx,
    if (ctx->num_so_targets) {
       for (unsigned i = 0; i < ctx->num_so_targets; i++) {
          struct zink_so_target *t = zink_so_target(ctx->so_targets[i]);
-         counter_buffers[i] = zink_resource(t->counter_buffer)->buffer;
-         counter_buffer_offsets[i] = t->counter_buffer_offset;
-         t->counter_buffer_valid = true;
-         zink_resource(ctx->so_targets[i]->buffer)->needs_xfb_barrier = true;
+         if (t) {
+            counter_buffers[i] = zink_resource(t->counter_buffer)->obj->buffer;
+            counter_buffer_offsets[i] = t->counter_buffer_offset;
+            t->counter_buffer_valid = true;
+         }
       }
-      screen->vk_CmdEndTransformFeedbackEXT(batch->cmdbuf, 0, ctx->num_so_targets, counter_buffers, counter_buffer_offsets);
+      screen->vk_CmdEndTransformFeedbackEXT(batch->state->cmdbuf, 0, ctx->num_so_targets, counter_buffers, counter_buffer_offsets);
    }
+   batch->has_work = true;
+}
+
+void
+zink_launch_grid(struct pipe_context *pctx, const struct pipe_grid_info *info)
+{
+   struct zink_context *ctx = zink_context(pctx);
+   struct zink_screen *screen = zink_screen(pctx->screen);
+   struct zink_batch *batch = &ctx->batch;
+
+   /* check memory usage and flush/stall as needed to avoid oom */
+   zink_maybe_flush_or_stall(ctx);
+   zink_update_samplers(ctx, true);
+
+   struct zink_compute_program *comp_program = get_compute_program(ctx);
+   if (!comp_program)
+      return;
+
+   zink_program_update_compute_pipeline_state(ctx, comp_program, info->block);
+   VkPipeline pipeline = zink_get_compute_pipeline(screen, comp_program,
+                                               &ctx->compute_pipeline_state);
+
+   struct set *persistent = NULL;
+   if (zink_program_has_descriptors(zink_program(comp_program)))
+      persistent = screen->descriptors_update(ctx, true);
+
+
+   vkCmdBindPipeline(batch->state->cmdbuf, VK_PIPELINE_BIND_POINT_COMPUTE, pipeline);
+
+   if (comp_program->shader->nir->info.system_values_read & BITFIELD64_BIT(SYSTEM_VALUE_WORK_DIM))
+      vkCmdPushConstants(batch->state->cmdbuf, comp_program->layout, VK_SHADER_STAGE_COMPUTE_BIT,
+                         offsetof(struct zink_cs_push_constant, work_dim), sizeof(uint32_t),
+                         &info->work_dim);
+
+   if (persistent)
+      flush_persistent_maps(screen, persistent);
+
+   if (info->indirect) {
+      vkCmdDispatchIndirect(batch->state->cmdbuf, zink_resource(info->indirect)->obj->buffer, info->indirect_offset);
+      zink_batch_reference_resource_rw(batch, zink_resource(info->indirect), false);
+   } else
+      vkCmdDispatch(batch->state->cmdbuf, info->grid[0], info->grid[1], info->grid[2]);
+   batch->has_work = true;
 }
diff --git a/src/gallium/drivers/zink/zink_extensions.py b/src/gallium/drivers/zink/zink_extensions.py
index 6d01052c39f..1880ce2f832 100644
--- a/src/gallium/drivers/zink/zink_extensions.py
+++ b/src/gallium/drivers/zink/zink_extensions.py
@@ -1,77 +1,13 @@
-# Copyright  2020 Hoe Hao Cheng
-#
-# Permission is hereby granted, free of charge, to any person obtaining a
-# copy of this software and associated documentation files (the "Software"),
-# to deal in the Software without restriction, including without limitation
-# the rights to use, copy, modify, merge, publish, distribute, sublicense,
-# and/or sell copies of the Software, and to permit persons to whom the
-# Software is furnished to do so, subject to the following conditions:
-#
-# The above copyright notice and this permission notice (including the next
-# paragraph) shall be included in all copies or substantial portions of the
-# Software.
-#
-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
-# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
-# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
-# IN THE SOFTWARE.
-# 
-
-class Version:
-    device_version  : (1,0,0)
-    struct_version  : (1,0)
-
-    def __init__(self, version, struct=()):
-        self.device_version = version
-
-        if not struct:
-            self.struct_version = (version[0], version[1])
-        else:
-            self.struct_version = struct
-
-    # e.g. "VK_MAKE_VERSION(1,2,0)"
-    def version(self):
-        return ("VK_MAKE_VERSION("
-               + str(self.device_version[0])
-               + ","
-               + str(self.device_version[1])
-               + ","
-               + str(self.device_version[2])
-               + ")")
-
-    # e.g. "10"
-    def struct(self):
-        return (str(self.struct_version[0])+str(self.struct_version[1]))
-
-    # the sType of the extension's struct
-    # e.g. VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_TRANSFORM_FEEDBACK_FEATURES_EXT
-    # for VK_EXT_transform_feedback and struct="FEATURES"
-    def stype(self, struct: str):
-        return ("VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_VULKAN_"
-                + str(self.struct_version[0]) + "_" + str(self.struct_version[1])
-                + '_' + struct)
-
 class Extension:
     name           : str   = None
     alias          : str   = None
     is_required    : bool  = False
-    enable_conds   : [str] = None
-
-    # these are specific to zink_device_info.py:
     has_properties : bool  = False
     has_features   : bool  = False
+    enable_conds   : [str] = None
     guard          : bool  = False
 
-    # these are specific to zink_instance.py:
-    core_since     : Version = None
-    instance_funcs : [str]   = None
-
-    def __init__(self, name, alias="", required=False, properties=False,
-                 features=False, conditions=None, guard=False, core_since=None,
-                 functions=None):
+    def __init__(self, name, alias="", required=False, properties=False, features=False, conditions=None, guard=False):
         self.name = name
         self.alias = alias
         self.is_required = required
@@ -79,8 +15,6 @@ class Extension:
         self.has_features = features
         self.enable_conds = conditions
         self.guard = guard
-        self.core_since = core_since
-        self.instance_funcs = functions
 
         if alias == "" and (properties == True or features == True):
             raise RuntimeError("alias must be available when properties and/or features are used")
@@ -127,5 +61,36 @@ class Extension:
     def vendor(self):
         return self.name.split('_')[1]
 
+
+class Version:
+    driver_version  : (1,0,0)
+    struct_version  : (1,0)
+
+    def __init__(self, version, struct):
+        self.device_version = version
+        self.struct_version = struct
+
+    # e.g. "VM_MAKE_VERSION(1,2,0)"
+    def version(self):
+        return ("VK_MAKE_VERSION("
+               + str(self.device_version[0])
+               + ","
+               + str(self.device_version[1])
+               + ","
+               + str(self.device_version[2])
+               + ")")
+
+    # e.g. "10"
+    def struct(self):
+        return (str(self.struct_version[0])+str(self.struct_version[1]))
+
+    # the sType of the extension's struct
+    # e.g. VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_TRANSFORM_FEEDBACK_FEATURES_EXT
+    # for VK_EXT_transform_feedback and struct="FEATURES"
+    def stype(self, struct: str):
+        return ("VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_VULKAN_"
+                + str(self.struct_version[0]) + "_" + str(self.struct_version[1])
+                + '_' + struct)
+
 # Type aliases
 Layer = Extension
diff --git a/src/gallium/drivers/zink/zink_fence.c b/src/gallium/drivers/zink/zink_fence.c
index e6f07045cae..28a0b9e7baf 100644
--- a/src/gallium/drivers/zink/zink_fence.c
+++ b/src/gallium/drivers/zink/zink_fence.c
@@ -22,73 +22,71 @@
  */
 
 #include "zink_batch.h"
+#include "zink_context.h"
 #include "zink_fence.h"
 
-#include "zink_query.h"
 #include "zink_resource.h"
 #include "zink_screen.h"
 
 #include "util/set.h"
 #include "util/u_memory.h"
 
-static void
-destroy_fence(struct zink_screen *screen, struct zink_fence *fence)
+
+void
+zink_fence_clear_resources(struct zink_screen *screen, struct zink_fence *fence)
 {
-   if (fence->fence)
-      vkDestroyFence(screen->dev, fence->fence, NULL);
-   util_dynarray_fini(&fence->resources);
-   FREE(fence);
+   simple_mtx_lock(&fence->resource_mtx);
+   /* unref all used resources */
+   set_foreach_remove(fence->resources, entry) {
+      struct zink_resource_object *obj = (struct zink_resource_object *)entry->key;
+      zink_batch_usage_unset(&obj->reads, fence->batch_id);
+      zink_batch_usage_unset(&obj->writes, fence->batch_id);
+      zink_resource_object_reference(screen, &obj, NULL);
+   }
+   simple_mtx_unlock(&fence->resource_mtx);
 }
 
-struct zink_fence *
-zink_create_fence(struct pipe_screen *pscreen, struct zink_batch *batch)
+static void
+destroy_fence(struct zink_screen *screen, struct zink_tc_fence *mfence)
 {
-   struct zink_screen *screen = zink_screen(pscreen);
-
-   VkFenceCreateInfo fci = {};
-   fci.sType = VK_STRUCTURE_TYPE_FENCE_CREATE_INFO;
+   struct zink_batch_state *bs = zink_batch_state(mfence->fence);
+   mfence->fence = NULL;
+   zink_batch_state_reference(screen, &bs, NULL);
+   tc_unflushed_batch_token_reference(&mfence->tc_token, NULL);
+   FREE(mfence);
+}
 
-   struct zink_fence *ret = CALLOC_STRUCT(zink_fence);
-   if (!ret) {
-      debug_printf("CALLOC_STRUCT failed\n");
+struct zink_tc_fence *
+zink_create_tc_fence(void)
+{
+   struct zink_tc_fence *mfence = CALLOC_STRUCT(zink_tc_fence);
+   if (!mfence)
       return NULL;
-   }
-
-   if (vkCreateFence(screen->dev, &fci, NULL, &ret->fence) != VK_SUCCESS) {
-      debug_printf("vkCreateFence failed\n");
-      goto fail;
-   }
-   ret->active_queries = batch->active_queries;
-   batch->active_queries = NULL;
-
-   ret->batch_id = batch->batch_id;
-   util_dynarray_init(&ret->resources, NULL);
-   set_foreach(batch->resources, entry) {
-      /* the fence needs its own reference to ensure it can safely access lifetime-dependent
-       * resource members
-       */
-      struct pipe_resource *r = NULL, *pres = (struct pipe_resource *)entry->key;
-      pipe_resource_reference(&r, pres);
-      util_dynarray_append(&ret->resources, struct pipe_resource*, pres);
-   }
-
-   pipe_reference_init(&ret->reference, 1);
-   return ret;
+   pipe_reference_init(&mfence->reference, 1);
+   util_queue_fence_init(&mfence->ready);
+   return mfence;
+}
 
-fail:
-   destroy_fence(screen, ret);
-   return NULL;
+struct pipe_fence_handle *
+zink_create_tc_fence_for_tc(struct pipe_context *pctx, struct tc_unflushed_batch_token *tc_token)
+{
+   struct zink_tc_fence *mfence = zink_create_tc_fence();
+   if (!mfence)
+      return NULL;
+   util_queue_fence_reset(&mfence->ready);
+   tc_unflushed_batch_token_reference(&mfence->tc_token, tc_token);
+   return (struct pipe_fence_handle*)mfence;
 }
 
 void
 zink_fence_reference(struct zink_screen *screen,
-                     struct zink_fence **ptr,
-                     struct zink_fence *fence)
+                     struct zink_tc_fence **ptr,
+                     struct zink_tc_fence *mfence)
 {
-   if (pipe_reference(&(*ptr)->reference, &fence->reference))
+   if (pipe_reference(&(*ptr)->reference, &mfence->reference))
       destroy_fence(screen, *ptr);
 
-   *ptr = fence;
+   *ptr = mfence;
 }
 
 static void
@@ -96,50 +94,129 @@ fence_reference(struct pipe_screen *pscreen,
                 struct pipe_fence_handle **pptr,
                 struct pipe_fence_handle *pfence)
 {
-   zink_fence_reference(zink_screen(pscreen), (struct zink_fence **)pptr,
-                        zink_fence(pfence));
+   zink_fence_reference(zink_screen(pscreen), (struct zink_tc_fence **)pptr,
+                        zink_tc_fence(pfence));
 }
 
-static inline void
-fence_remove_resource_access(struct zink_fence *fence, struct zink_resource *res)
+static bool
+tc_fence_finish(struct zink_context *ctx, struct zink_tc_fence *mfence, uint64_t *timeout_ns)
 {
-   p_atomic_set(&res->batch_uses[fence->batch_id], 0);
+   if (!util_queue_fence_is_signalled(&mfence->ready)) {
+      int64_t abs_timeout = os_time_get_absolute_timeout(*timeout_ns);
+      if (mfence->tc_token) {
+         /* Ensure that zink_flush will be called for
+          * this mfence, but only if we're in the API thread
+          * where the context is current.
+          *
+          * Note that the batch containing the flush may already
+          * be in flight in the driver thread, so the mfence
+          * may not be ready yet when this call returns.
+          */
+         threaded_context_flush(&ctx->base, mfence->tc_token, *timeout_ns == 0);
+      }
+
+      if (!timeout_ns)
+         return false;
+
+      /* this is a tc mfence, so we're just waiting on the queue mfence to complete
+       * after being signaled by the real mfence
+       */
+      if (*timeout_ns == PIPE_TIMEOUT_INFINITE) {
+         util_queue_fence_wait(&mfence->ready);
+      } else {
+         if (!util_queue_fence_wait_timeout(&mfence->ready, abs_timeout))
+            return false;
+      }
+      if (*timeout_ns && *timeout_ns != PIPE_TIMEOUT_INFINITE) {
+         int64_t time_ns = os_time_get_nano();
+         *timeout_ns = abs_timeout > time_ns ? abs_timeout - time_ns : 0;
+      }
+   }
+
+   return true;
 }
 
 bool
-zink_fence_finish(struct zink_screen *screen, struct zink_fence *fence,
-                  uint64_t timeout_ns)
+zink_vkfence_wait(struct zink_screen *screen, struct zink_fence *fence, uint64_t timeout_ns)
 {
-   bool success = vkWaitForFences(screen->dev, 1, &fence->fence, VK_TRUE,
-                                  timeout_ns) == VK_SUCCESS;
+   if (p_atomic_read(&fence->completed))
+      return true;
+
+   assert(fence->batch_id);
+   assert(fence->submitted);
+
+   bool success;
+
+   if (timeout_ns)
+      success = vkWaitForFences(screen->dev, 1, &fence->fence, VK_TRUE, timeout_ns) == VK_SUCCESS;
+   else
+      success = vkGetFenceStatus(screen->dev, fence->fence) == VK_SUCCESS;
+
    if (success) {
-      if (fence->active_queries)
-         zink_prune_queries(screen, fence);
-
-      /* unref all used resources */
-      util_dynarray_foreach(&fence->resources, struct pipe_resource*, pres) {
-         struct zink_resource *stencil, *res = zink_resource(*pres);
-         fence_remove_resource_access(fence, res);
-
-         /* we still hold a ref, so this doesn't need to be atomic */
-         zink_get_depth_stencil_resources((struct pipe_resource*)res, NULL, &stencil);
-         if (stencil)
-            fence_remove_resource_access(fence, stencil);
-         pipe_resource_reference(pres, NULL);
-      }
-      util_dynarray_clear(&fence->resources);
+      p_atomic_set(&fence->completed, true);
+      zink_fence_clear_resources(screen, fence);
    }
    return success;
 }
 
+static bool
+zink_fence_finish(struct zink_screen *screen, struct pipe_context *pctx, struct zink_tc_fence *mfence,
+                  uint64_t timeout_ns)
+{
+   pctx = threaded_context_unwrap_sync(pctx);
+   struct zink_context *ctx = zink_context(pctx);
+
+   if (pctx && mfence->deferred_ctx == pctx && mfence->deferred_id == ctx->curr_batch) {
+      zink_context(pctx)->batch.has_work = true;
+      /* this must be the current batch */
+      pctx->flush(pctx, NULL, !timeout_ns ? PIPE_FLUSH_ASYNC : 0);
+      if (!timeout_ns)
+         return false;
+   }
+
+   /* need to ensure the tc mfence has been flushed before we wait */
+   bool tc_finish = tc_fence_finish(ctx, mfence, &timeout_ns);
+   struct zink_fence *fence = mfence->fence;
+   if (!tc_finish)
+      return fence ? p_atomic_read(&fence->completed) : false;
+
+   /* this was an invalid flush, just return completed */
+   if (!mfence->fence)
+      return true;
+   /* if the zink fence has a different batch id then it must have completed and been recycled already */
+   if (mfence->fence->batch_id != mfence->batch_id)
+      return true;
+
+   bool success = zink_vkfence_wait(screen, fence, timeout_ns);
+   if (success && ctx)
+      ctx->last_finished = MAX2(mfence->batch_id, ctx->last_finished);
+   return success;
+}
+
 static bool
 fence_finish(struct pipe_screen *pscreen, struct pipe_context *pctx,
                   struct pipe_fence_handle *pfence, uint64_t timeout_ns)
 {
-   return zink_fence_finish(zink_screen(pscreen), zink_fence(pfence),
+   return zink_fence_finish(zink_screen(pscreen), pctx, zink_tc_fence(pfence),
                             timeout_ns);
 }
 
+void
+zink_fence_server_sync(struct pipe_context *pctx, struct pipe_fence_handle *pfence)
+{
+   struct zink_tc_fence *mfence = zink_tc_fence(pfence);
+
+   if (pctx && mfence->deferred_ctx == pctx)
+      return;
+
+   if (mfence->deferred_ctx) {
+      zink_context(pctx)->batch.has_work = true;
+      /* this must be the current batch */
+      pctx->flush(pctx, NULL, 0);
+   }
+   zink_fence_finish(zink_screen(pctx->screen), pctx, mfence, PIPE_TIMEOUT_INFINITE);
+}
+
 void
 zink_screen_fence_init(struct pipe_screen *pscreen)
 {
diff --git a/src/gallium/drivers/zink/zink_fence.h b/src/gallium/drivers/zink/zink_fence.h
index 06ef49947e6..305f2838320 100644
--- a/src/gallium/drivers/zink/zink_fence.h
+++ b/src/gallium/drivers/zink/zink_fence.h
@@ -24,41 +24,71 @@
 #ifndef ZINK_FENCE_H
 #define ZINK_FENCE_H
 
+#include "util/simple_mtx.h"
 #include "util/u_inlines.h"
-#include "util/u_dynarray.h"
+#include "util/u_queue.h"
 
 #include <vulkan/vulkan.h>
 
+struct pipe_context;
 struct pipe_screen;
+struct zink_batch;
+struct zink_batch_state;
+struct zink_context;
 struct zink_screen;
 
-struct zink_fence {
+struct tc_unflushed_batch_token;
+
+struct zink_tc_fence {
    struct pipe_reference reference;
-   unsigned batch_id : 2;
+   struct tc_unflushed_batch_token *tc_token;
+   struct util_queue_fence ready;
+   struct pipe_context *deferred_ctx;
+   uint32_t deferred_id;
+   struct zink_fence *fence;
+   uint32_t batch_id;
+};
+
+struct zink_fence {
    VkFence fence;
-   struct set *active_queries; /* zink_query objects which were active at some point in this batch */
-   struct util_dynarray resources;
+   uint32_t batch_id;
+   simple_mtx_t resource_mtx;
+   struct set *resources; /* resources need access removed asap, so they're on the fence */
+   bool submitted;
+   bool completed;
 };
 
 static inline struct zink_fence *
-zink_fence(struct pipe_fence_handle *pfence)
+zink_fence(void *pfence)
 {
    return (struct zink_fence *)pfence;
 }
 
-struct zink_fence *
-zink_create_fence(struct pipe_screen *pscreen, struct zink_batch *batch);
+static inline struct zink_tc_fence *
+zink_tc_fence(void *pfence)
+{
+   return (struct zink_tc_fence *)pfence;
+}
+
+struct zink_tc_fence *
+zink_create_tc_fence(void);
+
+struct pipe_fence_handle *
+zink_create_tc_fence_for_tc(struct pipe_context *pctx, struct tc_unflushed_batch_token *tc_token);
 
 void
 zink_fence_reference(struct zink_screen *screen,
-                     struct zink_fence **ptr,
-                     struct zink_fence *fence);
+                     struct zink_tc_fence **ptr,
+                     struct zink_tc_fence *fence);
 
-bool
-zink_fence_finish(struct zink_screen *screen, struct zink_fence *fence,
-                  uint64_t timeout_ns);
+void
+zink_fence_server_sync(struct pipe_context *pctx, struct pipe_fence_handle *pfence);
 
 void
 zink_screen_fence_init(struct pipe_screen *pscreen);
 
+bool
+zink_vkfence_wait(struct zink_screen *screen, struct zink_fence *fence, uint64_t timeout_ns);
+void
+zink_fence_clear_resources(struct zink_screen *screen, struct zink_fence *fence);
 #endif
diff --git a/src/gallium/drivers/zink/zink_format.c b/src/gallium/drivers/zink/zink_format.c
index 18b6d4ef61e..3db17d7a30a 100644
--- a/src/gallium/drivers/zink/zink_format.c
+++ b/src/gallium/drivers/zink/zink_format.c
@@ -124,9 +124,11 @@ static const VkFormat formats[PIPE_FORMAT_COUNT] = {
    // depth/stencil formats
    [PIPE_FORMAT_Z32_FLOAT] = VK_FORMAT_D32_SFLOAT,
    [PIPE_FORMAT_Z32_FLOAT_S8X24_UINT] = VK_FORMAT_D32_SFLOAT_S8_UINT,
+   [PIPE_FORMAT_X32_S8X24_UINT] = VK_FORMAT_D32_SFLOAT_S8_UINT,
    [PIPE_FORMAT_Z16_UNORM] = VK_FORMAT_D16_UNORM,
    [PIPE_FORMAT_Z16_UNORM_S8_UINT] = VK_FORMAT_D16_UNORM_S8_UINT,
    [PIPE_FORMAT_Z24X8_UNORM] = VK_FORMAT_X8_D24_UNORM_PACK32,
+   [PIPE_FORMAT_X24S8_UINT] = VK_FORMAT_X8_D24_UNORM_PACK32,
    [PIPE_FORMAT_Z24_UNORM_S8_UINT] = VK_FORMAT_D24_UNORM_S8_UINT,
    [PIPE_FORMAT_S8_UINT] = VK_FORMAT_S8_UINT,
 
diff --git a/src/gallium/drivers/zink/zink_framebuffer.c b/src/gallium/drivers/zink/zink_framebuffer.c
index cbd0a5bb0ff..fb85e6b3726 100644
--- a/src/gallium/drivers/zink/zink_framebuffer.c
+++ b/src/gallium/drivers/zink/zink_framebuffer.c
@@ -63,16 +63,8 @@ void
 zink_destroy_framebuffer(struct zink_screen *screen,
                          struct zink_framebuffer *fb)
 {
-   hash_table_foreach(&fb->objects, he) {
-#if defined(_WIN64) || defined(__x86_64__)
+   hash_table_foreach(&fb->objects, he)
       vkDestroyFramebuffer(screen->dev, he->data, NULL);
-#else
-      VkFramebuffer *ptr = he->data;
-      vkDestroyFramebuffer(screen->dev, *ptr, NULL);
-#endif
-   }
-   for (int i = 0; i < ARRAY_SIZE(fb->surfaces); ++i)
-      pipe_surface_reference(fb->surfaces + i, NULL);
 
    pipe_surface_reference(&fb->null_surface, NULL);
 
@@ -91,12 +83,7 @@ zink_init_framebuffer(struct zink_screen *screen, struct zink_framebuffer *fb, s
 
    struct hash_entry *he = _mesa_hash_table_search_pre_hashed(&fb->objects, hash, rp);
    if (he) {
-#if defined(_WIN64) || defined(__x86_64__)
       ret = (VkFramebuffer)he->data;
-#else
-      VkFramebuffer *ptr = he->data;
-      ret = *ptr;
-#endif
       goto out;
    }
 
@@ -111,17 +98,7 @@ zink_init_framebuffer(struct zink_screen *screen, struct zink_framebuffer *fb, s
 
    if (vkCreateFramebuffer(screen->dev, &fci, NULL, &ret) != VK_SUCCESS)
       return;
-#if defined(_WIN64) || defined(__x86_64__)
    _mesa_hash_table_insert_pre_hashed(&fb->objects, hash, rp, ret);
-#else
-   VkFramebuffer *ptr = ralloc(fb, VkFramebuffer);
-   if (!ptr) {
-      vkDestroyFramebuffer(screen->dev, ret, NULL);
-      return;
-   }
-   *ptr = ret;
-   _mesa_hash_table_insert_pre_hashed(&fb->objects, hash, rp, ptr);
-#endif
 out:
    fb->rp = rp;
    fb->fb = ret;
@@ -137,17 +114,23 @@ zink_create_framebuffer(struct zink_context *ctx,
    if (!fb)
       return NULL;
 
-   pipe_reference_init(&fb->reference, 1);
-
+   unsigned num_attachments = 0;
    for (int i = 0; i < state->num_attachments; i++) {
-      if (state->attachments[i])
-         pipe_surface_reference(&fb->surfaces[i], attachments[i]);
-      else {
+      struct zink_surface *surf;
+      if (state->attachments[i]) {
+         surf = zink_surface(attachments[i]);
+         /* no ref! */
+         fb->surfaces[i] = attachments[i];
+         num_attachments++;
+      } else {
          if (!fb->null_surface)
             fb->null_surface = framebuffer_null_surface_init(ctx, state);
+         surf = zink_surface(fb->null_surface);
          state->attachments[i] = zink_surface(fb->null_surface)->image_view;
       }
+      util_dynarray_append(&surf->framebuffer_refs, struct zink_framebuffer*, fb);
    }
+   pipe_reference_init(&fb->reference, 1 + num_attachments);
 
    if (!_mesa_hash_table_init(&fb->objects, fb, _mesa_hash_pointer, _mesa_key_pointer_equal))
       goto fail;
diff --git a/src/gallium/drivers/zink/zink_framebuffer.h b/src/gallium/drivers/zink/zink_framebuffer.h
index 58d590e74fc..9c09691cc30 100644
--- a/src/gallium/drivers/zink/zink_framebuffer.h
+++ b/src/gallium/drivers/zink/zink_framebuffer.h
@@ -70,17 +70,21 @@ zink_destroy_framebuffer(struct zink_screen *screen,
 void
 debug_describe_zink_framebuffer(char* buf, const struct zink_framebuffer *ptr);
 
-static inline void
+static inline bool
 zink_framebuffer_reference(struct zink_screen *screen,
                            struct zink_framebuffer **dst,
                            struct zink_framebuffer *src)
 {
    struct zink_framebuffer *old_dst = *dst;
+   bool ret = false;
 
    if (pipe_reference_described(&old_dst->reference, &src->reference,
-                                (debug_reference_descriptor)debug_describe_zink_framebuffer))
+                                (debug_reference_descriptor)debug_describe_zink_framebuffer)) {
       zink_destroy_framebuffer(screen, old_dst);
+      ret = true;
+   }
    *dst = src;
+   return ret;
 }
 
 #endif
diff --git a/src/gallium/drivers/zink/zink_helpers.h b/src/gallium/drivers/zink/zink_helpers.h
index 9fea18ac318..c3bb165ef97 100644
--- a/src/gallium/drivers/zink/zink_helpers.h
+++ b/src/gallium/drivers/zink/zink_helpers.h
@@ -34,4 +34,10 @@ zink_filter(enum pipe_tex_filter filter)
    unreachable("unexpected filter");
 }
 
+#include "util/os_time.h"
+
+#define TIMING_START(id) int64_t id##_0 = os_time_get_nano()
+#define TIMING_END(id) id##_0 = os_time_get_nano() - id##_0;
+#define TIMING_PRINT(id, name) printf(name" %ld\n", id##_0)
+
 #endif
diff --git a/src/gallium/drivers/zink/zink_instance.py b/src/gallium/drivers/zink/zink_instance.py
index 8758331a0be..4852cead86d 100644
--- a/src/gallium/drivers/zink/zink_instance.py
+++ b/src/gallium/drivers/zink/zink_instance.py
@@ -1,57 +1,18 @@
-# Copyright  2020 Hoe Hao Cheng
-#
-# Permission is hereby granted, free of charge, to any person obtaining a
-# copy of this software and associated documentation files (the "Software"),
-# to deal in the Software without restriction, including without limitation
-# the rights to use, copy, modify, merge, publish, distribute, sublicense,
-# and/or sell copies of the Software, and to permit persons to whom the
-# Software is furnished to do so, subject to the following conditions:
-#
-# The above copyright notice and this permission notice (including the next
-# paragraph) shall be included in all copies or substantial portions of the
-# Software.
-#
-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
-# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
-# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
-# IN THE SOFTWARE.
-# 
-# Authors:
-#    Hoe Hao Cheng <haochengho12907@gmail.com>
-#
-
 from mako.template import Template
 from os import path
-from xml.etree import ElementTree
-from zink_extensions import Extension,Layer,Version
+from zink_extensions import Extension,Layer
 import sys
 
-# constructor: Extension(name, core_since=None, functions=[])
-# The attributes:
-#  - core_since: the Vulkan version where this extension is promoted to core.
-#                When screen->loader_version is greater than or equal to this
-#                instance_info.have_{name} is set to true unconditionally. This
-#                is done because loading extensions that are promoted to core is
-#                considered to be an error.
-#
-#  - functions: functions which are added by the extension. The function names
-#               should not include the "vk" prefix and the vendor suffix - these
-#               will be added by the codegen accordingly.
 EXTENSIONS = [
     Extension("VK_EXT_debug_utils"),
     Extension("VK_KHR_maintenance2"),
-    Extension("VK_KHR_get_physical_device_properties2",
-        functions=["GetPhysicalDeviceFeatures2", "GetPhysicalDeviceProperties2"]),
-    Extension("VK_KHR_draw_indirect_count",
-        functions=["CmdDrawIndexedIndirectCount", "CmdDrawIndirectCount"]),
+    Extension("VK_KHR_get_physical_device_properties2"),
+    Extension("VK_KHR_draw_indirect_count"),
+    Extension("VK_KHR_timeline_semaphore"),
     Extension("VK_KHR_external_memory_capabilities"),
     Extension("VK_MVK_moltenvk"),
 ]
 
-# constructor: Layer(name, conditions=[])
 LAYERS = [
     # if we have debug_util, allow a validation layer to be added.
     Layer("VK_LAYER_KHRONOS_validation",
@@ -93,9 +54,6 @@ struct zink_instance_info {
 VkInstance
 zink_create_instance(struct zink_screen *screen);
 
-bool
-zink_load_instance_extensions(struct zink_screen *screen);
-
 #endif
 """
 
@@ -132,23 +90,12 @@ zink_create_instance(struct zink_screen *screen)
        if (extension_props) {
            if (vkEnumerateInstanceExtensionProperties(NULL, &extension_count, extension_props) == VK_SUCCESS) {
               for (uint32_t i = 0; i < extension_count; i++) {
-        %for ext in extensions:
-            %if not ext.core_since:
-                if (!strcmp(extension_props[i].extensionName, ${ext.extension_name_literal()})) {
+%for ext in extensions:
+                 if (!strcmp(extension_props[i].extensionName, ${ext.extension_name_literal()})) {
                     have_${ext.name_with_vendor()} = true;
                     extensions[num_extensions++] = ${ext.extension_name_literal()};
-                }
-            %else:
-                if (screen->loader_version < ${ext.core_since.version()}) {
-                   if (!strcmp(extension_props[i].extensionName, ${ext.extension_name_literal()})) {
-                        have_${ext.name_with_vendor()} = true;
-                        extensions[num_extensions++] = ${ext.extension_name_literal()};
-                   }
-                 } else {
-                    have_${ext.name_with_vendor()} = true;
                  }
-            %endif
-        %endfor
+%endfor
               }
            }
        free(extension_props);
@@ -230,40 +177,6 @@ zink_create_instance(struct zink_screen *screen)
 
    return instance;
 }
-
-bool
-zink_load_instance_extensions(struct zink_screen *screen)
-{
-   if (zink_debug & ZINK_DEBUG_VALIDATION) {
-      printf("zink: Loader %d.%d.%d \\n", VK_VERSION_MAJOR(screen->loader_version), VK_VERSION_MINOR(screen->loader_version), VK_VERSION_PATCH(screen->loader_version));
-   }
-
-%for ext in extensions:
-%if bool(ext.instance_funcs) and not ext.core_since:
-   if (screen->instance_info.have_${ext.name_with_vendor()}) {
-   %for func in ext.instance_funcs:
-      GET_PROC_ADDR_INSTANCE_LOCAL(screen->instance, ${func}${ext.vendor()});
-      screen->vk_${func} = vk_${func}${ext.vendor()};
-   %endfor
-   }
-%elif bool(ext.instance_funcs):
-   if (screen->instance_info.have_${ext.name_with_vendor()}) {
-      if (screen->loader_version < ${ext.core_since.version()}) {
-      %for func in ext.instance_funcs:
-         GET_PROC_ADDR_INSTANCE_LOCAL(screen->instance, ${func}${ext.vendor()});
-         screen->vk_${func} = vk_${func}${ext.vendor()};
-      %endfor
-      } else {
-      %for func in ext.instance_funcs:
-         GET_PROC_ADDR_INSTANCE(${func});
-      %endfor
-      }
-   }
-%endif
-%endfor
-
-   return true;
-}
 """
 
 
@@ -274,60 +187,21 @@ def replace_code(code: str, replacement: dict):
     return code
 
 
-# Parses e.g. "VK_VERSION_x_y" to integer tuple (x, y)
-# For any erroneous inputs, None is returned
-def parse_promotedto(promotedto: str):
-   result = None
-
-   if promotedto and promotedto.startswith("VK_VERSION_"):
-      (major, minor) = promotedto.split('_')[-2:]
-      result = (int(major), int(minor))
-
-   return result
-
-def parse_vkxml(path: str):
-    vkxml = ElementTree.parse(path)
-    all_extensions = dict()
-
-    for ext in vkxml.findall("extensions/extension"):
-        name = ext.get("name")
-        promotedto = parse_promotedto(ext.get("promotedto"))
-        
-        if not name:
-            print("found malformed extension entry in vk.xml")
-            exit(1)
-
-        all_extensions[name] = promotedto
-
-    return all_extensions
-
 if __name__ == "__main__":
     try:
         header_path = sys.argv[1]
         impl_path = sys.argv[2]
-        vkxml_path = sys.argv[3]
 
         header_path = path.abspath(header_path)
         impl_path = path.abspath(impl_path)
-        vkxml_path = path.abspath(vkxml_path)
     except:
-        print("usage: %s <path to .h> <path to .c> <path to vk.xml>" % sys.argv[0])
+        print("usage: %s <path to .h> <path to .c>" % sys.argv[0])
         exit(1)
 
-    all_extensions = parse_vkxml(vkxml_path)
-
     extensions = EXTENSIONS
     layers = LAYERS
     replacement = REPLACEMENTS
 
-    for ext in extensions:
-        if ext.name not in all_extensions:
-            print("the extension {} is not registered in vk.xml - a typo?".format(ext.name))
-            exit(1)
-        
-        if all_extensions[ext.name] is not None:
-            ext.core_since = Version((*all_extensions[ext.name], 0))
-
     with open(header_path, "w") as header_file:
         header = Template(header_code).render(extensions=extensions, layers=layers).strip()
         header = replace_code(header, replacement)
diff --git a/src/gallium/drivers/zink/zink_pipeline.c b/src/gallium/drivers/zink/zink_pipeline.c
index 4e20eb4ec3e..e6dfd460fa7 100644
--- a/src/gallium/drivers/zink/zink_pipeline.c
+++ b/src/gallium/drivers/zink/zink_pipeline.c
@@ -21,6 +21,8 @@
  * USE OR OTHER DEALINGS IN THE SOFTWARE.
  */
 
+#include "compiler/spirv/spirv.h"
+
 #include "zink_pipeline.h"
 
 #include "zink_compiler.h"
@@ -73,11 +75,19 @@ zink_create_gfx_pipeline(struct zink_screen *screen,
    }
 
    VkPipelineColorBlendStateCreateInfo blend_state = {};
+   VkPipelineColorBlendAdvancedStateCreateInfoEXT advanced_blend_state = {};
    blend_state.sType = VK_STRUCTURE_TYPE_PIPELINE_COLOR_BLEND_STATE_CREATE_INFO;
    blend_state.pAttachments = state->blend_state->attachments;
    blend_state.attachmentCount = state->num_attachments;
    blend_state.logicOpEnable = state->blend_state->logicop_enable;
    blend_state.logicOp = state->blend_state->logicop_func;
+   if (state->blend_state->advanced_blend) {
+      blend_state.pNext = &advanced_blend_state;
+      advanced_blend_state.sType = VK_STRUCTURE_TYPE_PIPELINE_COLOR_BLEND_ADVANCED_STATE_CREATE_INFO_EXT;
+      advanced_blend_state.blendOverlap = VK_BLEND_OVERLAP_UNCORRELATED_EXT;
+      advanced_blend_state.srcPremultiplied = VK_TRUE;
+      advanced_blend_state.dstPremultiplied = VK_TRUE;
+   }
 
    VkPipelineMultisampleStateCreateInfo ms_state = {};
    ms_state.sType = VK_STRUCTURE_TYPE_PIPELINE_MULTISAMPLE_STATE_CREATE_INFO;
@@ -85,6 +95,10 @@ zink_create_gfx_pipeline(struct zink_screen *screen,
    ms_state.alphaToCoverageEnable = state->blend_state->alpha_to_coverage;
    ms_state.alphaToOneEnable = state->blend_state->alpha_to_one;
    ms_state.pSampleMask = state->sample_mask ? &state->sample_mask : NULL;
+   if (state->rast_state->force_persample_interp) {
+      ms_state.sampleShadingEnable = VK_TRUE;
+      ms_state.minSampleShading = 1.0;
+   }
 
    VkPipelineViewportStateCreateInfo viewport_state = {};
    viewport_state.sType = VK_STRUCTURE_TYPE_PIPELINE_VIEWPORT_STATE_CREATE_INFO;
@@ -120,19 +134,26 @@ zink_create_gfx_pipeline(struct zink_screen *screen,
    depth_stencil_state.back = state->depth_stencil_alpha_state->stencil_back;
    depth_stencil_state.depthWriteEnable = state->depth_stencil_alpha_state->depth_write;
 
-   VkDynamicState dynamicStateEnables[] = {
-      VK_DYNAMIC_STATE_VIEWPORT,
-      VK_DYNAMIC_STATE_SCISSOR,
+   VkDynamicState dynamicStateEnables[24] = {
       VK_DYNAMIC_STATE_LINE_WIDTH,
       VK_DYNAMIC_STATE_DEPTH_BIAS,
       VK_DYNAMIC_STATE_BLEND_CONSTANTS,
       VK_DYNAMIC_STATE_STENCIL_REFERENCE,
    };
+   unsigned state_count = 4;
+   if (screen->info.have_EXT_extended_dynamic_state) {
+      dynamicStateEnables[state_count++] = VK_DYNAMIC_STATE_VIEWPORT_WITH_COUNT_EXT;
+      dynamicStateEnables[state_count++] = VK_DYNAMIC_STATE_SCISSOR_WITH_COUNT_EXT;
+      dynamicStateEnables[state_count++] = VK_DYNAMIC_STATE_VERTEX_INPUT_BINDING_STRIDE_EXT;
+   } else {
+      dynamicStateEnables[state_count++] = VK_DYNAMIC_STATE_VIEWPORT;
+      dynamicStateEnables[state_count++] = VK_DYNAMIC_STATE_SCISSOR;
+   }
 
    VkPipelineDynamicStateCreateInfo pipelineDynamicStateCreateInfo = {};
    pipelineDynamicStateCreateInfo.sType = VK_STRUCTURE_TYPE_PIPELINE_DYNAMIC_STATE_CREATE_INFO;
    pipelineDynamicStateCreateInfo.pDynamicStates = dynamicStateEnables;
-   pipelineDynamicStateCreateInfo.dynamicStateCount = ARRAY_SIZE(dynamicStateEnables);
+   pipelineDynamicStateCreateInfo.dynamicStateCount = state_count;
 
    VkGraphicsPipelineCreateInfo pci = {};
    pci.sType = VK_STRUCTURE_TYPE_GRAPHICS_PIPELINE_CREATE_INFO;
@@ -177,7 +198,7 @@ zink_create_gfx_pipeline(struct zink_screen *screen,
    pci.stageCount = num_stages;
 
    VkPipeline pipeline;
-   if (vkCreateGraphicsPipelines(screen->dev, VK_NULL_HANDLE, 1, &pci,
+   if (vkCreateGraphicsPipelines(screen->dev, screen->pipeline_cache, 1, &pci,
                                  NULL, &pipeline) != VK_SUCCESS) {
       debug_printf("vkCreateGraphicsPipelines failed\n");
       return VK_NULL_HANDLE;
@@ -185,3 +206,45 @@ zink_create_gfx_pipeline(struct zink_screen *screen,
 
    return pipeline;
 }
+
+VkPipeline
+zink_create_compute_pipeline(struct zink_screen *screen, struct zink_compute_program *comp, struct zink_compute_pipeline_state *state)
+{
+   VkComputePipelineCreateInfo pci = {};
+   pci.sType = VK_STRUCTURE_TYPE_COMPUTE_PIPELINE_CREATE_INFO;
+   pci.flags = VK_PIPELINE_CREATE_DISABLE_OPTIMIZATION_BIT;
+   pci.layout = comp->layout;
+
+   VkPipelineShaderStageCreateInfo stage = {};
+   stage.sType = VK_STRUCTURE_TYPE_PIPELINE_SHADER_STAGE_CREATE_INFO;
+   stage.stage = VK_SHADER_STAGE_COMPUTE_BIT;
+   stage.module = comp->module->shader;
+   stage.pName = "main";
+
+   VkSpecializationInfo sinfo = {};
+   VkSpecializationMapEntry me[3];
+   if (state->use_local_size) {
+      stage.pSpecializationInfo = &sinfo;
+      sinfo.mapEntryCount = 3;
+      sinfo.pMapEntries = &me[0];
+      sinfo.dataSize = sizeof(state->local_size);
+      sinfo.pData = &state->local_size[0];
+      uint32_t ids[] = {ZINK_WORKGROUP_SIZE_X, ZINK_WORKGROUP_SIZE_Y, ZINK_WORKGROUP_SIZE_Z};
+      for (int i = 0; i < 3; i++) {
+         me[i].size = sizeof(uint32_t);
+         me[i].constantID = ids[i];
+         me[i].offset = i * sizeof(uint32_t);
+      }
+   }
+
+   pci.stage = stage;
+
+   VkPipeline pipeline;
+   if (vkCreateComputePipelines(screen->dev, screen->pipeline_cache, 1, &pci,
+                                 NULL, &pipeline) != VK_SUCCESS) {
+      debug_printf("vkCreateComputePipelines failed\n");
+      return VK_NULL_HANDLE;
+   }
+
+   return pipeline;
+}
diff --git a/src/gallium/drivers/zink/zink_pipeline.h b/src/gallium/drivers/zink/zink_pipeline.h
index 830f479fa91..1f8b62f6a17 100644
--- a/src/gallium/drivers/zink/zink_pipeline.h
+++ b/src/gallium/drivers/zink/zink_pipeline.h
@@ -31,6 +31,7 @@
 struct zink_blend_state;
 struct zink_depth_stencil_alpha_state;
 struct zink_gfx_program;
+struct zink_compute_program;
 struct zink_rasterizer_state;
 struct zink_render_pass;
 struct zink_screen;
@@ -40,8 +41,6 @@ struct zink_gfx_pipeline_state {
    struct zink_render_pass *render_pass;
 
    struct zink_vertex_elements_hw_state *element_state;
-   VkVertexInputBindingDescription bindings[PIPE_MAX_ATTRIBS]; // combination of element_state and stride
-   VkVertexInputBindingDivisorDescriptionEXT divisors[PIPE_MAX_ATTRIBS];
    uint8_t divisors_present;
 
    uint32_t num_attachments;
@@ -59,12 +58,30 @@ struct zink_gfx_pipeline_state {
 
    bool primitive_restart;
 
+   /* Pre-hashed value for table lookup, invalid when zero.
+    * Members after this point are not included in pipeline state hash key */
+   uint32_t hash;
+   bool dirty;
+
    VkShaderModule modules[PIPE_SHADER_TYPES - 1];
+   uint32_t module_hash;
 
+   uint32_t combined_hash;
+   bool combined_dirty;
+
+   VkVertexInputBindingDivisorDescriptionEXT divisors[PIPE_MAX_ATTRIBS];
+   VkVertexInputBindingDescription bindings[PIPE_MAX_ATTRIBS]; // combination of element_state and stride
+   uint32_t vertex_buffers_enabled_mask;
+   bool have_EXT_extended_dynamic_state;
+};
+
+struct zink_compute_pipeline_state {
    /* Pre-hashed value for table lookup, invalid when zero.
     * Members after this point are not included in pipeline state hash key */
    uint32_t hash;
    bool dirty;
+   bool use_local_size;
+   uint32_t local_size[3];
 };
 
 VkPipeline
@@ -73,4 +90,6 @@ zink_create_gfx_pipeline(struct zink_screen *screen,
                          struct zink_gfx_pipeline_state *state,
                          VkPrimitiveTopology primitive_topology);
 
+VkPipeline
+zink_create_compute_pipeline(struct zink_screen *screen, struct zink_compute_program *comp, struct zink_compute_pipeline_state *state);
 #endif
diff --git a/src/gallium/drivers/zink/zink_program.c b/src/gallium/drivers/zink/zink_program.c
index 021866b8087..b84d43ddea2 100644
--- a/src/gallium/drivers/zink/zink_program.c
+++ b/src/gallium/drivers/zink/zink_program.c
@@ -25,7 +25,9 @@
 
 #include "zink_compiler.h"
 #include "zink_context.h"
+#include "zink_descriptors.h"
 #include "zink_render_pass.h"
+#include "zink_resource.h"
 #include "zink_screen.h"
 #include "zink_state.h"
 
@@ -35,17 +37,32 @@
 #include "util/u_memory.h"
 #include "tgsi/tgsi_from_mesa.h"
 
-struct pipeline_cache_entry {
+/* for pipeline cache */
+#define XXH_INLINE_ALL
+#include "util/xxhash.h"
+
+struct gfx_pipeline_cache_entry {
    struct zink_gfx_pipeline_state state;
    VkPipeline pipeline;
 };
 
+struct compute_pipeline_cache_entry {
+   struct zink_compute_pipeline_state state;
+   VkPipeline pipeline;
+};
+
 void
 debug_describe_zink_gfx_program(char *buf, const struct zink_gfx_program *ptr)
 {
    sprintf(buf, "zink_gfx_program");
 }
 
+void
+debug_describe_zink_compute_program(char *buf, const struct zink_compute_program *ptr)
+{
+   sprintf(buf, "zink_compute_program");
+}
+
 static void
 debug_describe_zink_shader_module(char *buf, const struct zink_shader_module *ptr)
 {
@@ -98,81 +115,22 @@ keybox_equals(const void *void_a, const void *void_b)
    return memcmp(a->data, b->data, a->size) == 0;
 }
 
-static VkDescriptorSetLayout
-create_desc_set_layout(VkDevice dev,
-                       struct zink_shader *stages[ZINK_SHADER_COUNT],
-                       unsigned *num_descriptors)
-{
-   VkDescriptorSetLayoutBinding bindings[PIPE_SHADER_TYPES * PIPE_MAX_CONSTANT_BUFFERS];
-   int num_bindings = 0;
-
-   for (int i = 0; i < ZINK_SHADER_COUNT; i++) {
-      struct zink_shader *shader = stages[i];
-      if (!shader)
-         continue;
-
-      VkShaderStageFlagBits stage_flags = zink_shader_stage(i);
-      for (int j = 0; j < shader->num_bindings; j++) {
-         assert(num_bindings < ARRAY_SIZE(bindings));
-         bindings[num_bindings].binding = shader->bindings[j].binding;
-         bindings[num_bindings].descriptorType = shader->bindings[j].type;
-         bindings[num_bindings].descriptorCount = shader->bindings[j].size;
-         bindings[num_bindings].stageFlags = stage_flags;
-         bindings[num_bindings].pImmutableSamplers = NULL;
-         ++num_bindings;
-      }
-   }
-
-   VkDescriptorSetLayoutCreateInfo dcslci = {};
-   dcslci.sType = VK_STRUCTURE_TYPE_DESCRIPTOR_SET_LAYOUT_CREATE_INFO;
-   dcslci.pNext = NULL;
-   dcslci.flags = 0;
-   dcslci.bindingCount = num_bindings;
-   dcslci.pBindings = bindings;
-
-   VkDescriptorSetLayout dsl;
-   if (vkCreateDescriptorSetLayout(dev, &dcslci, 0, &dsl) != VK_SUCCESS) {
-      debug_printf("vkCreateDescriptorSetLayout failed\n");
-      return VK_NULL_HANDLE;
-   }
-
-   *num_descriptors = num_bindings;
-   return dsl;
-}
-
-static VkPipelineLayout
-create_pipeline_layout(VkDevice dev, VkDescriptorSetLayout dsl)
+static void
+shader_key_vs_gen(struct zink_context *ctx, struct zink_shader *zs,
+                  struct zink_shader *shaders[ZINK_SHADER_COUNT], struct zink_shader_key *key)
 {
-   assert(dsl != VK_NULL_HANDLE);
-
-   VkPipelineLayoutCreateInfo plci = {};
-   plci.sType = VK_STRUCTURE_TYPE_PIPELINE_LAYOUT_CREATE_INFO;
-
-   plci.pSetLayouts = &dsl;
-   plci.setLayoutCount = 1;
+   struct zink_vs_key *vs_key = &key->key.vs;
+   key->size = sizeof(struct zink_vs_key);
 
-
-   VkPushConstantRange pcr = {};
-   pcr.stageFlags = VK_SHADER_STAGE_TESSELLATION_CONTROL_BIT;
-   pcr.offset = 0;
-   pcr.size = sizeof(float) * 6;
-   plci.pushConstantRangeCount = 1;
-   plci.pPushConstantRanges = &pcr;
-
-   VkPipelineLayout layout;
-   if (vkCreatePipelineLayout(dev, &plci, NULL, &layout) != VK_SUCCESS) {
-      debug_printf("vkCreatePipelineLayout failed!\n");
-      return VK_NULL_HANDLE;
-   }
-
-   return layout;
+   vs_key->shader_id = zs->shader_id;
+   vs_key->clip_halfz = ctx->rast_state->base.clip_halfz;
 }
 
- 
 static void
 shader_key_fs_gen(struct zink_context *ctx, struct zink_shader *zs,
                   struct zink_shader *shaders[ZINK_SHADER_COUNT], struct zink_shader_key *key)
 {
+   struct zink_screen *screen = zink_screen(ctx->base.screen);
    struct zink_fs_key *fs_key = &key->key.fs;
    key->size = sizeof(struct zink_fs_key);
 
@@ -185,6 +143,9 @@ shader_key_fs_gen(struct zink_context *ctx, struct zink_shader *zs,
     */
    if (zs->nir->info.outputs_written & (1 << FRAG_RESULT_SAMPLE_MASK))
       fs_key->samples = !!ctx->fb_state.samples;
+   fs_key->force_dual_color_blend = screen->driconf.dual_color_blend_by_location &&
+                                    ctx->gfx_pipeline_state.blend_state->dual_src_blend &&
+                                    ctx->gfx_pipeline_state.blend_state->attachments[1].blendEnable;
 }
 
 static void
@@ -199,25 +160,16 @@ shader_key_tcs_gen(struct zink_context *ctx, struct zink_shader *zs,
    tcs_key->vs_outputs_written = shaders[PIPE_SHADER_VERTEX]->nir->info.outputs_written;
 }
 
-static void
-shader_key_dummy_gen(struct zink_context *ctx, struct zink_shader *zs,
-                     struct zink_shader *shaders[ZINK_SHADER_COUNT], struct zink_shader_key *key)
-{
-   struct zink_fs_key *fs_key = &key->key.fs;
-   key->size = sizeof(uint32_t);
- 
-   fs_key->shader_id = zs->shader_id;
-}
-
 typedef void (*zink_shader_key_gen)(struct zink_context *ctx, struct zink_shader *zs,
                                     struct zink_shader *shaders[ZINK_SHADER_COUNT],
                                     struct zink_shader_key *key);
 static zink_shader_key_gen shader_key_vtbl[] =
 {
-   [MESA_SHADER_VERTEX] = shader_key_dummy_gen,
+   [MESA_SHADER_VERTEX] = shader_key_vs_gen,
    [MESA_SHADER_TESS_CTRL] = shader_key_tcs_gen,
-   [MESA_SHADER_TESS_EVAL] = shader_key_dummy_gen,
-   [MESA_SHADER_GEOMETRY] = shader_key_dummy_gen,
+   /* reusing vs key for now since we're only using clip_halfz */
+   [MESA_SHADER_TESS_EVAL] = shader_key_vs_gen,
+   [MESA_SHADER_GEOMETRY] = shader_key_vs_gen,
    [MESA_SHADER_FRAGMENT] = shader_key_fs_gen,
 };
 
@@ -225,14 +177,34 @@ static struct zink_shader_module *
 get_shader_module_for_stage(struct zink_context *ctx, struct zink_shader *zs, struct zink_gfx_program *prog)
 {
    gl_shader_stage stage = zs->nir->info.stage;
+   enum pipe_shader_type pstage = pipe_shader_type_from_mesa(stage);
    struct zink_shader_key key = {};
    VkShaderModule mod;
    struct zink_shader_module *zm;
    struct keybox *keybox;
    uint32_t hash;
+   bool needs_clamp_size = false;
+   bool needs_base_size = false;
 
    shader_key_vtbl[stage](ctx, zs, ctx->gfx_stages, &key);
-   keybox = make_keybox(NULL, stage, &key, key.size);
+   for (int i = 0; i < 3; i++) {
+      key.base.gl_clamp[i] = ctx->sampler_gl_clamp[pstage][i];
+      needs_clamp_size |= !!key.base.gl_clamp[i];
+   }
+
+   if (zs->nir->info.num_inlinable_uniforms &&
+       ctx->inlinable_uniforms_valid_mask & BITFIELD64_BIT(pstage)) {
+      key.inline_uniforms = true;
+      memcpy(key.base.inlined_uniform_values,
+             ctx->inlinable_uniforms[pstage],
+             zs->nir->info.num_inlinable_uniforms * 4);
+      needs_base_size = true;
+   }
+   if (needs_base_size)
+      key.size += sizeof(struct zink_shader_key_base);
+   else if (needs_clamp_size)
+      key.size += sizeof(key.base.gl_clamp);
+   keybox = make_keybox(prog->shader_cache, stage, &key, key.size);
    hash = keybox_hash(keybox);
    struct hash_entry *entry = _mesa_hash_table_search_pre_hashed(prog->shader_cache->shader_cache,
                                                                  hash, keybox);
@@ -287,9 +259,10 @@ zink_destroy_shader_cache(struct zink_screen *screen, struct zink_shader_cache *
    hash_table_foreach(sc->shader_cache, entry) {
       struct zink_shader_module *zm = entry->data;
       zink_shader_module_reference(screen, &zm, NULL);
+      _mesa_hash_table_remove(sc->shader_cache, entry);
    }
    _mesa_hash_table_destroy(sc->shader_cache, NULL);
-   free(sc);
+   ralloc_free(sc);
 }
 
 static inline void
@@ -310,6 +283,8 @@ update_shader_modules(struct zink_context *ctx, struct zink_shader *stages[ZINK_
 {
    struct zink_shader *dirty[ZINK_SHADER_COUNT] = {NULL};
 
+   if (!ctx->dirty_shader_stages)
+      return;
    /* we need to map pipe_shader_type -> gl_shader_stage so we can ensure that we're compiling
     * the shaders in pipeline order and have builtin input/output locations match up after being compacted
     */
@@ -334,25 +309,70 @@ update_shader_modules(struct zink_context *ctx, struct zink_shader *stages[ZINK_
          dirty[i]->has_tess_shader = dirty[MESA_SHADER_TESS_EVAL] || stages[PIPE_SHADER_TESS_EVAL];
          zm = get_shader_module_for_stage(ctx, dirty[i], prog);
          zink_shader_module_reference(zink_screen(ctx->base.screen), &prog->modules[type], zm);
-         /* we probably need a new pipeline when we switch shader modules */
-         ctx->gfx_pipeline_state.dirty = true;
       } else if (stages[type] && !disallow_reuse) /* reuse existing shader module */
          zink_shader_module_reference(zink_screen(ctx->base.screen), &prog->modules[type], ctx->curr_program->modules[type]);
+      ctx->gfx_pipeline_state.modules[type] = prog->modules[type] ? prog->modules[type]->shader : VK_NULL_HANDLE;
       prog->shaders[type] = stages[type];
    }
-   ctx->dirty_shader_stages = 0;
+   uint32_t hash = _mesa_hash_data(ctx->gfx_pipeline_state.modules, sizeof(ctx->gfx_pipeline_state.modules));
+   if (hash != ctx->gfx_pipeline_state.module_hash)
+      ctx->gfx_pipeline_state.combined_dirty = true;
+   ctx->gfx_pipeline_state.module_hash = hash;
+   unsigned clean = u_bit_consecutive(PIPE_SHADER_VERTEX, 5);;
+   ctx->dirty_shader_stages &= ~clean;
 }
 
 static uint32_t
 hash_gfx_pipeline_state(const void *key)
 {
+   const struct zink_gfx_pipeline_state *state = key;
+   uint32_t hash = 0;
+   if (!state->have_EXT_extended_dynamic_state) {
+      /* if we don't have dynamic states, we have to hash the enabled vertex buffer bindings */
+      uint32_t vertex_buffers_enabled_mask = state->vertex_buffers_enabled_mask;
+      hash = XXH32(&vertex_buffers_enabled_mask, sizeof(uint32_t), hash);
+      while (vertex_buffers_enabled_mask) {
+         unsigned idx = u_bit_scan(&vertex_buffers_enabled_mask);
+         hash = XXH32(&state->bindings[idx], sizeof(VkVertexInputBindingDescription), hash);
+      }
+   }
+   for (unsigned i = 0; i < state->divisors_present; i++)
+      hash = XXH32(&state->divisors[i], sizeof(VkVertexInputBindingDivisorDescriptionEXT), hash);
    return _mesa_hash_data(key, offsetof(struct zink_gfx_pipeline_state, hash));
 }
 
 static bool
 equals_gfx_pipeline_state(const void *a, const void *b)
 {
-   return !memcmp(a, b, offsetof(struct zink_gfx_pipeline_state, hash));
+   const struct zink_gfx_pipeline_state *sa = a;
+   const struct zink_gfx_pipeline_state *sb = b;
+   if (sa->vertex_buffers_enabled_mask != sb->vertex_buffers_enabled_mask)
+      return false;
+   if (!sa->have_EXT_extended_dynamic_state) {
+      /* if we don't have dynamic states, we have to hash the enabled vertex buffer bindings */
+      uint32_t mask_a = sa->vertex_buffers_enabled_mask;
+      uint32_t mask_b = sb->vertex_buffers_enabled_mask;
+      while (mask_a || mask_b) {
+         unsigned idx_a = u_bit_scan(&mask_a);
+         unsigned idx_b = u_bit_scan(&mask_b);
+         if (memcmp(&sa->bindings[idx_a], &sb->bindings[idx_b], sizeof(VkVertexInputBindingDescription)))
+            return false;
+      }
+   }
+   if (sa->divisors_present != sb->divisors_present)
+      return false;
+   if (sa->divisors_present && sb->divisors_present) {
+      uint32_t divisors_present_a = sa->divisors_present;
+      uint32_t divisors_present_b = sb->divisors_present;
+      while (divisors_present_a || divisors_present_b) {
+         unsigned idx_a = u_bit_scan(&divisors_present_a);
+         unsigned idx_b = u_bit_scan(&divisors_present_b);
+         if (memcmp(&sa->divisors[idx_a], &sb->divisors[idx_b], sizeof(VkVertexInputBindingDivisorDescriptionEXT)))
+            return false;
+      }
+   }
+   return !memcmp(sa->modules, sb->modules, sizeof(sa->modules)) &&
+          !memcmp(a, b, offsetof(struct zink_gfx_pipeline_state, hash));
 }
 
 static void
@@ -374,7 +394,7 @@ init_slot_map(struct zink_context *ctx, struct zink_gfx_program *prog)
        * the slots match up
        * TOOD: if we compact the slot map table, we can store it on the shader keys and reuse the cache
        */
-      prog->shader_cache = CALLOC_STRUCT(zink_shader_cache);
+      prog->shader_cache = ralloc(NULL, struct zink_shader_cache);
       pipe_reference_init(&prog->shader_cache->reference, 1);
       prog->shader_cache->shader_cache =  _mesa_hash_table_create(NULL, keybox_hash, keybox_equals);
    } else {
@@ -392,12 +412,48 @@ zink_update_gfx_program(struct zink_context *ctx, struct zink_gfx_program *prog)
    update_shader_modules(ctx, ctx->gfx_stages, prog, true);
 }
 
+VkPipelineLayout
+zink_pipeline_layout_create(struct zink_screen *screen, struct zink_program *pg, bool is_compute)
+{
+   VkPipelineLayoutCreateInfo plci = {};
+   plci.sType = VK_STRUCTURE_TYPE_PIPELINE_LAYOUT_CREATE_INFO;
+
+   plci.pSetLayouts = pg->dsl;
+   plci.setLayoutCount = pg->num_dsl;
+
+
+   VkPushConstantRange pcr[2] = {};
+   if (is_compute) {
+      pcr[0].stageFlags = VK_SHADER_STAGE_COMPUTE_BIT;
+      pcr[0].offset = 0;
+      pcr[0].size = sizeof(struct zink_cs_push_constant);
+      plci.pushConstantRangeCount = 1;
+   } else {
+      pcr[0].stageFlags = VK_SHADER_STAGE_VERTEX_BIT;
+      pcr[0].offset = offsetof(struct zink_gfx_push_constant, draw_mode_is_indexed);
+      pcr[0].size = 2 * sizeof(unsigned);
+      pcr[1].stageFlags = VK_SHADER_STAGE_TESSELLATION_CONTROL_BIT;
+      pcr[1].offset = offsetof(struct zink_gfx_push_constant, default_inner_level);
+      pcr[1].size = sizeof(float) * 6;
+      plci.pushConstantRangeCount = 2;
+   }
+   plci.pPushConstantRanges = &pcr[0];
+
+   VkPipelineLayout layout;
+   if (vkCreatePipelineLayout(screen->dev, &plci, NULL, &layout) != VK_SUCCESS) {
+      debug_printf("vkCreatePipelineLayout failed!\n");
+      return VK_NULL_HANDLE;
+   }
+
+   return layout;
+}
+
 struct zink_gfx_program *
 zink_create_gfx_program(struct zink_context *ctx,
                         struct zink_shader *stages[ZINK_SHADER_COUNT])
 {
    struct zink_screen *screen = zink_screen(ctx->base.screen);
-   struct zink_gfx_program *prog = CALLOC_STRUCT(zink_gfx_program);
+   struct zink_gfx_program *prog = rzalloc(NULL, struct zink_gfx_program);
    if (!prog)
       goto fail;
 
@@ -421,14 +477,9 @@ zink_create_gfx_program(struct zink_context *ctx,
          zink_gfx_program_reference(screen, NULL, prog);
       }
    }
+   p_atomic_dec(&prog->reference.count);
 
-   prog->dsl = create_desc_set_layout(screen->dev, stages,
-                                      &prog->num_descriptors);
-   if (!prog->dsl)
-      goto fail;
-
-   prog->layout = create_pipeline_layout(screen->dev, prog->dsl);
-   if (!prog->layout)
+   if (!screen->descriptor_program_init(ctx, (struct zink_program*)prog))
       goto fail;
 
    return prog;
@@ -439,6 +490,199 @@ fail:
    return NULL;
 }
 
+static uint32_t
+hash_compute_pipeline_state(const void *key)
+{
+   const struct zink_compute_pipeline_state *state = key;
+   uint32_t hash = _mesa_hash_data(state, offsetof(struct zink_compute_pipeline_state, hash));
+   if (state->use_local_size)
+      hash = XXH32(&state->local_size[0], sizeof(state->local_size), hash);
+   return hash;
+}
+
+void
+zink_program_update_compute_pipeline_state(struct zink_context *ctx, struct zink_compute_program *comp, const uint block[3])
+{
+   struct zink_shader *zs = comp->shader;
+   bool use_local_size = zs->nir->info.system_values_read & BITFIELD64_BIT(SYSTEM_VALUE_LOCAL_GROUP_SIZE);
+   if (ctx->compute_pipeline_state.use_local_size != use_local_size)
+      ctx->compute_pipeline_state.dirty = true;
+   ctx->compute_pipeline_state.use_local_size = use_local_size;
+
+   if (ctx->compute_pipeline_state.use_local_size) {
+      for (int i = 0; i < ARRAY_SIZE(ctx->compute_pipeline_state.local_size); i++) {
+         if (ctx->compute_pipeline_state.local_size[i] != block[i])
+            ctx->compute_pipeline_state.dirty = true;
+         ctx->compute_pipeline_state.local_size[i] = block[i];
+      }
+   } else
+      ctx->compute_pipeline_state.local_size[0] =
+      ctx->compute_pipeline_state.local_size[1] =
+      ctx->compute_pipeline_state.local_size[2] = 0;
+}
+
+static bool
+equals_compute_pipeline_state(const void *a, const void *b)
+{
+   return memcmp(a, b, offsetof(struct zink_compute_pipeline_state, hash)) == 0;
+}
+
+struct zink_compute_program *
+zink_create_compute_program(struct zink_context *ctx, struct zink_shader *shader)
+{
+   struct zink_screen *screen = zink_screen(ctx->base.screen);
+   struct zink_compute_program *comp = rzalloc(NULL, struct zink_compute_program);
+   if (!comp)
+      goto fail;
+
+   pipe_reference_init(&comp->reference, 1);
+   comp->is_compute = true;
+
+   if (!ctx->curr_compute || !ctx->curr_compute->shader_cache) {
+      /* TODO: cs shader keys? */
+      comp->shader_cache = ralloc(NULL, struct zink_shader_cache);
+      pipe_reference_init(&comp->shader_cache->reference, 1);
+      comp->shader_cache->shader_cache = _mesa_hash_table_create(NULL, _mesa_hash_u32, _mesa_key_u32_equal);
+   } else
+      zink_shader_cache_reference(zink_screen(ctx->base.screen), &comp->shader_cache, ctx->curr_compute->shader_cache);
+
+   if (ctx->dirty_shader_stages & (1 << PIPE_SHADER_COMPUTE)) {
+      struct hash_entry *he = _mesa_hash_table_search(comp->shader_cache->shader_cache, &shader->shader_id);
+      if (he)
+         comp->module = he->data;
+      else {
+         comp->module = CALLOC_STRUCT(zink_shader_module);
+         assert(comp->module);
+         pipe_reference_init(&comp->module->reference, 1);
+         comp->module->shader = zink_shader_compile(screen, shader, NULL, NULL, NULL);
+         assert(comp->module->shader);
+         _mesa_hash_table_insert(comp->shader_cache->shader_cache, &shader->shader_id, comp->module);
+      }
+   } else
+     comp->module = ctx->curr_compute->module;
+
+   struct zink_shader_module *zm = NULL;
+   zink_shader_module_reference(zink_screen(ctx->base.screen), &zm, comp->module);
+   ctx->dirty_shader_stages &= ~(1 << PIPE_SHADER_COMPUTE);
+
+   comp->pipelines = _mesa_hash_table_create(NULL, hash_compute_pipeline_state,
+                                             equals_compute_pipeline_state);
+
+   _mesa_set_add(shader->programs, comp);
+   comp->shader = shader;
+
+   if (!screen->descriptor_program_init(ctx, (struct zink_program*)comp))
+      goto fail;
+
+   return comp;
+
+fail:
+   if (comp)
+      zink_destroy_compute_program(screen, comp);
+   return NULL;
+}
+
+uint32_t
+zink_program_get_descriptor_usage(struct zink_context *ctx, enum pipe_shader_type stage, enum zink_descriptor_type type)
+{
+   struct zink_shader *zs = NULL;
+   switch (stage) {
+   case PIPE_SHADER_VERTEX:
+   case PIPE_SHADER_TESS_CTRL:
+   case PIPE_SHADER_TESS_EVAL:
+   case PIPE_SHADER_GEOMETRY:
+   case PIPE_SHADER_FRAGMENT:
+      zs = ctx->gfx_stages[stage];
+      break;
+   case PIPE_SHADER_COMPUTE: {
+      zs = ctx->compute_stage;
+      break;
+   }
+   default:
+      unreachable("unknown shader type");
+   }
+   if (!zs)
+      return 0;
+   switch (type) {
+   case ZINK_DESCRIPTOR_TYPE_UBO:
+      return zs->ubos_used;
+   case ZINK_DESCRIPTOR_TYPE_SSBO:
+      return zs->ssbos_used;
+   case ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW:
+      return zs->nir->info.textures_used;
+   case ZINK_DESCRIPTOR_TYPE_IMAGE:
+      return zs->nir->info.images_used;
+   default:
+      unreachable("unknown descriptor type!");
+   }
+   return 0;
+}
+
+bool
+zink_program_descriptor_is_buffer(struct zink_context *ctx, enum pipe_shader_type stage, enum zink_descriptor_type type, unsigned i)
+{
+   struct zink_shader *zs = NULL;
+   switch (stage) {
+   case PIPE_SHADER_VERTEX:
+   case PIPE_SHADER_TESS_CTRL:
+   case PIPE_SHADER_TESS_EVAL:
+   case PIPE_SHADER_GEOMETRY:
+   case PIPE_SHADER_FRAGMENT:
+      zs = ctx->gfx_stages[stage];
+      break;
+   case PIPE_SHADER_COMPUTE: {
+      zs = ctx->compute_stage;
+      break;
+   }
+   default:
+      unreachable("unknown shader type");
+   }
+   if (!zs)
+      return false;
+   return zink_shader_descriptor_is_buffer(zs, type, i);
+}
+
+static unsigned
+get_num_bindings(struct zink_shader *zs, enum zink_descriptor_type type)
+{
+   switch (type) {
+   case ZINK_DESCRIPTOR_TYPE_UBO:
+   case ZINK_DESCRIPTOR_TYPE_SSBO:
+      return zs->num_bindings[type];
+   default:
+      break;
+   }
+   unsigned num_bindings = 0;
+   for (int i = 0; i < zs->num_bindings[type]; i++)
+      num_bindings += zs->bindings[type][i].size;
+   return num_bindings;
+}
+
+unsigned
+zink_program_num_bindings_typed(const struct zink_program *pg, enum zink_descriptor_type type, bool is_compute)
+{
+   unsigned num_bindings = 0;
+   if (is_compute) {
+      struct zink_compute_program *comp = (void*)pg;
+      return get_num_bindings(comp->shader, type);
+   }
+   struct zink_gfx_program *prog = (void*)pg;
+   for (unsigned i = 0; i < ZINK_SHADER_COUNT; i++) {
+      if (prog->shaders[i])
+         num_bindings += get_num_bindings(prog->shaders[i], type);
+   }
+   return num_bindings;
+}
+
+unsigned
+zink_program_num_bindings(const struct zink_program *pg, bool is_compute)
+{
+   unsigned num_bindings = 0;
+   for (unsigned i = 0; i < ZINK_DESCRIPTOR_TYPES; i++)
+      num_bindings += zink_program_num_bindings_typed(pg, i, is_compute);
+   return num_bindings;
+}
+
 static void
 gfx_program_remove_shader(struct zink_gfx_program *prog, struct zink_shader *shader)
 {
@@ -456,9 +700,6 @@ zink_destroy_gfx_program(struct zink_screen *screen,
    if (prog->layout)
       vkDestroyPipelineLayout(screen->dev, prog->layout, NULL);
 
-   if (prog->dsl)
-      vkDestroyDescriptorSetLayout(screen->dev, prog->dsl, NULL);
-
    for (int i = 0; i < ZINK_SHADER_COUNT; ++i) {
       if (prog->shaders[i])
          gfx_program_remove_shader(prog, prog->shaders[i]);
@@ -468,7 +709,7 @@ zink_destroy_gfx_program(struct zink_screen *screen,
 
    for (int i = 0; i < ARRAY_SIZE(prog->pipelines); ++i) {
       hash_table_foreach(prog->pipelines[i], entry) {
-         struct pipeline_cache_entry *pc_entry = entry->data;
+         struct gfx_pipeline_cache_entry *pc_entry = entry->data;
 
          vkDestroyPipeline(screen->dev, pc_entry->pipeline, NULL);
          free(pc_entry);
@@ -476,8 +717,34 @@ zink_destroy_gfx_program(struct zink_screen *screen,
       _mesa_hash_table_destroy(prog->pipelines[i], NULL);
    }
    zink_shader_cache_reference(screen, &prog->shader_cache, NULL);
+   screen->descriptor_program_deinit(screen, zink_program(prog));
+
+   ralloc_free(prog);
+}
+
+void
+zink_destroy_compute_program(struct zink_screen *screen,
+                         struct zink_compute_program *comp)
+{
+   if (comp->layout)
+      vkDestroyPipelineLayout(screen->dev, comp->layout, NULL);
+
+   if (comp->shader)
+      _mesa_set_remove_key(comp->shader->programs, comp);
+   if (comp->module)
+      zink_shader_module_reference(screen, &comp->module, NULL);
 
-   FREE(prog);
+   hash_table_foreach(comp->pipelines, entry) {
+      struct compute_pipeline_cache_entry *pc_entry = entry->data;
+
+      vkDestroyPipeline(screen->dev, pc_entry->pipeline, NULL);
+      free(pc_entry);
+   }
+   _mesa_hash_table_destroy(comp->pipelines, NULL);
+   zink_shader_cache_reference(screen, &comp->shader_cache, NULL);
+   screen->descriptor_program_deinit(screen, zink_program(comp));
+
+   ralloc_free(comp);
 }
 
 static VkPrimitiveTopology
@@ -534,13 +801,15 @@ zink_get_gfx_pipeline(struct zink_screen *screen,
    struct hash_entry *entry = NULL;
    
    if (state->dirty) {
-      for (unsigned i = 0; i < ZINK_SHADER_COUNT; i++)
-         state->modules[i] = prog->modules[i] ? prog->modules[i]->shader : VK_NULL_HANDLE;
-
+      state->combined_dirty = true;
       state->hash = hash_gfx_pipeline_state(state);
       state->dirty = false;
    }
-   entry = _mesa_hash_table_search_pre_hashed(prog->pipelines[vkmode], state->hash, state);
+   if (state->combined_dirty) {
+      state->combined_hash = XXH32(&state->module_hash, sizeof(uint32_t), state->hash);
+      state->combined_dirty = false;
+   }
+   entry = _mesa_hash_table_search_pre_hashed(prog->pipelines[vkmode], state->combined_hash, state);
 
    if (!entry) {
       VkPipeline pipeline = zink_create_gfx_pipeline(screen, prog,
@@ -548,18 +817,51 @@ zink_get_gfx_pipeline(struct zink_screen *screen,
       if (pipeline == VK_NULL_HANDLE)
          return VK_NULL_HANDLE;
 
-      struct pipeline_cache_entry *pc_entry = CALLOC_STRUCT(pipeline_cache_entry);
+      struct gfx_pipeline_cache_entry *pc_entry = CALLOC_STRUCT(gfx_pipeline_cache_entry);
+      if (!pc_entry)
+         return VK_NULL_HANDLE;
+
+      memcpy(&pc_entry->state, state, sizeof(*state));
+      pc_entry->pipeline = pipeline;
+
+      entry = _mesa_hash_table_insert_pre_hashed(prog->pipelines[vkmode], state->combined_hash, state, pc_entry);
+      assert(entry);
+   }
+
+   return ((struct gfx_pipeline_cache_entry *)(entry->data))->pipeline;
+}
+
+VkPipeline
+zink_get_compute_pipeline(struct zink_screen *screen,
+                      struct zink_compute_program *comp,
+                      struct zink_compute_pipeline_state *state)
+{
+   struct hash_entry *entry = NULL;
+
+   if (state->dirty) {
+      state->hash = hash_compute_pipeline_state(state);
+      state->dirty = false;
+   }
+   entry = _mesa_hash_table_search_pre_hashed(comp->pipelines, state->hash, state);
+
+   if (!entry) {
+      VkPipeline pipeline = zink_create_compute_pipeline(screen, comp, state);
+
+      if (pipeline == VK_NULL_HANDLE)
+         return VK_NULL_HANDLE;
+
+      struct compute_pipeline_cache_entry *pc_entry = CALLOC_STRUCT(compute_pipeline_cache_entry);
       if (!pc_entry)
          return VK_NULL_HANDLE;
 
       memcpy(&pc_entry->state, state, sizeof(*state));
       pc_entry->pipeline = pipeline;
 
-      entry = _mesa_hash_table_insert_pre_hashed(prog->pipelines[vkmode], state->hash, state, pc_entry);
+      entry = _mesa_hash_table_insert_pre_hashed(comp->pipelines, state->hash, state, pc_entry);
       assert(entry);
    }
 
-   return ((struct pipeline_cache_entry *)(entry->data))->pipeline;
+   return ((struct compute_pipeline_cache_entry *)(entry->data))->pipeline;
 }
 
 
@@ -580,9 +882,15 @@ static void
 bind_stage(struct zink_context *ctx, enum pipe_shader_type stage,
            struct zink_shader *shader)
 {
-   assert(stage < PIPE_SHADER_COMPUTE);
-   ctx->gfx_stages[stage] = shader;
+   if (stage == PIPE_SHADER_COMPUTE)
+      ctx->compute_stage = shader;
+   else
+      ctx->gfx_stages[stage] = shader;
    ctx->dirty_shader_stages |= 1 << stage;
+   if (shader && shader->nir->info.num_inlinable_uniforms)
+      ctx->shader_has_inlinable_uniforms_mask |= 1 << stage;
+   else
+      ctx->shader_has_inlinable_uniforms_mask &= ~(1 << stage);
 }
 
 static void
@@ -678,6 +986,25 @@ zink_delete_shader_state(struct pipe_context *pctx, void *cso)
    zink_shader_free(zink_context(pctx), cso);
 }
 
+static void *
+zink_create_cs_state(struct pipe_context *pctx,
+                     const struct pipe_compute_state *shader)
+{
+   struct nir_shader *nir;
+   if (shader->ir_type != PIPE_SHADER_IR_NIR)
+      nir = zink_tgsi_to_nir(pctx->screen, shader->prog);
+   else
+      nir = (struct nir_shader *)shader->prog;
+
+   return zink_shader_create(zink_screen(pctx->screen), nir, NULL);
+}
+
+static void
+zink_bind_cs_state(struct pipe_context *pctx,
+                   void *cso)
+{
+   bind_stage(zink_context(pctx), PIPE_SHADER_COMPUTE, cso);
+}
 
 void
 zink_program_init(struct zink_context *ctx)
@@ -701,4 +1028,8 @@ zink_program_init(struct zink_context *ctx)
    ctx->base.create_tes_state = zink_create_tes_state;
    ctx->base.bind_tes_state = zink_bind_tes_state;
    ctx->base.delete_tes_state = zink_delete_shader_state;
+
+   ctx->base.create_compute_state = zink_create_cs_state;
+   ctx->base.bind_compute_state = zink_bind_cs_state;
+   ctx->base.delete_compute_state = zink_delete_shader_state;
 }
diff --git a/src/gallium/drivers/zink/zink_program.h b/src/gallium/drivers/zink/zink_program.h
index 9d726f67fe3..8309a167ff3 100644
--- a/src/gallium/drivers/zink/zink_program.h
+++ b/src/gallium/drivers/zink/zink_program.h
@@ -31,14 +31,30 @@
 #include "util/u_inlines.h"
 
 #include "zink_context.h"
+#include "zink_compiler.h"
 #include "zink_shader_keys.h"
 
 struct zink_screen;
 struct zink_shader;
 struct zink_gfx_pipeline_state;
+struct zink_descriptor_set;
 
 struct hash_table;
 struct set;
+struct util_dynarray;
+
+struct zink_program;
+
+struct zink_gfx_push_constant {
+   unsigned draw_mode_is_indexed;
+   unsigned draw_id;
+   float default_inner_level[2];
+   float default_outer_level[4];
+};
+
+struct zink_cs_push_constant {
+   unsigned work_dim;
+};
 
 /* a shader module is used for directly reusing a shader module between programs,
  * e.g., in the case where we're swapping out only one shader,
@@ -55,20 +71,90 @@ struct zink_shader_cache {
    struct hash_table *shader_cache;
 };
 
+struct zink_program {
+   struct pipe_reference reference;
+   struct zink_batch_usage batch_uses;
+   bool is_compute;
+
+   struct zink_program_descriptor_data *descriptor_data;
+
+   VkPipelineLayout layout;
+   VkDescriptorSetLayout dsl[ZINK_DESCRIPTOR_TYPES];
+   unsigned num_dsl;
+};
+
+static inline struct zink_program *
+zink_program(void *prog)
+{
+   return prog;
+}
+
 struct zink_gfx_program {
    struct pipe_reference reference;
+   struct zink_batch_usage batch_uses;
+   bool is_compute;
+
+   struct zink_program_descriptor_data *descriptor_data;
+
+   VkPipelineLayout layout;
+   VkDescriptorSetLayout dsl[ZINK_DESCRIPTOR_TYPES];
+   unsigned num_dsl;
 
    struct zink_shader_module *modules[ZINK_SHADER_COUNT]; // compute stage doesn't belong here
    struct zink_shader *shaders[ZINK_SHADER_COUNT];
    struct zink_shader_cache *shader_cache;
    unsigned char shader_slot_map[VARYING_SLOT_MAX];
    unsigned char shader_slots_reserved;
-   VkDescriptorSetLayout dsl;
-   VkPipelineLayout layout;
-   unsigned num_descriptors;
    struct hash_table *pipelines[11]; // number of draw modes we support
 };
 
+struct zink_compute_program {
+   struct pipe_reference reference;
+   struct zink_batch_usage batch_uses;
+   bool is_compute;
+
+   struct zink_program_descriptor_data *descriptor_data;
+
+   VkPipelineLayout layout;
+   VkDescriptorSetLayout dsl[ZINK_DESCRIPTOR_TYPES];
+   unsigned num_dsl;
+
+   struct zink_shader_module *module;
+   struct zink_shader *shader;
+   struct zink_shader_cache *shader_cache;
+   struct hash_table *pipelines;
+};
+
+static inline enum zink_descriptor_type
+zink_desc_type_from_vktype(VkDescriptorType type)
+{
+   switch (type) {
+   case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER:
+   case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER_DYNAMIC:
+      return ZINK_DESCRIPTOR_TYPE_UBO;
+   case VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER:
+   case VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER:
+      return ZINK_DESCRIPTOR_TYPE_SAMPLER_VIEW;
+   case VK_DESCRIPTOR_TYPE_STORAGE_BUFFER:
+      return ZINK_DESCRIPTOR_TYPE_SSBO;
+   case VK_DESCRIPTOR_TYPE_STORAGE_IMAGE:
+   case VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER:
+      return ZINK_DESCRIPTOR_TYPE_IMAGE;
+   default:
+      unreachable("unhandled descriptor type");
+   }
+   return 0;
+   
+}
+
+unsigned
+zink_program_num_bindings_typed(const struct zink_program *pg, enum zink_descriptor_type type, bool is_compute);
+
+unsigned
+zink_program_num_bindings(const struct zink_program *pg, bool is_compute);
+
+bool
+zink_program_descriptor_is_buffer(struct zink_context *ctx, enum pipe_shader_type stage, enum zink_descriptor_type type, unsigned i);
 
 void
 zink_update_gfx_program(struct zink_context *ctx, struct zink_gfx_program *prog);
@@ -90,19 +176,70 @@ zink_get_gfx_pipeline(struct zink_screen *screen,
 void
 zink_program_init(struct zink_context *ctx);
 
+uint32_t
+zink_program_get_descriptor_usage(struct zink_context *ctx, enum pipe_shader_type stage, enum zink_descriptor_type type);
+
 void
 debug_describe_zink_gfx_program(char* buf, const struct zink_gfx_program *ptr);
 
-static inline void
+static inline bool
 zink_gfx_program_reference(struct zink_screen *screen,
                            struct zink_gfx_program **dst,
                            struct zink_gfx_program *src)
 {
    struct zink_gfx_program *old_dst = dst ? *dst : NULL;
+   bool ret = false;
 
    if (pipe_reference_described(old_dst ? &old_dst->reference : NULL, &src->reference,
-                                (debug_reference_descriptor)debug_describe_zink_gfx_program))
+                                (debug_reference_descriptor)debug_describe_zink_gfx_program)) {
       zink_destroy_gfx_program(screen, old_dst);
+      ret = true;
+   }
    if (dst) *dst = src;
+   return ret;
 }
+
+struct zink_compute_program *
+zink_create_compute_program(struct zink_context *ctx, struct zink_shader *shader);
+void
+zink_destroy_compute_program(struct zink_screen *screen,
+                         struct zink_compute_program *comp);
+
+void
+debug_describe_zink_compute_program(char* buf, const struct zink_compute_program *ptr);
+
+static inline bool
+zink_compute_program_reference(struct zink_screen *screen,
+                           struct zink_compute_program **dst,
+                           struct zink_compute_program *src)
+{
+   struct zink_compute_program *old_dst = dst ? *dst : NULL;
+   bool ret = false;
+
+   if (pipe_reference_described(old_dst ? &old_dst->reference : NULL, &src->reference,
+                                (debug_reference_descriptor)debug_describe_zink_compute_program)) {
+      zink_destroy_compute_program(screen, old_dst);
+      ret = true;
+   }
+   if (dst) *dst = src;
+   return ret;
+}
+
+VkPipelineLayout
+zink_pipeline_layout_create(struct zink_screen *screen, struct zink_program *pg, bool is_compute);
+
+void
+zink_program_update_compute_pipeline_state(struct zink_context *ctx, struct zink_compute_program *comp, const uint block[3]);
+
+VkPipeline
+zink_get_compute_pipeline(struct zink_screen *screen,
+                      struct zink_compute_program *comp,
+                      struct zink_compute_pipeline_state *state);
+
+static inline bool
+zink_program_has_descriptors(const struct zink_program *pg)
+{
+   return pg->dsl[0] || pg->dsl[1] || pg->dsl[2] || pg->dsl[3];
+}
+
 #endif
diff --git a/src/gallium/drivers/zink/zink_query.c b/src/gallium/drivers/zink/zink_query.c
index f75dcce2b5a..01da363b5b6 100644
--- a/src/gallium/drivers/zink/zink_query.c
+++ b/src/gallium/drivers/zink/zink_query.c
@@ -11,20 +11,28 @@
 #include "util/u_inlines.h"
 #include "util/u_memory.h"
 
-#define NUM_QUERIES 50
+#define NUM_QUERIES 5000
+
+struct zink_query_buffer {
+   struct list_head list;
+   unsigned num_results;
+   struct pipe_resource *buffer;
+   struct pipe_resource *xfb_buffers[PIPE_MAX_VERTEX_STREAMS - 1];
+};
 
 struct zink_query {
+   struct threaded_query base;
    enum pipe_query_type type;
 
    VkQueryPool query_pool;
-   VkQueryPool xfb_query_pool;
-   unsigned curr_query, num_queries, last_start;
+   VkQueryPool xfb_query_pool[PIPE_MAX_VERTEX_STREAMS - 1];
+   unsigned curr_query, last_start;
 
    VkQueryType vkqtype;
    unsigned index;
-   bool use_64bit;
    bool precise;
    bool xfb_running;
+   bool xfb_overflow;
 
    bool active; /* query is considered active by vk */
    bool needs_reset; /* query is considered active by vk and cannot be destroyed */
@@ -37,11 +45,56 @@ struct zink_query {
    bool have_gs[NUM_QUERIES]; /* geometry shaders use GEOMETRY_SHADER_PRIMITIVES_BIT */
    bool have_xfb[NUM_QUERIES]; /* xfb was active during this query */
 
-   unsigned batch_id : 2; //batch that the query was started in
-
-   union pipe_query_result accumulated_result;
+   struct zink_batch_usage batch_id; //batch that the query was started in
+   struct list_head buffers;
+   struct zink_query_buffer *curr_qbo;
 };
 
+static inline unsigned
+get_num_results(enum pipe_query_type query_type)
+{
+   switch (query_type) {
+   case PIPE_QUERY_OCCLUSION_COUNTER:
+   case PIPE_QUERY_OCCLUSION_PREDICATE:
+   case PIPE_QUERY_OCCLUSION_PREDICATE_CONSERVATIVE:
+   case PIPE_QUERY_TIME_ELAPSED:
+   case PIPE_QUERY_TIMESTAMP:
+   case PIPE_QUERY_PIPELINE_STATISTICS_SINGLE:
+      return 1;
+   case PIPE_QUERY_PRIMITIVES_GENERATED:
+   case PIPE_QUERY_SO_OVERFLOW_ANY_PREDICATE:
+   case PIPE_QUERY_SO_OVERFLOW_PREDICATE:
+   case PIPE_QUERY_PRIMITIVES_EMITTED:
+      return 2;
+   case PIPE_QUERY_PIPELINE_STATISTICS:
+      return 11;
+   default:
+      debug_printf("unknown query: %s\n",
+                   util_str_query_type(query_type, true));
+      unreachable("zink: unknown query type");
+   }
+}
+
+static VkQueryPipelineStatisticFlags
+pipeline_statistic_convert(enum pipe_statistics_query_index idx)
+{
+   unsigned map[] = {
+      [PIPE_STAT_QUERY_IA_VERTICES] = VK_QUERY_PIPELINE_STATISTIC_INPUT_ASSEMBLY_VERTICES_BIT,
+      [PIPE_STAT_QUERY_IA_PRIMITIVES] = VK_QUERY_PIPELINE_STATISTIC_INPUT_ASSEMBLY_PRIMITIVES_BIT,
+      [PIPE_STAT_QUERY_VS_INVOCATIONS] = VK_QUERY_PIPELINE_STATISTIC_VERTEX_SHADER_INVOCATIONS_BIT,
+      [PIPE_STAT_QUERY_GS_INVOCATIONS] = VK_QUERY_PIPELINE_STATISTIC_GEOMETRY_SHADER_INVOCATIONS_BIT,
+      [PIPE_STAT_QUERY_GS_PRIMITIVES] = VK_QUERY_PIPELINE_STATISTIC_GEOMETRY_SHADER_PRIMITIVES_BIT,
+      [PIPE_STAT_QUERY_C_INVOCATIONS] = VK_QUERY_PIPELINE_STATISTIC_CLIPPING_INVOCATIONS_BIT,
+      [PIPE_STAT_QUERY_C_PRIMITIVES] = VK_QUERY_PIPELINE_STATISTIC_CLIPPING_PRIMITIVES_BIT,
+      [PIPE_STAT_QUERY_PS_INVOCATIONS] = VK_QUERY_PIPELINE_STATISTIC_FRAGMENT_SHADER_INVOCATIONS_BIT,
+      [PIPE_STAT_QUERY_HS_INVOCATIONS] = VK_QUERY_PIPELINE_STATISTIC_TESSELLATION_CONTROL_SHADER_PATCHES_BIT,
+      [PIPE_STAT_QUERY_DS_INVOCATIONS] = VK_QUERY_PIPELINE_STATISTIC_TESSELLATION_EVALUATION_SHADER_INVOCATIONS_BIT,
+      [PIPE_STAT_QUERY_CS_INVOCATIONS] = VK_QUERY_PIPELINE_STATISTIC_COMPUTE_SHADER_INVOCATIONS_BIT
+   };
+   assert(idx < ARRAY_SIZE(map));
+   return map[idx];
+}
+
 static void
 timestamp_to_nanoseconds(struct zink_screen *screen, uint64_t *timestamp)
 {
@@ -58,27 +111,26 @@ timestamp_to_nanoseconds(struct zink_screen *screen, uint64_t *timestamp)
 }
 
 static VkQueryType
-convert_query_type(unsigned query_type, bool *use_64bit, bool *precise)
+convert_query_type(unsigned query_type, bool *precise)
 {
-   *use_64bit = false;
    *precise = false;
    switch (query_type) {
    case PIPE_QUERY_OCCLUSION_COUNTER:
+   case PIPE_QUERY_OCCLUSION_PREDICATE:
       *precise = true;
-      *use_64bit = true;
       /* fallthrough */
-   case PIPE_QUERY_OCCLUSION_PREDICATE:
    case PIPE_QUERY_OCCLUSION_PREDICATE_CONSERVATIVE:
       return VK_QUERY_TYPE_OCCLUSION;
    case PIPE_QUERY_TIME_ELAPSED:
    case PIPE_QUERY_TIMESTAMP:
-      *use_64bit = true;
       return VK_QUERY_TYPE_TIMESTAMP;
+   case PIPE_QUERY_PIPELINE_STATISTICS_SINGLE:
    case PIPE_QUERY_PIPELINE_STATISTICS:
    case PIPE_QUERY_PRIMITIVES_GENERATED:
       return VK_QUERY_TYPE_PIPELINE_STATISTICS;
+   case PIPE_QUERY_SO_OVERFLOW_ANY_PREDICATE:
+   case PIPE_QUERY_SO_OVERFLOW_PREDICATE:
    case PIPE_QUERY_PRIMITIVES_EMITTED:
-      *use_64bit = true;
       return VK_QUERY_TYPE_TRANSFORM_FEEDBACK_STREAM_EXT;
    default:
       debug_printf("unknown query: %s\n",
@@ -93,6 +145,93 @@ is_time_query(struct zink_query *query)
    return query->type == PIPE_QUERY_TIMESTAMP || query->type == PIPE_QUERY_TIME_ELAPSED;
 }
 
+static bool
+is_so_overflow_query(struct zink_query *query)
+{
+   return query->type == PIPE_QUERY_SO_OVERFLOW_ANY_PREDICATE || query->type == PIPE_QUERY_SO_OVERFLOW_PREDICATE;
+}
+
+static bool
+is_bool_query(struct zink_query *query)
+{
+   return is_so_overflow_query(query) ||
+          query->type == PIPE_QUERY_OCCLUSION_PREDICATE ||
+          query->type == PIPE_QUERY_OCCLUSION_PREDICATE_CONSERVATIVE ||
+          query->type == PIPE_QUERY_GPU_FINISHED;
+}
+
+static bool
+qbo_append(struct pipe_screen *screen, struct zink_query *query)
+{
+   if (query->curr_qbo && query->curr_qbo->list.next)
+      return true;
+   struct zink_query_buffer *qbo = CALLOC_STRUCT(zink_query_buffer);
+   if (!qbo)
+      return false;
+   qbo->buffer = pipe_buffer_create(screen, PIPE_BIND_QUERY_BUFFER,
+                                  PIPE_USAGE_STREAM,
+                                  /* this is the maximum possible size of the results in a given buffer */
+                                  NUM_QUERIES * get_num_results(query->type) * sizeof(uint64_t));
+   if (!qbo->buffer)
+      goto fail;
+   if (query->type == PIPE_QUERY_PRIMITIVES_GENERATED) {
+      /* need separate xfb buffer */
+      qbo->xfb_buffers[0] = pipe_buffer_create(screen, PIPE_BIND_QUERY_BUFFER,
+                                     PIPE_USAGE_STREAM,
+                                     /* this is the maximum possible size of the results in a given buffer */
+                                     NUM_QUERIES * get_num_results(query->type) * sizeof(uint64_t));
+      if (!qbo->xfb_buffers[0])
+         goto fail;
+   } else if (query->type == PIPE_QUERY_SO_OVERFLOW_ANY_PREDICATE) {
+      /* need to monitor all xfb streams */
+      for (unsigned i = 0; i < ARRAY_SIZE(qbo->xfb_buffers); i++) {
+         /* need separate xfb buffer */
+         qbo->xfb_buffers[i] = pipe_buffer_create(screen, PIPE_BIND_QUERY_BUFFER,
+                                        PIPE_USAGE_STREAM,
+                                        /* this is the maximum possible size of the results in a given buffer */
+                                        NUM_QUERIES * get_num_results(query->type) * sizeof(uint64_t));
+         if (!qbo->xfb_buffers[i])
+            goto fail;
+      }
+   }
+   list_addtail(&qbo->list, &query->buffers);
+
+   return true;
+fail:
+   pipe_resource_reference(&qbo->buffer, NULL);
+   for (unsigned i = 0; i < ARRAY_SIZE(qbo->xfb_buffers); i++)
+      pipe_resource_reference(&qbo->xfb_buffers[i], NULL);
+   FREE(qbo);
+   return false;
+}
+
+static void
+destroy_query(struct zink_screen *screen, struct zink_query *query)
+{
+   assert(!p_atomic_read(&query->fences));
+   if (query->query_pool)
+      vkDestroyQueryPool(screen->dev, query->query_pool, NULL);
+   struct zink_query_buffer *qbo, *next;
+   LIST_FOR_EACH_ENTRY_SAFE(qbo, next, &query->buffers, list) {
+      pipe_resource_reference(&qbo->buffer, NULL);
+      for (unsigned i = 0; i < ARRAY_SIZE(qbo->xfb_buffers); i++)
+         pipe_resource_reference(&qbo->xfb_buffers[i], NULL);
+      FREE(qbo);
+   }
+   for (unsigned i = 0; i < ARRAY_SIZE(query->xfb_query_pool); i++) {
+      if (query->xfb_query_pool[i])
+         vkDestroyQueryPool(screen->dev, query->xfb_query_pool[i], NULL);
+   }
+   FREE(query);
+}
+
+static void
+reset_qbo(struct zink_query *q)
+{
+   q->curr_qbo = list_first_entry(&q->buffers, struct zink_query_buffer, list);
+   q->curr_qbo->num_results = 0;
+}
+
 static struct pipe_query *
 zink_create_query(struct pipe_context *pctx,
                   unsigned query_type, unsigned index)
@@ -103,58 +242,63 @@ zink_create_query(struct pipe_context *pctx,
 
    if (!query)
       return NULL;
+   list_inithead(&query->buffers);
 
    query->index = index;
    query->type = query_type;
-   query->vkqtype = convert_query_type(query_type, &query->use_64bit, &query->precise);
+   query->vkqtype = convert_query_type(query_type, &query->precise);
    if (query->vkqtype == -1)
       return NULL;
 
-   query->num_queries = NUM_QUERIES;
    query->curr_query = 0;
 
    pool_create.sType = VK_STRUCTURE_TYPE_QUERY_POOL_CREATE_INFO;
    pool_create.queryType = query->vkqtype;
-   pool_create.queryCount = query->num_queries;
+   pool_create.queryCount = NUM_QUERIES;
    if (query_type == PIPE_QUERY_PRIMITIVES_GENERATED)
      pool_create.pipelineStatistics = VK_QUERY_PIPELINE_STATISTIC_GEOMETRY_SHADER_PRIMITIVES_BIT |
                                       VK_QUERY_PIPELINE_STATISTIC_INPUT_ASSEMBLY_PRIMITIVES_BIT;
+   else if (query_type == PIPE_QUERY_PIPELINE_STATISTICS)
+      /* TODO? this is broken if the user is expecting cs invocations */
+      pool_create.pipelineStatistics = 0xffffffff;
+   else if (query_type == PIPE_QUERY_PIPELINE_STATISTICS_SINGLE)
+      pool_create.pipelineStatistics = pipeline_statistic_convert(index);
 
    VkResult status = vkCreateQueryPool(screen->dev, &pool_create, NULL, &query->query_pool);
-   if (status != VK_SUCCESS) {
-      FREE(query);
-      return NULL;
-   }
+   if (status != VK_SUCCESS)
+      goto fail;
    if (query_type == PIPE_QUERY_PRIMITIVES_GENERATED) {
       /* if xfb is active, we need to use an xfb query, otherwise we need pipeline statistics */
       pool_create.sType = VK_STRUCTURE_TYPE_QUERY_POOL_CREATE_INFO;
       pool_create.queryType = VK_QUERY_TYPE_TRANSFORM_FEEDBACK_STREAM_EXT;
-      pool_create.queryCount = query->num_queries;
+      pool_create.queryCount = NUM_QUERIES;
 
-      status = vkCreateQueryPool(screen->dev, &pool_create, NULL, &query->xfb_query_pool);
-      if (status != VK_SUCCESS) {
-         vkDestroyQueryPool(screen->dev, query->query_pool, NULL);
-         FREE(query);
-         return NULL;
+      status = vkCreateQueryPool(screen->dev, &pool_create, NULL, &query->xfb_query_pool[0]);
+      if (status != VK_SUCCESS)
+         goto fail;
+   } else if (query_type == PIPE_QUERY_SO_OVERFLOW_ANY_PREDICATE) {
+      /* need to monitor all xfb streams */
+      for (unsigned i = 0; i < ARRAY_SIZE(query->xfb_query_pool); i++) {
+         status = vkCreateQueryPool(screen->dev, &pool_create, NULL, &query->xfb_query_pool[i]);
+         if (status != VK_SUCCESS)
+            goto fail;
       }
    }
-   struct zink_batch *batch = zink_batch_no_rp(zink_context(pctx));
-   vkCmdResetQueryPool(batch->cmdbuf, query->query_pool, 0, query->num_queries);
+   if (!qbo_append(pctx->screen, query))
+      goto fail;
+   struct zink_batch *batch = &zink_context(pctx)->batch;
+   batch->has_work = true;
+   vkCmdResetQueryPool(batch->state->cmdbuf, query->query_pool, 0, NUM_QUERIES);
    if (query->type == PIPE_QUERY_PRIMITIVES_GENERATED)
-      vkCmdResetQueryPool(batch->cmdbuf, query->xfb_query_pool, 0, query->num_queries);
-   if (query->type == PIPE_QUERY_TIMESTAMP)
+      vkCmdResetQueryPool(batch->state->cmdbuf, query->xfb_query_pool[0], 0, NUM_QUERIES);
+   if (query->type == PIPE_QUERY_TIMESTAMP) {
       query->active = true;
+      reset_qbo(query);
+   }
    return (struct pipe_query *)query;
-}
-
-static void
-destroy_query(struct zink_screen *screen, struct zink_query *query)
-{
-   assert(!p_atomic_read(&query->fences));
-   vkDestroyQueryPool(screen->dev, query->query_pool, NULL);
-   if (query->type == PIPE_QUERY_PRIMITIVES_GENERATED)
-      vkDestroyQueryPool(screen->dev, query->xfb_query_pool, NULL);
-   FREE(query);
+fail:
+   destroy_query(screen, query);
+   return NULL;
 }
 
 static void
@@ -175,88 +319,24 @@ zink_destroy_query(struct pipe_context *pctx,
 }
 
 void
-zink_prune_queries(struct zink_screen *screen, struct zink_fence *fence)
+zink_prune_query(struct zink_screen *screen, struct zink_query *query)
 {
-   set_foreach(fence->active_queries, entry) {
-      struct zink_query *query = (void*)entry->key;
-      if (!p_atomic_dec_return(&query->fences)) {
-         if (p_atomic_read(&query->dead))
-            destroy_query(screen, query);
-      }
+   if (!p_atomic_dec_return(&query->fences)) {
+      if (p_atomic_read(&query->dead))
+         destroy_query(screen, query);
    }
-   _mesa_set_destroy(fence->active_queries, NULL);
-   fence->active_queries = NULL;
 }
 
-static bool
-get_query_result(struct pipe_context *pctx,
-                      struct pipe_query *q,
-                      bool wait,
-                      union pipe_query_result *result)
+static void
+check_query_results(struct zink_query *query, union pipe_query_result *result,
+                    int num_results, uint64_t *results, uint64_t *xfb_results)
 {
-   struct zink_screen *screen = zink_screen(pctx->screen);
-   struct zink_query *query = (struct zink_query *)q;
-   VkQueryResultFlagBits flags = 0;
-
-   if (wait)
-      flags |= VK_QUERY_RESULT_WAIT_BIT;
-
-   if (query->use_64bit)
-      flags |= VK_QUERY_RESULT_64_BIT;
-
-   if (result != &query->accumulated_result) {
-      if (query->type == PIPE_QUERY_TIMESTAMP)
-         util_query_clear_result(result, query->type);
-      else {
-         memcpy(result, &query->accumulated_result, sizeof(query->accumulated_result));
-         util_query_clear_result(&query->accumulated_result, query->type);
-      }
-   } else
-      flags |= VK_QUERY_RESULT_PARTIAL_BIT;
-
-   // union pipe_query_result results[NUM_QUERIES * 2];
-   /* xfb queries return 2 results */
-   uint64_t results[NUM_QUERIES * 2];
-   memset(results, 0, sizeof(results));
-   uint64_t xfb_results[NUM_QUERIES * 2];
-   memset(xfb_results, 0, sizeof(xfb_results));
-   int num_results = query->curr_query - query->last_start;
-   int result_size = 1;
-      /* these query types emit 2 values */
-   if (query->type == PIPE_QUERY_PRIMITIVES_GENERATED ||
-       query->type == PIPE_QUERY_PRIMITIVES_EMITTED)
-      result_size = 2;
-
-   /* verify that we have the expected number of results pending */
-   assert(query->curr_query <= ARRAY_SIZE(results) / result_size);
-   VkResult status = vkGetQueryPoolResults(screen->dev, query->query_pool,
-                                           query->last_start, num_results,
-                                           sizeof(results),
-                                           results,
-                                           sizeof(uint64_t),
-                                           flags);
-   if (status != VK_SUCCESS)
-      return false;
-
-   if (query->type == PIPE_QUERY_PRIMITIVES_GENERATED) {
-      status = vkGetQueryPoolResults(screen->dev, query->xfb_query_pool,
-                                              query->last_start, num_results,
-                                              sizeof(xfb_results),
-                                              xfb_results,
-                                              2 * sizeof(uint64_t),
-                                              flags | VK_QUERY_RESULT_64_BIT);
-      if (status != VK_SUCCESS)
-         return false;
-
-   }
-
    uint64_t last_val = 0;
+   int result_size = get_num_results(query->type);
    for (int i = 0; i < num_results * result_size; i += result_size) {
       switch (query->type) {
       case PIPE_QUERY_OCCLUSION_PREDICATE:
       case PIPE_QUERY_OCCLUSION_PREDICATE_CONSERVATIVE:
-      case PIPE_QUERY_SO_OVERFLOW_PREDICATE:
-      case PIPE_QUERY_SO_OVERFLOW_ANY_PREDICATE:
       case PIPE_QUERY_GPU_FINISHED:
          result->b |= results[i] != 0;
          break;
@@ -266,7 +346,7 @@ get_query_result(struct pipe_context *pctx,
          /* the application can sum the differences between all N queries to determine the total execution time.
           * - 17.5. Timestamp Queries
           */
-         if (query->type != PIPE_QUERY_TIME_ELAPSED || i > 0)
+         if (query->type != PIPE_QUERY_TIME_ELAPSED || i)
             result->u64 += results[i] - last_val;
          last_val = results[i];
          break;
@@ -278,7 +358,7 @@ get_query_result(struct pipe_context *pctx,
             result->u64 += xfb_results[i + 1];
          else
             /* if a given draw had a geometry shader, we need to use the second result */
-            result->u32 += ((uint32_t*)results)[i + query->have_gs[query->last_start + i / 2]];
+            result->u64 += results[i + query->have_gs[query->last_start + i / 2]];
          break;
       case PIPE_QUERY_PRIMITIVES_EMITTED:
          /* A query pool created with this type will capture 2 integers -
@@ -288,6 +368,24 @@ get_query_result(struct pipe_context *pctx,
           */
          result->u64 += results[i];
          break;
+      case PIPE_QUERY_SO_OVERFLOW_ANY_PREDICATE:
+      case PIPE_QUERY_SO_OVERFLOW_PREDICATE:
+         /* A query pool created with this type will capture 2 integers -
+          * numPrimitivesWritten and numPrimitivesNeeded -
+          * for the specified vertex stream output from the last vertex processing stage.
+          * - from VK_EXT_transform_feedback spec
+          */
+         result->b |= results[i] != results[i + 1];
+         break;
+      case PIPE_QUERY_PIPELINE_STATISTICS_SINGLE:
+         result->u64 += results[i];
+         break;
+      case PIPE_QUERY_PIPELINE_STATISTICS: {
+         uint64_t *statistics = (uint64_t*)&result->pipeline_statistics;
+         for (unsigned j = 0; j < 11; j++)
+            statistics[j] += results[i + j];
+         break;
+      }
 
       default:
          debug_printf("unhandled query type: %s\n",
@@ -295,6 +393,73 @@ get_query_result(struct pipe_context *pctx,
          unreachable("unexpected query type");
       }
    }
+}
+
+static bool
+get_query_result(struct pipe_context *pctx,
+                      struct pipe_query *q,
+                      bool wait,
+                      union pipe_query_result *result)
+{
+   struct zink_screen *screen = zink_screen(pctx->screen);
+   struct zink_query *query = (struct zink_query *)q;
+   unsigned flags = PIPE_MAP_READ;
+
+   if (!wait)
+      flags |= PIPE_MAP_DONTBLOCK;
+
+   util_query_clear_result(result, query->type);
+
+   int num_results = query->curr_query - query->last_start;
+   int result_size = get_num_results(query->type) * sizeof(uint64_t);
+
+   if (query->type == PIPE_QUERY_PIPELINE_STATISTICS)
+      num_results = 1;
+
+   struct zink_query_buffer *qbo;
+   struct pipe_transfer *xfer;
+   LIST_FOR_EACH_ENTRY(qbo, &query->buffers, list) {
+      uint64_t *xfb_results = NULL;
+      uint64_t *results;
+      bool is_timestamp = query->type == PIPE_QUERY_TIMESTAMP || query->type == PIPE_QUERY_TIMESTAMP_DISJOINT;
+      results = pipe_buffer_map_range(pctx, qbo->buffer, zink_resource(qbo->buffer)->obj->offset,
+                                      (is_timestamp ? 1 : qbo->num_results) * result_size, flags, &xfer);
+      if (!results) {
+         if (wait)
+            debug_printf("zink: qbo read failed!");
+         break;
+      }
+      struct pipe_transfer *xfb_xfer = NULL;
+      if (query->type == PIPE_QUERY_PRIMITIVES_GENERATED) {
+         xfb_results = pipe_buffer_map_range(pctx, qbo->xfb_buffers[0], zink_resource(qbo->xfb_buffers[0])->obj->offset,
+                                         qbo->num_results * result_size, flags, &xfb_xfer);
+         if (!xfb_results) {
+            if (wait)
+               debug_printf("zink: xfb qbo read failed!");
+         }
+      }
+      check_query_results(query, result, is_timestamp ? 1 : qbo->num_results, results, xfb_results);
+      pipe_buffer_unmap(pctx, xfer);
+      if (xfb_xfer)
+         pipe_buffer_unmap(pctx, xfb_xfer);
+      if (query->type == PIPE_QUERY_SO_OVERFLOW_ANY_PREDICATE) {
+         for (unsigned i = 0; i < ARRAY_SIZE(qbo->xfb_buffers) && !result->b; i++) {
+            uint64_t *results = pipe_buffer_map_range(pctx, qbo->xfb_buffers[i],
+                                              zink_resource(qbo->xfb_buffers[i])->obj->offset,
+                                              qbo->num_results * result_size, flags, &xfer);
+            if (!results) {
+               if (wait)
+                  debug_printf("zink: qbo read failed!");
+               break;
+            }
+            check_query_results(query, result, num_results, results, xfb_results);
+            pipe_buffer_unmap(pctx, xfer);
+         }
+         /* if overflow is detected we can stop */
+         if (result->b)
+            break;
+      }
+   }
 
    if (is_time_query(query))
       timestamp_to_nanoseconds(screen, &result->u64);
@@ -302,6 +467,70 @@ get_query_result(struct pipe_context *pctx,
    return TRUE;
 }
 
+static void
+force_cpu_read(struct zink_context *ctx, struct pipe_query *pquery, enum pipe_query_value_type result_type, struct pipe_resource *pres, unsigned offset)
+{
+   struct pipe_context *pctx = &ctx->base;
+   unsigned result_size = result_type <= PIPE_QUERY_TYPE_U32 ? sizeof(uint32_t) : sizeof(uint64_t);
+   struct zink_query *query = (struct zink_query*)pquery;
+   union pipe_query_result result;
+   uint32_t batch_id = p_atomic_read(&query->batch_id.usage);
+
+   zink_wait_on_batch(ctx, batch_id);
+
+   bool success = get_query_result(pctx, pquery, true, &result);
+   if (!success) {
+      debug_printf("zink: getting query result failed\n");
+      return;
+   }
+
+   if (result_type <= PIPE_QUERY_TYPE_U32) {
+      uint32_t u32;
+      uint32_t limit;
+      if (result_type == PIPE_QUERY_TYPE_I32)
+         limit = INT_MAX;
+      else
+         limit = UINT_MAX;
+      if (is_bool_query(query))
+         u32 = result.b;
+      else
+         u32 = result.u64 > limit ? limit : result.u64;
+      pipe_buffer_write(pctx, pres, offset, result_size, &u32);
+   } else {
+      uint64_t u64;
+      if (is_bool_query(query))
+         u64 = result.b;
+      else
+         u64 = result.u64;
+      pipe_buffer_write(pctx, pres, offset, result_size, &u64);
+   }
+}
+
+static void
+copy_pool_results_to_buffer(struct zink_context *ctx, struct zink_query *query, VkQueryPool pool,
+                            unsigned query_id, struct zink_resource *res, unsigned offset,
+                            int num_results, VkQueryResultFlags flags)
+{
+   struct zink_batch *batch = &ctx->batch;
+   unsigned type_size = (flags & VK_QUERY_RESULT_64_BIT) ? sizeof(uint64_t) : sizeof(uint32_t);
+   unsigned base_result_size = get_num_results(query->type) * type_size;
+   unsigned result_size = base_result_size * num_results;
+   if (flags & VK_QUERY_RESULT_WITH_AVAILABILITY_BIT)
+      result_size += type_size;
+   /* if it's a single query that doesn't need special handling, we can copy it and be done */
+   zink_batch_reference_resource_rw(batch, res, true);
+   zink_resource_buffer_barrier(ctx, batch, res, VK_ACCESS_TRANSFER_WRITE_BIT, 0);
+   util_range_add(&res->base.b, &res->valid_buffer_range, offset, offset + result_size);
+   vkCmdCopyQueryPoolResults(batch->state->cmdbuf, pool, query_id, num_results, res->obj->buffer,
+                             offset, 0, flags);
+}
+
+static void
+copy_results_to_buffer(struct zink_context *ctx, struct zink_query *query, struct zink_resource *res, unsigned offset, int num_results, VkQueryResultFlags flags)
+{
+   copy_pool_results_to_buffer(ctx, query, query->query_pool, query->last_start, res, offset, num_results, flags);
+}
+
 static void
 reset_pool(struct zink_context *ctx, struct zink_batch *batch, struct zink_query *q)
 {
@@ -309,17 +538,68 @@ reset_pool(struct zink_context *ctx, struct zink_batch *batch, struct zink_query
     *
     * - vkCmdResetQueryPool spec
     */
-   batch = zink_batch_no_rp(ctx);
+   zink_batch_no_rp(ctx);
 
-   if (q->type != PIPE_QUERY_TIMESTAMP)
-      get_query_result(&ctx->base, (struct pipe_query*)q, false, &q->accumulated_result);
-   vkCmdResetQueryPool(batch->cmdbuf, q->query_pool, 0, q->num_queries);
+   vkCmdResetQueryPool(batch->state->cmdbuf, q->query_pool, 0, NUM_QUERIES);
    if (q->type == PIPE_QUERY_PRIMITIVES_GENERATED)
-      vkCmdResetQueryPool(batch->cmdbuf, q->xfb_query_pool, 0, q->num_queries);
+      vkCmdResetQueryPool(batch->state->cmdbuf, q->xfb_query_pool[0], 0, NUM_QUERIES);
+   else if (q->type == PIPE_QUERY_SO_OVERFLOW_ANY_PREDICATE) {
+      for (unsigned i = 0; i < ARRAY_SIZE(q->xfb_query_pool); i++)
+         vkCmdResetQueryPool(batch->state->cmdbuf, q->xfb_query_pool[i], 0, NUM_QUERIES);
+   }
    memset(q->have_gs, 0, sizeof(q->have_gs));
    memset(q->have_xfb, 0, sizeof(q->have_xfb));
    q->last_start = q->curr_query = 0;
    q->needs_reset = false;
+   /* create new qbo for non-timestamp queries */
+   if (q->type != PIPE_QUERY_TIMESTAMP) {
+      if (qbo_append(ctx->base.screen, q))
+         reset_qbo(q);
+      else
+         debug_printf("zink: qbo alloc failed on reset!");
+   }
+}
+
+static inline unsigned
+get_buffer_offset(struct zink_query *q, struct pipe_resource *pres, unsigned query_id)
+{
+   return zink_resource(pres)->obj->offset + (query_id - q->last_start) * get_num_results(q->type) * sizeof(uint64_t);
+}
+
+static void
+update_qbo(struct zink_context *ctx, struct zink_query *q)
+{
+   struct zink_query_buffer *qbo = q->curr_qbo;
+   unsigned offset = 0;
+   bool is_timestamp = q->type == PIPE_QUERY_TIMESTAMP || q->type == PIPE_QUERY_TIMESTAMP_DISJOINT;
+   /* timestamp queries just write to offset 0 always */
+   if (!is_timestamp)
+      offset = get_buffer_offset(q, qbo->buffer, q->curr_query);
+   copy_pool_results_to_buffer(ctx, q, q->query_pool, q->curr_query, zink_resource(qbo->buffer),
+                          offset,
+                          1, VK_QUERY_RESULT_64_BIT);
+
+   if (q->type == PIPE_QUERY_PRIMITIVES_EMITTED ||
+            q->type == PIPE_QUERY_PRIMITIVES_GENERATED ||
+            q->type == PIPE_QUERY_SO_OVERFLOW_PREDICATE) {
+      copy_pool_results_to_buffer(ctx, q,
+                                  q->xfb_query_pool[0] ? q->xfb_query_pool[0] : q->query_pool,
+                                  q->curr_query,
+                                  zink_resource(qbo->xfb_buffers[0] ? qbo->xfb_buffers[0] : qbo->buffer),
+                             get_buffer_offset(q, qbo->xfb_buffers[0] ? qbo->xfb_buffers[0] : qbo->buffer, q->curr_query),
+                             1, VK_QUERY_RESULT_64_BIT);
+   }
+
+   else if (q->type == PIPE_QUERY_SO_OVERFLOW_ANY_PREDICATE) {
+      for (unsigned i = 0; i < ARRAY_SIZE(q->xfb_query_pool); i++) {
+         copy_pool_results_to_buffer(ctx, q, q->xfb_query_pool[i], q->curr_query, zink_resource(qbo->xfb_buffers[i]),
+                                get_buffer_offset(q, qbo->xfb_buffers[i], q->curr_query),
+                                1, VK_QUERY_RESULT_64_BIT);
+      }
+   }
+
+   if (!is_timestamp)
+      q->curr_qbo->num_results++;
 }
 
 static void
@@ -329,33 +609,49 @@ begin_query(struct zink_context *ctx, struct zink_batch *batch, struct zink_quer
 
    if (q->needs_reset)
       reset_pool(ctx, batch, q);
-   assert(q->curr_query < q->num_queries);
+   assert(q->curr_query < NUM_QUERIES);
    q->active = true;
-   if (q->type == PIPE_QUERY_TIME_ELAPSED)
-      vkCmdWriteTimestamp(batch->cmdbuf, VK_PIPELINE_STAGE_TOP_OF_PIPE_BIT, q->query_pool, q->curr_query++);
+   batch->has_work = true;
+   if (q->type == PIPE_QUERY_TIME_ELAPSED) {
+      vkCmdWriteTimestamp(batch->state->cmdbuf, VK_PIPELINE_STAGE_TOP_OF_PIPE_BIT, q->query_pool, q->curr_query);
+      update_qbo(ctx, q);
+      q->curr_query++;
+   }
    /* ignore the rest of begin_query for timestamps */
    if (is_time_query(q))
       return;
    if (q->precise)
       flags |= VK_QUERY_CONTROL_PRECISE_BIT;
-   if (q->type == PIPE_QUERY_PRIMITIVES_EMITTED || q->type == PIPE_QUERY_PRIMITIVES_GENERATED) {
-      zink_screen(ctx->base.screen)->vk_CmdBeginQueryIndexedEXT(batch->cmdbuf,
-                                                                q->xfb_query_pool ? q->xfb_query_pool : q->query_pool,
+   if (q->type == PIPE_QUERY_PRIMITIVES_EMITTED ||
+       q->type == PIPE_QUERY_PRIMITIVES_GENERATED ||
+       q->type == PIPE_QUERY_SO_OVERFLOW_PREDICATE) {
+      zink_screen(ctx->base.screen)->vk_CmdBeginQueryIndexedEXT(batch->state->cmdbuf,
+                                                                q->xfb_query_pool[0] ? q->xfb_query_pool[0] : q->query_pool,
                                                                 q->curr_query,
                                                                 flags,
                                                                 q->index);
       q->xfb_running = true;
+   } else if (q->type == PIPE_QUERY_SO_OVERFLOW_ANY_PREDICATE) {
+      zink_screen(ctx->base.screen)->vk_CmdBeginQueryIndexedEXT(batch->state->cmdbuf,
+                                                                q->query_pool,
+                                                                q->curr_query,
+                                                                flags,
+                                                                0);
+      for (unsigned i = 0; i < ARRAY_SIZE(q->xfb_query_pool); i++)
+         zink_screen(ctx->base.screen)->vk_CmdBeginQueryIndexedEXT(batch->state->cmdbuf,
+                                                                   q->xfb_query_pool[i],
+                                                                   q->curr_query,
+                                                                   flags,
+                                                                   i + 1);
+      q->xfb_running = true;
    }
    if (q->vkqtype != VK_QUERY_TYPE_TRANSFORM_FEEDBACK_STREAM_EXT)
-      vkCmdBeginQuery(batch->cmdbuf, q->query_pool, q->curr_query, flags);
-   if (!batch->active_queries)
-      batch->active_queries = _mesa_set_create(NULL, _mesa_hash_pointer, _mesa_key_pointer_equal);
-   assert(batch->active_queries);
+      vkCmdBeginQuery(batch->state->cmdbuf, q->query_pool, q->curr_query, flags);
    if (q->type == PIPE_QUERY_PRIMITIVES_GENERATED)
       list_addtail(&q->stats_list, &ctx->primitives_generated_queries);
    p_atomic_inc(&q->fences);
-   q->batch_id = batch->batch_id;
-   _mesa_set_add(batch->active_queries, q);
+   zink_batch_usage_set(&q->batch_id, batch->state->fence.batch_id);
+   _mesa_set_add(batch->state->active_queries, q);
 }
 
 static bool
@@ -364,11 +660,11 @@ zink_begin_query(struct pipe_context *pctx,
 {
    struct zink_query *query = (struct zink_query *)q;
    struct zink_context *ctx = zink_context(pctx);
-   struct zink_batch *batch = zink_curr_batch(ctx);
+   struct zink_batch *batch = &ctx->batch;
 
    query->last_start = query->curr_query;
-
-   util_query_clear_result(&query->accumulated_result, query->type);
+   /* drop all past results */
+   reset_qbo(query);
 
    begin_query(ctx, batch, query);
 
@@ -379,18 +675,35 @@ static void
 end_query(struct zink_context *ctx, struct zink_batch *batch, struct zink_query *q)
 {
    struct zink_screen *screen = zink_screen(ctx->base.screen);
+   struct zink_query_buffer *qbo = q->curr_qbo;
+   assert(qbo);
+   batch->has_work = true;
    q->active = q->type == PIPE_QUERY_TIMESTAMP;
    if (is_time_query(q)) {
-      vkCmdWriteTimestamp(batch->cmdbuf, VK_PIPELINE_STAGE_BOTTOM_OF_PIPE_BIT,
+      vkCmdWriteTimestamp(batch->state->cmdbuf, VK_PIPELINE_STAGE_BOTTOM_OF_PIPE_BIT,
                           q->query_pool, q->curr_query);
-      q->batch_id = batch->batch_id;
-   } else if (q->type == PIPE_QUERY_PRIMITIVES_EMITTED || q->type == PIPE_QUERY_PRIMITIVES_GENERATED)
-      screen->vk_CmdEndQueryIndexedEXT(batch->cmdbuf, q->xfb_query_pool ? q->xfb_query_pool : q->query_pool, q->curr_query, q->index);
+      zink_batch_usage_set(&q->batch_id, batch->state->fence.batch_id);
+   } else if (q->type == PIPE_QUERY_PRIMITIVES_EMITTED ||
+            q->type == PIPE_QUERY_PRIMITIVES_GENERATED ||
+            q->type == PIPE_QUERY_SO_OVERFLOW_PREDICATE) {
+      screen->vk_CmdEndQueryIndexedEXT(batch->state->cmdbuf, q->xfb_query_pool[0] ? q->xfb_query_pool[0] :
+                                                                                    q->query_pool,
+                                       q->curr_query, q->index);
+   }
+
+   else if (q->type == PIPE_QUERY_SO_OVERFLOW_ANY_PREDICATE) {
+      screen->vk_CmdEndQueryIndexedEXT(batch->state->cmdbuf, q->query_pool, q->curr_query, 0);
+      for (unsigned i = 0; i < ARRAY_SIZE(q->xfb_query_pool); i++) {
+         screen->vk_CmdEndQueryIndexedEXT(batch->state->cmdbuf, q->xfb_query_pool[i], q->curr_query, i + 1);
+      }
+   }
    if (q->vkqtype != VK_QUERY_TYPE_TRANSFORM_FEEDBACK_STREAM_EXT && !is_time_query(q))
-      vkCmdEndQuery(batch->cmdbuf, q->query_pool, q->curr_query);
+      vkCmdEndQuery(batch->state->cmdbuf, q->query_pool, q->curr_query);
+
+   update_qbo(ctx, q);
    if (q->type == PIPE_QUERY_PRIMITIVES_GENERATED)
       list_delinit(&q->stats_list);
-   if (++q->curr_query == q->num_queries) {
+   if (++q->curr_query == NUM_QUERIES) {
       /* always reset on start; this ensures we can actually submit the batch that the current query is on */
       q->needs_reset = true;
    }
@@ -402,7 +715,7 @@ zink_end_query(struct pipe_context *pctx,
 {
    struct zink_context *ctx = zink_context(pctx);
    struct zink_query *query = (struct zink_query *)q;
-   struct zink_batch *batch = zink_curr_batch(ctx);
+   struct zink_batch *batch = &ctx->batch;
 
    if (query->type == PIPE_QUERY_PRIMITIVES_GENERATED)
       list_delinit(&query->stats_list);
@@ -418,19 +731,20 @@ zink_get_query_result(struct pipe_context *pctx,
                       bool wait,
                       union pipe_query_result *result)
 {
-   if (wait) {
-      zink_fence_wait(pctx);
-   } else
-      pctx->flush(pctx, NULL, 0);
+   struct zink_query *query = (void*)q;
+   struct zink_context *ctx = zink_context(pctx);
+   uint32_t batch_id = p_atomic_read(&query->batch_id.usage);
+
+   if (wait)
+      zink_wait_on_batch(ctx, batch_id);
+
    return get_query_result(pctx, q, wait, result);
 }
 
 void
 zink_suspend_queries(struct zink_context *ctx, struct zink_batch *batch)
 {
-   if (!batch->active_queries)
-      return;
-   set_foreach(batch->active_queries, entry) {
+   set_foreach(batch->state->active_queries, entry) {
       struct zink_query *query = (void*)entry->key;
       /* if a query isn't active here then we don't need to reactivate it on the next batch */
       if (query->active) {
@@ -471,7 +785,7 @@ zink_set_active_query_state(struct pipe_context *pctx, bool enable)
    struct zink_context *ctx = zink_context(pctx);
    ctx->queries_disabled = !enable;
 
-   struct zink_batch *batch = zink_curr_batch(ctx);
+   struct zink_batch *batch = &ctx->batch;
    if (ctx->queries_disabled)
       zink_suspend_queries(ctx, batch);
    else
@@ -491,7 +805,8 @@ zink_render_condition(struct pipe_context *pctx,
    VkQueryResultFlagBits flags = 0;
 
    if (query == NULL) {
-      screen->vk_CmdEndConditionalRenderingEXT(batch->cmdbuf);
+      zink_clear_apply_conditionals(ctx);
+      screen->vk_CmdEndConditionalRenderingEXT(batch->state->cmdbuf);
       ctx->render_condition_active = false;
       return;
    }
@@ -515,27 +830,101 @@ zink_render_condition(struct pipe_context *pctx,
    if (mode == PIPE_RENDER_COND_WAIT || mode == PIPE_RENDER_COND_BY_REGION_WAIT)
       flags |= VK_QUERY_RESULT_WAIT_BIT;
 
-   if (query->use_64bit)
-      flags |= VK_QUERY_RESULT_64_BIT;
+   flags |= VK_QUERY_RESULT_64_BIT;
    int num_results = query->curr_query - query->last_start;
-   vkCmdCopyQueryPoolResults(batch->cmdbuf, query->query_pool, query->last_start, num_results,
-                             res->buffer, 0, 0, flags);
+   if (query->type != PIPE_QUERY_PRIMITIVES_GENERATED &&
+       !is_so_overflow_query(query)) {
+      copy_results_to_buffer(ctx, query, res, 0, num_results, flags);
+      batch = &ctx->batch;
+   } else {
+      /* these need special handling */
+      force_cpu_read(ctx, pquery, PIPE_QUERY_TYPE_U32, pres, 0);
+      batch = &ctx->batch;
+      zink_batch_reference_resource_rw(batch, res, false);
+   }
 
    VkConditionalRenderingFlagsEXT begin_flags = 0;
    if (condition)
       begin_flags = VK_CONDITIONAL_RENDERING_INVERTED_BIT_EXT;
    VkConditionalRenderingBeginInfoEXT begin_info = {};
    begin_info.sType = VK_STRUCTURE_TYPE_CONDITIONAL_RENDERING_BEGIN_INFO_EXT;
-   begin_info.buffer = res->buffer;
+   begin_info.buffer = res->obj->buffer;
    begin_info.flags = begin_flags;
-   screen->vk_CmdBeginConditionalRenderingEXT(batch->cmdbuf, &begin_info);
+   screen->vk_CmdBeginConditionalRenderingEXT(batch->state->cmdbuf, &begin_info);
    ctx->render_condition_active = true;
 
-   zink_batch_reference_resource_rw(batch, res, true);
-
    pipe_resource_reference(&pres, NULL);
 }
 
+static void
+zink_get_query_result_resource(struct pipe_context *pctx,
+                               struct pipe_query *pquery,
+                               bool wait,
+                               enum pipe_query_value_type result_type,
+                               int index,
+                               struct pipe_resource *pres,
+                               unsigned offset)
+{
+   struct zink_context *ctx = zink_context(pctx);
+   struct zink_screen *screen = zink_screen(pctx->screen);
+   struct zink_query *query = (struct zink_query*)pquery;
+   struct zink_resource *res = zink_resource(pres);
+   unsigned result_size = result_type <= PIPE_QUERY_TYPE_U32 ? sizeof(uint32_t) : sizeof(uint64_t);
+   VkQueryResultFlagBits size_flags = result_type <= PIPE_QUERY_TYPE_U32 ? 0 : VK_QUERY_RESULT_64_BIT;
+   unsigned num_queries = query->curr_query - query->last_start;
+   unsigned query_id = query->last_start;
+   unsigned fences = p_atomic_read(&query->fences);
+
+   if (index == -1) {
+      /* VK_QUERY_RESULT_WITH_AVAILABILITY_BIT will ALWAYS write some kind of result data
+       * in addition to the availability result, which is a problem if we're just trying to get availability data
+       *
+       * if we know that there's no valid buffer data in the preceding buffer range, then we can just
+       * stomp on it with a glorious queued buffer copy instead of forcing a stall to manually write to the
+       * buffer
+       */
+
+      if (fences) {
+         struct pipe_resource *staging = pipe_buffer_create(pctx->screen, 0, PIPE_USAGE_STAGING, result_size * 2);
+         copy_results_to_buffer(ctx, query, zink_resource(staging), 0, 1, size_flags | VK_QUERY_RESULT_WITH_AVAILABILITY_BIT | VK_QUERY_RESULT_PARTIAL_BIT);
+         zink_copy_buffer(ctx, &ctx->batch, res, zink_resource(staging), offset, result_size, result_size);
+         pipe_resource_reference(&staging, NULL);
+      } else {
+         uint64_t u64[2] = {0};
+         if (vkGetQueryPoolResults(screen->dev, query->query_pool, query_id, 1, 2 * result_size, u64,
+                                   0, size_flags | VK_QUERY_RESULT_WITH_AVAILABILITY_BIT | VK_QUERY_RESULT_PARTIAL_BIT) != VK_SUCCESS) {
+            debug_printf("zink: getting query result failed\n");
+            return;
+         }
+         pipe_buffer_write(pctx, pres, offset, result_size, (unsigned char*)u64 + result_size);
+      }
+      return;
+   }
+
+   if (!is_time_query(query) && !is_bool_query(query)) {
+      if (num_queries == 1 && query->type != PIPE_QUERY_PRIMITIVES_GENERATED &&
+                              query->type != PIPE_QUERY_PRIMITIVES_EMITTED &&
+                              !is_bool_query(query)) {
+         if (size_flags == VK_QUERY_RESULT_64_BIT)
+            /* internal qbo always writes 64bit value so we can just direct copy */
+            zink_copy_buffer(ctx, NULL, res, zink_resource(query->curr_qbo->buffer), offset,
+                             get_buffer_offset(query, query->curr_qbo->buffer, query->last_start),
+                             result_size);
+         else
+            /* have to do a new copy for 32bit */
+            copy_results_to_buffer(ctx, query, res, offset, 1, size_flags);
+         return;
+      }
+   }
+
+   /* TODO: use CS to aggregate results */
+
+   /* unfortunately, there's no way to accumulate results from multiple queries on the gpu without either
+    * clobbering all but the last result or writing the results sequentially, so we have to manually write the result
+    */
+   force_cpu_read(ctx, pquery, result_type, pres, offset);
+}
+
 static uint64_t
 zink_get_timestamp(struct pipe_context *pctx)
 {
@@ -562,6 +951,7 @@ zink_context_query_init(struct pipe_context *pctx)
    pctx->begin_query = zink_begin_query;
    pctx->end_query = zink_end_query;
    pctx->get_query_result = zink_get_query_result;
+   pctx->get_query_result_resource = zink_get_query_result_resource;
    pctx->set_active_query_state = zink_set_active_query_state;
    pctx->render_condition = zink_render_condition;
    pctx->get_timestamp = zink_get_timestamp;
diff --git a/src/gallium/drivers/zink/zink_query.h b/src/gallium/drivers/zink/zink_query.h
index d9606c2bdaf..357fefae43f 100644
--- a/src/gallium/drivers/zink/zink_query.h
+++ b/src/gallium/drivers/zink/zink_query.h
@@ -27,6 +27,7 @@
 struct zink_batch;
 struct zink_context;
 struct zink_fence;
+struct zink_query;
 struct zink_screen;
 
 void
@@ -36,7 +37,7 @@ void
 zink_resume_queries(struct zink_context *ctx, struct zink_batch *batch);
 
 void
-zink_prune_queries(struct zink_screen *screen, struct zink_fence *fence);
+zink_prune_query(struct zink_screen *screen, struct zink_query *query);
 
 void
 zink_query_update_gs_states(struct zink_context *ctx);
diff --git a/src/gallium/drivers/zink/zink_render_pass.c b/src/gallium/drivers/zink/zink_render_pass.c
index b1eb9015bd8..85949f82e79 100644
--- a/src/gallium/drivers/zink/zink_render_pass.c
+++ b/src/gallium/drivers/zink/zink_render_pass.c
@@ -40,12 +40,17 @@ create_render_pass(VkDevice dev, struct zink_render_pass_state *state)
       attachments[i].flags = 0;
       attachments[i].format = rt->format;
       attachments[i].samples = rt->samples;
-      attachments[i].loadOp = VK_ATTACHMENT_LOAD_OP_LOAD;
+      attachments[i].loadOp = rt->clear_color ? VK_ATTACHMENT_LOAD_OP_CLEAR :
+                                                state->swapchain_init && rt->swapchain ?
+                                                VK_ATTACHMENT_LOAD_OP_DONT_CARE :
+                                                VK_ATTACHMENT_LOAD_OP_LOAD;
       attachments[i].storeOp = VK_ATTACHMENT_STORE_OP_STORE;
       attachments[i].stencilLoadOp = VK_ATTACHMENT_LOAD_OP_DONT_CARE;
       attachments[i].stencilStoreOp = VK_ATTACHMENT_STORE_OP_DONT_CARE;
-      attachments[i].initialLayout = VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL;
-      attachments[i].finalLayout = VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL;
+      attachments[i].initialLayout = state->swapchain_init && rt->swapchain ?
+                                     VK_IMAGE_LAYOUT_UNDEFINED :
+                                     VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL;
+      attachments[i].finalLayout = rt->swapchain ? VK_IMAGE_LAYOUT_PRESENT_SRC_KHR : VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL;
       color_refs[i].attachment = i;
       color_refs[i].layout = VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL;
    }
@@ -56,9 +61,9 @@ create_render_pass(VkDevice dev, struct zink_render_pass_state *state)
       attachments[num_attachments].flags = 0;
       attachments[num_attachments].format = rt->format;
       attachments[num_attachments].samples = rt->samples;
-      attachments[num_attachments].loadOp = VK_ATTACHMENT_LOAD_OP_LOAD;
+      attachments[num_attachments].loadOp = rt->clear_color ? VK_ATTACHMENT_LOAD_OP_CLEAR : VK_ATTACHMENT_LOAD_OP_LOAD;
       attachments[num_attachments].storeOp = VK_ATTACHMENT_STORE_OP_STORE;
-      attachments[num_attachments].stencilLoadOp = VK_ATTACHMENT_LOAD_OP_LOAD;
+      attachments[num_attachments].stencilLoadOp = rt->clear_stencil ? VK_ATTACHMENT_LOAD_OP_CLEAR : VK_ATTACHMENT_LOAD_OP_LOAD;
       attachments[num_attachments].stencilStoreOp = VK_ATTACHMENT_STORE_OP_STORE;
       attachments[num_attachments].initialLayout = VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL;
       attachments[num_attachments].finalLayout = VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL;
diff --git a/src/gallium/drivers/zink/zink_render_pass.h b/src/gallium/drivers/zink/zink_render_pass.h
index 577b92c9065..328bea4a6dd 100644
--- a/src/gallium/drivers/zink/zink_render_pass.h
+++ b/src/gallium/drivers/zink/zink_render_pass.h
@@ -34,11 +34,18 @@ struct zink_screen;
 struct zink_rt_attrib {
   VkFormat format;
   VkSampleCountFlagBits samples;
+  bool clear_color;
+  bool clear_stencil;
+  bool swapchain;
 };
 
 struct zink_render_pass_state {
    uint8_t num_cbufs : 4; /* PIPE_MAX_COLOR_BUFS = 8 */
    uint8_t have_zsbuf : 1;
+   bool swapchain_init;
+#ifndef NDEBUG
+   uint32_t clears; //for extra verification
+#endif
    struct zink_rt_attrib rts[PIPE_MAX_COLOR_BUFS + 1];
    unsigned num_rts;
 };
diff --git a/src/gallium/drivers/zink/zink_resource.c b/src/gallium/drivers/zink/zink_resource.c
index 8ce09755c30..b8cc185b85e 100644
--- a/src/gallium/drivers/zink/zink_resource.c
+++ b/src/gallium/drivers/zink/zink_resource.c
@@ -25,6 +25,8 @@
 
 #include "zink_batch.h"
 #include "zink_context.h"
+#include "zink_fence.h"
+#include "zink_program.h"
 #include "zink_screen.h"
 
 #include "vulkan/wsi/wsi_common.h"
@@ -35,6 +37,7 @@
 #include "util/u_transfer_helper.h"
 #include "util/u_inlines.h"
 #include "util/u_memory.h"
+#include "util/u_upload_mgr.h"
 
 #include "frontend/sw_winsys.h"
 
@@ -46,6 +49,112 @@
 #include "drm-uapi/drm_fourcc.h"
 #endif
 
+static void
+zink_transfer_flush_region(struct pipe_context *pctx,
+                           struct pipe_transfer *ptrans,
+                           const struct pipe_box *box);
+static void *
+zink_transfer_map(struct pipe_context *pctx,
+                  struct pipe_resource *pres,
+                  unsigned level,
+                  unsigned usage,
+                  const struct pipe_box *box,
+                  struct pipe_transfer **transfer);
+static void
+zink_transfer_unmap(struct pipe_context *pctx,
+                    struct pipe_transfer *ptrans);
+
+void
+debug_describe_zink_resource_object(char *buf, const struct zink_resource_object *ptr)
+{
+   sprintf(buf, "zink_resource_object");
+}
+
+static uint32_t
+get_resource_usage(struct zink_resource *res)
+{
+   uint32_t reads = p_atomic_read(&res->obj->reads.usage);
+   uint32_t writes = p_atomic_read(&res->obj->writes.usage);
+   uint32_t batch_uses = 0;
+   if (reads)
+      batch_uses |= ZINK_RESOURCE_ACCESS_READ;
+   if (writes)
+      batch_uses |= ZINK_RESOURCE_ACCESS_WRITE;
+   return batch_uses;
+}
+
+static void
+resource_sync_reads(struct zink_context *ctx, struct zink_resource *res)
+{
+   uint32_t reads = p_atomic_read(&res->obj->reads.usage);
+   assert(reads);
+   zink_wait_on_batch(ctx, reads);
+}
+
+static void
+resource_sync_writes_from_batch_id(struct zink_context *ctx, struct zink_resource *res)
+{
+   uint32_t writes = p_atomic_read(&res->obj->writes.usage);
+
+   zink_wait_on_batch(ctx, writes);
+}
+
+static uint32_t
+mem_hash(const void *key)
+{
+   return _mesa_hash_data(key, sizeof(struct mem_key));
+}
+
+static bool
+mem_equals(const void *a, const void *b)
+{
+   return !memcmp(a, b, sizeof(struct mem_key));
+}
+
+static void
+cache_or_free_mem(struct zink_screen *screen, struct zink_resource_object *obj)
+{
+   if (obj->mkey.flags) {
+      simple_mtx_lock(&screen->mem_cache_mtx);
+      struct hash_entry *he = _mesa_hash_table_search_pre_hashed(screen->resource_mem_cache, obj->mem_hash, &obj->mkey);
+      struct util_dynarray *array = he ? (void*)he->data : NULL;
+      if (!array) {
+         struct mem_key *mkey = rzalloc(screen->resource_mem_cache, struct mem_key);
+         memcpy(mkey, &obj->mkey, sizeof(struct mem_key));
+         array = rzalloc(screen->resource_mem_cache, struct util_dynarray);
+         util_dynarray_init(array, screen->resource_mem_cache);
+         _mesa_hash_table_insert_pre_hashed(screen->resource_mem_cache, obj->mem_hash, mkey, array);
+      }
+      if (util_dynarray_num_elements(array, VkDeviceMemory) < 2) {
+         util_dynarray_append(array, VkDeviceMemory, obj->mem);
+         simple_mtx_unlock(&screen->mem_cache_mtx);
+         return;
+      }
+      simple_mtx_unlock(&screen->mem_cache_mtx);
+   }
+   vkFreeMemory(screen->dev, obj->mem, NULL);
+}
+
+void
+zink_destroy_resource_object(struct zink_screen *screen, struct zink_resource_object *obj)
+{
+   assert(!obj->map_count);
+   if (obj->is_buffer) {
+      if (obj->sbuffer)
+         vkDestroyBuffer(screen->dev, obj->sbuffer, NULL);
+      vkDestroyBuffer(screen->dev, obj->buffer, NULL);
+   } else {
+      if (obj->simage)
+         vkDestroyImage(screen->dev, obj->simage, NULL);
+      vkDestroyImage(screen->dev, obj->image, NULL);
+   }
+   simple_mtx_destroy(&obj->map_mtx);
+
+   zink_descriptor_set_refs_clear(&obj->desc_set_refs, obj);
+   cache_or_free_mem(screen, obj);
+   FREE(obj);
+}
+
 static void
 zink_resource_destroy(struct pipe_screen *pscreen,
                       struct pipe_resource *pres)
@@ -53,11 +162,10 @@ zink_resource_destroy(struct pipe_screen *pscreen,
    struct zink_screen *screen = zink_screen(pscreen);
    struct zink_resource *res = zink_resource(pres);
    if (pres->target == PIPE_BUFFER)
-      vkDestroyBuffer(screen->dev, res->buffer, NULL);
-   else
-      vkDestroyImage(screen->dev, res->image, NULL);
+      util_range_destroy(&res->valid_buffer_range);
 
-   vkFreeMemory(screen->dev, res->mem, NULL);
+   zink_resource_object_reference(screen, &res->obj, NULL);
+   threaded_resource_deinit(pres);
    FREE(res);
 }
 
@@ -94,159 +202,185 @@ aspect_from_format(enum pipe_format fmt)
      return VK_IMAGE_ASPECT_COLOR_BIT;
 }
 
-static struct pipe_resource *
-resource_create(struct pipe_screen *pscreen,
-                const struct pipe_resource *templ,
-                struct winsys_handle *whandle,
-                unsigned external_usage)
+static VkBufferCreateInfo
+create_bci(struct zink_screen *screen, const struct pipe_resource *templ, unsigned bind)
 {
-   struct zink_screen *screen = zink_screen(pscreen);
-   struct zink_resource *res = CALLOC_STRUCT(zink_resource);
+   VkBufferCreateInfo bci = {};
+   bci.sType = VK_STRUCTURE_TYPE_BUFFER_CREATE_INFO;
+   bci.size = templ->width0;
 
-   res->base = *templ;
+   bci.usage = VK_BUFFER_USAGE_TRANSFER_SRC_BIT |
+               VK_BUFFER_USAGE_TRANSFER_DST_BIT;
 
-   pipe_reference_init(&res->base.reference, 1);
-   res->base.screen = pscreen;
+   VkFormatProperties props = screen->format_props[templ->format];
 
-   VkMemoryRequirements reqs = {};
-   VkMemoryPropertyFlags flags = 0;
+   if (templ->usage != PIPE_USAGE_STAGING)
+      bci.usage |= VK_BUFFER_USAGE_UNIFORM_TEXEL_BUFFER_BIT;
 
-   res->internal_format = templ->format;
-   if (templ->target == PIPE_BUFFER) {
-      VkBufferCreateInfo bci = {};
-      bci.sType = VK_STRUCTURE_TYPE_BUFFER_CREATE_INFO;
-      bci.size = templ->width0;
-
-      bci.usage = VK_BUFFER_USAGE_TRANSFER_SRC_BIT |
-                  VK_BUFFER_USAGE_TRANSFER_DST_BIT;
-
-      /* apparently gallium thinks this is the jack-of-all-trades bind type */
-      if (templ->bind & PIPE_BIND_SAMPLER_VIEW) {
-         bci.usage |= VK_BUFFER_USAGE_UNIFORM_TEXEL_BUFFER_BIT |
-                      VK_BUFFER_USAGE_UNIFORM_BUFFER_BIT |
-                      VK_BUFFER_USAGE_INDIRECT_BUFFER_BIT |
-                      VK_BUFFER_USAGE_INDEX_BUFFER_BIT |
-                      VK_BUFFER_USAGE_VERTEX_BUFFER_BIT |
-                      VK_BUFFER_USAGE_TRANSFORM_FEEDBACK_BUFFER_BIT_EXT;
-         VkFormatProperties props;
-         vkGetPhysicalDeviceFormatProperties(screen->pdev, zink_get_format(screen, templ->format), &props);
-         if (props.bufferFeatures & VK_BUFFER_USAGE_STORAGE_BUFFER_BIT)
-            bci.usage |= VK_BUFFER_USAGE_STORAGE_BUFFER_BIT;
-         if (props.bufferFeatures & VK_BUFFER_USAGE_STORAGE_TEXEL_BUFFER_BIT)
-            bci.usage |= VK_BUFFER_USAGE_STORAGE_TEXEL_BUFFER_BIT;
-      }
+   if (props.bufferFeatures & VK_BUFFER_USAGE_STORAGE_BUFFER_BIT)
+      bci.usage |= VK_BUFFER_USAGE_STORAGE_BUFFER_BIT;
 
-      if (templ->bind & PIPE_BIND_VERTEX_BUFFER)
-         bci.usage |= VK_BUFFER_USAGE_VERTEX_BUFFER_BIT |
-                      VK_BUFFER_USAGE_INDEX_BUFFER_BIT |
-                      VK_BUFFER_USAGE_UNIFORM_TEXEL_BUFFER_BIT;
+   /* apparently gallium thinks this is the jack-of-all-trades bind type */
+   if (bind & (PIPE_BIND_SAMPLER_VIEW | PIPE_BIND_QUERY_BUFFER)) {
+      bci.usage |= VK_BUFFER_USAGE_UNIFORM_TEXEL_BUFFER_BIT |
+                   VK_BUFFER_USAGE_UNIFORM_BUFFER_BIT |
+                   VK_BUFFER_USAGE_INDIRECT_BUFFER_BIT |
+                   VK_BUFFER_USAGE_INDEX_BUFFER_BIT |
+                   VK_BUFFER_USAGE_VERTEX_BUFFER_BIT |
+                   VK_BUFFER_USAGE_TRANSFORM_FEEDBACK_BUFFER_BIT_EXT;
+   }
 
-      if (templ->bind & PIPE_BIND_INDEX_BUFFER)
-         bci.usage |= VK_BUFFER_USAGE_INDEX_BUFFER_BIT;
+   if (bind & PIPE_BIND_VERTEX_BUFFER)
+      bci.usage |= VK_BUFFER_USAGE_VERTEX_BUFFER_BIT |
+                   VK_BUFFER_USAGE_INDEX_BUFFER_BIT |
+                   VK_BUFFER_USAGE_UNIFORM_TEXEL_BUFFER_BIT |
+                   VK_BUFFER_USAGE_UNIFORM_BUFFER_BIT |
+                   VK_BUFFER_USAGE_TRANSFORM_FEEDBACK_BUFFER_BIT_EXT;
 
-      if (templ->bind & PIPE_BIND_CONSTANT_BUFFER)
-         bci.usage |= VK_BUFFER_USAGE_UNIFORM_BUFFER_BIT;
+   if (bind & PIPE_BIND_INDEX_BUFFER)
+      bci.usage |= VK_BUFFER_USAGE_INDEX_BUFFER_BIT;
 
-      if (templ->bind & PIPE_BIND_SHADER_BUFFER)
-         bci.usage |= VK_BUFFER_USAGE_STORAGE_BUFFER_BIT;
+   if (bind & PIPE_BIND_CONSTANT_BUFFER)
+      bci.usage |= VK_BUFFER_USAGE_UNIFORM_BUFFER_BIT;
 
-      if (templ->bind & PIPE_BIND_COMMAND_ARGS_BUFFER)
-         bci.usage |= VK_BUFFER_USAGE_INDIRECT_BUFFER_BIT;
+   if (bind & PIPE_BIND_SHADER_IMAGE && props.bufferFeatures & VK_BUFFER_USAGE_STORAGE_TEXEL_BUFFER_BIT)
+      bci.usage |= VK_BUFFER_USAGE_STORAGE_TEXEL_BUFFER_BIT;
 
-      if (templ->bind == (PIPE_BIND_STREAM_OUTPUT | PIPE_BIND_CUSTOM)) {
-         bci.usage |= VK_BUFFER_USAGE_TRANSFORM_FEEDBACK_COUNTER_BUFFER_BIT_EXT;
-      } else if (templ->bind & PIPE_BIND_STREAM_OUTPUT) {
-         bci.usage |= VK_BUFFER_USAGE_VERTEX_BUFFER_BIT | VK_BUFFER_USAGE_TRANSFORM_FEEDBACK_BUFFER_BIT_EXT;
-      }
+   if (bind & PIPE_BIND_COMMAND_ARGS_BUFFER)
+      bci.usage |= VK_BUFFER_USAGE_INDIRECT_BUFFER_BIT;
 
-      if (vkCreateBuffer(screen->dev, &bci, NULL, &res->buffer) !=
-          VK_SUCCESS) {
-         FREE(res);
-         return NULL;
-      }
+   if (bind == (PIPE_BIND_STREAM_OUTPUT | PIPE_BIND_CUSTOM)) {
+      bci.usage |= VK_BUFFER_USAGE_TRANSFORM_FEEDBACK_COUNTER_BUFFER_BIT_EXT;
+   } else if (bind & PIPE_BIND_STREAM_OUTPUT) {
+      bci.usage |= VK_BUFFER_USAGE_VERTEX_BUFFER_BIT |
+                   VK_BUFFER_USAGE_INDEX_BUFFER_BIT |
+                   VK_BUFFER_USAGE_UNIFORM_BUFFER_BIT |
+                   VK_BUFFER_USAGE_TRANSFORM_FEEDBACK_BUFFER_BIT_EXT;
+   }
+   return bci;
+}
 
-      vkGetBufferMemoryRequirements(screen->dev, res->buffer, &reqs);
-      flags |= VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT;
-   } else {
-      res->format = zink_get_format(screen, templ->format);
+static VkImageCreateInfo
+create_ici(struct zink_screen *screen, const struct pipe_resource *templ, unsigned bind)
+{
+   VkImageCreateInfo ici = {};
+   ici.sType = VK_STRUCTURE_TYPE_IMAGE_CREATE_INFO;
+   ici.flags = VK_IMAGE_CREATE_MUTABLE_FORMAT_BIT;
+
+   switch (templ->target) {
+   case PIPE_TEXTURE_1D:
+   case PIPE_TEXTURE_1D_ARRAY:
+      ici.imageType = VK_IMAGE_TYPE_1D;
+      break;
+
+   case PIPE_TEXTURE_CUBE:
+   case PIPE_TEXTURE_CUBE_ARRAY:
+      ici.flags |= VK_IMAGE_CREATE_CUBE_COMPATIBLE_BIT;
+      /* fall-through */
+   case PIPE_TEXTURE_2D:
+   case PIPE_TEXTURE_2D_ARRAY:
+   case PIPE_TEXTURE_RECT:
+      ici.imageType = VK_IMAGE_TYPE_2D;
+      break;
+
+   case PIPE_TEXTURE_3D:
+      ici.imageType = VK_IMAGE_TYPE_3D;
+      if (bind & PIPE_BIND_RENDER_TARGET)
+         ici.flags |= VK_IMAGE_CREATE_2D_ARRAY_COMPATIBLE_BIT;
+      break;
+
+   case PIPE_BUFFER:
+      unreachable("PIPE_BUFFER should already be handled");
+
+   default:
+      unreachable("Unknown target");
+   }
 
-      VkImageCreateInfo ici = {};
-      ici.sType = VK_STRUCTURE_TYPE_IMAGE_CREATE_INFO;
-      ici.flags = VK_IMAGE_CREATE_MUTABLE_FORMAT_BIT;
-
-      switch (templ->target) {
-      case PIPE_TEXTURE_1D:
-      case PIPE_TEXTURE_1D_ARRAY:
-         ici.imageType = VK_IMAGE_TYPE_1D;
-         break;
-
-      case PIPE_TEXTURE_CUBE:
-      case PIPE_TEXTURE_CUBE_ARRAY:
-         ici.flags |= VK_IMAGE_CREATE_CUBE_COMPATIBLE_BIT;
-         /* fall-through */
-      case PIPE_TEXTURE_2D:
-      case PIPE_TEXTURE_2D_ARRAY:
-      case PIPE_TEXTURE_RECT:
-         ici.imageType = VK_IMAGE_TYPE_2D;
-         break;
-
-      case PIPE_TEXTURE_3D:
-         ici.imageType = VK_IMAGE_TYPE_3D;
-         if (templ->bind & PIPE_BIND_RENDER_TARGET)
-            ici.flags |= VK_IMAGE_CREATE_2D_ARRAY_COMPATIBLE_BIT;
-         break;
-
-      case PIPE_BUFFER:
-         unreachable("PIPE_BUFFER should already be handled");
-
-      default:
-         unreachable("Unknown target");
-      }
+   ici.format = zink_get_format(screen, templ->format);
+   ici.extent.width = templ->width0;
+   ici.extent.height = templ->height0;
+   ici.extent.depth = templ->depth0;
+   ici.mipLevels = templ->last_level + 1;
+   ici.arrayLayers = MAX2(templ->array_size, 1);
+   ici.samples = templ->nr_samples ? templ->nr_samples : VK_SAMPLE_COUNT_1_BIT;
+   ici.tiling = bind & PIPE_BIND_LINEAR ? VK_IMAGE_TILING_LINEAR : VK_IMAGE_TILING_OPTIMAL;
+
+   if (templ->target == PIPE_TEXTURE_CUBE ||
+       templ->target == PIPE_TEXTURE_CUBE_ARRAY)
+      ici.arrayLayers *= 6;
+
+   if (templ->usage == PIPE_USAGE_STAGING)
+      ici.tiling = VK_IMAGE_TILING_LINEAR;
+
+   /* sadly, gallium doesn't let us know if it'll ever need this, so we have to assume */
+   ici.usage = VK_IMAGE_USAGE_TRANSFER_SRC_BIT |
+               VK_IMAGE_USAGE_TRANSFER_DST_BIT |
+               VK_IMAGE_USAGE_SAMPLED_BIT;
+
+   if ((templ->nr_samples < 2 || screen->info.feats.features.shaderStorageImageMultisample) &&
+       (bind & PIPE_BIND_SHADER_IMAGE)) {
+      VkFormatProperties props = screen->format_props[templ->format];
+      if ((ici.tiling == VK_IMAGE_TILING_LINEAR && props.linearTilingFeatures & VK_FORMAT_FEATURE_STORAGE_IMAGE_BIT) ||
+          (ici.tiling == VK_IMAGE_TILING_OPTIMAL && props.optimalTilingFeatures & VK_FORMAT_FEATURE_STORAGE_IMAGE_BIT))
+         ici.usage |= VK_IMAGE_USAGE_STORAGE_BIT;
+   }
 
-      ici.format = res->format;
-      ici.extent.width = templ->width0;
-      ici.extent.height = templ->height0;
-      ici.extent.depth = templ->depth0;
-      ici.mipLevels = templ->last_level + 1;
-      ici.arrayLayers = MAX2(templ->array_size, 1);
-      ici.samples = templ->nr_samples ? templ->nr_samples : VK_SAMPLE_COUNT_1_BIT;
-      ici.tiling = templ->bind & PIPE_BIND_LINEAR ? VK_IMAGE_TILING_LINEAR : VK_IMAGE_TILING_OPTIMAL;
+   if (bind & PIPE_BIND_RENDER_TARGET)
+      ici.usage |= VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT;
 
-      if (templ->target == PIPE_TEXTURE_CUBE ||
-          templ->target == PIPE_TEXTURE_CUBE_ARRAY)
-         ici.arrayLayers *= 6;
+   if (bind & PIPE_BIND_DEPTH_STENCIL)
+      ici.usage |= VK_IMAGE_USAGE_DEPTH_STENCIL_ATTACHMENT_BIT;
 
-      if (templ->bind & (PIPE_BIND_DISPLAY_TARGET |
-                         PIPE_BIND_SHARED)) {
-         ici.tiling = VK_IMAGE_TILING_LINEAR;
-      }
+   if (templ->flags & PIPE_RESOURCE_FLAG_SPARSE)
+      ici.usage |= VK_IMAGE_USAGE_TRANSIENT_ATTACHMENT_BIT;
 
-      if (templ->usage == PIPE_USAGE_STAGING)
-         ici.tiling = VK_IMAGE_TILING_LINEAR;
+   if (bind & PIPE_BIND_STREAM_OUTPUT)
+      ici.usage |= VK_IMAGE_USAGE_INPUT_ATTACHMENT_BIT;
 
-      /* sadly, gallium doesn't let us know if it'll ever need this, so we have to assume */
-      ici.usage = VK_IMAGE_USAGE_TRANSFER_SRC_BIT |
-                  VK_IMAGE_USAGE_TRANSFER_DST_BIT |
-                  VK_IMAGE_USAGE_SAMPLED_BIT;
+   ici.sharingMode = VK_SHARING_MODE_EXCLUSIVE;
+   ici.initialLayout = VK_IMAGE_LAYOUT_UNDEFINED;
+   return ici;
+}
 
-      if (templ->bind & PIPE_BIND_SHADER_IMAGE)
-         ici.usage |= VK_IMAGE_USAGE_STORAGE_BIT;
+static struct zink_resource_object *
+resource_object_create(struct zink_screen *screen, const struct pipe_resource *templ, struct winsys_handle *whandle, bool *optimal_tiling)
+{
+   struct zink_resource_object *obj = CALLOC_STRUCT(zink_resource_object);
+   if (!obj)
+      return NULL;
 
-      if (templ->bind & PIPE_BIND_RENDER_TARGET)
-         ici.usage |= VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT;
+   VkMemoryRequirements reqs = {};
+   VkMemoryPropertyFlags flags = 0;
 
-      if (templ->bind & PIPE_BIND_DEPTH_STENCIL)
-         ici.usage |= VK_IMAGE_USAGE_DEPTH_STENCIL_ATTACHMENT_BIT;
+   pipe_reference_init(&obj->reference, 1);
+   util_dynarray_init(&obj->desc_set_refs.refs, NULL);
+   simple_mtx_init(&obj->map_mtx, mtx_plain);
+   if (templ->target == PIPE_BUFFER) {
+      VkBufferCreateInfo bci = create_bci(screen, templ, templ->bind);
 
-      if (templ->flags & PIPE_RESOURCE_FLAG_SPARSE)
-         ici.usage |= VK_IMAGE_USAGE_TRANSIENT_ATTACHMENT_BIT;
+      if (vkCreateBuffer(screen->dev, &bci, NULL, &obj->buffer) != VK_SUCCESS)
+         goto fail1;
 
-      if (templ->bind & PIPE_BIND_STREAM_OUTPUT)
-         ici.usage |= VK_IMAGE_USAGE_INPUT_ATTACHMENT_BIT;
+      vkGetBufferMemoryRequirements(screen->dev, obj->buffer, &reqs);
+      flags = VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT;
+      obj->is_buffer = true;
+   } else {
+      VkImageCreateInfo ici = create_ici(screen, templ, templ->bind);
+      VkExternalMemoryImageCreateInfo emici = {};
 
-      ici.sharingMode = VK_SHARING_MODE_EXCLUSIVE;
-      ici.initialLayout = VK_IMAGE_LAYOUT_UNDEFINED;
-      res->layout = VK_IMAGE_LAYOUT_UNDEFINED;
+      if (optimal_tiling)
+         *optimal_tiling = ici.tiling != VK_IMAGE_TILING_LINEAR;
+
+      if (templ->bind & (PIPE_BIND_DISPLAY_TARGET |
+                         PIPE_BIND_SHARED)) {
+         ici.tiling = VK_IMAGE_TILING_LINEAR;
+      }
+
+      if (templ->bind & PIPE_BIND_SHARED) {
+         emici.sType = VK_STRUCTURE_TYPE_EXTERNAL_MEMORY_IMAGE_CREATE_INFO;
+         emici.handleTypes = VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_FD_BIT;
+         ici.pNext = &emici;
+      }
 
       struct wsi_image_create_info image_wsi_info = {
          VK_STRUCTURE_TYPE_WSI_IMAGE_CREATE_INFO_MESA,
@@ -257,22 +391,24 @@ resource_create(struct pipe_screen *pscreen,
       if (screen->needs_mesa_wsi && (templ->bind & PIPE_BIND_SCANOUT))
          ici.pNext = &image_wsi_info;
 
-      VkResult result = vkCreateImage(screen->dev, &ici, NULL, &res->image);
-      if (result != VK_SUCCESS) {
-         FREE(res);
-         return NULL;
-      }
-
-      res->optimal_tiling = ici.tiling != VK_IMAGE_TILING_LINEAR;
-      res->aspect = aspect_from_format(templ->format);
+      VkResult result = vkCreateImage(screen->dev, &ici, NULL, &obj->image);
+      if (result != VK_SUCCESS)
+         goto fail1;
 
-      vkGetImageMemoryRequirements(screen->dev, res->image, &reqs);
+      vkGetImageMemoryRequirements(screen->dev, obj->image, &reqs);
       if (templ->usage == PIPE_USAGE_STAGING || (screen->winsys && (templ->bind & (PIPE_BIND_SCANOUT|PIPE_BIND_DISPLAY_TARGET|PIPE_BIND_SHARED))))
-        flags |= VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT;
+        flags = VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT;
       else
-        flags |= VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT;
+        flags = VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT;
    }
 
+   if (templ->flags & PIPE_RESOURCE_FLAG_MAP_COHERENT)
+      flags |= VK_MEMORY_PROPERTY_HOST_COHERENT_BIT;
+   else if (!(flags & VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT) &&
+            !screen->winsys &&
+            !(templ->bind & (PIPE_BIND_SCANOUT|PIPE_BIND_DISPLAY_TARGET|PIPE_BIND_SHARED)))
+      flags |= VK_MEMORY_PROPERTY_HOST_CACHED_BIT;
+
    VkMemoryAllocateInfo mai = {};
    mai.sType = VK_STRUCTURE_TYPE_MEMORY_ALLOCATE_INFO;
    mai.allocationSize = reqs.size;
@@ -313,41 +449,100 @@ resource_create(struct pipe_screen *pscreen,
       mai.pNext = &memory_wsi_info;
    }
 
-   if (vkAllocateMemory(screen->dev, &mai, NULL, &res->mem) != VK_SUCCESS)
-      goto fail;
+   if (!mai.pNext && !(templ->flags & PIPE_RESOURCE_FLAG_MAP_COHERENT)) {
+      obj->mkey.reqs = reqs;
+      obj->mkey.flags = flags;
+      obj->mem_hash = mem_hash(&obj->mkey);
+      simple_mtx_lock(&screen->mem_cache_mtx);
 
-   res->offset = 0;
-   res->size = reqs.size;
+      struct hash_entry *he = _mesa_hash_table_search_pre_hashed(screen->resource_mem_cache, obj->mem_hash, &obj->mkey);
+
+      struct util_dynarray *array = he ? (void*)he->data : NULL;
+      if (array && util_dynarray_num_elements(array, VkDeviceMemory)) {
+         obj->mem = util_dynarray_pop(array, VkDeviceMemory);
+      }
+      simple_mtx_unlock(&screen->mem_cache_mtx);
+   }
+
+   if (!obj->mem && vkAllocateMemory(screen->dev, &mai, NULL, &obj->mem) != VK_SUCCESS)
+      goto fail2;
+
+   obj->offset = 0;
+   obj->size = reqs.size;
 
    if (templ->target == PIPE_BUFFER)
-      vkBindBufferMemory(screen->dev, res->buffer, res->mem, res->offset);
+      vkBindBufferMemory(screen->dev, obj->buffer, obj->mem, obj->offset);
    else
-      vkBindImageMemory(screen->dev, res->image, res->mem, res->offset);
+      vkBindImageMemory(screen->dev, obj->image, obj->mem, obj->offset);
+   return obj;
+
+fail2:
+   if (templ->target == PIPE_BUFFER)
+      vkDestroyBuffer(screen->dev, obj->buffer, NULL);
+   else
+      vkDestroyImage(screen->dev, obj->image, NULL);
+fail1:
+   FREE(obj);
+   return NULL;
+}
+
+static const struct u_resource_vtbl zink_resource_vtbl = {
+   NULL,
+   zink_resource_destroy,
+   zink_transfer_map,
+   zink_transfer_flush_region,
+   zink_transfer_unmap,
+};
+
+static struct pipe_resource *
+resource_create(struct pipe_screen *pscreen,
+                const struct pipe_resource *templ,
+                struct winsys_handle *whandle,
+                unsigned external_usage)
+{
+   struct zink_screen *screen = zink_screen(pscreen);
+   struct zink_resource *res = CALLOC_STRUCT(zink_resource);
+
+   res->base.b = *templ;
+
+   res->base.vtbl = &zink_resource_vtbl;
+   threaded_resource_init(&res->base.b);
+   pipe_reference_init(&res->base.b.reference, 1);
+   res->base.b.screen = pscreen;
+
+   bool optimal_tiling = false;
+   res->obj = resource_object_create(screen, templ, whandle, &optimal_tiling);
+   if (!res->obj) {
+      FREE(res);
+      return NULL;
+   }
+
+   res->internal_format = templ->format;
+   if (templ->target == PIPE_BUFFER) {
+      util_range_init(&res->valid_buffer_range);
+   } else {
+      res->format = zink_get_format(screen, templ->format);
+      res->layout = VK_IMAGE_LAYOUT_UNDEFINED;
+      res->optimal_tiling = optimal_tiling;
+      res->aspect = aspect_from_format(templ->format);
+   }
 
    if (screen->winsys && (templ->bind & (PIPE_BIND_DISPLAY_TARGET |
                                          PIPE_BIND_SCANOUT |
                                          PIPE_BIND_SHARED))) {
-      struct sw_winsys *winsys = screen->winsys;
-      res->dt = winsys->displaytarget_create(screen->winsys,
-                                             res->base.bind,
-                                             res->base.format,
-                                             templ->width0,
-                                             templ->height0,
-                                             64, NULL,
-                                             &res->dt_stride);
+      if (screen->winsys) {
+         struct sw_winsys *winsys = screen->winsys;
+         res->dt = winsys->displaytarget_create(screen->winsys,
+                                                res->base.b.bind,
+                                                res->base.b.format,
+                                                templ->width0,
+                                                templ->height0,
+                                                64, NULL,
+                                                &res->dt_stride);
+      }
    }
 
-   return &res->base;
-
-fail:
-   if (templ->target == PIPE_BUFFER)
-      vkDestroyBuffer(screen->dev, res->buffer, NULL);
-   else
-      vkDestroyImage(screen->dev, res->image, NULL);
-
-   FREE(res);
-
-   return NULL;
+   return &res->base.b;
 }
 
 static struct pipe_resource *
@@ -367,13 +562,13 @@ zink_resource_get_handle(struct pipe_screen *pscreen,
    struct zink_resource *res = zink_resource(tex);
    struct zink_screen *screen = zink_screen(pscreen);
 
-   if (res->base.target != PIPE_BUFFER) {
+   if (res->base.b.target != PIPE_BUFFER) {
       VkImageSubresource sub_res = {};
       VkSubresourceLayout sub_res_layout = {};
 
       sub_res.aspectMask = res->aspect;
 
-      vkGetImageSubresourceLayout(screen->dev, res->image, &sub_res, &sub_res_layout);
+      vkGetImageSubresourceLayout(screen->dev, res->obj->image, &sub_res, &sub_res_layout);
 
       whandle->stride = sub_res_layout.rowPitch;
    }
@@ -383,7 +578,7 @@ zink_resource_get_handle(struct pipe_screen *pscreen,
       VkMemoryGetFdInfoKHR fd_info = {};
       int fd;
       fd_info.sType = VK_STRUCTURE_TYPE_MEMORY_GET_FD_INFO_KHR;
-      fd_info.memory = res->mem;
+      fd_info.memory = res->obj->mem;
       fd_info.handleType = VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_FD_BIT;
       VkResult result = (*screen->vk_GetMemoryFdKHR)(screen->dev, &fd_info, &fd);
       if (result != VK_SUCCESS)
@@ -414,90 +609,254 @@ zink_resource_from_handle(struct pipe_screen *pscreen,
 }
 
 static bool
+invalidate_buffer(struct zink_context *ctx, struct zink_resource *res)
+{
+   struct zink_screen *screen = zink_screen(ctx->base.screen);
+
+   assert(res->base.b.target == PIPE_BUFFER);
+
+   if (res->valid_buffer_range.start > res->valid_buffer_range.end)
+      return false;
+
+   util_range_set_empty(&res->valid_buffer_range);
+   if (!get_resource_usage(res))
+      return false;
+
+   struct zink_resource_object *old_obj = res->obj;
+   struct zink_resource_object *new_obj = resource_object_create(screen, &res->base.b, NULL, NULL);
+   if (!new_obj) {
+      debug_printf("new backing resource alloc failed!");
+      return false;
+   }
+   res->obj = new_obj;
+   res->access_stage = 0;
+   res->access = 0;
+   zink_resource_rebind(ctx, res);
+   zink_descriptor_set_refs_clear(&old_obj->desc_set_refs, old_obj);
+   zink_resource_object_reference(screen, &old_obj, NULL);
+   return true;
+}
+
+
+static void
+zink_resource_invalidate(struct pipe_context *pctx, struct pipe_resource *pres)
+{
+   if (pres->target == PIPE_BUFFER)
+      invalidate_buffer(zink_context(pctx), zink_resource(pres));
+}
+
+static void
 zink_transfer_copy_bufimage(struct zink_context *ctx,
-                            struct zink_resource *res,
-                            struct zink_resource *staging_res,
-                            struct zink_transfer *trans,
-                            bool buf2img)
+                            struct zink_resource *dst,
+                            struct zink_resource *src,
+                            struct zink_transfer *trans)
+{
+   assert((trans->base.b.usage & (PIPE_MAP_DEPTH_ONLY | PIPE_MAP_STENCIL_ONLY)) !=
+          (PIPE_MAP_DEPTH_ONLY | PIPE_MAP_STENCIL_ONLY));
+
+   bool buf2img = src->base.b.target == PIPE_BUFFER;
+
+   struct pipe_box box = trans->base.b.box;
+   int x = box.x;
+   if (buf2img)
+      box.x = src->obj->offset + trans->offset;
+
+   zink_copy_image_buffer(ctx, NULL, dst, src, trans->base.b.level, buf2img ? x : dst->obj->offset,
+                           box.y, box.z, trans->base.b.level, &box, trans->base.b.usage);
+}
+
+bool
+zink_resource_has_usage(struct zink_resource *res, enum zink_resource_access usage)
+{
+   uint32_t batch_uses = get_resource_usage(res);
+   return batch_uses & usage;
+}
+
+static VkMappedMemoryRange
+init_mem_range(struct zink_screen *screen, struct zink_resource *res, VkDeviceSize offset, VkDeviceSize size)
 {
-   struct zink_batch *batch = zink_batch_no_rp(ctx);
+   assert(res->obj->size);
+   VkDeviceSize align = offset % screen->info.props.limits.nonCoherentAtomSize;
+   offset -= align;
+   size += align;
+   VkDeviceSize align_size = align64(size, screen->info.props.limits.nonCoherentAtomSize);
+   if (res->base.b.width0 > offset &&
+       res->base.b.width0 - offset >= align_size)
+      size = align_size;
+   else
+      size = VK_WHOLE_SIZE;
+   VkMappedMemoryRange range = {
+      VK_STRUCTURE_TYPE_MAPPED_MEMORY_RANGE,
+      NULL,
+      res->obj->mem,
+      offset,
+      size
+   };
+   assert(range.size);
+   return range;
+}
 
-   if (buf2img) {
-      if (res->layout != VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL) {
-         zink_resource_barrier(batch->cmdbuf, res, res->aspect,
-                               VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL);
+bool
+zink_resource_has_curr_read_usage(struct zink_context *ctx, struct zink_resource *res)
+{
+   return zink_batch_usage_matches(&res->obj->reads, ctx->curr_batch);
+}
+
+static uint32_t
+get_most_recent_access(struct zink_resource *res, enum zink_resource_access flags)
+{
+   uint32_t usage[3]; // read, write, failure
+   uint32_t latest = ARRAY_SIZE(usage) - 1;
+   usage[latest] = 0;
+
+   if (flags & ZINK_RESOURCE_ACCESS_READ) {
+      usage[0] = p_atomic_read(&res->obj->reads.usage);
+      if (usage[0] > usage[latest]) {
+         latest = 0;
       }
-   } else {
-      if (res->layout != VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL) {
-         zink_resource_barrier(batch->cmdbuf, res, res->aspect,
-                               VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL);
+   }
+   if (flags & ZINK_RESOURCE_ACCESS_WRITE) {
+      usage[1] = p_atomic_read(&res->obj->writes.usage);
+      if (usage[1] > usage[latest]) {
+         latest = 1;
       }
    }
+   return usage[latest];
+}
 
-   VkBufferImageCopy copyRegion = {};
-   copyRegion.bufferOffset = staging_res->offset;
-   copyRegion.bufferRowLength = 0;
-   copyRegion.bufferImageHeight = 0;
-   copyRegion.imageSubresource.mipLevel = trans->base.level;
-   copyRegion.imageSubresource.layerCount = 1;
-   if (res->base.array_size > 1) {
-      copyRegion.imageSubresource.baseArrayLayer = trans->base.box.z;
-      copyRegion.imageSubresource.layerCount = trans->base.box.depth;
-      copyRegion.imageExtent.depth = 1;
-   } else {
-      copyRegion.imageOffset.z = trans->base.box.z;
-      copyRegion.imageExtent.depth = trans->base.box.depth;
+static void *
+map_resource(struct zink_screen *screen, struct zink_resource *res)
+{
+   simple_mtx_lock(&res->obj->map_mtx);
+   VkResult result = VK_SUCCESS;
+   if (!res->obj->map_count)
+      result = vkMapMemory(screen->dev, res->obj->mem, res->obj->offset,
+                           res->obj->size, 0, &res->obj->map);
+   res->obj->map_count++;
+   simple_mtx_unlock(&res->obj->map_mtx);
+   return result == VK_SUCCESS ? res->obj->map : NULL;
+}
+
+static void
+unmap_resource(struct zink_screen *screen, struct zink_resource *res)
+{
+   simple_mtx_lock(&res->obj->map_mtx);
+   res->obj->map_count--;
+   if (!res->obj->map_count) {
+      res->obj->map = NULL;
+      vkUnmapMemory(screen->dev, res->obj->mem);
    }
-   copyRegion.imageOffset.x = trans->base.box.x;
-   copyRegion.imageOffset.y = trans->base.box.y;
+   simple_mtx_unlock(&res->obj->map_mtx);
+}
 
-   copyRegion.imageExtent.width = trans->base.box.width;
-   copyRegion.imageExtent.height = trans->base.box.height;
+static void *
+buffer_transfer_map(struct zink_context *ctx, struct zink_resource *res, unsigned usage,
+                    const struct pipe_box *box, struct zink_transfer *trans)
+{
+   struct zink_screen *screen = zink_screen(ctx->base.screen);
+   void *ptr = NULL;
+
+   /* See if the buffer range being mapped has never been initialized,
+    * in which case it can be mapped unsynchronized. */
+   if (!(usage & (PIPE_MAP_UNSYNCHRONIZED | TC_TRANSFER_MAP_NO_INFER_UNSYNCHRONIZED)) &&
+       usage & PIPE_MAP_WRITE && !res->base.is_shared &&
+       !util_ranges_intersect(&res->valid_buffer_range, box->x, box->x + box->width)) {
+      usage |= PIPE_MAP_UNSYNCHRONIZED;
+   }
 
-   zink_batch_reference_resource_rw(batch, res, buf2img);
-   zink_batch_reference_resource_rw(batch, staging_res, !buf2img);
+   /* If discarding the entire range, discard the whole resource instead. */
+   if (usage & PIPE_MAP_DISCARD_RANGE && box->x == 0 && box->width == res->base.b.width0) {
+      usage |= PIPE_MAP_DISCARD_WHOLE_RESOURCE;
+   }
 
-   /* we're using u_transfer_helper_deinterleave, which means we'll be getting PIPE_MAP_* usage
-    * to indicate whether to copy either the depth or stencil aspects
-    */
-   unsigned aspects = 0;
-   assert((trans->base.usage & (PIPE_MAP_DEPTH_ONLY | PIPE_MAP_STENCIL_ONLY)) !=
-          (PIPE_MAP_DEPTH_ONLY | PIPE_MAP_STENCIL_ONLY));
-   if (trans->base.usage & PIPE_MAP_DEPTH_ONLY)
-      aspects = VK_IMAGE_ASPECT_DEPTH_BIT;
-   else if (trans->base.usage & PIPE_MAP_STENCIL_ONLY)
-      aspects = VK_IMAGE_ASPECT_STENCIL_BIT;
-   else {
-      aspects = aspect_from_format(res->base.format);
-   }
-   while (aspects) {
-      int aspect = 1 << u_bit_scan(&aspects);
-      copyRegion.imageSubresource.aspectMask = aspect;
-
-      /* this may or may not work with multisampled depth/stencil buffers depending on the driver implementation:
-       *
-       * srcImage must have a sample count equal to VK_SAMPLE_COUNT_1_BIT
-       * - vkCmdCopyImageToBuffer spec
-       *
-       * dstImage must have a sample count equal to VK_SAMPLE_COUNT_1_BIT
-       * - vkCmdCopyBufferToImage spec
+   if (usage & PIPE_MAP_DISCARD_WHOLE_RESOURCE &&
+       !(usage & (PIPE_MAP_UNSYNCHRONIZED | TC_TRANSFER_MAP_NO_INVALIDATE))) {
+      assert(usage & PIPE_MAP_WRITE);
+
+      if (invalidate_buffer(ctx, res)) {
+         /* At this point, the buffer is always idle. */
+         usage |= PIPE_MAP_UNSYNCHRONIZED;
+      } else {
+         /* Fall back to a temporary buffer. */
+         usage |= PIPE_MAP_DISCARD_RANGE;
+      }
+   }
+
+   if ((usage & PIPE_MAP_WRITE) &&
+       (usage & PIPE_MAP_DISCARD_RANGE ||
+        zink_resource_has_usage(res, ZINK_RESOURCE_ACCESS_RW)) &&
+       !(usage & (PIPE_MAP_UNSYNCHRONIZED | PIPE_MAP_PERSISTENT))) {
+
+      /* Check if mapping this buffer would cause waiting for the GPU.
        */
-      if (buf2img)
-         vkCmdCopyBufferToImage(batch->cmdbuf, staging_res->buffer, res->image, res->layout, 1, &copyRegion);
-      else
-         vkCmdCopyImageToBuffer(batch->cmdbuf, res->image, res->layout, staging_res->buffer, 1, &copyRegion);
+
+      uint32_t latest_access = get_most_recent_access(res, ZINK_RESOURCE_ACCESS_RW);
+      if (zink_resource_has_curr_read_usage(ctx, res) ||
+          (latest_access && !zink_check_batch_completion(ctx, latest_access))) {
+         /* Do a wait-free write-only transfer using a temporary buffer. */
+         unsigned offset;
+
+         /* If we are not called from the driver thread, we have
+          * to use the uploader from u_threaded_context, which is
+          * local to the calling thread.
+          */
+         struct u_upload_mgr *mgr;
+         if (usage & TC_TRANSFER_MAP_THREADED_UNSYNC)
+            mgr = ctx->tc->base.stream_uploader;
+         else
+            mgr = ctx->base.stream_uploader;
+         u_upload_alloc(mgr, 0, box->width + box->x,
+                     screen->info.props.limits.minMemoryMapAlignment, &offset,
+                     (struct pipe_resource **)&trans->staging_res, (void **)&ptr);
+         res = zink_resource(trans->staging_res);
+         trans->offset = offset;
+         res->obj->map_count++;
+         res->obj->map = ptr;
+      } else {
+         /* At this point, the buffer is always idle (we checked it above). */
+         usage |= PIPE_MAP_UNSYNCHRONIZED;
+      }
+   } else if ((usage & PIPE_MAP_READ) && !(usage & PIPE_MAP_PERSISTENT)) {
+      assert(!(usage & (TC_TRANSFER_MAP_THREADED_UNSYNC | PIPE_MAP_THREAD_SAFE)));
+      uint32_t latest_write = get_most_recent_access(res, ZINK_RESOURCE_ACCESS_WRITE);
+      if (latest_write) {
+         if (usage & PIPE_MAP_DONTBLOCK) {
+            if (latest_write == ctx->curr_batch ||
+                zink_check_batch_completion(ctx, latest_write))
+               return NULL;
+         } else
+            zink_wait_on_batch(ctx, latest_write);
+      }
    }
 
-   return true;
-}
+   if (!ptr) {
+      ptr = map_resource(screen, res);
+      if (!ptr)
+         return NULL;
+   }
 
-static uint32_t
-get_resource_usage(struct zink_resource *res)
-{
-   uint32_t batch_uses = 0;
-   for (unsigned i = 0; i < 4; i++)
-      batch_uses |= p_atomic_read(&res->batch_uses[i]) << i;
-   return batch_uses;
+   if (!(res->base.b.flags & PIPE_RESOURCE_FLAG_MAP_COHERENT)
+#if defined(MVK_VERSION)
+      // Work around for MoltenVk limitation specifically on coherent memory
+      // MoltenVk returns blank memory ranges when there should be data present
+      // This is a known limitation of MoltenVK.
+      // See https://github.com/KhronosGroup/MoltenVK/blob/master/Docs/MoltenVK_Runtime_UserGuide.md#known-moltenvk-limitations
+
+       || screen->have_moltenvk
+#endif
+      ) {
+      VkDeviceSize size = box->width;
+      VkDeviceSize offset = trans->offset + box->x;
+      VkMappedMemoryRange range = init_mem_range(screen, res, offset, size);
+      if (vkInvalidateMappedMemoryRanges(screen->dev, 1, &range) != VK_SUCCESS) {
+         vkUnmapMemory(screen->dev, res->obj->mem);
+         return NULL;
+      }
+   }
+   trans->base.b.usage = usage;
+   if (usage & PIPE_MAP_WRITE)
+      util_range_add(&res->base.b, &res->valid_buffer_range, box->x, box->x + box->width);
+   return ptr;
 }
 
 static void *
@@ -511,70 +870,46 @@ zink_transfer_map(struct pipe_context *pctx,
    struct zink_context *ctx = zink_context(pctx);
    struct zink_screen *screen = zink_screen(pctx->screen);
    struct zink_resource *res = zink_resource(pres);
-   uint32_t batch_uses = get_resource_usage(res);
 
-   struct zink_transfer *trans = slab_alloc(&ctx->transfer_pool);
+   struct zink_transfer *trans;
+
+   if (usage & PIPE_MAP_THREAD_SAFE)
+      trans = malloc(sizeof(*trans));
+   else if (usage & TC_TRANSFER_MAP_THREADED_UNSYNC)
+      trans = slab_alloc(&ctx->transfer_pool_unsync);
+   else
+      trans = slab_alloc(&ctx->transfer_pool);
    if (!trans)
       return NULL;
 
    memset(trans, 0, sizeof(*trans));
-   pipe_resource_reference(&trans->base.resource, pres);
+   pipe_resource_reference(&trans->base.b.resource, pres);
 
-   trans->base.resource = pres;
-   trans->base.level = level;
-   trans->base.usage = usage;
-   trans->base.box = *box;
+   trans->base.b.resource = pres;
+   trans->base.b.level = level;
+   trans->base.b.usage = usage;
+   trans->base.b.box = *box;
 
-   void *ptr;
+   void *ptr, *base;
    if (pres->target == PIPE_BUFFER) {
-      if (!(usage & PIPE_MAP_UNSYNCHRONIZED)) {
-         if ((usage & PIPE_MAP_READ && batch_uses >= ZINK_RESOURCE_ACCESS_WRITE) ||
-             (usage & PIPE_MAP_WRITE && batch_uses)) {
-            /* need to wait for rendering to finish
-             * TODO: optimize/fix this to be much less obtrusive
-             * mesa/mesa#2966
-             */
-            zink_fence_wait(pctx);
-         }
-      }
-
-
-      VkResult result = vkMapMemory(screen->dev, res->mem, res->offset, res->size, 0, &ptr);
-      if (result != VK_SUCCESS)
-         return NULL;
-
-#if defined(__APPLE__)
-      if (!(usage & PIPE_MAP_DISCARD_WHOLE_RESOURCE)) {
-         // Work around for MoltenVk limitation
-         // MoltenVk returns blank memory ranges when there should be data present
-         // This is a known limitation of MoltenVK.
-         // See https://github.com/KhronosGroup/MoltenVK/blob/master/Docs/MoltenVK_Runtime_UserGuide.md#known-moltenvk-limitations
-         VkMappedMemoryRange range = {
-            VK_STRUCTURE_TYPE_MAPPED_MEMORY_RANGE,
-            NULL,
-            res->mem,
-            res->offset,
-            res->size
-         };
-         result = vkFlushMappedMemoryRanges(screen->dev, 1, &range);
-         if (result != VK_SUCCESS)
-            return NULL;
-      }
-#endif
-
-      trans->base.stride = 0;
-      trans->base.layer_stride = 0;
-      ptr = ((uint8_t *)ptr) + box->x;
+      base = buffer_transfer_map(ctx, res, usage, box, trans);
+      ptr = ((uint8_t *)base) + box->x;
    } else {
-      if (res->optimal_tiling || ((res->base.usage != PIPE_USAGE_STAGING))) {
+      if (usage & PIPE_MAP_WRITE && !(usage & PIPE_MAP_READ))
+         /* this is like a blit, so we can potentially dump some clears or maybe we have to  */
+         zink_fb_clears_apply_or_discard(ctx, pres, zink_rect_from_box(box), false);
+      else if (usage & PIPE_MAP_READ)
+         /* if the map region intersects with any clears then we have to apply them */
+         zink_fb_clears_apply_region(ctx, pres, zink_rect_from_box(box));
+      if (res->optimal_tiling || ((res->base.b.usage != PIPE_USAGE_STAGING))) {
          enum pipe_format format = pres->format;
          if (usage & PIPE_MAP_DEPTH_ONLY)
             format = util_format_get_depth_only(pres->format);
          else if (usage & PIPE_MAP_STENCIL_ONLY)
             format = PIPE_FORMAT_S8_UINT;
-         trans->base.stride = util_format_get_stride(format, box->width);
-         trans->base.layer_stride = util_format_get_2d_size(format,
-                                                            trans->base.stride,
+         trans->base.b.stride = util_format_get_stride(format, box->width);
+         trans->base.b.layer_stride = util_format_get_2d_size(format,
+                                                            trans->base.b.stride,
                                                             box->height);
 
          struct pipe_resource templ = *pres;
@@ -582,7 +917,7 @@ zink_transfer_map(struct pipe_context *pctx,
          templ.usage = PIPE_USAGE_STAGING;
          templ.target = PIPE_BUFFER;
          templ.bind = 0;
-         templ.width0 = trans->base.layer_stride * box->depth;
+         templ.width0 = trans->base.b.layer_stride * box->depth;
          templ.height0 = templ.depth0 = 0;
          templ.last_level = 0;
          templ.array_size = 1;
@@ -595,52 +930,101 @@ zink_transfer_map(struct pipe_context *pctx,
          struct zink_resource *staging_res = zink_resource(trans->staging_res);
 
          if (usage & PIPE_MAP_READ) {
-            struct zink_context *ctx = zink_context(pctx);
-            bool ret = zink_transfer_copy_bufimage(ctx, res,
-                                                   staging_res, trans,
-                                                   false);
-            if (ret == false)
-               return NULL;
-
+            zink_transfer_copy_bufimage(ctx, staging_res, res, trans);
             /* need to wait for rendering to finish */
             zink_fence_wait(pctx);
          }
 
-         VkResult result = vkMapMemory(screen->dev, staging_res->mem,
-                                       staging_res->offset,
-                                       staging_res->size, 0, &ptr);
-         if (result != VK_SUCCESS)
+         ptr = base = map_resource(screen, staging_res);
+         if (!base)
             return NULL;
-
       } else {
          assert(!res->optimal_tiling);
-         if (batch_uses >= ZINK_RESOURCE_ACCESS_WRITE)
-            zink_fence_wait(pctx);
-         VkResult result = vkMapMemory(screen->dev, res->mem, res->offset, res->size, 0, &ptr);
-         if (result != VK_SUCCESS)
+         base = map_resource(screen, res);
+         if (!base)
             return NULL;
+         /* special case compute reads since they aren't handled by zink_fence_wait() */
+         if (zink_resource_has_usage(res, ZINK_RESOURCE_ACCESS_READ))
+            resource_sync_reads(ctx, res);
+         if (zink_resource_has_usage(res, ZINK_RESOURCE_ACCESS_RW)) {
+            if (usage & PIPE_MAP_READ)
+               resource_sync_writes_from_batch_id(ctx, res);
+            else
+               zink_fence_wait(pctx);
+         }
          VkImageSubresource isr = {
             res->aspect,
             level,
             0
          };
          VkSubresourceLayout srl;
-         vkGetImageSubresourceLayout(screen->dev, res->image, &isr, &srl);
-         trans->base.stride = srl.rowPitch;
-         trans->base.layer_stride = srl.arrayPitch;
-         const struct util_format_description *desc = util_format_description(res->base.format);
+         vkGetImageSubresourceLayout(screen->dev, res->obj->image, &isr, &srl);
+         trans->base.b.stride = srl.rowPitch;
+         trans->base.b.layer_stride = srl.arrayPitch;
+         trans->offset = srl.offset;
+         trans->depthPitch = srl.depthPitch;
+         const struct util_format_description *desc = util_format_description(res->base.b.format);
          unsigned offset = srl.offset +
                            box->z * srl.depthPitch +
                            (box->y / desc->block.height) * srl.rowPitch +
                            (box->x / desc->block.width) * (desc->block.bits / 8);
-         ptr = ((uint8_t *)ptr) + offset;
+         if (!(res->base.b.flags & PIPE_RESOURCE_FLAG_MAP_COHERENT)) {
+            VkDeviceSize size = box->width * box->height * desc->block.bits / 8;
+            VkMappedMemoryRange range = init_mem_range(screen, res, offset, size);
+            vkFlushMappedMemoryRanges(screen->dev, 1, &range);
+         }
+         ptr = ((uint8_t *)base) + offset;
       }
    }
+   if ((usage & PIPE_MAP_PERSISTENT) && !(usage & PIPE_MAP_COHERENT)) {
+      res->persistent_maps++;
+      ctx->num_persistent_maps++;
+   }
 
-   *transfer = &trans->base;
+   *transfer = &trans->base.b;
    return ptr;
 }
 
+static void
+zink_transfer_flush_region(struct pipe_context *pctx,
+                           struct pipe_transfer *ptrans,
+                           const struct pipe_box *box)
+{
+   struct zink_context *ctx = zink_context(pctx);
+   struct zink_resource *res = zink_resource(ptrans->resource);
+   struct zink_transfer *trans = (struct zink_transfer *)ptrans;
+
+   if (trans->base.b.usage & PIPE_MAP_WRITE) {
+      struct zink_screen *screen = zink_screen(pctx->screen);
+      struct zink_resource *m = trans->staging_res ? zink_resource(trans->staging_res) :
+                                                     res;
+      VkDeviceSize size, offset;
+      if (res->obj->is_buffer) {
+         size = box->width;
+         offset = trans->offset + box->x;
+      } else {
+         const struct util_format_description *desc = util_format_description(res->base.b.format);
+         size = box->width * box->height * desc->block.bits / 8;
+         offset = trans->offset +
+                  box->z * trans->depthPitch +
+                  (box->y / desc->block.height) * trans->base.b.stride +
+                  (box->x / desc->block.width) * (desc->block.bits / 8);
+      }
+      if (!(res->base.b.flags & PIPE_RESOURCE_FLAG_MAP_COHERENT)) {
+         VkMappedMemoryRange range = init_mem_range(screen, m, offset, size);
+         vkFlushMappedMemoryRanges(screen->dev, 1, &range);
+      }
+      if (trans->staging_res) {
+         struct zink_resource *staging_res = zink_resource(trans->staging_res);
+
+         if (ptrans->resource->target == PIPE_BUFFER)
+            zink_copy_buffer(ctx, NULL, res, staging_res, box->x, box->x + trans->offset + m->obj->offset, box->width);
+         else
+            zink_transfer_copy_bufimage(ctx, res, staging_res, trans);
+      }
+   }
+}
+
 static void
 zink_transfer_unmap(struct pipe_context *pctx,
                     struct pipe_transfer *ptrans)
@@ -649,24 +1033,54 @@ zink_transfer_unmap(struct pipe_context *pctx,
    struct zink_screen *screen = zink_screen(pctx->screen);
    struct zink_resource *res = zink_resource(ptrans->resource);
    struct zink_transfer *trans = (struct zink_transfer *)ptrans;
-   if (trans->staging_res) {
-      struct zink_resource *staging_res = zink_resource(trans->staging_res);
-      vkUnmapMemory(screen->dev, staging_res->mem);
 
-      if (trans->base.usage & PIPE_MAP_WRITE) {
-         struct zink_context *ctx = zink_context(pctx);
-         uint32_t batch_uses = get_resource_usage(res);
-         if (batch_uses >= ZINK_RESOURCE_ACCESS_WRITE)
-            zink_fence_wait(pctx);
-         zink_transfer_copy_bufimage(ctx, res, staging_res, trans, true);
-      }
+   if (!(trans->base.b.usage & (PIPE_MAP_FLUSH_EXPLICIT | PIPE_MAP_COHERENT))) {
+      zink_transfer_flush_region(pctx, ptrans, &ptrans->box);
+   }
 
-      pipe_resource_reference(&trans->staging_res, NULL);
+   if (trans->staging_res) {
+      unmap_resource(screen, zink_resource(trans->staging_res));
    } else
-      vkUnmapMemory(screen->dev, res->mem);
+      unmap_resource(screen, res);
+   if ((trans->base.b.usage & PIPE_MAP_PERSISTENT) && !(trans->base.b.usage & PIPE_MAP_COHERENT)) {
+      res->persistent_maps--;
+      ctx->num_persistent_maps--;
+   }
+
+   if (trans->staging_res)
+      pipe_resource_reference(&trans->staging_res, NULL);
+   pipe_resource_reference(&trans->base.b.resource, NULL);
+
+   if (trans->base.b.usage & PIPE_MAP_THREAD_SAFE) {
+      free(trans);
+   } else {
+      /* Don't use pool_transfers_unsync. We are always in the driver
+       * thread. Freeing an object into a different pool is allowed.
+       */
+      slab_free(&ctx->transfer_pool, ptrans);
+   }
+}
+
+static void
+zink_buffer_subdata(struct pipe_context *ctx, struct pipe_resource *buffer,
+                    unsigned usage, unsigned offset, unsigned size, const void *data)
+{
+   struct pipe_transfer *transfer = NULL;
+   struct pipe_box box;
+   uint8_t *map = NULL;
+
+   usage |= PIPE_MAP_WRITE;
+
+   if (!(usage & PIPE_MAP_DIRECTLY))
+      usage |= PIPE_MAP_DISCARD_RANGE;
+
+   u_box_1d(offset, size, &box);
+   map = zink_transfer_map(ctx, buffer, 0, usage, &box, &transfer);
+   if (!map)
+      return;
 
-   pipe_resource_reference(&trans->base.resource, NULL);
-   slab_free(&ctx->transfer_pool, ptrans);
+   memcpy(map, data, size);
+   zink_transfer_unmap(ctx, transfer);
 }
 
 static struct pipe_resource *
@@ -682,8 +1096,43 @@ zink_resource_get_separate_stencil(struct pipe_resource *pres)
 
 }
 
+bool
+zink_resource_object_init_storage(struct zink_context *ctx, struct zink_resource *res)
+{
+   struct zink_screen *screen = zink_screen(ctx->base.screen);
+   /* base resource already has the cap */
+   if (res->base.b.bind & PIPE_BIND_SHADER_IMAGE)
+      return true;
+   if (res->obj->is_buffer) {
+      if (res->obj->sbuffer)
+         return true;
+      VkBufferCreateInfo bci = create_bci(screen, &res->base.b, res->base.b.bind | PIPE_BIND_SHADER_IMAGE);
+      VkBuffer buffer;
+      if (vkCreateBuffer(screen->dev, &bci, NULL, &buffer) != VK_SUCCESS)
+         return false;
+      vkBindBufferMemory(screen->dev, buffer, res->obj->mem, res->obj->offset);
+      res->obj->sbuffer = res->obj->buffer;
+      res->obj->buffer = buffer;
+   } else {
+      if (res->obj->simage)
+         return true;
+      VkImageCreateInfo ici = create_ici(screen, &res->base.b, res->base.b.bind | PIPE_BIND_SHADER_IMAGE);
+      ici.initialLayout = VK_IMAGE_LAYOUT_PREINITIALIZED;
+      VkImage image;
+      if (vkCreateImage(screen->dev, &ici, NULL, &image) != VK_SUCCESS)
+         return false;
+      vkBindImageMemory(screen->dev, image, res->obj->mem, res->obj->offset);
+      res->obj->simage = res->obj->image;
+      res->obj->image = image;
+   }
+
+   zink_resource_rebind(ctx, res);
+
+   return true;
+}
+
 void
-zink_resource_setup_transfer_layouts(struct zink_batch *batch, struct zink_resource *src, struct zink_resource *dst)
+zink_resource_setup_transfer_layouts(struct zink_context *ctx, struct zink_resource *src, struct zink_resource *dst)
 {
    if (src == dst) {
       /* The Vulkan 1.1 specification says the following about valid usage
@@ -702,16 +1151,20 @@ zink_resource_setup_transfer_layouts(struct zink_batch *batch, struct zink_resou
        * VK_IMAGE_LAYOUT_GENERAL. And since this isn't a present-related
        * operation, VK_IMAGE_LAYOUT_GENERAL seems most appropriate.
        */
-      zink_resource_barrier(batch->cmdbuf, src, src->aspect,
-                            VK_IMAGE_LAYOUT_GENERAL);
+      zink_resource_image_barrier(ctx, NULL, src,
+                                  VK_IMAGE_LAYOUT_GENERAL,
+                                  VK_ACCESS_TRANSFER_READ_BIT | VK_ACCESS_TRANSFER_WRITE_BIT,
+                                  VK_PIPELINE_STAGE_TRANSFER_BIT);
    } else {
-      if (src->layout != VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL)
-         zink_resource_barrier(batch->cmdbuf, src, src->aspect,
-                               VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL);
-
-      if (dst->layout != VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL)
-         zink_resource_barrier(batch->cmdbuf, dst, dst->aspect,
-                               VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL);
+      zink_resource_image_barrier(ctx, NULL, src,
+                                  VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL,
+                                  VK_ACCESS_TRANSFER_READ_BIT,
+                                  VK_PIPELINE_STAGE_TRANSFER_BIT);
+
+      zink_resource_image_barrier(ctx, NULL, dst,
+                                  VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL,
+                                  VK_ACCESS_TRANSFER_WRITE_BIT,
+                                  VK_PIPELINE_STAGE_TRANSFER_BIT);
    }
 }
 
@@ -755,23 +1208,27 @@ static const struct u_transfer_vtbl transfer_vtbl = {
    .resource_destroy      = zink_resource_destroy,
    .transfer_map          = zink_transfer_map,
    .transfer_unmap        = zink_transfer_unmap,
-   .transfer_flush_region = u_default_transfer_flush_region,
+   .transfer_flush_region = zink_transfer_flush_region,
    .get_internal_format   = zink_resource_get_internal_format,
    .set_stencil           = zink_resource_set_separate_stencil,
    .get_stencil           = zink_resource_get_separate_stencil,
 };
 
-void
+bool
 zink_screen_resource_init(struct pipe_screen *pscreen)
 {
+   struct zink_screen *screen = zink_screen(pscreen);
    pscreen->resource_create = zink_resource_create;
    pscreen->resource_destroy = zink_resource_destroy;
    pscreen->transfer_helper = u_transfer_helper_create(&transfer_vtbl, true, true, false, false);
 
-   if (zink_screen(pscreen)->info.have_KHR_external_memory_fd) {
+   if (screen->info.have_KHR_external_memory_fd) {
       pscreen->resource_get_handle = zink_resource_get_handle;
       pscreen->resource_from_handle = zink_resource_from_handle;
    }
+   simple_mtx_init(&screen->mem_cache_mtx, mtx_plain);
+   screen->resource_mem_cache = _mesa_hash_table_create(NULL, mem_hash, mem_equals);
+   return !!screen->resource_mem_cache;
 }
 
 void
@@ -781,6 +1238,7 @@ zink_context_resource_init(struct pipe_context *pctx)
    pctx->transfer_unmap = u_transfer_helper_deinterleave_transfer_unmap;
 
    pctx->transfer_flush_region = u_transfer_helper_transfer_flush_region;
-   pctx->buffer_subdata = u_default_buffer_subdata;
+   pctx->buffer_subdata = zink_buffer_subdata;
    pctx->texture_subdata = u_default_texture_subdata;
+   pctx->invalidate_resource = zink_resource_invalidate;
 }
diff --git a/src/gallium/drivers/zink/zink_resource.h b/src/gallium/drivers/zink/zink_resource.h
index 572a3ecc363..b8d7723e78b 100644
--- a/src/gallium/drivers/zink/zink_resource.h
+++ b/src/gallium/drivers/zink/zink_resource.h
@@ -27,43 +27,92 @@
 struct pipe_screen;
 struct sw_displaytarget;
 struct zink_batch;
+struct zink_context;
 
+#include "util/simple_mtx.h"
 #include "util/u_transfer.h"
+#include "util/u_range.h"
+#include "util/u_dynarray.h"
+#include "util/u_threaded_context.h"
+
+#include "zink_batch.h"
+#include "zink_descriptors.h"
 
 #include <vulkan/vulkan.h>
 
-#define ZINK_RESOURCE_ACCESS_READ 1
-#define ZINK_RESOURCE_ACCESS_WRITE 16
+enum zink_resource_access {
+   ZINK_RESOURCE_ACCESS_READ = 1,
+   ZINK_RESOURCE_ACCESS_WRITE = 32,
+   ZINK_RESOURCE_ACCESS_RW = ZINK_RESOURCE_ACCESS_READ | ZINK_RESOURCE_ACCESS_WRITE,
+};
+
+struct mem_key {
+   VkMemoryRequirements reqs;
+   VkMemoryPropertyFlags flags;
+};
+
+struct zink_resource_object {
+   struct pipe_reference reference;
+   union {
+      VkBuffer buffer;
+      VkImage image;
+   };
+
+   union {
+      VkBuffer sbuffer;
+      VkImage simage;
+   };
+   bool storage_init; //layout was set for image
+
+   VkDeviceMemory mem;
+   uint32_t mem_hash;
+   struct mem_key mkey;
+   VkDeviceSize offset, size;
+   struct zink_descriptor_refs desc_set_refs;
+
+   struct zink_batch_usage reads;
+   struct zink_batch_usage writes;
+   simple_mtx_t map_mtx;
+   unsigned map_count;
+   void *map;
+   bool is_buffer;
+};
 
 struct zink_resource {
-   struct pipe_resource base;
+   struct threaded_resource base;
 
    enum pipe_format internal_format:16;
 
+   VkPipelineStageFlagBits access_stage;
+   VkAccessFlags access;
+
+   struct zink_resource_object *obj;
    union {
-      VkBuffer buffer;
+      struct util_range valid_buffer_range;
       struct {
          VkFormat format;
-         VkImage image;
          VkImageLayout layout;
          VkImageAspectFlags aspect;
          bool optimal_tiling;
+         uint16_t image_bind_count[2]; //gfx, compute
       };
    };
-   VkDeviceMemory mem;
-   VkDeviceSize offset, size;
+   uint16_t write_bind_count[2]; //gfx, compute
+   uint16_t bind_count[2]; //gfx, compute
 
    struct sw_displaytarget *dt;
    unsigned dt_stride;
+   unsigned persistent_maps; //if nonzero, requires vkFlushMappedMemoryRanges during batch use
 
-   /* this has to be atomic for fence access, so we can't use a bitmask and make everything neat */
-   uint8_t batch_uses[4];
-   bool needs_xfb_barrier;
+   uint32_t bind_history; // enum zink_descriptor_type bitmask
+   uint32_t bind_stages;
 };
 
 struct zink_transfer {
-   struct pipe_transfer base;
+   struct threaded_transfer base;
    struct pipe_resource *staging_res;
+   unsigned offset;
+   unsigned depthPitch;
 };
 
 static inline struct zink_resource *
@@ -72,7 +121,7 @@ zink_resource(struct pipe_resource *r)
    return (struct zink_resource *)r;
 }
 
-void
+bool
 zink_screen_resource_init(struct pipe_screen *pscreen);
 
 void
@@ -84,5 +133,33 @@ zink_get_depth_stencil_resources(struct pipe_resource *res,
                                  struct zink_resource **out_s);
 
 void
-zink_resource_setup_transfer_layouts(struct zink_batch *batch, struct zink_resource *src, struct zink_resource *dst);
+zink_resource_setup_transfer_layouts(struct zink_context *ctx, struct zink_resource *src, struct zink_resource *dst);
+
+bool
+zink_resource_has_usage(struct zink_resource *res, enum zink_resource_access usage);
+
+bool
+zink_resource_has_curr_read_usage(struct zink_context *ctx, struct zink_resource *res);
+
+void
+zink_destroy_resource_object(struct zink_screen *screen, struct zink_resource_object *resource_object);
+
+void
+debug_describe_zink_resource_object(char *buf, const struct zink_resource_object *ptr);
+
+static inline void
+zink_resource_object_reference(struct zink_screen *screen,
+                             struct zink_resource_object **dst,
+                             struct zink_resource_object *src)
+{
+   struct zink_resource_object *old_dst = dst ? *dst : NULL;
+
+   if (pipe_reference_described(old_dst ? &old_dst->reference : NULL, &src->reference,
+                                (debug_reference_descriptor)debug_describe_zink_resource_object))
+      zink_destroy_resource_object(screen, old_dst);
+   if (dst) *dst = src;
+}
+
+bool
+zink_resource_object_init_storage(struct zink_context *ctx, struct zink_resource *res);
 #endif
diff --git a/src/gallium/drivers/zink/zink_screen.c b/src/gallium/drivers/zink/zink_screen.c
index eb2efc21257..3225feeb54e 100644
--- a/src/gallium/drivers/zink/zink_screen.c
+++ b/src/gallium/drivers/zink/zink_screen.c
@@ -26,6 +26,7 @@
 #include "zink_compiler.h"
 #include "zink_context.h"
 #include "zink_device_info.h"
+#include "zink_descriptors.h"
 #include "zink_fence.h"
 #include "zink_format.h"
 #include "zink_instance.h"
@@ -33,13 +34,18 @@
 #include "zink_resource.h"
 
 #include "os/os_process.h"
+#include "util/os_misc.h"
 #include "util/u_debug.h"
 #include "util/format/u_format.h"
+#include "util/hash_table.h"
 #include "util/u_math.h"
 #include "util/u_memory.h"
 #include "util/u_screen.h"
 #include "util/u_string.h"
 #include "util/u_transfer_helper.h"
+#include "util/xmlconfig.h"
+
+#include "util/u_cpu_detect.h"
 
 #include "frontend/sw_winsys.h"
 
@@ -93,6 +99,105 @@ get_video_mem(struct zink_screen *screen)
    return (int)(size >> 20);
 }
 
+static void
+disk_cache_init(struct zink_screen *screen)
+{
+#ifdef ENABLE_SHADER_CACHE
+   static char buf[1000];
+   snprintf(buf, sizeof(buf), "zink_%x04x", screen->info.props.vendorID);
+
+   screen->disk_cache = disk_cache_create(buf, screen->info.props.deviceName, 0);
+   if (screen->disk_cache)
+      disk_cache_compute_key(screen->disk_cache, buf, strlen(buf), screen->disk_cache_key);
+#endif
+}
+
+void
+zink_screen_update_pipeline_cache(struct zink_screen *screen)
+{
+   size_t size = 0;
+
+   if (!screen->disk_cache)
+      return;
+   if (vkGetPipelineCacheData(screen->dev, screen->pipeline_cache, &size, NULL) != VK_SUCCESS)
+      return;
+   if (screen->pipeline_cache_size == size)
+      return;
+   void *data = malloc(size);
+   if (!data)
+      return;
+   if (vkGetPipelineCacheData(screen->dev, screen->pipeline_cache, &size, data) == VK_SUCCESS) {
+      screen->pipeline_cache_size = size;
+      disk_cache_put(screen->disk_cache, screen->disk_cache_key, data, size, NULL);
+   }
+   free(data);
+}
+
+static int
+zink_get_compute_param(struct pipe_screen *pscreen, enum pipe_shader_ir ir_type,
+                       enum pipe_compute_cap param, void *ret)
+{
+   struct zink_screen *screen = zink_screen(pscreen);
+#define RET(x) do {                  \
+   if (ret)                          \
+      memcpy(ret, x, sizeof(x));     \
+   return sizeof(x);                 \
+} while (0)
+
+   switch (param) {
+   case PIPE_COMPUTE_CAP_ADDRESS_BITS:
+      RET((uint32_t []){ 64 });
+
+   case PIPE_COMPUTE_CAP_IR_TARGET:
+      if (ret)
+         strcpy(ret, "nir");
+      return 4;
+
+   case PIPE_COMPUTE_CAP_GRID_DIMENSION:
+      RET((uint64_t []) { 3 });
+
+   case PIPE_COMPUTE_CAP_MAX_GRID_SIZE:
+      RET(((uint64_t []) { screen->info.props.limits.maxComputeWorkGroupCount[0],
+                           screen->info.props.limits.maxComputeWorkGroupCount[1],
+                           screen->info.props.limits.maxComputeWorkGroupCount[2] }));
+
+   case PIPE_COMPUTE_CAP_MAX_BLOCK_SIZE:
+      /* MaxComputeWorkGroupSize[0..2] */
+      RET(((uint64_t []) {screen->info.props.limits.maxComputeWorkGroupSize[0],
+                          screen->info.props.limits.maxComputeWorkGroupSize[1],
+                          screen->info.props.limits.maxComputeWorkGroupSize[2]}));
+
+   case PIPE_COMPUTE_CAP_MAX_THREADS_PER_BLOCK:
+   case PIPE_COMPUTE_CAP_MAX_VARIABLE_THREADS_PER_BLOCK:
+      RET((uint64_t []) { screen->info.props.limits.maxComputeWorkGroupInvocations });
+
+   case PIPE_COMPUTE_CAP_MAX_PRIVATE_SIZE:
+   case PIPE_COMPUTE_CAP_MAX_LOCAL_SIZE:
+      RET((uint64_t []) { screen->info.props.limits.maxComputeSharedMemorySize });
+
+   case PIPE_COMPUTE_CAP_IMAGES_SUPPORTED:
+      RET((uint32_t []) { 1 });
+
+   case PIPE_COMPUTE_CAP_SUBGROUP_SIZE:
+      RET((uint32_t []) { screen->info.props11.subgroupSize });
+
+   case PIPE_COMPUTE_CAP_MAX_MEM_ALLOC_SIZE:
+   case PIPE_COMPUTE_CAP_MAX_GLOBAL_SIZE:
+      RET((uint64_t []) { 1 << 30 }); /* does vulkan even expose this? */
+
+   case PIPE_COMPUTE_CAP_MAX_CLOCK_FREQUENCY:
+      RET((uint32_t []) { 400 }); /* does vulkan even expose this? */
+
+   case PIPE_COMPUTE_CAP_MAX_COMPUTE_UNITS:
+      RET((uint32_t []) { 8 }); /* does vulkan even expose this? */
+   case PIPE_COMPUTE_CAP_MAX_INPUT_SIZE:
+      RET((uint64_t []) { 1024 }); /* does vulkan even expose this? */
+
+   default:
+      unreachable("unknown compute param");
+   }
+}
+
 static int
 zink_get_param(struct pipe_screen *pscreen, enum pipe_cap param)
 {
@@ -104,7 +209,32 @@ zink_get_param(struct pipe_screen *pscreen, enum pipe_cap param)
    case PIPE_CAP_DRAW_INDIRECT:
    case PIPE_CAP_TEXTURE_QUERY_LOD:
    case PIPE_CAP_GLSL_TESS_LEVELS_AS_INPUTS:
+   case PIPE_CAP_COPY_BETWEEN_COMPRESSED_AND_PLAIN_FORMATS:
+   case PIPE_CAP_FORCE_PERSAMPLE_INTERP:
+   case PIPE_CAP_FRAMEBUFFER_NO_ATTACHMENT:
+   case PIPE_CAP_ROBUST_BUFFER_ACCESS_BEHAVIOR:
+   case PIPE_CAP_BUFFER_MAP_PERSISTENT_COHERENT:
    case PIPE_CAP_CLEAR_TEXTURE:
+   case PIPE_CAP_TGSI_ARRAY_COMPONENTS:
+   case PIPE_CAP_QUERY_BUFFER_OBJECT:
+   case PIPE_CAP_TEXTURE_MIRROR_CLAMP_TO_EDGE:
+   case PIPE_CAP_CONDITIONAL_RENDER_INVERTED:
+   case PIPE_CAP_CLIP_HALFZ:
+   case PIPE_CAP_TGSI_TXQS:
+   case PIPE_CAP_TEXTURE_BARRIER:
+   case PIPE_CAP_TGSI_VOTE:
+   case PIPE_CAP_DRAW_PARAMETERS:
+   case PIPE_CAP_POLYGON_OFFSET_CLAMP:
+   case PIPE_CAP_QUERY_SO_OVERFLOW:
+   case PIPE_CAP_ANISOTROPIC_FILTER:
+   case PIPE_CAP_QUERY_PIPELINE_STATISTICS:
+   case PIPE_CAP_QUERY_PIPELINE_STATISTICS_SINGLE:
+   case PIPE_CAP_GL_SPIRV:
+   case PIPE_CAP_TEXTURE_HALF_FLOAT_LINEAR:
+   case PIPE_CAP_CLEAR_SCISSORED:
+   case PIPE_CAP_INVALIDATE_BUFFER:
+   case PIPE_CAP_NIR_ATOMICS_AS_DEREF:
+   case PIPE_CAP_PREFER_REAL_BUFFER_IN_CONSTBUF0:
       return 1;
 
    case PIPE_CAP_MULTI_DRAW_INDIRECT:
@@ -113,11 +243,15 @@ zink_get_param(struct pipe_screen *pscreen, enum pipe_cap param)
              screen->vk_CmdDrawIndexedIndirectCount;
 
    case PIPE_CAP_START_INSTANCE:
-      return screen->info.feats11.shaderDrawParameters;
+      return screen->loader_version >= VK_MAKE_VERSION(1,1,0) &&
+             screen->info.feats11.shaderDrawParameters;
 
    case PIPE_CAP_VERTEX_ELEMENT_INSTANCE_DIVISOR:
       return screen->info.have_EXT_vertex_attribute_divisor;
 
+   case PIPE_CAP_BLEND_EQUATION_ADVANCED:
+      return screen->info.have_EXT_blend_operation_advanced;
+
    case PIPE_CAP_MAX_VERTEX_STREAMS:
       return screen->info.tf_props.maxTransformFeedbackStreams;
 
@@ -165,6 +299,7 @@ zink_get_param(struct pipe_screen *pscreen, enum pipe_cap param)
 
    case PIPE_CAP_INDEP_BLEND_ENABLE:
    case PIPE_CAP_INDEP_BLEND_FUNC:
+   case PIPE_CAP_NO_DITHERING:
       return 1;
 
    case PIPE_CAP_MAX_STREAM_OUTPUT_BUFFERS:
@@ -192,18 +327,19 @@ zink_get_param(struct pipe_screen *pscreen, enum pipe_cap param)
    case PIPE_CAP_VERTEX_COLOR_UNCLAMPED:
       return 1;
 
+   case PIPE_CAP_SHADER_STENCIL_EXPORT:
+      return screen->info.have_EXT_shader_stencil_export;
+
    case PIPE_CAP_CONDITIONAL_RENDER:
      return screen->info.have_EXT_conditional_rendering;
 
    case PIPE_CAP_GLSL_FEATURE_LEVEL_COMPATIBILITY:
       return 130;
    case PIPE_CAP_GLSL_FEATURE_LEVEL:
-      return 410;
+      return 460;
 
-#if 0 /* TODO: Enable me */
    case PIPE_CAP_COMPUTE:
-      return 1;
-#endif
+      return screen->compute_queue != UINT_MAX;
 
    case PIPE_CAP_CONSTANT_BUFFER_OFFSET_ALIGNMENT:
       return screen->info.props.limits.minUniformBufferOffsetAlignment;
@@ -226,7 +362,7 @@ zink_get_param(struct pipe_screen *pscreen, enum pipe_cap param)
       return screen->info.props.limits.minTexelBufferOffsetAlignment;
 
    case PIPE_CAP_PREFER_BLIT_BASED_TEXTURE_TRANSFER:
-      return 0; /* unsure */
+      return 1;
 
    case PIPE_CAP_MAX_TEXTURE_BUFFER_SIZE:
       return screen->info.props.limits.maxTexelBufferElements;
@@ -237,6 +373,9 @@ zink_get_param(struct pipe_screen *pscreen, enum pipe_cap param)
    case PIPE_CAP_MAX_VIEWPORTS:
       return screen->info.props.limits.maxViewports;
 
+   case PIPE_CAP_IMAGE_LOAD_FORMATTED:
+      return screen->info.feats.features.shaderStorageImageExtendedFormats;
+
    case PIPE_CAP_MIXED_FRAMEBUFFER_SIZES:
       return 1;
 
@@ -271,15 +410,13 @@ zink_get_param(struct pipe_screen *pscreen, enum pipe_cap param)
    case PIPE_CAP_MAX_VERTEX_ATTRIB_STRIDE:
       return screen->info.props.limits.maxVertexInputBindingStride;
 
-#if 0 /* TODO: Enable me */
    case PIPE_CAP_SAMPLER_VIEW_TARGET:
+   case PIPE_CAP_EMULATE_ARGB:
       return 1;
-#endif
 
-#if 0 /* TODO: Enable me */
-   case PIPE_CAP_CLIP_HALFZ:
-      return 1;
-#endif
+   case PIPE_CAP_TGSI_VS_LAYER_VIEWPORT:
+      return screen->info.feats12.shaderOutputLayer &&
+             screen->info.feats12.shaderOutputViewportIndex;
 
 #if 0 /* TODO: Enable me */
    case PIPE_CAP_TEXTURE_FLOAT_LINEAR:
@@ -290,10 +427,8 @@ zink_get_param(struct pipe_screen *pscreen, enum pipe_cap param)
    case PIPE_CAP_SHAREABLE_SHADERS:
       return 1;
 
-#if 0 /* TODO: Enable me. Enables GL_ARB_shader_storage_buffer_object */
    case PIPE_CAP_SHADER_BUFFER_OFFSET_ALIGNMENT:
       return screen->info.props.limits.minStorageBufferOffsetAlignment;
-#endif
 
    case PIPE_CAP_PCI_GROUP:
    case PIPE_CAP_PCI_BUS:
@@ -327,9 +462,6 @@ zink_get_param(struct pipe_screen *pscreen, enum pipe_cap param)
    case PIPE_CAP_TGSI_FS_COORD_PIXEL_CENTER_INTEGER:
       return 0;
 
-   case PIPE_CAP_BUFFER_MAP_PERSISTENT_COHERENT:
-      return 0;
-
    case PIPE_CAP_NIR_COMPACT_ARRAYS:
       return 1;
 
@@ -406,7 +538,8 @@ zink_get_shader_param(struct pipe_screen *pscreen,
       case PIPE_SHADER_TESS_CTRL:
       case PIPE_SHADER_TESS_EVAL:
          if (screen->info.feats.features.tessellationShader &&
-             screen->instance_info.have_KHR_maintenance2)
+             (screen->instance_info.have_KHR_maintenance2 ||
+              VK_MAKE_VERSION(1,1,0) <= screen->loader_version))
             return INT_MAX;
          break;
 
@@ -415,6 +548,10 @@ zink_get_shader_param(struct pipe_screen *pscreen,
             return INT_MAX;
          break;
 
+      case PIPE_SHADER_COMPUTE:
+         if (screen->compute_queue != UINT_MAX)
+            return INT_MAX;
+         break;
       default:
          break;
       }
@@ -444,8 +581,8 @@ zink_get_shader_param(struct pipe_screen *pscreen,
          max = screen->info.props.limits.maxGeometryInputComponents;
          break;
       case PIPE_SHADER_FRAGMENT:
-         max = screen->info.props.limits.maxFragmentInputComponents / 4;
-         break;
+         /* vulkan drivers may exclude certain builtins here, but that doesn't affect us */
+         return MIN2(128 / 4, PIPE_MAX_SHADER_INPUTS);
       default:
          return 0; /* unsupported stage */
       }
@@ -527,19 +664,19 @@ zink_get_shader_param(struct pipe_screen *pscreen,
       return 32; /* arbitrary */
 
    case PIPE_SHADER_CAP_MAX_SHADER_BUFFERS:
-      /* TODO: this limitation is dumb, and will need some fixes in mesa */
       return MIN2(screen->info.props.limits.maxPerStageDescriptorStorageBuffers, PIPE_MAX_SHADER_BUFFERS);
 
    case PIPE_SHADER_CAP_SUPPORTED_IRS:
-      return (1 << PIPE_SHADER_IR_NIR) | (1 << PIPE_SHADER_IR_TGSI);
-
-   case PIPE_SHADER_CAP_MAX_SHADER_IMAGES:
-#if 0 /* TODO: needs compiler support */
-      return MIN2(screen->info.props.limits.maxPerStageDescriptorStorageImages,
-                  PIPE_MAX_SHADER_IMAGES);
-#else
+      return (1 << PIPE_SHADER_IR_NIR) | (1 << PIPE_SHADER_IR_TGSI) | (1 << PIPE_SHADER_IR_NIR_SERIALIZED);
+
+   case PIPE_SHADER_CAP_MAX_SHADER_IMAGES: {
+      if (screen->info.feats.features.shaderStorageImageExtendedFormats ||
+          (screen->info.feats.features.shaderStorageImageWriteWithoutFormat &&
+           screen->info.feats.features.shaderStorageImageReadWithoutFormat))
+         return MIN2(screen->info.props.limits.maxPerStageDescriptorStorageImages,
+                     PIPE_MAX_SHADER_IMAGES);
       return 0;
-#endif
+   }
 
    case PIPE_SHADER_CAP_LOWER_IF_THRESHOLD:
    case PIPE_SHADER_CAP_TGSI_SKIP_MERGE_REGISTERS:
@@ -627,15 +764,30 @@ zink_is_format_supported(struct pipe_screen *pscreen,
              !(screen->info.props.limits.sampledImageColorSampleCounts & sample_mask))
             return false;
       }
+      if (bind & PIPE_BIND_SHADER_IMAGE) {
+          if (!screen->info.feats.features.shaderStorageImageMultisample)
+             return false;
+      }
    }
 
-   VkFormatProperties props;
-   vkGetPhysicalDeviceFormatProperties(screen->pdev, vkformat, &props);
+   VkFormatProperties props = screen->format_props[format];
 
    if (target == PIPE_BUFFER) {
       if (bind & PIPE_BIND_VERTEX_BUFFER &&
           !(props.bufferFeatures & VK_FORMAT_FEATURE_VERTEX_BUFFER_BIT))
          return false;
+
+      if (bind & PIPE_BIND_SAMPLER_VIEW) {
+         if (!(props.bufferFeatures & VK_FORMAT_FEATURE_UNIFORM_TEXEL_BUFFER_BIT))
+            return false;
+         /* we can't swizzle these, so we can't sample from them either */
+         if (util_format_is_argb(format) || util_format_is_abgr(format))
+            return false;
+      }
+
+      if (bind & PIPE_BIND_SHADER_IMAGE &&
+          !(props.bufferFeatures & VK_FORMAT_FEATURE_STORAGE_TEXEL_BUFFER_BIT))
+         return false;
    } else {
       /* all other targets are texture-targets */
       if (bind & PIPE_BIND_RENDER_TARGET &&
@@ -647,12 +799,23 @@ zink_is_format_supported(struct pipe_screen *pscreen,
         return false;
 
       if (bind & PIPE_BIND_SAMPLER_VIEW &&
-          !(props.optimalTilingFeatures & VK_FORMAT_FEATURE_SAMPLED_IMAGE_BIT))
-         return false;
+         !(props.optimalTilingFeatures & VK_FORMAT_FEATURE_SAMPLED_IMAGE_BIT))
+            return false;
+
+      if ((bind & PIPE_BIND_SAMPLER_VIEW) || (bind & PIPE_BIND_RENDER_TARGET)) {
+         /* if this is a 3-component texture, force gallium to give us 4 components by rejecting this one */
+         unsigned bpb = util_format_get_blocksizebits(format);
+         if (bpb == 24 || bpb == 48 || bpb == 96)
+            return false;
+      }
 
       if (bind & PIPE_BIND_DEPTH_STENCIL &&
           !(props.optimalTilingFeatures & VK_FORMAT_FEATURE_DEPTH_STENCIL_ATTACHMENT_BIT))
          return false;
+
+      if (bind & PIPE_BIND_SHADER_IMAGE &&
+          !(props.optimalTilingFeatures & VK_FORMAT_FEATURE_STORAGE_IMAGE_BIT))
+         return false;
    }
 
    if (util_format_is_compressed(format)) {
@@ -665,6 +828,15 @@ zink_is_format_supported(struct pipe_screen *pscreen,
    return true;
 }
 
+static void
+resource_cache_entry_destroy(struct zink_screen *screen, struct hash_entry *he)
+{
+   struct util_dynarray *array = (void*)he->data;
+   util_dynarray_foreach(array, VkDeviceMemory, mem)
+      vkFreeMemory(screen->dev, *mem, NULL);
+   util_dynarray_fini(array);
+}
+
 static void
 zink_destroy_screen(struct pipe_screen *pscreen)
 {
@@ -675,6 +847,17 @@ zink_destroy_screen(struct pipe_screen *pscreen)
    }
 
    u_transfer_helper_destroy(pscreen->transfer_helper);
+   zink_screen_update_pipeline_cache(screen);
+   if (screen->disk_cache)
+      disk_cache_wait_for_idle(screen->disk_cache);
+   disk_cache_destroy(screen->disk_cache);
+   simple_mtx_lock(&screen->mem_cache_mtx);
+   hash_table_foreach(screen->resource_mem_cache, he)
+      resource_cache_entry_destroy(screen, he);
+   _mesa_hash_table_destroy(screen->resource_mem_cache, NULL);
+   simple_mtx_unlock(&screen->mem_cache_mtx);
+   simple_mtx_destroy(&screen->mem_cache_mtx);
+   vkDestroyPipelineCache(screen->dev, screen->pipeline_cache, NULL);
 
    vkDestroyDevice(screen->dev, NULL);
    vkDestroyInstance(screen->instance, NULL);
@@ -719,10 +902,14 @@ update_queue_props(struct zink_screen *screen)
    vkGetPhysicalDeviceQueueFamilyProperties(screen->pdev, &num_queues, props);
 
    for (uint32_t i = 0; i < num_queues; i++) {
-      if (props[i].queueFlags & VK_QUEUE_GRAPHICS_BIT) {
+      if (!screen->timestamp_valid_bits && (props[i].queueFlags & VK_QUEUE_GRAPHICS_BIT)) {
          screen->gfx_queue = i;
+         screen->max_queues = props[i].queueCount;
          screen->timestamp_valid_bits = props[i].timestampValidBits;
-         break;
+      }
+      if ((screen->compute_queue == UINT_MAX) && (props[i].queueFlags & VK_QUEUE_COMPUTE_BIT)) {
+         screen->compute_queue = i;
+         screen->compute_timestamp_valid_bits = props[i].timestampValidBits;
       }
    }
    free(props);
@@ -750,10 +937,10 @@ zink_flush_frontbuffer(struct pipe_screen *pscreen,
       isr.mipLevel = level;
       isr.arrayLayer = layer;
       VkSubresourceLayout layout;
-      vkGetImageSubresourceLayout(screen->dev, res->image, &isr, &layout);
+      vkGetImageSubresourceLayout(screen->dev, res->obj->image, &isr, &layout);
 
       void *ptr;
-      VkResult result = vkMapMemory(screen->dev, res->mem, res->offset, res->size, 0, &ptr);
+      VkResult result = vkMapMemory(screen->dev, res->obj->mem, res->obj->offset, res->obj->size, 0, &ptr);
       if (result != VK_SUCCESS) {
          debug_printf("failed to map memory for display\n");
          return;
@@ -763,7 +950,7 @@ zink_flush_frontbuffer(struct pipe_screen *pscreen,
          uint8_t *dst = (uint8_t *)map + i * res->dt_stride;
          memcpy(dst, src, res->dt_stride);
       }
-      vkUnmapMemory(screen->dev, res->mem);
+      vkUnmapMemory(screen->dev, res->obj->mem);
    }
 
    winsys->displaytarget_unmap(winsys, res->dt);
@@ -783,7 +970,7 @@ zink_is_depth_format_supported(struct zink_screen *screen, VkFormat format)
 }
 
 static enum pipe_format
-emulate_x8(enum pipe_format format)
+emulate_format(enum pipe_format format)
 {
    /* convert missing X8 variants to A8 */
    switch (format) {
@@ -793,6 +980,29 @@ emulate_x8(enum pipe_format format)
    case PIPE_FORMAT_B8G8R8X8_SRGB:
       return PIPE_FORMAT_B8G8R8A8_SRGB;
 
+   case PIPE_FORMAT_R8G8B8X8_UNORM:
+      return PIPE_FORMAT_R8G8B8A8_UNORM;
+
+   case PIPE_FORMAT_A8R8G8B8_UNORM:
+      return PIPE_FORMAT_R8G8B8A8_UNORM;
+   case PIPE_FORMAT_A8R8G8B8_UINT:
+      return PIPE_FORMAT_R8G8B8A8_UINT;
+   case PIPE_FORMAT_A8R8G8B8_SRGB:
+      return PIPE_FORMAT_R8G8B8A8_SRGB;
+   case PIPE_FORMAT_A8B8G8R8_UNORM:
+      return PIPE_FORMAT_B8G8R8A8_UNORM;
+   case PIPE_FORMAT_A8B8G8R8_UINT:
+      return PIPE_FORMAT_B8G8R8A8_UINT;
+   case PIPE_FORMAT_A8B8G8R8_SRGB:
+      return PIPE_FORMAT_B8G8R8A8_SRGB;
+   case PIPE_FORMAT_A8B8G8R8_SINT:
+      return PIPE_FORMAT_B8G8R8A8_SINT;
+   case PIPE_FORMAT_A8B8G8R8_SSCALED:
+      return PIPE_FORMAT_B8G8R8A8_SSCALED;
+   case PIPE_FORMAT_A8B8G8R8_USCALED:
+      return PIPE_FORMAT_B8G8R8A8_USCALED;
+
+
    default:
       return format;
    }
@@ -801,7 +1011,7 @@ emulate_x8(enum pipe_format format)
 VkFormat
 zink_get_format(struct zink_screen *screen, enum pipe_format format)
 {
-   VkFormat ret = zink_pipe_format_to_vk_format(emulate_x8(format));
+   VkFormat ret = zink_pipe_format_to_vk_format(emulate_format(format));
 
    if (ret == VK_FORMAT_X8_D24_UNORM_PACK32 &&
        !screen->have_X8_D24_UNORM_PACK32) {
@@ -825,6 +1035,82 @@ zink_get_format(struct zink_screen *screen, enum pipe_format format)
    return ret;
 }
 
+void
+zink_screen_init_descriptor_funcs(struct zink_screen *screen, bool fallback)
+{
+   if (screen->info.have_KHR_push_descriptor &&
+       screen->info.have_KHR_descriptor_update_template &&
+       !fallback &&
+       !getenv("ZINK_CACHE_DESCRIPTORS")) {
+#define LAZY(FUNC) screen->FUNC = zink_##FUNC##_lazy
+      LAZY(descriptor_program_init);
+      LAZY(descriptor_program_deinit);
+      LAZY(context_invalidate_descriptor_state);
+      LAZY(batch_descriptor_init);
+      LAZY(batch_descriptor_reset);
+      LAZY(batch_descriptor_deinit);
+      LAZY(descriptors_init);
+      LAZY(descriptors_deinit);
+      LAZY(descriptors_update);
+      screen->lazy_descriptors = true;
+#undef LAZY
+   } else {
+#define DEFAULT(FUNC) screen->FUNC = zink_##FUNC
+      DEFAULT(descriptor_program_init);
+      DEFAULT(descriptor_program_deinit);
+      DEFAULT(context_invalidate_descriptor_state);
+      DEFAULT(batch_descriptor_init);
+      DEFAULT(batch_descriptor_reset);
+      DEFAULT(batch_descriptor_deinit);
+      DEFAULT(descriptors_init);
+      DEFAULT(descriptors_deinit);
+      DEFAULT(descriptors_update);
+      screen->lazy_descriptors = false;
+#undef DEFAULT
+   }
+}
+
+static bool
+load_instance_extensions(struct zink_screen *screen)
+{
+   if (zink_debug & ZINK_DEBUG_VALIDATION) {
+      printf("zink: Loader %d.%d.%d \n", VK_VERSION_MAJOR(screen->loader_version), VK_VERSION_MINOR(screen->loader_version), VK_VERSION_PATCH(screen->loader_version));
+   }
+
+   if (screen->instance_info.have_KHR_get_physical_device_properties2) {
+      // Not Vk 1.1+ so if VK_KHR_get_physical_device_properties2 the use it
+      GET_PROC_ADDR_INSTANCE_LOCAL(screen->instance, GetPhysicalDeviceFeatures2KHR);
+      GET_PROC_ADDR_INSTANCE_LOCAL(screen->instance, GetPhysicalDeviceProperties2KHR);
+      screen->vk_GetPhysicalDeviceFeatures2 = vk_GetPhysicalDeviceFeatures2KHR;
+      screen->vk_GetPhysicalDeviceProperties2 = vk_GetPhysicalDeviceProperties2KHR;
+   } else if (VK_MAKE_VERSION(1,1,0) <= screen->loader_version) {
+      // Get Vk 1.1+ Instance functions
+      GET_PROC_ADDR_INSTANCE(GetPhysicalDeviceFeatures2);
+      GET_PROC_ADDR_INSTANCE(GetPhysicalDeviceProperties2);
+   }
+
+   if (screen->instance_info.have_KHR_draw_indirect_count) {
+      GET_PROC_ADDR_INSTANCE_LOCAL(screen->instance, CmdDrawIndirectCountKHR);
+      GET_PROC_ADDR_INSTANCE_LOCAL(screen->instance, CmdDrawIndexedIndirectCountKHR);
+      screen->vk_CmdDrawIndirectCount = vk_CmdDrawIndirectCountKHR;
+      screen->vk_CmdDrawIndexedIndirectCount = vk_CmdDrawIndexedIndirectCountKHR;
+   } else if (VK_MAKE_VERSION(1,2,0) <= screen->loader_version) {
+      // Get Vk 1.1+ Instance functions
+      GET_PROC_ADDR_INSTANCE(CmdDrawIndirectCount);
+      GET_PROC_ADDR_INSTANCE(CmdDrawIndexedIndirectCount);
+   }
+
+   if (screen->instance_info.have_KHR_timeline_semaphore) {
+      GET_PROC_ADDR_INSTANCE_LOCAL(screen->instance, WaitSemaphoresKHR);
+      screen->vk_WaitSemaphores = vk_WaitSemaphoresKHR;
+   } else if (VK_MAKE_VERSION(1,1,0) <= screen->loader_version) {
+      // Get Vk 1.1+ Instance functions
+      GET_PROC_ADDR_INSTANCE(WaitSemaphores);
+   }
+
+   return true;
+}
+
 static bool
 load_device_extensions(struct zink_screen *screen)
 {
@@ -869,6 +1155,19 @@ load_device_extensions(struct zink_screen *screen)
    if (screen->info.have_EXT_extended_dynamic_state) {
       GET_PROC_ADDR(CmdSetViewportWithCountEXT);
       GET_PROC_ADDR(CmdSetScissorWithCountEXT);
+      GET_PROC_ADDR(CmdBindVertexBuffers2EXT);
+   }
+
+   if (screen->info.have_KHR_push_descriptor) {
+      GET_PROC_ADDR(CmdPushDescriptorSetKHR);
+   }
+
+   if (screen->info.have_KHR_descriptor_update_template) {
+      GET_PROC_ADDR(CreateDescriptorUpdateTemplate);
+      GET_PROC_ADDR(DestroyDescriptorUpdateTemplate);
+      GET_PROC_ADDR(UpdateDescriptorSetWithTemplate);
+      if (screen->info.have_KHR_push_descriptor)
+         GET_PROC_ADDR(CmdPushDescriptorSetWithTemplateKHR);
    }
 
    screen->have_triangle_fans = true;
@@ -991,12 +1290,23 @@ check_device_needs_mesa_wsi(struct zink_screen *screen)
        (screen->info.props.vendorID == 0x14E4 &&
         screen->info.props.deviceID == 42) ||
        /* RADV */
-       screen->info.driver_props.driverID == VK_DRIVER_ID_MESA_RADV_KHR
+       screen->info.props.vendorID == 0x1002
       ) {
       screen->needs_mesa_wsi = true;
    }
 }
 
+static void
+populate_format_props(struct zink_screen *screen)
+{
+   for (unsigned i = 0; i < PIPE_FORMAT_COUNT; i++) {
+      VkFormat format = zink_get_format(screen, i);
+      if (!format)
+         continue;
+      vkGetPhysicalDeviceFormatProperties(screen->pdev, format, &screen->format_props[i]);
+   }
+}
+
 static uint32_t
 zink_get_loader_version(void)
 {
@@ -1020,17 +1330,25 @@ zink_create_logical_device(struct zink_screen *screen)
 {
    VkDevice dev = VK_NULL_HANDLE;
 
-   VkDeviceQueueCreateInfo qci = {};
-   float dummy = 0.0f;
-   qci.sType = VK_STRUCTURE_TYPE_DEVICE_QUEUE_CREATE_INFO;
-   qci.queueFamilyIndex = screen->gfx_queue;
-   qci.queueCount = 1;
-   qci.pQueuePriorities = &dummy;
-
    VkDeviceCreateInfo dci = {};
    dci.sType = VK_STRUCTURE_TYPE_DEVICE_CREATE_INFO;
    dci.queueCreateInfoCount = 1;
-   dci.pQueueCreateInfos = &qci;
+
+   VkDeviceQueueCreateInfo qci[2] = {};
+   float dummy = 0.0f;
+   qci[0].sType = VK_STRUCTURE_TYPE_DEVICE_QUEUE_CREATE_INFO;
+   qci[0].queueFamilyIndex = screen->gfx_queue;
+   qci[0].queueCount = screen->threaded && screen->max_queues > 1 ? 2 : 1;
+   qci[0].pQueuePriorities = &dummy;
+   if (screen->compute_queue != UINT_MAX && screen->compute_queue != screen->gfx_queue) {
+      qci[1].sType = VK_STRUCTURE_TYPE_DEVICE_QUEUE_CREATE_INFO;
+      qci[1].queueFamilyIndex = screen->compute_queue;
+      qci[1].queueCount = screen->threaded && screen->max_queues > 1 ? 2 : 1;
+      qci[1].pQueuePriorities = &dummy;
+      dci.queueCreateInfoCount++;
+   }
+
+   dci.pQueueCreateInfos = qci;
    /* extensions don't have bool members in pEnabledFeatures.
     * this requires us to pass the whole VkPhysicalDeviceFeatures2 struct
     */
@@ -1047,6 +1365,15 @@ zink_create_logical_device(struct zink_screen *screen)
    return dev;
 }
 
+static void
+pre_hash_descriptor_states(struct zink_screen *screen)
+{
+   VkImageViewCreateInfo null_info = {.sType = VK_STRUCTURE_TYPE_IMAGE_VIEW_CREATE_INFO};
+   VkBufferViewCreateInfo null_binfo = {.sType = VK_STRUCTURE_TYPE_BUFFER_VIEW_CREATE_INFO};
+   screen->null_descriptor_hashes.image_view = _mesa_hash_data(&null_info, sizeof(VkImageViewCreateInfo));
+   screen->null_descriptor_hashes.buffer_view = _mesa_hash_data(&null_binfo, sizeof(VkBufferViewCreateInfo));
+}
+
 static struct zink_screen *
 zink_internal_create_screen(const struct pipe_screen_config *config)
 {
@@ -1054,6 +1381,11 @@ zink_internal_create_screen(const struct pipe_screen_config *config)
    if (!screen)
       return NULL;
 
+   util_cpu_detect();
+   screen->threaded = util_cpu_caps.nr_cpus > 1 && debug_get_bool_option("GALLIUM_THREAD", util_cpu_caps.nr_cpus > 1);
+
+   screen->compute_queue = UINT_MAX;
+   screen->gfx_queue = UINT_MAX;
    zink_debug = debug_get_option_zink_debug();
 
    screen->loader_version = zink_get_loader_version();
@@ -1061,7 +1393,7 @@ zink_internal_create_screen(const struct pipe_screen_config *config)
    if (!screen->instance)
       goto fail;
 
-   if (!zink_load_instance_extensions(screen))
+   if (!load_instance_extensions(screen))
       goto fail;
 
    if (screen->instance_info.have_EXT_debug_utils && !create_debug(screen))
@@ -1069,6 +1401,7 @@ zink_internal_create_screen(const struct pipe_screen_config *config)
 
    screen->pdev = choose_pdev(screen->instance);
    update_queue_props(screen);
+   assert(screen->gfx_queue < UINT_MAX);
 
    screen->have_X8_D24_UNORM_PACK32 = zink_is_depth_format_supported(screen,
                                               VK_FORMAT_X8_D24_UNORM_PACK32);
@@ -1097,6 +1430,7 @@ zink_internal_create_screen(const struct pipe_screen_config *config)
    screen->base.get_name = zink_get_name;
    screen->base.get_vendor = zink_get_vendor;
    screen->base.get_device_vendor = zink_get_device_vendor;
+   screen->base.get_compute_param = zink_get_compute_param;
    screen->base.get_param = zink_get_param;
    screen->base.get_paramf = zink_get_paramf;
    screen->base.get_shader_param = zink_get_shader_param;
@@ -1105,14 +1439,45 @@ zink_internal_create_screen(const struct pipe_screen_config *config)
    screen->base.context_create = zink_context_create;
    screen->base.flush_frontbuffer = zink_flush_frontbuffer;
    screen->base.destroy = zink_destroy_screen;
+   screen->base.finalize_nir = zink_shader_finalize;
 
-   zink_screen_resource_init(&screen->base);
+   if (!zink_screen_resource_init(&screen->base))
+      goto fail;
    zink_screen_fence_init(&screen->base);
 
    zink_screen_init_compiler(screen);
+   disk_cache_init(screen);
+   populate_format_props(screen);
+   pre_hash_descriptor_states(screen);
+
+   VkPipelineCacheCreateInfo pcci;
+   pcci.sType = VK_STRUCTURE_TYPE_PIPELINE_CACHE_CREATE_INFO;
+   pcci.pNext = NULL;
+   /* we're single-threaded now, so we don't need synchronization */
+   pcci.flags = screen->info.have_EXT_pipeline_creation_cache_control ? VK_PIPELINE_CACHE_CREATE_EXTERNALLY_SYNCHRONIZED_BIT_EXT : 0;
+   pcci.initialDataSize = 0;
+   pcci.pInitialData = NULL;
+   if (screen->disk_cache) {
+      pcci.pInitialData = disk_cache_get(screen->disk_cache, screen->disk_cache_key, &screen->pipeline_cache_size);
+      pcci.initialDataSize = screen->pipeline_cache_size;
+   }
+   vkCreatePipelineCache(screen->dev, &pcci, NULL, &screen->pipeline_cache);
+   free((void*)pcci.pInitialData);
 
    slab_create_parent(&screen->transfer_pool, sizeof(struct zink_transfer), 16);
 
+   if (config) {
+      screen->driconf.dual_color_blend_by_location = driQueryOptionb(config->options, "dual_color_blend_by_location");
+      screen->driconf.inline_uniforms = true;
+   }
+
+   if (!os_get_total_physical_memory(&screen->total_mem)) {
+      zink_destroy_screen(&screen->base);
+      return NULL;
+   }
+
+   zink_screen_init_descriptor_funcs(screen, false);
+
    return screen;
 
 fail:
diff --git a/src/gallium/drivers/zink/zink_screen.h b/src/gallium/drivers/zink/zink_screen.h
index aa16a2ad500..924e4422db0 100644
--- a/src/gallium/drivers/zink/zink_screen.h
+++ b/src/gallium/drivers/zink/zink_screen.h
@@ -30,6 +30,8 @@
 #include "pipe/p_screen.h"
 #include "util/slab.h"
 #include "compiler/nir/nir.h"
+#include "util/disk_cache.h"
+#include "util/simple_mtx.h"
 
 #include <vulkan/vulkan.h>
 
@@ -39,6 +41,14 @@
 #endif
 
 extern uint32_t zink_debug;
+struct hash_table;
+
+struct zink_batch_state;
+struct zink_context;
+struct zink_descriptor_layout_key;
+struct zink_program;
+struct zink_shader;
+enum zink_descriptor_type;
 
 #define ZINK_DEBUG_NIR 0x1
 #define ZINK_DEBUG_SPIRV 0x2
@@ -47,13 +57,23 @@ extern uint32_t zink_debug;
 
 struct zink_screen {
    struct pipe_screen base;
+   bool threaded;
 
    struct sw_winsys *winsys;
 
    struct slab_parent_pool transfer_pool;
+   VkPipelineCache pipeline_cache;
+   size_t pipeline_cache_size;
+   struct disk_cache *disk_cache;
+   cache_key disk_cache_key;
+
+   simple_mtx_t mem_cache_mtx;
+   struct hash_table *resource_mem_cache;
 
    unsigned shader_id;
 
+   uint64_t total_mem;
+
    VkInstance instance;
    struct zink_instance_info instance_info;
 
@@ -67,7 +87,10 @@ struct zink_screen {
    bool have_triangle_fans;
 
    uint32_t gfx_queue;
+   uint32_t max_queues;
+   uint32_t compute_queue;
    uint32_t timestamp_valid_bits;
+   uint32_t compute_timestamp_valid_bits;
    VkDevice dev;
    VkDebugUtilsMessengerEXT debugUtilsCallbackHandle;
 
@@ -83,6 +106,27 @@ struct zink_screen {
    PFN_vkCmdDrawIndirectCount vk_CmdDrawIndirectCount;
    PFN_vkCmdDrawIndexedIndirectCount vk_CmdDrawIndexedIndirectCount;
 
+   PFN_vkWaitSemaphores vk_WaitSemaphores;
+
+   PFN_vkCmdPushDescriptorSetKHR vk_CmdPushDescriptorSetKHR;
+   PFN_vkCreateDescriptorUpdateTemplate vk_CreateDescriptorUpdateTemplate;
+   PFN_vkDestroyDescriptorUpdateTemplate vk_DestroyDescriptorUpdateTemplate;
+   PFN_vkUpdateDescriptorSetWithTemplate vk_UpdateDescriptorSetWithTemplate;
+   PFN_vkCmdPushDescriptorSetWithTemplateKHR vk_CmdPushDescriptorSetWithTemplateKHR;
+   bool (*descriptor_program_init)(struct zink_context *ctx, struct zink_program *pg);
+   void (*descriptor_program_deinit)(struct zink_screen *screen, struct zink_program *pg);
+   struct set *(*descriptors_update)(struct zink_context *ctx, bool is_compute);
+   void (*context_update_descriptor_states)(struct zink_context *ctx, bool is_compute);
+   void (*context_invalidate_descriptor_state)(struct zink_context *ctx, enum pipe_shader_type shader,
+                                               enum zink_descriptor_type type,
+                                               unsigned start, unsigned count);
+   bool (*batch_descriptor_init)(struct zink_batch_state *bs);
+   void (*batch_descriptor_reset)(struct zink_screen *screen, struct zink_batch_state *bs);
+   void (*batch_descriptor_deinit)(struct zink_screen *screen, struct zink_batch_state *bs);
+   bool (*descriptors_init)(struct zink_context *ctx);
+   void (*descriptors_deinit)(struct zink_context *ctx);
+   bool lazy_descriptors;
+
    PFN_vkGetMemoryFdKHR vk_GetMemoryFdKHR;
    PFN_vkCmdBeginConditionalRenderingEXT vk_CmdBeginConditionalRenderingEXT;
    PFN_vkCmdEndConditionalRenderingEXT vk_CmdEndConditionalRenderingEXT;
@@ -99,6 +143,7 @@ struct zink_screen {
 
    PFN_vkCmdSetViewportWithCountEXT vk_CmdSetViewportWithCountEXT;
    PFN_vkCmdSetScissorWithCountEXT vk_CmdSetScissorWithCountEXT;
+   PFN_vkCmdBindVertexBuffers2EXT vk_CmdBindVertexBuffers2EXT;
 
    PFN_vkCreateDebugUtilsMessengerEXT vk_CreateDebugUtilsMessengerEXT;
    PFN_vkDestroyDebugUtilsMessengerEXT vk_DestroyDebugUtilsMessengerEXT;
@@ -112,6 +157,17 @@ struct zink_screen {
    PFN_vkUseIOSurfaceMVK vk_UseIOSurfaceMVK;
    PFN_vkGetIOSurfaceMVK vk_GetIOSurfaceMVK;
 #endif
+
+   struct {
+      bool dual_color_blend_by_location;
+      bool inline_uniforms;
+   } driconf;
+
+   VkFormatProperties format_props[PIPE_FORMAT_COUNT];
+   struct {
+      uint32_t image_view;
+      uint32_t buffer_view;
+   } null_descriptor_hashes;
 };
 
 static inline struct zink_screen *
@@ -144,4 +200,9 @@ zink_is_depth_format_supported(struct zink_screen *screen, VkFormat format);
 
 #define GET_PROC_ADDR_INSTANCE_LOCAL(instance, x) PFN_vk##x vk_##x = (PFN_vk##x)vkGetInstanceProcAddr(instance, "vk"#x)
 
+void
+zink_screen_update_pipeline_cache(struct zink_screen *screen);
+
+void
+zink_screen_init_descriptor_funcs(struct zink_screen *screen, bool fallback);
 #endif
diff --git a/src/gallium/drivers/zink/zink_shader_keys.h b/src/gallium/drivers/zink/zink_shader_keys.h
index 962651f1e6d..045176946d1 100644
--- a/src/gallium/drivers/zink/zink_shader_keys.h
+++ b/src/gallium/drivers/zink/zink_shader_keys.h
@@ -26,10 +26,16 @@
 #ifndef ZINK_SHADER_KEYS_H
 # define ZINK_SHADER_KEYS_H
 
+struct zink_vs_key {
+   unsigned shader_id;
+   bool clip_halfz;
+};
+
 struct zink_fs_key {
    unsigned shader_id;
    //bool flat_shade;
    bool samples;
+   bool force_dual_color_blend;
 };
 
 struct zink_tcs_key {
@@ -38,6 +44,11 @@ struct zink_tcs_key {
    uint64_t vs_outputs_written;
 };
 
+struct zink_shader_key_base {
+   uint32_t gl_clamp[3]; //bitmasks of slots
+   uint32_t inlined_uniform_values[MAX_INLINABLE_UNIFORMS];
+};
+
 /* a shader key is used for swapping out shader modules based on pipeline states,
  * e.g., if sampleCount changes, we must verify that the fs doesn't need a recompile
  *       to account for GL ignoring gl_SampleMask in some cases when VK will not
@@ -45,9 +56,13 @@ struct zink_tcs_key {
  */
 struct zink_shader_key {
    union {
+      /* reuse vs key for now with tes/gs since we only use clip_halfz */
+      struct zink_vs_key vs;
       struct zink_fs_key fs;
       struct zink_tcs_key tcs;
    } key;
+   struct zink_shader_key_base base;
+   unsigned inline_uniforms:1;
    uint32_t size;
 };
 
@@ -57,6 +72,11 @@ zink_fs_key(const struct zink_shader_key *key)
    return &key->key.fs;
 }
 
+static inline const struct zink_vs_key *zink_vs_key(const struct zink_shader_key *key)
+{
+   return &key->key.vs;
+}
+
 
 
 #endif
diff --git a/src/gallium/drivers/zink/zink_state.c b/src/gallium/drivers/zink/zink_state.c
index 734cc03a40d..5ffa4577172 100644
--- a/src/gallium/drivers/zink/zink_state.c
+++ b/src/gallium/drivers/zink/zink_state.c
@@ -26,6 +26,8 @@
 #include "zink_context.h"
 #include "zink_screen.h"
 
+#include "compiler/shader_enums.h"
+#include "util/u_dual_blend.h"
 #include "util/u_memory.h"
 
 #include <math.h>
@@ -177,6 +179,30 @@ blend_op(enum pipe_blend_func func)
    unreachable("unexpected blend function");
 }
 
+static VkBlendOp
+advanced_blend_op(enum gl_advanced_blend_mode mode)
+{
+   switch (mode) {
+   case BLEND_MULTIPLY: return VK_BLEND_OP_MULTIPLY_EXT;
+   case BLEND_SCREEN: return VK_BLEND_OP_SCREEN_EXT;
+   case BLEND_OVERLAY: return VK_BLEND_OP_OVERLAY_EXT;
+   case BLEND_DARKEN: return VK_BLEND_OP_DARKEN_EXT;
+   case BLEND_LIGHTEN: return VK_BLEND_OP_LIGHTEN_EXT;
+   case BLEND_COLORDODGE: return VK_BLEND_OP_COLORDODGE_EXT;
+   case BLEND_COLORBURN: return VK_BLEND_OP_COLORBURN_EXT;
+   case BLEND_HARDLIGHT: return VK_BLEND_OP_HARDLIGHT_EXT;
+   case BLEND_SOFTLIGHT: return VK_BLEND_OP_SOFTLIGHT_EXT;
+   case BLEND_DIFFERENCE: return VK_BLEND_OP_DIFFERENCE_EXT;
+   case BLEND_EXCLUSION: return VK_BLEND_OP_EXCLUSION_EXT;
+   case BLEND_HSL_HUE: return VK_BLEND_OP_HSL_HUE_EXT;
+   case BLEND_HSL_SATURATION: return VK_BLEND_OP_HSL_SATURATION_EXT;
+   case BLEND_HSL_COLOR: return VK_BLEND_OP_HSL_COLOR_EXT;
+   case BLEND_HSL_LUMINOSITY: return VK_BLEND_OP_HSL_LUMINOSITY_EXT;
+   default:
+      unreachable("unknown advanced blend mode");
+   }
+}
+
 static VkLogicOp
 logic_op(enum pipe_logicop func)
 {
@@ -220,6 +246,7 @@ static void *
 zink_create_blend_state(struct pipe_context *pctx,
                         const struct pipe_blend_state *blend_state)
 {
+   struct zink_screen *screen = zink_screen(pctx->screen);
    struct zink_blend_state *cso = CALLOC_STRUCT(zink_blend_state);
    if (!cso)
       return NULL;
@@ -240,6 +267,11 @@ zink_create_blend_state(struct pipe_context *pctx,
    cso->alpha_to_one = blend_state->alpha_to_one;
 
    cso->need_blend_constants = false;
+   cso->advanced_blend = blend_state->advanced_blend_func && screen->info.have_EXT_blend_operation_advanced;
+   if (cso->advanced_blend) {
+      cso->logicop_enable = VK_TRUE;
+      cso->logicop_func = VK_LOGIC_OP_COPY;
+   }
 
    for (int i = 0; i < PIPE_MAX_COLOR_BUFS; ++i) {
       const struct pipe_rt_blend_state *rt = blend_state->rt;
@@ -248,7 +280,18 @@ zink_create_blend_state(struct pipe_context *pctx,
 
       VkPipelineColorBlendAttachmentState att = { };
 
-      if (rt->blend_enable) {
+      if (cso->advanced_blend) {
+         assert(i < screen->info.blend_props.advancedBlendMaxColorAttachments);
+         att.blendEnable = VK_TRUE;
+         att.srcColorBlendFactor = VK_BLEND_FACTOR_SRC_COLOR;
+         att.dstColorBlendFactor = VK_BLEND_FACTOR_DST_COLOR;
+         att.colorBlendOp = advanced_blend_op(blend_state->advanced_blend_func);
+         att.srcAlphaBlendFactor = VK_BLEND_FACTOR_SRC_ALPHA;
+         att.dstAlphaBlendFactor = VK_BLEND_FACTOR_DST_ALPHA;
+         att.alphaBlendOp = att.colorBlendOp;
+      } else if (rt->blend_enable) {
+         if (blend_state->advanced_blend_func)
+            debug_printf("ignoring advanced blend mode due to missing EXT_blend_operation_advanced extension");
          att.blendEnable = VK_TRUE;
          att.srcColorBlendFactor = blend_factor(fix_blendfactor(rt->rgb_src_factor, cso->alpha_to_one));
          att.dstColorBlendFactor = blend_factor(fix_blendfactor(rt->rgb_dst_factor, cso->alpha_to_one));
@@ -275,6 +318,7 @@ zink_create_blend_state(struct pipe_context *pctx,
 
       cso->attachments[i] = att;
    }
+   cso->dual_src_blend = util_blend_state_is_dual(blend_state, 0);
 
    return cso;
 }
@@ -434,6 +478,7 @@ zink_create_rasterizer_state(struct pipe_context *pctx,
    assert(rs_state->depth_clip_far == rs_state->depth_clip_near);
    state->hw_state.depth_clamp = rs_state->depth_clip_near == 0;
    state->hw_state.rasterizer_discard = rs_state->rasterizer_discard;
+   state->hw_state.force_persample_interp = rs_state->force_persample_interp;
 
    assert(rs_state->fill_front <= PIPE_POLYGON_MODE_POINT);
    if (rs_state->fill_back != rs_state->fill_front)
@@ -463,6 +508,7 @@ static void
 zink_bind_rasterizer_state(struct pipe_context *pctx, void *cso)
 {
    struct zink_context *ctx = zink_context(pctx);
+   bool clip_halfz = ctx->rast_state ? ctx->rast_state->base.clip_halfz : false;
    ctx->rast_state = cso;
 
    if (ctx->rast_state) {
@@ -471,6 +517,11 @@ zink_bind_rasterizer_state(struct pipe_context *pctx, void *cso)
          ctx->gfx_pipeline_state.dirty = true;
       }
 
+      if (clip_halfz != ctx->rast_state->base.clip_halfz)
+         ctx->dirty_shader_stages |= BITFIELD64_BIT(PIPE_SHADER_VERTEX) |
+                                     BITFIELD64_BIT(PIPE_SHADER_TESS_EVAL) |
+                                     BITFIELD64_BIT(PIPE_SHADER_GEOMETRY);
+
       if (ctx->line_width != ctx->rast_state->line_width) {
          ctx->line_width = ctx->rast_state->line_width;
          ctx->gfx_pipeline_state.dirty = true;
diff --git a/src/gallium/drivers/zink/zink_state.h b/src/gallium/drivers/zink/zink_state.h
index 141f4c39126..f204c400379 100644
--- a/src/gallium/drivers/zink/zink_state.h
+++ b/src/gallium/drivers/zink/zink_state.h
@@ -49,6 +49,7 @@ struct zink_rasterizer_hw_state {
    VkFrontFace front_face;
    VkPolygonMode polygon_mode;
    VkCullModeFlags cull_mode;
+   bool force_persample_interp;
 };
 
 struct zink_rasterizer_state {
@@ -69,6 +70,8 @@ struct zink_blend_state {
    VkBool32 alpha_to_one;
 
    bool need_blend_constants;
+   bool advanced_blend;
+   bool dual_src_blend;
 };
 
 struct zink_depth_stencil_alpha_hw_state {
diff --git a/src/gallium/drivers/zink/zink_surface.c b/src/gallium/drivers/zink/zink_surface.c
index d5d0a3c6dbc..32de457e43c 100644
--- a/src/gallium/drivers/zink/zink_surface.c
+++ b/src/gallium/drivers/zink/zink_surface.c
@@ -22,6 +22,7 @@
  */
 
 #include "zink_context.h"
+#include "zink_framebuffer.h"
 #include "zink_resource.h"
 #include "zink_screen.h"
 #include "zink_surface.h"
@@ -30,36 +31,16 @@
 #include "util/u_inlines.h"
 #include "util/u_memory.h"
 
-static struct pipe_surface *
-zink_create_surface(struct pipe_context *pctx,
-                    struct pipe_resource *pres,
-                    const struct pipe_surface *templ)
+VkImageViewCreateInfo
+create_ivci(struct zink_screen *screen,
+            struct zink_resource *res,
+            const struct pipe_surface *templ)
 {
-   struct zink_screen *screen = zink_screen(pctx->screen);
-   unsigned int level = templ->u.tex.level;
-
-   struct zink_surface *surface = CALLOC_STRUCT(zink_surface);
-   if (!surface)
-      return NULL;
-
-   pipe_resource_reference(&surface->base.texture, pres);
-   pipe_reference_init(&surface->base.reference, 1);
-   surface->base.context = pctx;
-   surface->base.format = templ->format;
-   surface->base.width = u_minify(pres->width0, level);
-   surface->base.height = u_minify(pres->height0, level);
-   surface->base.nr_samples = templ->nr_samples;
-   surface->base.u.tex.level = level;
-   surface->base.u.tex.first_layer = templ->u.tex.first_layer;
-   surface->base.u.tex.last_layer = templ->u.tex.last_layer;
-
-   struct zink_resource *res = zink_resource(pres);
-
    VkImageViewCreateInfo ivci = {};
    ivci.sType = VK_STRUCTURE_TYPE_IMAGE_VIEW_CREATE_INFO;
-   ivci.image = res->image;
+   ivci.image = res->obj->image;
 
-   switch (pres->target) {
+   switch (res->base.b.target) {
    case PIPE_TEXTURE_1D:
       ivci.viewType = VK_IMAGE_VIEW_TYPE_1D;
       break;
@@ -108,30 +89,177 @@ zink_create_surface(struct pipe_context *pctx,
    ivci.subresourceRange.baseArrayLayer = templ->u.tex.first_layer;
    ivci.subresourceRange.layerCount = 1 + templ->u.tex.last_layer - templ->u.tex.first_layer;
 
-   if (pres->target == PIPE_TEXTURE_CUBE ||
-       pres->target == PIPE_TEXTURE_CUBE_ARRAY)
-      ivci.subresourceRange.layerCount *= 6;
+   if (res->base.b.target == PIPE_TEXTURE_CUBE ||
+       res->base.b.target == PIPE_TEXTURE_CUBE_ARRAY) {
+      if (ivci.subresourceRange.layerCount != 6)
+         ivci.subresourceRange.layerCount = VK_REMAINING_ARRAY_LAYERS;
+   }
+
+   return ivci;
+}
+
+static struct zink_surface *
+create_surface(struct pipe_context *pctx,
+               struct pipe_resource *pres,
+               const struct pipe_surface *templ,
+               VkImageViewCreateInfo *ivci)
+{
+   struct zink_screen *screen = zink_screen(pctx->screen);
+   unsigned int level = templ->u.tex.level;
+
+   struct zink_surface *surface = CALLOC_STRUCT(zink_surface);
+   if (!surface)
+      return NULL;
+
+   pipe_resource_reference(&surface->base.texture, pres);
+   pipe_reference_init(&surface->base.reference, 1);
+   surface->base.context = pctx;
+   surface->base.format = templ->format;
+   surface->base.width = u_minify(pres->width0, level);
+   surface->base.height = u_minify(pres->height0, level);
+   surface->base.nr_samples = templ->nr_samples;
+   surface->base.u.tex.level = level;
+   surface->base.u.tex.first_layer = templ->u.tex.first_layer;
+   surface->base.u.tex.last_layer = templ->u.tex.last_layer;
+   util_dynarray_init(&surface->framebuffer_refs, NULL);
 
-   if (vkCreateImageView(screen->dev, &ivci, NULL,
+   if (vkCreateImageView(screen->dev, ivci, NULL,
                          &surface->image_view) != VK_SUCCESS) {
       FREE(surface);
       return NULL;
    }
 
+   return surface;
+}
+
+static uint32_t
+hash_ivci(const void *key)
+{
+   return _mesa_hash_data((char*)key + offsetof(VkImageViewCreateInfo, flags), sizeof(VkImageViewCreateInfo) - offsetof(VkImageViewCreateInfo, flags));
+}
+
+struct pipe_surface *
+zink_get_surface(struct zink_context *ctx,
+            struct pipe_resource *pres,
+            const struct pipe_surface *templ,
+            VkImageViewCreateInfo *ivci)
+{
+   struct zink_surface* surface = NULL;
+   uint32_t hash = hash_ivci(ivci);
+
+   simple_mtx_lock(&ctx->surface_mtx);
+   struct hash_entry *entry = _mesa_hash_table_search_pre_hashed(&ctx->surface_cache, hash, ivci);
+   simple_mtx_unlock(&ctx->surface_mtx);
+
+   if (!entry) {
+      /* create a new surface */
+      surface = create_surface(&ctx->base, pres, templ, ivci);
+      surface->hash = hash;
+      surface->ivci = *ivci;
+      simple_mtx_lock(&ctx->surface_mtx);
+      entry = _mesa_hash_table_insert_pre_hashed(&ctx->surface_cache, hash, &surface->ivci, surface);
+      simple_mtx_unlock(&ctx->surface_mtx);
+      if (!entry)
+         return NULL;
+
+      surface = entry->data;
+   } else {
+      surface = entry->data;
+      if (zink_resource(surface->base.texture)->obj->simage)
+         zink_rebind_surface(ctx, &surface->base);
+      p_atomic_inc(&surface->base.reference.count);
+   }
+
    return &surface->base;
 }
 
+static struct pipe_surface *
+zink_create_surface(struct pipe_context *pctx,
+                    struct pipe_resource *pres,
+                    const struct pipe_surface *templ)
+{
+
+   VkImageViewCreateInfo ivci = create_ivci(zink_screen(pctx->screen),
+                                            zink_resource(pres), templ);
+
+   return zink_get_surface(zink_context(pctx), pres, templ, &ivci);
+}
+
+static void
+surface_clear_fb_refs(struct zink_context *ctx, struct pipe_surface *psurface)
+{
+   struct zink_surface *surface = zink_surface(psurface);
+   struct zink_screen *screen = zink_screen(ctx->base.screen);
+   util_dynarray_foreach(&surface->framebuffer_refs, struct zink_framebuffer*, fb_ref) {
+      struct zink_framebuffer *fb = *fb_ref;
+      for (unsigned i = 0; i < fb->state.num_attachments; i++) {
+         if (fb->surfaces[i] == psurface) {
+            simple_mtx_lock(&ctx->framebuffer_mtx);
+            fb->surfaces[i] = NULL;
+            _mesa_hash_table_remove_key(&ctx->framebuffer_cache, &fb->state);
+            zink_framebuffer_reference(screen, &fb, NULL);
+            simple_mtx_unlock(&ctx->framebuffer_mtx);
+            break;
+         }
+         /* null surface doesn't get a ref but it will double-free
+          * if the pointer isn't unset
+          */
+         if (fb->null_surface == psurface)
+            fb->null_surface = NULL;
+      }
+   }
+}
+
 static void
 zink_surface_destroy(struct pipe_context *pctx,
                      struct pipe_surface *psurface)
 {
+   struct zink_context *ctx = zink_context(pctx);
    struct zink_screen *screen = zink_screen(pctx->screen);
    struct zink_surface *surface = zink_surface(psurface);
+   simple_mtx_lock(&ctx->surface_mtx);
+   struct hash_entry *he = _mesa_hash_table_search_pre_hashed(&ctx->surface_cache, surface->hash, &surface->ivci);
+   assert(he);
+   _mesa_hash_table_remove(&ctx->surface_cache, he);
+   simple_mtx_unlock(&ctx->surface_mtx);
+   surface_clear_fb_refs(ctx, psurface);
+   util_dynarray_fini(&surface->framebuffer_refs);
    pipe_resource_reference(&psurface->texture, NULL);
+   if (surface->simage_view)
+      vkDestroyImageView(screen->dev, surface->simage_view, NULL);
    vkDestroyImageView(screen->dev, surface->image_view, NULL);
    FREE(surface);
 }
 
+bool
+zink_rebind_surface(struct zink_context *ctx, struct pipe_surface *psurface)
+{
+   struct zink_surface *surface = zink_surface(psurface);
+   struct zink_screen *screen = zink_screen(ctx->base.screen);
+   if (surface->simage_view)
+      return false;
+   VkImageViewCreateInfo ivci = create_ivci(zink_screen(ctx->base.screen),
+                                            zink_resource(psurface->texture), psurface);
+   VkImageView image_view;
+   if (vkCreateImageView(screen->dev, &ivci, NULL, &image_view) != VK_SUCCESS) {
+      abort(); //paniccccccccccccccc
+      return false;
+   }
+   simple_mtx_lock(&ctx->surface_mtx);
+   struct hash_entry *entry = _mesa_hash_table_search_pre_hashed(&ctx->surface_cache, surface->hash, &surface->ivci);
+   assert(entry);
+   _mesa_hash_table_remove(&ctx->surface_cache, entry);
+   surface_clear_fb_refs(ctx, psurface);
+   surface->hash = hash_ivci(&ivci);
+   surface->ivci = ivci;
+   entry = _mesa_hash_table_insert_pre_hashed(&ctx->surface_cache, surface->hash, &surface->ivci, surface);
+   assert(entry);
+   surface->simage_view = surface->image_view;
+   surface->image_view = image_view;
+   simple_mtx_unlock(&ctx->surface_mtx);
+   return true;
+}
+
 void
 zink_context_surface_init(struct pipe_context *context)
 {
diff --git a/src/gallium/drivers/zink/zink_surface.h b/src/gallium/drivers/zink/zink_surface.h
index a85a4981c20..60d5631cbf5 100644
--- a/src/gallium/drivers/zink/zink_surface.h
+++ b/src/gallium/drivers/zink/zink_surface.h
@@ -25,14 +25,19 @@
  #define ZINK_SURFACE_H
 
 #include "pipe/p_state.h"
-
+#include "zink_batch.h"
 #include <vulkan/vulkan.h>
 
 struct pipe_context;
 
 struct zink_surface {
    struct pipe_surface base;
+   VkImageViewCreateInfo ivci;
    VkImageView image_view;
+   VkImageView simage_view;//old iview after storage replacement/rebind
+   uint32_t hash;
+   struct zink_batch_usage batch_uses;
+   struct util_dynarray framebuffer_refs;
 };
 
 static inline struct zink_surface *
@@ -44,4 +49,17 @@ zink_surface(struct pipe_surface *pipe)
 void
 zink_context_surface_init(struct pipe_context *context);
 
+VkImageViewCreateInfo
+create_ivci(struct zink_screen *screen,
+            struct zink_resource *res,
+            const struct pipe_surface *templ);
+
+struct pipe_surface *
+zink_get_surface(struct zink_context *ctx,
+            struct pipe_resource *pres,
+            const struct pipe_surface *templ,
+            VkImageViewCreateInfo *ivci);
+
+bool
+zink_rebind_surface(struct zink_context *ctx, struct pipe_surface *psurface);
 #endif
diff --git a/src/gallium/frontends/dri/dri_context.c b/src/gallium/frontends/dri/dri_context.c
index 39eae5f7587..6e53ca90ad4 100644
--- a/src/gallium/frontends/dri/dri_context.c
+++ b/src/gallium/frontends/dri/dri_context.c
@@ -192,9 +192,8 @@ dri_create_context(gl_api api, const struct gl_config * visual,
    ctx->stapi = stapi;
 
    if (ctx->st->cso_context) {
-      ctx->pp = pp_init(ctx->st->pipe, screen->pp_enabled, ctx->st->cso_context,
-                        ctx->st);
-      ctx->hud = hud_create(ctx->st->cso_context, ctx->st,
+      ctx->pp = pp_init(ctx->st->pipe, screen->pp_enabled, ctx->st->cso_context);
+      ctx->hud = hud_create(ctx->st->cso_context,
                             share_ctx ? share_ctx->hud : NULL);
    }
 
diff --git a/src/gallium/frontends/dri/dri_screen.c b/src/gallium/frontends/dri/dri_screen.c
index c117f593d1a..20ee86f52b7 100644
--- a/src/gallium/frontends/dri/dri_screen.c
+++ b/src/gallium/frontends/dri/dri_screen.c
@@ -163,12 +163,6 @@ dri_fill_in_modes(struct dri_screen *screen)
 
       /* Required by Android, for HAL_PIXEL_FORMAT_RGBX_8888. */
       MESA_FORMAT_R8G8B8X8_UNORM,
-
-      /* Required by Android, for HAL_PIXEL_FORMAT_RGBA_8888. */
-      MESA_FORMAT_R8G8B8A8_SRGB,
-
-      /* Required by Android, for HAL_PIXEL_FORMAT_RGBX_8888. */
-      MESA_FORMAT_R8G8B8X8_SRGB,
    };
    static const enum pipe_format pipe_formats[] = {
       PIPE_FORMAT_B10G10R10A2_UNORM,
@@ -184,8 +178,6 @@ dri_fill_in_modes(struct dri_screen *screen)
       PIPE_FORMAT_R16G16B16X16_FLOAT,
       PIPE_FORMAT_RGBA8888_UNORM,
       PIPE_FORMAT_RGBX8888_UNORM,
-      PIPE_FORMAT_RGBA8888_SRGB,
-      PIPE_FORMAT_RGBX8888_SRGB,
    };
    mesa_format format;
    __DRIconfig **configs = NULL;
@@ -276,9 +268,7 @@ dri_fill_in_modes(struct dri_screen *screen)
       /* Expose only BGRA ordering if the loader doesn't support RGBA ordering. */
       if (!allow_rgba_ordering &&
           (mesa_formats[format] == MESA_FORMAT_R8G8B8A8_UNORM ||
-           mesa_formats[format] == MESA_FORMAT_R8G8B8X8_UNORM ||
-           mesa_formats[format] == MESA_FORMAT_R8G8B8A8_SRGB  ||
-           mesa_formats[format] == MESA_FORMAT_R8G8B8X8_SRGB))
+           mesa_formats[format] == MESA_FORMAT_R8G8B8X8_UNORM))
          continue;
 
       if (!allow_rgb10 &&
diff --git a/src/gallium/frontends/glx/xlib/xm_api.c b/src/gallium/frontends/glx/xlib/xm_api.c
index 458d7d42696..c3ce20faba4 100644
--- a/src/gallium/frontends/glx/xlib/xm_api.c
+++ b/src/gallium/frontends/glx/xlib/xm_api.c
@@ -1029,7 +1029,7 @@ XMesaContext XMesaCreateContext( XMesaVisual v, XMesaContext share_list,
 
    c->st->st_manager_private = (void *) c;
 
-   c->hud = hud_create(c->st->cso_context, c->st, NULL);
+   c->hud = hud_create(c->st->cso_context, NULL);
 
    return c;
 
diff --git a/src/gallium/frontends/lavapipe/lvp_device.c b/src/gallium/frontends/lavapipe/lvp_device.c
index 3d05e424278..93426cea1f6 100644
--- a/src/gallium/frontends/lavapipe/lvp_device.c
+++ b/src/gallium/frontends/lavapipe/lvp_device.c
@@ -52,6 +52,8 @@ lvp_physical_device_init(struct lvp_physical_device *device,
    if (!device->pscreen)
       return vk_error(instance, VK_ERROR_OUT_OF_HOST_MEMORY);
 
+   fprintf(stderr, "WARNING: lavapipe is not a conformant vulkan implementation, testing use only.\n");
+
    device->max_images = device->pscreen->get_shader_param(device->pscreen, PIPE_SHADER_FRAGMENT, PIPE_SHADER_CAP_MAX_SHADER_IMAGES);
    lvp_physical_device_get_supported_extensions(device, &device->supported_extensions);
    result = lvp_init_wsi(device);
@@ -631,19 +633,6 @@ void lvp_GetPhysicalDeviceProperties2(
    }
 }
 
-static void lvp_get_physical_device_queue_family_properties(
-   VkQueueFamilyProperties*                    pQueueFamilyProperties)
-{
-   *pQueueFamilyProperties = (VkQueueFamilyProperties) {
-      .queueFlags = VK_QUEUE_GRAPHICS_BIT |
-      VK_QUEUE_COMPUTE_BIT |
-      VK_QUEUE_TRANSFER_BIT,
-      .queueCount = 1,
-      .timestampValidBits = 64,
-      .minImageTransferGranularity = (VkExtent3D) { 1, 1, 1 },
-   };
-}
-
 void lvp_GetPhysicalDeviceQueueFamilyProperties(
    VkPhysicalDevice                            physicalDevice,
    uint32_t*                                   pCount,
@@ -655,21 +644,15 @@ void lvp_GetPhysicalDeviceQueueFamilyProperties(
    }
 
    assert(*pCount >= 1);
-   lvp_get_physical_device_queue_family_properties(pQueueFamilyProperties);
-}
-
-void lvp_GetPhysicalDeviceQueueFamilyProperties2(
-   VkPhysicalDevice                            physicalDevice,
-   uint32_t*                                   pCount,
-   VkQueueFamilyProperties2                   *pQueueFamilyProperties)
-{
-   if (pQueueFamilyProperties == NULL) {
-      *pCount = 1;
-      return;
-   }
 
-   assert(*pCount >= 1);
-   lvp_get_physical_device_queue_family_properties(&pQueueFamilyProperties->queueFamilyProperties);
+   *pQueueFamilyProperties = (VkQueueFamilyProperties) {
+      .queueFlags = VK_QUEUE_GRAPHICS_BIT |
+      VK_QUEUE_COMPUTE_BIT |
+      VK_QUEUE_TRANSFER_BIT,
+      .queueCount = 1,
+      .timestampValidBits = 64,
+      .minImageTransferGranularity = (VkExtent3D) { 1, 1, 1 },
+   };
 }
 
 void lvp_GetPhysicalDeviceMemoryProperties(
@@ -692,14 +675,6 @@ void lvp_GetPhysicalDeviceMemoryProperties(
    };
 }
 
-void lvp_GetPhysicalDeviceMemoryProperties2(
-   VkPhysicalDevice                            physicalDevice,
-   VkPhysicalDeviceMemoryProperties2          *pMemoryProperties)
-{
-   lvp_GetPhysicalDeviceMemoryProperties(physicalDevice,
-                                         &pMemoryProperties->memoryProperties);
-}
-
 PFN_vkVoidFunction lvp_GetInstanceProcAddr(
    VkInstance                                  _instance,
    const char*                                 pName)
@@ -905,8 +880,6 @@ VkResult lvp_CreateDevice(
    const VkAllocationCallbacks*                pAllocator,
    VkDevice*                                   pDevice)
 {
-   fprintf(stderr, "WARNING: lavapipe is not a conformant vulkan implementation, testing use only.\n");
-
    LVP_FROM_HANDLE(lvp_physical_device, physical_device, physicalDevice);
    struct lvp_device *device;
 
diff --git a/src/gallium/frontends/nine/device9.c b/src/gallium/frontends/nine/device9.c
index badf65258b5..30791ab914a 100644
--- a/src/gallium/frontends/nine/device9.c
+++ b/src/gallium/frontends/nine/device9.c
@@ -233,7 +233,7 @@ NineDevice9_ctor( struct NineDevice9 *This,
     if (!This->cso_sw) { return E_OUTOFMEMORY; }
 
     /* Create first, it messes up our state. */
-    This->hud = hud_create(This->context.cso, NULL, NULL); /* NULL result is fine */
+    This->hud = hud_create(This->context.cso, NULL); /* NULL result is fine */
 
     /* Available memory counter. Updated only for allocations with this device
      * instance. This is the Win 7 behavior.
diff --git a/src/gallium/frontends/nine/nine_state.c b/src/gallium/frontends/nine/nine_state.c
index 6bbdd032f47..e99c2a082d7 100644
--- a/src/gallium/frontends/nine/nine_state.c
+++ b/src/gallium/frontends/nine/nine_state.c
@@ -961,7 +961,6 @@ static void
 update_textures_and_samplers(struct NineDevice9 *device)
 {
     struct nine_context *context = &device->context;
-    struct pipe_context *pipe = context->pipe;
     struct pipe_sampler_view *view[NINE_MAX_SAMPLERS];
     unsigned num_textures;
     unsigned i;
@@ -1013,7 +1012,7 @@ update_textures_and_samplers(struct NineDevice9 *device)
         context->bound_samplers_mask_ps |= (1 << s);
     }
 
-    pipe->set_sampler_views(pipe, PIPE_SHADER_FRAGMENT, 0, num_textures, view);
+    cso_set_sampler_views(context->cso, PIPE_SHADER_FRAGMENT, num_textures, view);
 
     if (commit_samplers)
         cso_single_sampler_done(context->cso, PIPE_SHADER_FRAGMENT);
@@ -1061,7 +1060,7 @@ update_textures_and_samplers(struct NineDevice9 *device)
         context->bound_samplers_mask_vs |= (1 << i);
     }
 
-    pipe->set_sampler_views(pipe, PIPE_SHADER_VERTEX, 0, num_textures, view);
+    cso_set_sampler_views(context->cso, PIPE_SHADER_VERTEX, num_textures, view);
 
     if (commit_samplers)
         cso_single_sampler_done(context->cso, PIPE_SHADER_VERTEX);
@@ -2913,8 +2912,8 @@ nine_context_clear(struct NineDevice9 *device)
     cso_set_samplers(cso, PIPE_SHADER_VERTEX, 0, NULL);
     cso_set_samplers(cso, PIPE_SHADER_FRAGMENT, 0, NULL);
 
-    pipe->set_sampler_views(pipe, PIPE_SHADER_VERTEX, 0, NINE_MAX_SAMPLERS_VS, NULL);
-    pipe->set_sampler_views(pipe, PIPE_SHADER_FRAGMENT, 0, NINE_MAX_SAMPLERS_PS, NULL);
+    cso_set_sampler_views(cso, PIPE_SHADER_VERTEX, 0, NULL);
+    cso_set_sampler_views(cso, PIPE_SHADER_FRAGMENT, 0, NULL);
 
     pipe->set_vertex_buffers(pipe, 0, device->caps.MaxStreams, NULL);
 
diff --git a/src/gallium/frontends/osmesa/osmesa.c b/src/gallium/frontends/osmesa/osmesa.c
index 38ac4b7a52f..7d06c8d9df9 100644
--- a/src/gallium/frontends/osmesa/osmesa.c
+++ b/src/gallium/frontends/osmesa/osmesa.c
@@ -820,8 +820,7 @@ OSMesaMakeCurrent(OSMesaContext osmesa, void *buffer, GLenum type,
       if (any_pp_enabled) {
          osmesa->pp = pp_init(osmesa->stctx->pipe,
                               osmesa->pp_enabled,
-                              osmesa->stctx->cso_context,
-                              osmesa->stctx);
+                              osmesa->stctx->cso_context);
 
          pp_init_fbos(osmesa->pp, width, height);
       }
diff --git a/src/gallium/frontends/va/image.c b/src/gallium/frontends/va/image.c
index 76cb403ed99..7a0d391e470 100644
--- a/src/gallium/frontends/va/image.c
+++ b/src/gallium/frontends/va/image.c
@@ -200,7 +200,6 @@ vlVaDeriveImage(VADriverContextP ctx, VASurfaceID surface, VAImage *image)
    vlVaSurface *surf;
    vlVaBuffer *img_buf;
    VAImage *img;
-   VAStatus status;
    struct pipe_screen *screen;
    struct pipe_surface **surfaces;
    struct pipe_video_buffer *new_buffer = NULL;
@@ -244,9 +243,10 @@ vlVaDeriveImage(VADriverContextP ctx, VASurfaceID surface, VAImage *image)
          if ((strcmp(derive_interlaced_allowlist[i], proc) == 0))
             break;
 
-      if (i >= ARRAY_SIZE(derive_interlaced_allowlist) ||
-          !screen->get_video_param(screen, PIPE_VIDEO_PROFILE_UNKNOWN,
-                                   PIPE_VIDEO_ENTRYPOINT_BITSTREAM,
+      if (i >= ARRAY_SIZE(derive_interlaced_allowlist))
+         return VA_STATUS_ERROR_OPERATION_FAILED;
+
+      if (!screen->get_video_param(screen, PIPE_VIDEO_PROFILE_UNKNOWN, PIPE_VIDEO_ENTRYPOINT_BITSTREAM,
                                    PIPE_VIDEO_CAP_SUPPORTS_PROGRESSIVE))
          return VA_STATUS_ERROR_OPERATION_FAILED;
    }
@@ -318,8 +318,9 @@ vlVaDeriveImage(VADriverContextP ctx, VASurfaceID surface, VAImage *image)
 
          /* not all devices support non-interlaced buffers */
          if (!new_buffer) {
-            status = VA_STATUS_ERROR_OPERATION_FAILED;
-            goto exit_on_error;
+            FREE(img);
+            mtx_unlock(&drv->mutex);
+            return VA_STATUS_ERROR_OPERATION_FAILED;
          }
 
          /* convert the interlaced to the progressive */
@@ -356,14 +357,16 @@ vlVaDeriveImage(VADriverContextP ctx, VASurfaceID surface, VAImage *image)
    default:
       /* VaDeriveImage only supports contiguous planes. But there is now a
          more generic api vlVaExportSurfaceHandle. */
-      status = VA_STATUS_ERROR_OPERATION_FAILED;
-      goto exit_on_error;
+      FREE(img);
+      mtx_unlock(&drv->mutex);
+      return VA_STATUS_ERROR_OPERATION_FAILED;
    }
 
    img_buf = CALLOC(1, sizeof(vlVaBuffer));
    if (!img_buf) {
-      status = VA_STATUS_ERROR_ALLOCATION_FAILED;
-      goto exit_on_error;
+      FREE(img);
+      mtx_unlock(&drv->mutex);
+      return VA_STATUS_ERROR_ALLOCATION_FAILED;
    }
 
    img->image_id = handle_table_add(drv->htab, img);
@@ -381,11 +384,6 @@ vlVaDeriveImage(VADriverContextP ctx, VASurfaceID surface, VAImage *image)
    *image = *img;
 
    return VA_STATUS_SUCCESS;
-
-exit_on_error:
-   FREE(img);
-   mtx_unlock(&drv->mutex);
-   return status;
 }
 
 VAStatus
diff --git a/src/gallium/frontends/wgl/stw_context.c b/src/gallium/frontends/wgl/stw_context.c
index 9a909ebfcc0..8f7150eb6df 100644
--- a/src/gallium/frontends/wgl/stw_context.c
+++ b/src/gallium/frontends/wgl/stw_context.c
@@ -278,7 +278,7 @@ stw_create_context_attribs(HDC hdc, INT iLayerPlane, DHGLRC hShareContext,
    ctx->st->st_manager_private = (void *) ctx;
 
    if (ctx->st->cso_context) {
-      ctx->hud = hud_create(ctx->st->cso_context, ctx->st, NULL);
+      ctx->hud = hud_create(ctx->st->cso_context, NULL);
    }
 
    stw_lock_contexts(stw_dev);
diff --git a/src/gallium/frontends/xa/xa_composite.c b/src/gallium/frontends/xa/xa_composite.c
index 45bc031e8c0..34d78027e27 100644
--- a/src/gallium/frontends/xa/xa_composite.c
+++ b/src/gallium/frontends/xa/xa_composite.c
@@ -504,8 +504,8 @@ bind_samplers(struct xa_context *ctx,
 
     cso_set_samplers(ctx->cso, PIPE_SHADER_FRAGMENT, num_samplers,
 		     (const struct pipe_sampler_state **)samplers);
-    pipe->set_sampler_views(pipe, PIPE_SHADER_FRAGMENT, 0, num_samplers,
-                            ctx->bound_sampler_views);
+    cso_set_sampler_views(ctx->cso, PIPE_SHADER_FRAGMENT, num_samplers,
+				   ctx->bound_sampler_views);
     ctx->num_bound_samplers = num_samplers;
 }
 
diff --git a/src/gallium/frontends/xa/xa_context.c b/src/gallium/frontends/xa/xa_context.c
index dc9586dd4b0..8593644fb09 100644
--- a/src/gallium/frontends/xa/xa_context.c
+++ b/src/gallium/frontends/xa/xa_context.c
@@ -327,7 +327,7 @@ xa_solid_prepare(struct xa_context *ctx, struct xa_surface *dst,
     renderer_bind_destination(ctx, ctx->srf);
     bind_solid_blend_state(ctx);
     cso_set_samplers(ctx->cso, PIPE_SHADER_FRAGMENT, 0, NULL);
-    ctx->pipe->set_sampler_views(ctx->pipe, PIPE_SHADER_FRAGMENT, 0, XA_MAX_SAMPLERS, NULL);
+    cso_set_sampler_views(ctx->cso, PIPE_SHADER_FRAGMENT, 0, NULL);
 
     shader = xa_shaders_get(ctx->shaders, vs_traits, fs_traits);
     cso_set_vertex_shader_handle(ctx->cso, shader.vs);
diff --git a/src/gallium/frontends/xa/xa_renderer.c b/src/gallium/frontends/xa/xa_renderer.c
index 91d129b0338..89548ad7019 100644
--- a/src/gallium/frontends/xa/xa_renderer.c
+++ b/src/gallium/frontends/xa/xa_renderer.c
@@ -442,7 +442,7 @@ renderer_copy_prepare(struct xa_context *r,
 	u_sampler_view_default_template(&templ,
 					src_texture, src_texture->format);
 	src_view = pipe->create_sampler_view(pipe, src_texture, &templ);
-	pipe->set_sampler_views(pipe, PIPE_SHADER_FRAGMENT, 0, 1, &src_view);
+	cso_set_sampler_views(r->cso, PIPE_SHADER_FRAGMENT, 1, &src_view);
 	pipe_sampler_view_reference(&src_view, NULL);
     }
 
diff --git a/src/gallium/frontends/xa/xa_yuv.c b/src/gallium/frontends/xa/xa_yuv.c
index ce49aa23d8b..97a1833ff15 100644
--- a/src/gallium/frontends/xa/xa_yuv.c
+++ b/src/gallium/frontends/xa/xa_yuv.c
@@ -93,7 +93,7 @@ xa_yuv_bind_samplers(struct xa_context *r, struct xa_surface *yuv[])
     }
     r->num_bound_samplers = 3;
     cso_set_samplers(r->cso, PIPE_SHADER_FRAGMENT, 3, (const struct pipe_sampler_state **)samplers);
-    r->pipe->set_sampler_views(r->pipe, PIPE_SHADER_FRAGMENT, 0, 3, r->bound_sampler_views);
+    cso_set_sampler_views(r->cso, PIPE_SHADER_FRAGMENT, 3, r->bound_sampler_views);
 }
 
 static void
diff --git a/src/gallium/include/frontend/api.h b/src/gallium/include/frontend/api.h
index 70afed3499a..b0cafc7073c 100644
--- a/src/gallium/include/frontend/api.h
+++ b/src/gallium/include/frontend/api.h
@@ -146,15 +146,6 @@ enum st_attachment_type {
 #define ST_FLUSH_WAIT                     (1 << 2)
 #define ST_FLUSH_FENCE_FD                 (1 << 3)
 
-/**
- * State invalidation flags to notify frontends that states have been changed
- * behind their back.
- */
-#define ST_INVALIDATE_FS_SAMPLER_VIEWS    (1 << 0)
-#define ST_INVALIDATE_FS_CONSTBUF0        (1 << 1)
-#define ST_INVALIDATE_VS_CONSTBUF0        (1 << 2)
-#define ST_INVALIDATE_VERTEX_BUFFERS      (1 << 3)
-
 /**
  * Value to st_manager->get_param function.
  */
@@ -439,12 +430,6 @@ struct st_context_iface
     * Called from the main thread.
     */
    void (*thread_finish)(struct st_context_iface *stctxi);
-
-   /**
-    * Invalidate states to notify the frontend that states have been changed
-    * behind its back.
-    */
-   void (*invalidate_state)(struct st_context_iface *stctxi, unsigned flags);
 };
 
 
diff --git a/src/gallium/include/pipe/p_defines.h b/src/gallium/include/pipe/p_defines.h
index 3b984b716ca..aa219f9a577 100644
--- a/src/gallium/include/pipe/p_defines.h
+++ b/src/gallium/include/pipe/p_defines.h
@@ -977,6 +977,8 @@ enum pipe_cap
    PIPE_CAP_SHADER_ATOMIC_INT64,
    PIPE_CAP_DEVICE_PROTECTED_CONTENT,
    PIPE_CAP_PREFER_REAL_BUFFER_IN_CONSTBUF0,
+   PIPE_CAP_NO_DITHERING,
+   PIPE_CAP_EMULATE_ARGB,
 };
 
 /**
diff --git a/src/gallium/targets/haiku-softpipe/GalliumContext.cpp b/src/gallium/targets/haiku-softpipe/GalliumContext.cpp
index 760381a24fd..15eea948d07 100644
--- a/src/gallium/targets/haiku-softpipe/GalliumContext.cpp
+++ b/src/gallium/targets/haiku-softpipe/GalliumContext.cpp
@@ -222,7 +222,7 @@ GalliumContext::CreateContext(HGLWinsysContext *wsContext)
 	// Init Gallium3D Post Processing
 	// TODO: no pp filters are enabled yet through postProcessEnable
 	context->postProcess = pp_init(stContext->pipe, context->postProcessEnable,
-		stContext->cso_context, &stContext->iface);
+		stContext->cso_context);
 
 	context_id contextNext = -1;
 	Lock();
diff --git a/src/gallium/tests/trivial/quad-tex.c b/src/gallium/tests/trivial/quad-tex.c
index cc4e011d7a9..fe345a94ae7 100644
--- a/src/gallium/tests/trivial/quad-tex.c
+++ b/src/gallium/tests/trivial/quad-tex.c
@@ -322,7 +322,7 @@ static void draw(struct program *p)
 	cso_set_samplers(p->cso, PIPE_SHADER_FRAGMENT, 1, samplers);
 
 	/* texture sampler view */
-	p->pipe->set_sampler_views(p->pipe, PIPE_SHADER_FRAGMENT, 0, 1, &p->view);
+	cso_set_sampler_views(p->cso, PIPE_SHADER_FRAGMENT, 1, &p->view);
 
 	/* shaders */
 	cso_set_fragment_shader_handle(p->cso, p->fs);
diff --git a/src/gallium/winsys/virgl/drm/virgl_drm_winsys.c b/src/gallium/winsys/virgl/drm/virgl_drm_winsys.c
index 2be575fb21d..43f21566ec3 100644
--- a/src/gallium/winsys/virgl/drm/virgl_drm_winsys.c
+++ b/src/gallium/winsys/virgl/drm/virgl_drm_winsys.c
@@ -62,8 +62,7 @@ static inline boolean can_cache_resource(uint32_t bind)
           bind == VIRGL_BIND_INDEX_BUFFER ||
           bind == VIRGL_BIND_VERTEX_BUFFER ||
           bind == VIRGL_BIND_CUSTOM ||
-          bind == VIRGL_BIND_STAGING ||
-          bind == VIRGL_BIND_DEPTH_STENCIL;
+          bind == VIRGL_BIND_STAGING;
 }
 
 static void virgl_hw_res_destroy(struct virgl_drm_winsys *qdws,
diff --git a/src/intel/common/gen_mi_builder.h b/src/intel/common/gen_mi_builder.h
index 5b801705e1a..ddd8459ef07 100644
--- a/src/intel/common/gen_mi_builder.h
+++ b/src/intel/common/gen_mi_builder.h
@@ -369,56 +369,21 @@ _gen_mi_copy_no_unref(struct gen_mi_builder *b,
 
    case GEN_MI_VALUE_TYPE_MEM64:
    case GEN_MI_VALUE_TYPE_REG64:
+      /* If the destination is 64 bits, we have to copy in two halves */
+      _gen_mi_copy_no_unref(b, gen_mi_value_half(dst, false),
+                               gen_mi_value_half(src, false));
       switch (src.type) {
       case GEN_MI_VALUE_TYPE_IMM:
-         if (dst.type == GEN_MI_VALUE_TYPE_REG64) {
-            uint32_t *dw = (uint32_t *)__gen_get_batch_dwords(b->user_data,
-                                                              GENX(MI_LOAD_REGISTER_IMM_length) + 2);
-            gen_mi_builder_pack(b, GENX(MI_LOAD_REGISTER_IMM), dw, lri) {
-               lri.DWordLength = GENX(MI_LOAD_REGISTER_IMM_length) + 2 -
-                                 GENX(MI_LOAD_REGISTER_IMM_length_bias);
-            }
-            dw[1] = dst.reg;
-            dw[2] = src.imm;
-            dw[3] = dst.reg + 4;
-            dw[4] = src.imm >> 32;
-         } else {
-#if GEN_GEN >= 8
-            assert(dst.type == GEN_MI_VALUE_TYPE_MEM64);
-            uint32_t *dw = (uint32_t *)__gen_get_batch_dwords(b->user_data,
-                                                              GENX(MI_STORE_DATA_IMM_length) + 1);
-            gen_mi_builder_pack(b, GENX(MI_STORE_DATA_IMM), dw, sdm) {
-               sdm.DWordLength = GENX(MI_STORE_DATA_IMM_length) + 1 -
-                                 GENX(MI_STORE_DATA_IMM_length_bias);
-               sdm.StoreQword = true;
-               sdm.Address = dst.addr;
-            }
-            dw[3] = src.imm;
-            dw[4] = src.imm >> 32;
-#else
-         _gen_mi_copy_no_unref(b, gen_mi_value_half(dst, false),
-                                  gen_mi_value_half(src, false));
+      case GEN_MI_VALUE_TYPE_MEM64:
+      case GEN_MI_VALUE_TYPE_REG64:
+         /* TODO: Use MI_STORE_DATA_IMM::StoreQWord when we have it */
          _gen_mi_copy_no_unref(b, gen_mi_value_half(dst, true),
                                   gen_mi_value_half(src, true));
-#endif
-         }
          break;
-      case GEN_MI_VALUE_TYPE_REG32:
-      case GEN_MI_VALUE_TYPE_MEM32:
-         _gen_mi_copy_no_unref(b, gen_mi_value_half(dst, false),
-                                  gen_mi_value_half(src, false));
+      default:
          _gen_mi_copy_no_unref(b, gen_mi_value_half(dst, true),
                                   gen_mi_imm(0));
          break;
-      case GEN_MI_VALUE_TYPE_REG64:
-      case GEN_MI_VALUE_TYPE_MEM64:
-         _gen_mi_copy_no_unref(b, gen_mi_value_half(dst, false),
-                                  gen_mi_value_half(src, false));
-         _gen_mi_copy_no_unref(b, gen_mi_value_half(dst, true),
-                                  gen_mi_value_half(src, true));
-         break;
-      default:
-         unreachable("Invalid gen_mi_value type");
       }
       break;
 
@@ -967,13 +932,6 @@ gen_mi_store_address(struct gen_mi_builder *b,
 static inline void
 gen_mi_self_mod_barrier(struct gen_mi_builder *b)
 {
-   /* First make sure all the memory writes from previous modifying commands
-    * have landed. We want to do this before going through the CS cache,
-    * otherwise we could be fetching memory that hasn't been written to yet.
-    */
-   gen_mi_builder_emit(b, GENX(PIPE_CONTROL), pc) {
-      pc.CommandStreamerStallEnable = true;
-   }
    /* Documentation says Gen11+ should be able to invalidate the command cache
     * but experiment show it doesn't work properly, so for now just get over
     * the CS prefetch.
diff --git a/src/intel/compiler/brw_compiler.h b/src/intel/compiler/brw_compiler.h
index 0721a24f893..bfec4e8f0db 100644
--- a/src/intel/compiler/brw_compiler.h
+++ b/src/intel/compiler/brw_compiler.h
@@ -1581,19 +1581,6 @@ void brw_debug_key_recompile(const struct brw_compiler *c, void *log,
                              const struct brw_base_prog_key *old_key,
                              const struct brw_base_prog_key *key);
 
-/* Shared Local Memory Size is specified as powers of two,
- * and also have a Gen-dependent minimum value if not zero.
- */
-static inline uint32_t
-calculate_gen_slm_size(unsigned gen, uint32_t bytes)
-{
-   assert(bytes <= 64 * 1024);
-   if (bytes > 0)
-      return MAX2(util_next_power_of_two(bytes), gen >= 9 ? 1024 : 4096);
-   else
-      return 0;
-}
-
 static inline uint32_t
 encode_slm_size(unsigned gen, uint32_t bytes)
 {
@@ -1608,19 +1595,18 @@ encode_slm_size(unsigned gen, uint32_t bytes)
     * -------------------------------------------------------------------
     * Gen9+  |    0 |    1 |    2 |    3 |    4 |     5 |     6 |     7 |
     */
+   assert(bytes <= 64 * 1024);
 
    if (bytes > 0) {
-      slm_size = calculate_gen_slm_size(gen, bytes);
-      assert(util_is_power_of_two_nonzero(slm_size));
+      /* Shared Local Memory Size is specified as powers of two. */
+      slm_size = util_next_power_of_two(bytes);
 
       if (gen >= 9) {
-         /* Turn an exponent of 10 (1024 kB) into 1. */
-         assert(slm_size >= 1024);
-         slm_size = ffs(slm_size) - 10;
+         /* Use a minimum of 1kB; turn an exponent of 10 (1024 kB) into 1. */
+         slm_size = ffs(MAX2(slm_size, 1024)) - 10;
       } else {
-         assert(slm_size >= 4096);
-         /* Convert to the pre-Gen9 representation. */
-         slm_size = slm_size / 4096;
+         /* Use a minimum of 4kB; convert to the pre-Gen9 representation. */
+         slm_size = MAX2(slm_size, 4096) / 4096;
       }
    }
 
diff --git a/src/intel/dev/gen_device_info.c b/src/intel/dev/gen_device_info.c
index 24225156465..5e26dc10b8c 100644
--- a/src/intel/dev/gen_device_info.c
+++ b/src/intel/dev/gen_device_info.c
@@ -764,7 +764,6 @@ static const struct gen_device_info gen_device_info_cfl_gt1 = {
     * leading to some vertices to go missing if we use too much URB.
     */
    .urb.max_entries[MESA_SHADER_VERTEX] = 928,
-   .urb.max_entries[MESA_SHADER_GEOMETRY] = 256,
    .simulator_id = 24,
 };
 static const struct gen_device_info gen_device_info_cfl_gt2 = {
diff --git a/src/intel/perf/gen_perf.c b/src/intel/perf/gen_perf.c
index 4530bb02e33..0b88fb2c594 100644
--- a/src/intel/perf/gen_perf.c
+++ b/src/intel/perf/gen_perf.c
@@ -1088,35 +1088,6 @@ gen_perf_query_result_accumulate(struct gen_perf_query_result *result,
 
 }
 
-#define GET_FIELD(word, field) (((word)  & field ## _MASK) >> field ## _SHIFT)
-
-void
-gen_perf_query_result_read_gt_frequency(struct gen_perf_query_result *result,
-                                        const struct gen_device_info *devinfo,
-                                        const uint32_t start,
-                                        const uint32_t end)
-{
-   switch (devinfo->gen) {
-   case 7:
-   case 8:
-      result->gt_frequency[0] = GET_FIELD(start, GEN7_RPSTAT1_CURR_GT_FREQ) * 50ULL;
-      result->gt_frequency[1] = GET_FIELD(end, GEN7_RPSTAT1_CURR_GT_FREQ) * 50ULL;
-      break;
-   case 9:
-   case 11:
-   case 12:
-      result->gt_frequency[0] = GET_FIELD(start, GEN9_RPSTAT0_CURR_GT_FREQ) * 50ULL / 3ULL;
-      result->gt_frequency[1] = GET_FIELD(end, GEN9_RPSTAT0_CURR_GT_FREQ) * 50ULL / 3ULL;
-      break;
-   default:
-      unreachable("unexpected gen");
-   }
-
-   /* Put the numbers into Hz. */
-   result->gt_frequency[0] *= 1000000ULL;
-   result->gt_frequency[1] *= 1000000ULL;
-}
-
 void
 gen_perf_query_result_clear(struct gen_perf_query_result *result)
 {
diff --git a/src/intel/perf/gen_perf.h b/src/intel/perf/gen_perf.h
index 4348c731109..790719ccbad 100644
--- a/src/intel/perf/gen_perf.h
+++ b/src/intel/perf/gen_perf.h
@@ -150,11 +150,6 @@ struct gen_perf_query_result {
     */
    uint64_t unslice_frequency[2];
 
-   /**
-    * Frequency of the whole GT at the begin and end of the query.
-    */
-   uint64_t gt_frequency[2];
-
    /**
     * Timestamp of the query.
     */
@@ -206,8 +201,6 @@ struct gen_perf_registers {
 };
 
 struct gen_perf_query_info {
-   struct gen_perf_config *perf;
-
    enum gen_perf_query_type {
       GEN_PERF_QUERY_TYPE_OA,
       GEN_PERF_QUERY_TYPE_RAW,
@@ -362,14 +355,6 @@ void gen_perf_query_result_read_frequencies(struct gen_perf_query_result *result
                                             const struct gen_device_info *devinfo,
                                             const uint32_t *start,
                                             const uint32_t *end);
-
-/** Store the GT frequency as reported by the RPSTAT register.
- */
-void gen_perf_query_result_read_gt_frequency(struct gen_perf_query_result *result,
-                                             const struct gen_device_info *devinfo,
-                                             const uint32_t start,
-                                             const uint32_t end);
-
 /** Accumulate the delta between 2 OA reports into result for a given query.
  */
 void gen_perf_query_result_accumulate(struct gen_perf_query_result *result,
@@ -404,26 +389,6 @@ gen_perf_new(void *ctx)
    return perf;
 }
 
-/** Whether we have the ability to hold off preemption on a batch so we don't
- * have to look at the OA buffer to subtract unrelated workloads off the
- * values captured through MI_* commands.
- */
-static inline bool
-gen_perf_has_hold_preemption(const struct gen_perf_config *perf)
-{
-   return perf->i915_perf_version >= 3;
-}
-
-/** Whether we have the ability to lock EU array power configuration for the
- * duration of the performance recording. This is useful on Gen11 where the HW
- * architecture requires half the EU for particular workloads.
- */
-static inline bool
-gen_perf_has_global_sseu(const struct gen_perf_config *perf)
-{
-   return perf->i915_perf_version >= 4;
-}
-
 uint32_t gen_perf_get_n_passes(struct gen_perf_config *perf,
                                const uint32_t *counter_indices,
                                uint32_t counter_indices_count,
diff --git a/src/intel/perf/gen_perf.py b/src/intel/perf/gen_perf.py
index 94fd6f619f4..0d0aae90b5f 100644
--- a/src/intel/perf/gen_perf.py
+++ b/src/intel/perf/gen_perf.py
@@ -681,7 +681,6 @@ def main():
 
             c("struct gen_perf_query_info *query = rzalloc(perf, struct gen_perf_query_info);\n")
             c("\n")
-            c("query->perf = perf;\n")
             c("query->kind = GEN_PERF_QUERY_TYPE_OA;\n")
             c("query->name = \"" + set.name + "\";\n")
             c("query->symbol_name = \"" + set.symbol_name + "\";\n")
diff --git a/src/intel/perf/gen_perf_mdapi.c b/src/intel/perf/gen_perf_mdapi.c
index 2452b99f59f..19f1be28bbc 100644
--- a/src/intel/perf/gen_perf_mdapi.c
+++ b/src/intel/perf/gen_perf_mdapi.c
@@ -34,8 +34,8 @@
 int
 gen_perf_query_result_write_mdapi(void *data, uint32_t data_size,
                                   const struct gen_device_info *devinfo,
-                                  const struct gen_perf_query_info *query,
-                                  const struct gen_perf_query_result *result)
+                                  const struct gen_perf_query_result *result,
+                                  uint64_t freq_start, uint64_t freq_end)
 {
    switch (devinfo->gen) {
    case 7: {
@@ -57,8 +57,8 @@ gen_perf_query_result_write_mdapi(void *data, uint32_t data_size,
       mdapi_data->ReportsCount = result->reports_accumulated;
       mdapi_data->TotalTime =
          gen_device_info_timebase_scale(devinfo, result->accumulator[0]);
-      mdapi_data->CoreFrequency = result->gt_frequency[1];
-      mdapi_data->CoreFrequencyChanged = result->gt_frequency[1] != result->gt_frequency[0];
+      mdapi_data->CoreFrequency = freq_end;
+      mdapi_data->CoreFrequencyChanged = freq_end != freq_start;
       mdapi_data->SplitOccured = result->query_disjoint;
       return sizeof(*mdapi_data);
    }
@@ -82,8 +82,8 @@ gen_perf_query_result_write_mdapi(void *data, uint32_t data_size,
       mdapi_data->BeginTimestamp =
          gen_device_info_timebase_scale(devinfo, result->begin_timestamp);
       mdapi_data->GPUTicks = result->accumulator[1];
-      mdapi_data->CoreFrequency = result->gt_frequency[1];
-      mdapi_data->CoreFrequencyChanged = result->gt_frequency[1] != result->gt_frequency[0];
+      mdapi_data->CoreFrequency = freq_end;
+      mdapi_data->CoreFrequencyChanged = freq_end != freq_start;
       mdapi_data->SliceFrequency =
          (result->slice_frequency[0] + result->slice_frequency[1]) / 2ULL;
       mdapi_data->UnsliceFrequency =
@@ -113,8 +113,8 @@ gen_perf_query_result_write_mdapi(void *data, uint32_t data_size,
       mdapi_data->BeginTimestamp =
          gen_device_info_timebase_scale(devinfo, result->begin_timestamp);
       mdapi_data->GPUTicks = result->accumulator[1];
-      mdapi_data->CoreFrequency = result->gt_frequency[1];
-      mdapi_data->CoreFrequencyChanged = result->gt_frequency[1] != result->gt_frequency[0];
+      mdapi_data->CoreFrequency = freq_end;
+      mdapi_data->CoreFrequencyChanged = freq_end != freq_start;
       mdapi_data->SliceFrequency =
          (result->slice_frequency[0] + result->slice_frequency[1]) / 2ULL;
       mdapi_data->UnsliceFrequency =
diff --git a/src/intel/perf/gen_perf_mdapi.h b/src/intel/perf/gen_perf_mdapi.h
index acf1edd6e79..8be8d2033ac 100644
--- a/src/intel/perf/gen_perf_mdapi.h
+++ b/src/intel/perf/gen_perf_mdapi.h
@@ -129,8 +129,8 @@ struct mdapi_pipeline_metrics {
 
 int gen_perf_query_result_write_mdapi(void *data, uint32_t data_size,
                                       const struct gen_device_info *devinfo,
-                                      const struct gen_perf_query_info *query,
-                                      const struct gen_perf_query_result *result);
+                                      const struct gen_perf_query_result *result,
+                                      uint64_t freq_start, uint64_t freq_end);
 
 static inline void gen_perf_query_mdapi_write_perfcntr(void *data, uint32_t data_size,
                                                        const struct gen_device_info *devinfo,
diff --git a/src/intel/perf/gen_perf_private.h b/src/intel/perf/gen_perf_private.h
index acca9a2b48a..e3e877dd89e 100644
--- a/src/intel/perf/gen_perf_private.h
+++ b/src/intel/perf/gen_perf_private.h
@@ -76,8 +76,6 @@ gen_perf_append_query_info(struct gen_perf_config *perf, int max_counters)
    query = &perf->queries[perf->n_queries - 1];
    memset(query, 0, sizeof(*query));
 
-   query->perf = perf;
-
    if (max_counters > 0) {
       query->max_counters = max_counters;
       query->counters =
diff --git a/src/intel/perf/gen_perf_query.c b/src/intel/perf/gen_perf_query.c
index e6d38b6bb72..ab78e5fc693 100644
--- a/src/intel/perf/gen_perf_query.c
+++ b/src/intel/perf/gen_perf_query.c
@@ -218,6 +218,11 @@ struct gen_perf_query_object
           */
          bool results_accumulated;
 
+         /**
+          * Frequency of the GT at begin and end of the query.
+          */
+         uint64_t gt_frequency[2];
+
          /**
           * Accumulated OA results between begin and end of the query.
           */
@@ -343,6 +348,8 @@ gen_perf_close(struct gen_perf_context *perfquery,
    }
 }
 
+#define NUM_PERF_PROPERTIES(array) (ARRAY_SIZE(array) / 2)
+
 static bool
 gen_perf_open(struct gen_perf_context *perf_ctx,
               int metrics_set_id,
@@ -351,40 +358,28 @@ gen_perf_open(struct gen_perf_context *perf_ctx,
               int drm_fd,
               uint32_t ctx_id)
 {
-   uint64_t properties[DRM_I915_PERF_PROP_MAX * 2];
-   uint32_t p = 0;
-
-   /* Single context sampling */
-   properties[p++] = DRM_I915_PERF_PROP_CTX_HANDLE;
-   properties[p++] = ctx_id;
-
-   /* Include OA reports in samples */
-   properties[p++] = DRM_I915_PERF_PROP_SAMPLE_OA;
-   properties[p++] = true;
+   uint64_t properties[] = {
+      /* Single context sampling */
+      DRM_I915_PERF_PROP_CTX_HANDLE, ctx_id,
 
-   /* OA unit configuration */
-   properties[p++] = DRM_I915_PERF_PROP_OA_METRICS_SET;
-   properties[p++] = metrics_set_id;
+      /* Include OA reports in samples */
+      DRM_I915_PERF_PROP_SAMPLE_OA, true,
 
-   properties[p++] = DRM_I915_PERF_PROP_OA_FORMAT;
-   properties[p++] = report_format;
-
-   properties[p++] = DRM_I915_PERF_PROP_OA_EXPONENT;
-   properties[p++] = period_exponent;
-
-   /* SSEU configuration */
-   if (gen_perf_has_global_sseu(perf_ctx->perf)) {
-      properties[p++] = DRM_I915_PERF_PROP_GLOBAL_SSEU;
-      properties[p++] = to_user_pointer(&perf_ctx->perf->sseu);
-   }
-
-   assert(p <= ARRAY_SIZE(properties));
+      /* OA unit configuration */
+      DRM_I915_PERF_PROP_OA_METRICS_SET, metrics_set_id,
+      DRM_I915_PERF_PROP_OA_FORMAT, report_format,
+      DRM_I915_PERF_PROP_OA_EXPONENT, period_exponent,
 
+      /* SSEU configuration */
+      DRM_I915_PERF_PROP_GLOBAL_SSEU, to_user_pointer(&perf_ctx->perf->sseu),
+   };
    struct drm_i915_perf_open_param param = {
       .flags = I915_PERF_FLAG_FD_CLOEXEC |
                I915_PERF_FLAG_FD_NONBLOCK |
                I915_PERF_FLAG_DISABLED,
-      .num_properties = p / 2,
+      .num_properties = perf_ctx->perf->i915_perf_version >= 4 ?
+                        NUM_PERF_PROPERTIES(properties) :
+                        NUM_PERF_PROPERTIES(properties) - 1,
       .properties_ptr = (uintptr_t) properties,
    };
    int fd = gen_ioctl(drm_fd, DRM_IOCTL_I915_PERF_OPEN, &param);
@@ -1400,6 +1395,37 @@ gen_perf_delete_query(struct gen_perf_context *perf_ctx,
    free(query);
 }
 
+#define GET_FIELD(word, field) (((word)  & field ## _MASK) >> field ## _SHIFT)
+
+static void
+read_gt_frequency(struct gen_perf_context *perf_ctx,
+                  struct gen_perf_query_object *obj)
+{
+   const struct gen_device_info *devinfo = perf_ctx->devinfo;
+   uint32_t start = *((uint32_t *)(obj->oa.map + MI_FREQ_START_OFFSET_BYTES)),
+      end = *((uint32_t *)(obj->oa.map + MI_FREQ_END_OFFSET_BYTES));
+
+   switch (devinfo->gen) {
+   case 7:
+   case 8:
+      obj->oa.gt_frequency[0] = GET_FIELD(start, GEN7_RPSTAT1_CURR_GT_FREQ) * 50ULL;
+      obj->oa.gt_frequency[1] = GET_FIELD(end, GEN7_RPSTAT1_CURR_GT_FREQ) * 50ULL;
+      break;
+   case 9:
+   case 11:
+   case 12:
+      obj->oa.gt_frequency[0] = GET_FIELD(start, GEN9_RPSTAT0_CURR_GT_FREQ) * 50ULL / 3ULL;
+      obj->oa.gt_frequency[1] = GET_FIELD(end, GEN9_RPSTAT0_CURR_GT_FREQ) * 50ULL / 3ULL;
+      break;
+   default:
+      unreachable("unexpected gen");
+   }
+
+   /* Put the numbers into Hz. */
+   obj->oa.gt_frequency[0] *= 1000000ULL;
+   obj->oa.gt_frequency[1] *= 1000000ULL;
+}
+
 static int
 get_oa_counter_data(struct gen_perf_context *perf_ctx,
                     struct gen_perf_query_object *query,
@@ -1504,6 +1530,7 @@ gen_perf_get_query_data(struct gen_perf_context *perf_ctx,
          while (!read_oa_samples_for_query(perf_ctx, query, current_batch))
             ;
 
+         read_gt_frequency(perf_ctx, query);
          uint32_t *begin_report = query->oa.map;
          uint32_t *end_report = query->oa.map + MI_RPC_BO_END_OFFSET_BYTES;
          gen_perf_query_result_read_frequencies(&query->oa.result,
@@ -1522,8 +1549,9 @@ gen_perf_get_query_data(struct gen_perf_context *perf_ctx,
          const struct gen_device_info *devinfo = perf_ctx->devinfo;
 
          written = gen_perf_query_result_write_mdapi((uint8_t *)data, data_size,
-                                                     devinfo, query->queryinfo,
-                                                     &query->oa.result);
+                                                     devinfo, &query->oa.result,
+                                                     query->oa.gt_frequency[0],
+                                                     query->oa.gt_frequency[1]);
       }
       break;
 
diff --git a/src/intel/perf/gen_perf_regs.h b/src/intel/perf/gen_perf_regs.h
index f97e387e46b..1b54fe29d6d 100644
--- a/src/intel/perf/gen_perf_regs.h
+++ b/src/intel/perf/gen_perf_regs.h
@@ -39,13 +39,6 @@
 #define  GEN9_RPSTAT0_PREV_GT_FREQ_SHIFT   0
 #define  GEN9_RPSTAT0_PREV_GT_FREQ_MASK    INTEL_MASK(8, 0)
 
-/* Programmable perf 64bits counters (used for GTRequestQueueFull counter on
- * gen7-11)
- */
-#define PERF_CNT_1_DW0                     0x91b8
-#define PERF_CNT_2_DW0                     0x91c0
-#define PERF_CNT_VALUE_MASK                ((1ull << 44) - 1)
-
 /* Pipeline statistic counters */
 #define IA_VERTICES_COUNT          0x2310
 #define IA_PRIMITIVES_COUNT        0x2318
diff --git a/src/intel/vulkan/anv_descriptor_set.c b/src/intel/vulkan/anv_descriptor_set.c
index e3895a77234..a2cf5ddb053 100644
--- a/src/intel/vulkan/anv_descriptor_set.c
+++ b/src/intel/vulkan/anv_descriptor_set.c
@@ -236,7 +236,7 @@ anv_descriptor_requires_bindless(const struct anv_physical_device *pdevice,
       VK_DESCRIPTOR_BINDING_UPDATE_UNUSED_WHILE_PENDING_BIT_EXT |
       VK_DESCRIPTOR_BINDING_PARTIALLY_BOUND_BIT_EXT;
 
-   return (binding->flags & flags_requiring_bindless) != 0;
+   return anv_descriptor_supports_bindless(pdevice, binding, sampler) &&(binding->flags & flags_requiring_bindless) != 0;
 }
 
 void anv_GetDescriptorSetLayoutSupport(
diff --git a/src/intel/vulkan/anv_device.c b/src/intel/vulkan/anv_device.c
index 5638f30890c..bcc33923b2a 100644
--- a/src/intel/vulkan/anv_device.c
+++ b/src/intel/vulkan/anv_device.c
@@ -43,7 +43,6 @@
 #include "util/driconf.h"
 #include "git_sha1.h"
 #include "vk_util.h"
-#include "vk_deferred_operation.h"
 #include "common/gen_aux_map.h"
 #include "common/gen_defines.h"
 #include "common/gen_uuid.h"
@@ -4698,46 +4697,3 @@ void anv_GetPrivateDataEXT(
                                    objectType, objectHandle,
                                    privateDataSlot, pData);
 }
-
-VkResult anv_CreateDeferredOperationKHR(
-    VkDevice                                    _device,
-    const VkAllocationCallbacks*                pAllocator,
-    VkDeferredOperationKHR*                     pDeferredOperation)
-{
-   ANV_FROM_HANDLE(anv_device, device, _device);
-   return vk_create_deferred_operation(&device->vk, pAllocator,
-                                       pDeferredOperation);
-}
-
-void anv_DestroyDeferredOperationKHR(
-    VkDevice                                    _device,
-    VkDeferredOperationKHR                      operation,
-    const VkAllocationCallbacks*                pAllocator)
-{
-   ANV_FROM_HANDLE(anv_device, device, _device);
-   vk_destroy_deferred_operation(&device->vk, operation, pAllocator);
-}
-
-uint32_t anv_GetDeferredOperationMaxConcurrencyKHR(
-    VkDevice                                    _device,
-    VkDeferredOperationKHR                      operation)
-{
-   ANV_FROM_HANDLE(anv_device, device, _device);
-   return vk_get_deferred_operation_max_concurrency(&device->vk, operation);
-}
-
-VkResult anv_GetDeferredOperationResultKHR(
-    VkDevice                                    _device,
-    VkDeferredOperationKHR                      operation)
-{
-   ANV_FROM_HANDLE(anv_device, device, _device);
-   return vk_get_deferred_operation_result(&device->vk, operation);
-}
-
-VkResult anv_DeferredOperationJoinKHR(
-    VkDevice                                    _device,
-    VkDeferredOperationKHR                      operation)
-{
-   ANV_FROM_HANDLE(anv_device, device, _device);
-   return vk_deferred_operation_join(&device->vk, operation);
-}
diff --git a/src/intel/vulkan/anv_extensions.py b/src/intel/vulkan/anv_extensions.py
index 35744a4fd55..93aa2e406eb 100644
--- a/src/intel/vulkan/anv_extensions.py
+++ b/src/intel/vulkan/anv_extensions.py
@@ -59,7 +59,6 @@ EXTENSIONS = [
     Extension('VK_KHR_copy_commands2',                    1, True),
     Extension('VK_KHR_create_renderpass2',                1, True),
     Extension('VK_KHR_dedicated_allocation',              3, True),
-    Extension('VK_KHR_deferred_host_operations',          1, True),
     Extension('VK_KHR_depth_stencil_resolve',             1, True),
     Extension('VK_KHR_descriptor_update_template',        1, True),
     Extension('VK_KHR_device_group',                      4, True),
@@ -91,7 +90,7 @@ EXTENSIONS = [
     Extension('VK_KHR_multiview',                         1, True),
     Extension('VK_KHR_performance_query',                 1,
               'device->use_softpin && device->perf && ' +
-              '(device->perf->i915_perf_version >= 3 || INTEL_DEBUG & DEBUG_NO_OACONFIG) && ' +
+              'device->perf->i915_perf_version >= 3 && ' +
               'device->use_call_secondary'),
     Extension('VK_KHR_pipeline_executable_properties',    1, True),
     Extension('VK_KHR_push_descriptor',                   2, True),
diff --git a/src/intel/vulkan/anv_image.c b/src/intel/vulkan/anv_image.c
index 21c5d5c3eef..fcebe0f770f 100644
--- a/src/intel/vulkan/anv_image.c
+++ b/src/intel/vulkan/anv_image.c
@@ -733,25 +733,6 @@ choose_drm_format_mod(const struct anv_physical_device *device,
       return NULL;
 }
 
-static VkImageUsageFlags
-anv_image_create_usage(const VkImageCreateInfo *pCreateInfo,
-                       VkImageUsageFlags usage)
-{
-   /* Add TRANSFER_SRC usage for multisample attachment images. This is
-    * because we might internally use the TRANSFER_SRC layout on them for
-    * blorp operations associated with resolving those into other attachments
-    * at the end of a subpass.
-    *
-    * Without this additional usage, we compute an incorrect AUX state in
-    * anv_layout_to_aux_state().
-    */
-   if (pCreateInfo->samples > VK_SAMPLE_COUNT_1_BIT &&
-       (usage & (VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT |
-                 VK_IMAGE_USAGE_DEPTH_STENCIL_ATTACHMENT_BIT)))
-      usage |= VK_IMAGE_USAGE_TRANSFER_SRC_BIT;
-   return usage;
-}
-
 VkResult
 anv_image_create(VkDevice _device,
                  const struct anv_image_create_info *create_info,
@@ -801,7 +782,7 @@ anv_image_create(VkDevice _device,
    image->levels = pCreateInfo->mipLevels;
    image->array_size = pCreateInfo->arrayLayers;
    image->samples = pCreateInfo->samples;
-   image->usage = anv_image_create_usage(pCreateInfo, pCreateInfo->usage);
+   image->usage = pCreateInfo->usage;
    image->create_flags = pCreateInfo->flags;
    image->tiling = pCreateInfo->tiling;
    image->disjoint = pCreateInfo->flags & VK_IMAGE_CREATE_DISJOINT_BIT;
@@ -814,11 +795,8 @@ anv_image_create(VkDevice _device,
       const VkImageStencilUsageCreateInfoEXT *stencil_usage_info =
          vk_find_struct_const(pCreateInfo->pNext,
                               IMAGE_STENCIL_USAGE_CREATE_INFO_EXT);
-      if (stencil_usage_info) {
-         image->stencil_usage =
-            anv_image_create_usage(pCreateInfo,
-                                   stencil_usage_info->stencilUsage);
-      }
+      if (stencil_usage_info)
+         image->stencil_usage = stencil_usage_info->stencilUsage;
    }
 
    /* In case of external format, We don't know format yet,
diff --git a/src/intel/vulkan/anv_pass.c b/src/intel/vulkan/anv_pass.c
index 1818f6c587b..af23b87969d 100644
--- a/src/intel/vulkan/anv_pass.c
+++ b/src/intel/vulkan/anv_pass.c
@@ -23,7 +23,6 @@
 
 #include "anv_private.h"
 
-#include "vk_format_info.h"
 #include "vk_util.h"
 
 static void
@@ -407,70 +406,6 @@ num_subpass_attachments2(const VkSubpassDescription2KHR *desc)
           (ds_resolve && ds_resolve->pDepthStencilResolveAttachment);
 }
 
-static bool
-vk_image_layout_depth_only(VkImageLayout layout)
-{
-   switch (layout) {
-   case VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_OPTIMAL:
-   case VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL:
-      return true;
-
-   default:
-      return false;
-   }
-}
-
-/* From the Vulkan Specification 1.2.166 - VkAttachmentReference2:
- *
- *   "If layout only specifies the layout of the depth aspect of the
- *    attachment, the layout of the stencil aspect is specified by the
- *    stencilLayout member of a VkAttachmentReferenceStencilLayout structure
- *    included in the pNext chain. Otherwise, layout describes the layout for
- *    all relevant image aspects."
- */
-static VkImageLayout
-stencil_ref_layout(const VkAttachmentReference2KHR *att_ref)
-{
-   if (!vk_image_layout_depth_only(att_ref->layout))
-      return att_ref->layout;
-
-   const VkAttachmentReferenceStencilLayoutKHR *stencil_ref =
-      vk_find_struct_const(att_ref->pNext,
-                           ATTACHMENT_REFERENCE_STENCIL_LAYOUT_KHR);
-   if (!stencil_ref)
-      return VK_IMAGE_LAYOUT_UNDEFINED;
-   return stencil_ref->stencilLayout;
-}
-
-/* From the Vulkan Specification 1.2.166 - VkAttachmentDescription2:
- *
- *   "If format is a depth/stencil format, and initialLayout only specifies
- *    the initial layout of the depth aspect of the attachment, the initial
- *    layout of the stencil aspect is specified by the stencilInitialLayout
- *    member of a VkAttachmentDescriptionStencilLayout structure included in
- *    the pNext chain. Otherwise, initialLayout describes the initial layout
- *    for all relevant image aspects."
- */
-static VkImageLayout
-stencil_desc_layout(const VkAttachmentDescription2KHR *att_desc, bool final)
-{
-   if (!vk_format_has_stencil(att_desc->format))
-      return VK_IMAGE_LAYOUT_UNDEFINED;
-
-   const VkImageLayout main_layout =
-      final ? att_desc->finalLayout : att_desc->initialLayout;
-   if (!vk_image_layout_depth_only(main_layout))
-      return main_layout;
-
-   const VkAttachmentDescriptionStencilLayoutKHR *stencil_desc =
-      vk_find_struct_const(att_desc->pNext,
-                           ATTACHMENT_DESCRIPTION_STENCIL_LAYOUT_KHR);
-   assert(stencil_desc);
-   return final ?
-      stencil_desc->stencilFinalLayout :
-      stencil_desc->stencilInitialLayout;
-}
-
 VkResult anv_CreateRenderPass2(
     VkDevice                                    _device,
     const VkRenderPassCreateInfo2KHR*           pCreateInfo,
@@ -515,6 +450,10 @@ VkResult anv_CreateRenderPass2(
    pass->subpass_flushes = subpass_flushes;
 
    for (uint32_t i = 0; i < pCreateInfo->attachmentCount; i++) {
+      const VkAttachmentDescriptionStencilLayoutKHR *stencil_layout =
+         vk_find_struct_const(pCreateInfo->pAttachments[i].pNext,
+                              ATTACHMENT_DESCRIPTION_STENCIL_LAYOUT_KHR);
+
       pass->attachments[i] = (struct anv_render_pass_attachment) {
          .format                 = pCreateInfo->pAttachments[i].format,
          .samples                = pCreateInfo->pAttachments[i].samples,
@@ -524,10 +463,12 @@ VkResult anv_CreateRenderPass2(
          .initial_layout         = pCreateInfo->pAttachments[i].initialLayout,
          .final_layout           = pCreateInfo->pAttachments[i].finalLayout,
 
-         .stencil_initial_layout = stencil_desc_layout(&pCreateInfo->pAttachments[i],
-                                                       false),
-         .stencil_final_layout   = stencil_desc_layout(&pCreateInfo->pAttachments[i],
-                                                       true),
+         .stencil_initial_layout = (stencil_layout ?
+                                    stencil_layout->stencilInitialLayout :
+                                    pCreateInfo->pAttachments[i].initialLayout),
+         .stencil_final_layout   = (stencil_layout ?
+                                    stencil_layout->stencilFinalLayout :
+                                    pCreateInfo->pAttachments[i].finalLayout),
       };
    }
 
@@ -546,11 +487,17 @@ VkResult anv_CreateRenderPass2(
          subpass_attachments += desc->inputAttachmentCount;
 
          for (uint32_t j = 0; j < desc->inputAttachmentCount; j++) {
+            const VkAttachmentReferenceStencilLayoutKHR *stencil_layout =
+               vk_find_struct_const(desc->pInputAttachments[j].pNext,
+                                    ATTACHMENT_REFERENCE_STENCIL_LAYOUT_KHR);
+
             subpass->input_attachments[j] = (struct anv_subpass_attachment) {
                .usage =          VK_IMAGE_USAGE_INPUT_ATTACHMENT_BIT,
                .attachment =     desc->pInputAttachments[j].attachment,
                .layout =         desc->pInputAttachments[j].layout,
-               .stencil_layout = stencil_ref_layout(&desc->pInputAttachments[j]),
+               .stencil_layout = (stencil_layout ?
+                                  stencil_layout->stencilLayout :
+                                  desc->pInputAttachments[j].layout),
             };
          }
       }
@@ -584,11 +531,17 @@ VkResult anv_CreateRenderPass2(
       if (desc->pDepthStencilAttachment) {
          subpass->depth_stencil_attachment = subpass_attachments++;
 
+         const VkAttachmentReferenceStencilLayoutKHR *stencil_attachment =
+            vk_find_struct_const(desc->pDepthStencilAttachment->pNext,
+                                 ATTACHMENT_REFERENCE_STENCIL_LAYOUT_KHR);
+
          *subpass->depth_stencil_attachment = (struct anv_subpass_attachment) {
             .usage =          VK_IMAGE_USAGE_DEPTH_STENCIL_ATTACHMENT_BIT,
             .attachment =     desc->pDepthStencilAttachment->attachment,
             .layout =         desc->pDepthStencilAttachment->layout,
-            .stencil_layout = stencil_ref_layout(desc->pDepthStencilAttachment),
+            .stencil_layout = stencil_attachment ?
+                              stencil_attachment->stencilLayout :
+                              desc->pDepthStencilAttachment->layout,
          };
       }
 
@@ -599,11 +552,17 @@ VkResult anv_CreateRenderPass2(
       if (ds_resolve && ds_resolve->pDepthStencilResolveAttachment) {
          subpass->ds_resolve_attachment = subpass_attachments++;
 
+         const VkAttachmentReferenceStencilLayoutKHR *stencil_resolve_attachment =
+            vk_find_struct_const(ds_resolve->pDepthStencilResolveAttachment->pNext,
+                                 ATTACHMENT_REFERENCE_STENCIL_LAYOUT_KHR);
+
          *subpass->ds_resolve_attachment = (struct anv_subpass_attachment) {
             .usage =          VK_IMAGE_USAGE_TRANSFER_DST_BIT,
             .attachment =     ds_resolve->pDepthStencilResolveAttachment->attachment,
             .layout =         ds_resolve->pDepthStencilResolveAttachment->layout,
-            .stencil_layout = stencil_ref_layout(ds_resolve->pDepthStencilResolveAttachment),
+            .stencil_layout = stencil_resolve_attachment ?
+                              stencil_resolve_attachment->stencilLayout :
+                              ds_resolve->pDepthStencilResolveAttachment->layout,
          };
          subpass->depth_resolve_mode = ds_resolve->depthResolveMode;
          subpass->stencil_resolve_mode = ds_resolve->stencilResolveMode;
diff --git a/src/intel/vulkan/anv_perf.c b/src/intel/vulkan/anv_perf.c
index 0a323994cb6..ba6f677875e 100644
--- a/src/intel/vulkan/anv_perf.c
+++ b/src/intel/vulkan/anv_perf.c
@@ -56,10 +56,8 @@ anv_get_perf(const struct gen_device_info *devinfo, int fd)
    /* We need DRM_I915_PERF_PROP_HOLD_PREEMPTION support, only available in
     * perf revision 2.
     */
-   if (!(INTEL_DEBUG & DEBUG_NO_OACONFIG)) {
-      if (!gen_perf_has_hold_preemption(perf))
-         goto err;
-   }
+   if (perf->i915_perf_version < 3)
+      goto err;
 
    return perf;
 
@@ -106,7 +104,7 @@ anv_device_perf_open(struct anv_device *device, uint64_t metric_id)
     * enabled we would use only half on Gen11 because of functional
     * requirements.
     */
-   if (gen_perf_has_global_sseu(device->physical->perf)) {
+   if (device->physical->perf->i915_perf_version >= 4) {
       properties[p++] = DRM_I915_PERF_PROP_GLOBAL_SSEU;
       properties[p++] = (uintptr_t) &device->physical->perf->sseu;
    }
diff --git a/src/intel/vulkan/genX_query.c b/src/intel/vulkan/genX_query.c
index 5994488960d..3fd662cc062 100644
--- a/src/intel/vulkan/genX_query.c
+++ b/src/intel/vulkan/genX_query.c
@@ -528,22 +528,29 @@ VkResult genX(GetQueryPoolResults)(
       case VK_QUERY_TYPE_PERFORMANCE_QUERY_INTEL: {
          if (!write_results)
             break;
-         const struct gen_perf_query_info *query = &device->physical->perf->queries[0];
          const void *query_data = query_slot(pool, firstQuery + i);
          const uint32_t *oa_begin = query_data + intel_perf_mi_rpc_offset(false);
          const uint32_t *oa_end = query_data + intel_perf_mi_rpc_offset(true);
          const uint32_t *rpstat_begin = query_data + intel_perf_rpstart_offset(false);
          const uint32_t *rpstat_end = query_data + intel_perf_mi_rpc_offset(true);
          struct gen_perf_query_result result;
+         uint32_t core_freq[2];
+#if GEN_GEN < 9
+         core_freq[0] = ((*rpstat_begin >> 7) & 0x7f) * 1000000ULL;
+         core_freq[1] = ((*rpstat_end >> 7) & 0x7f) * 1000000ULL;
+#else
+         core_freq[0] = ((*rpstat_begin >> 23) & 0x1ff) * 1000000ULL;
+         core_freq[1] = ((*rpstat_end >> 23) & 0x1ff) * 1000000ULL;
+#endif
          gen_perf_query_result_clear(&result);
-         gen_perf_query_result_accumulate(&result, query, oa_begin, oa_end);
+         gen_perf_query_result_accumulate(&result, &device->physical->perf->queries[0],
+                                          oa_begin, oa_end);
          gen_perf_query_result_read_frequencies(&result, &device->info,
                                                 oa_begin, oa_end);
-         gen_perf_query_result_read_gt_frequency(&result, &device->info,
-                                                 *rpstat_begin, *rpstat_end);
          gen_perf_query_result_write_mdapi(pData, stride,
                                            &device->info,
-                                           query, &result);
+                                           &result,
+                                           core_freq[0], core_freq[1]);
 #if GEN_GEN >= 8 && GEN_GEN <= 11
          gen_perf_query_mdapi_write_perfcntr(pData, stride, &device->info,
                                              query_data + intel_perf_counter(false),
diff --git a/src/intel/vulkan/vk_format_info.h b/src/intel/vulkan/vk_format_info.h
index 4e72c244742..006e1f4a6ad 100644
--- a/src/intel/vulkan/vk_format_info.h
+++ b/src/intel/vulkan/vk_format_info.h
@@ -164,11 +164,4 @@ vk_format_has_depth(VkFormat format)
    return aspects & VK_IMAGE_ASPECT_DEPTH_BIT;
 }
 
-static inline bool
-vk_format_has_stencil(VkFormat format)
-{
-   const VkImageAspectFlags aspects = vk_format_aspects(format);
-   return aspects & VK_IMAGE_ASPECT_STENCIL_BIT;
-}
-
 #endif /* VK_FORMAT_INFO_H */
diff --git a/src/mapi/glapi/gen/ARB_draw_elements_base_vertex.xml b/src/mapi/glapi/gen/ARB_draw_elements_base_vertex.xml
index 08ea7a31ca0..0350dd735a7 100644
--- a/src/mapi/glapi/gen/ARB_draw_elements_base_vertex.xml
+++ b/src/mapi/glapi/gen/ARB_draw_elements_base_vertex.xml
@@ -8,7 +8,7 @@
 
 <category name="GL_ARB_draw_elements_base_vertex" number="62">
 
-    <function name="DrawElementsBaseVertex" es2="3.2" marshal="custom">
+    <function name="DrawElementsBaseVertex" es2="3.2" exec="dynamic" marshal="custom">
         <param name="mode" type="GLenum"/>
         <param name="count" type="GLsizei"/>
         <param name="type" type="GLenum"/>
@@ -16,7 +16,7 @@
         <param name="basevertex" type="GLint"/>
     </function>
 
-    <function name="DrawRangeElementsBaseVertex" es2="3.2" marshal="custom">
+    <function name="DrawRangeElementsBaseVertex" es2="3.2" exec="dynamic" marshal="custom">
         <param name="mode" type="GLenum"/>
         <param name="start" type="GLuint"/>
         <param name="end" type="GLuint"/>
@@ -26,7 +26,7 @@
         <param name="basevertex" type="GLint"/>
     </function>
 
-    <function name="MultiDrawElementsBaseVertex" marshal="custom">
+    <function name="MultiDrawElementsBaseVertex" exec="dynamic" marshal="custom">
         <param name="mode" type="GLenum"/>
         <param name="count" type="const GLsizei *" count="primcount"/>
         <param name="type" type="GLenum"/>
diff --git a/src/mapi/glapi/gen/ARB_draw_indirect.xml b/src/mapi/glapi/gen/ARB_draw_indirect.xml
index 541647a0c38..6dc1d6feeec 100644
--- a/src/mapi/glapi/gen/ARB_draw_indirect.xml
+++ b/src/mapi/glapi/gen/ARB_draw_indirect.xml
@@ -9,14 +9,14 @@
     <enum name="DRAW_INDIRECT_BUFFER_BINDING"           value="0x8F43"/>
 
     <function name="DrawArraysIndirect" es2="3.1"
-              marshal="async"
+              marshal="draw"
               marshal_sync="_mesa_glthread_has_non_vbo_vertices_or_indirect(ctx)">
         <param name="mode" type="GLenum"/>
         <param name="indirect" type="const GLvoid *"/>
     </function>
 
     <function name="DrawElementsIndirect" es2="3.1"
-              marshal="async"
+              marshal="draw"
               marshal_sync="_mesa_glthread_has_non_vbo_vertices_or_indices_or_indirect(ctx)">
         <param name="mode" type="GLenum"/>
         <param name="type" type="GLenum"/>
@@ -28,7 +28,7 @@
 
 <category name="GL_ARB_multi_draw_indirect" number="133">
 
-    <function name="MultiDrawArraysIndirect" marshal="async"
+    <function name="MultiDrawArraysIndirect" marshal="draw"
               marshal_sync="_mesa_glthread_has_non_vbo_vertices_or_indirect(ctx)">
         <param name="mode" type="GLenum"/>
         <param name="indirect" type="const GLvoid *"/>
@@ -36,7 +36,7 @@
         <param name="stride" type="GLsizei"/>
     </function>
 
-    <function name="MultiDrawElementsIndirect" marshal="async"
+    <function name="MultiDrawElementsIndirect" marshal="draw"
               marshal_sync="_mesa_glthread_has_non_vbo_vertices_or_indices_or_indirect(ctx)">
         <param name="mode" type="GLenum"/>
         <param name="type" type="GLenum"/>
diff --git a/src/mapi/glapi/gen/ARB_indirect_parameters.xml b/src/mapi/glapi/gen/ARB_indirect_parameters.xml
index f08201d4a22..2739f153875 100644
--- a/src/mapi/glapi/gen/ARB_indirect_parameters.xml
+++ b/src/mapi/glapi/gen/ARB_indirect_parameters.xml
@@ -8,7 +8,7 @@
     <enum name="PARAMETER_BUFFER_ARB"                   value="0x80EE"/>
     <enum name="PARAMETER_BUFFER_BINDING_ARB"           value="0x80EF"/>
 
-    <function name="MultiDrawArraysIndirectCountARB"
+    <function name="MultiDrawArraysIndirectCountARB" marshal="draw"
               marshal_sync="_mesa_glthread_has_non_vbo_vertices(ctx)">
         <param name="mode" type="GLenum"/>
         <param name="indirect" type="GLintptr"/>
@@ -18,7 +18,7 @@
     </function>
 
     <!-- Use "...has_non_vbo_vertices", because indices always come from a buffer object. -->
-    <function name="MultiDrawElementsIndirectCountARB"
+    <function name="MultiDrawElementsIndirectCountARB" marshal="draw"
               marshal_sync="_mesa_glthread_has_non_vbo_vertices(ctx)">
         <param name="mode" type="GLenum"/>
         <param name="type" type="GLenum"/>
diff --git a/src/mapi/glapi/gen/EXT_direct_state_access.xml b/src/mapi/glapi/gen/EXT_direct_state_access.xml
index 1a7ba6b2393..4314973cc01 100644
--- a/src/mapi/glapi/gen/EXT_direct_state_access.xml
+++ b/src/mapi/glapi/gen/EXT_direct_state_access.xml
@@ -93,12 +93,10 @@
       <param name="n" type="GLdouble" />
       <param name="f" type="GLdouble" />
    </function>
-   <function name="MatrixPushEXT" offset="assign"
-             marshal_call_after="_mesa_glthread_MatrixPushEXT(ctx, matrixMode);">
+   <function name="MatrixPushEXT" offset="assign">
       <param name="matrixMode" type="GLenum" />
    </function>
-   <function name="MatrixPopEXT" offset="assign"
-             marshal_call_after="_mesa_glthread_MatrixPopEXT(ctx, matrixMode);">
+   <function name="MatrixPopEXT" offset="assign">
       <param name="matrixMode" type="GLenum" />
    </function>
 
diff --git a/src/mapi/glapi/gen/EXT_transform_feedback.xml b/src/mapi/glapi/gen/EXT_transform_feedback.xml
index 01aa2a364ec..604ab7cd0d5 100644
--- a/src/mapi/glapi/gen/EXT_transform_feedback.xml
+++ b/src/mapi/glapi/gen/EXT_transform_feedback.xml
@@ -108,7 +108,7 @@
   <function name="ResumeTransformFeedback" es2="3.0" no_error="true">
   </function>
 
-  <function name="DrawTransformFeedback">
+  <function name="DrawTransformFeedback" marshal="draw">
     <param name="mode" type="GLenum"/>
     <param name="id" type="GLuint"/>
   </function>
diff --git a/src/mapi/glapi/gen/NV_half_float.xml b/src/mapi/glapi/gen/NV_half_float.xml
index d3be839b120..b248e6d7b45 100644
--- a/src/mapi/glapi/gen/NV_half_float.xml
+++ b/src/mapi/glapi/gen/NV_half_float.xml
@@ -11,7 +11,7 @@
     </function>
 
     <function name="Vertex2hvNV" exec="dynamic" deprecated="3.1">
-        <param name="v" type="const GLhalfNV *" count="2"/>
+        <param name="v" type="const GLhalfNV *"/>
     </function>
 
     <function name="Vertex3hNV" exec="dynamic" deprecated="3.1">
@@ -21,7 +21,7 @@
     </function>
 
     <function name="Vertex3hvNV" exec="dynamic" deprecated="3.1">
-        <param name="v" type="const GLhalfNV *" count="3"/>
+        <param name="v" type="const GLhalfNV *"/>
     </function>
 
     <function name="Vertex4hNV" exec="dynamic" deprecated="3.1">
@@ -32,7 +32,7 @@
     </function>
 
     <function name="Vertex4hvNV" exec="dynamic" deprecated="3.1">
-        <param name="v" type="const GLhalfNV *" count="4"/>
+        <param name="v" type="const GLhalfNV *"/>
     </function>
 
     <function name="Normal3hNV" exec="dynamic" deprecated="3.1">
@@ -71,7 +71,7 @@
     </function>
 
     <function name="TexCoord1hvNV" exec="dynamic" deprecated="3.1">
-        <param name="v" type="const GLhalfNV *" count="1"/>
+        <param name="v" type="const GLhalfNV *"/>
     </function>
 
     <function name="TexCoord2hNV" exec="dynamic" deprecated="3.1">
@@ -80,7 +80,7 @@
     </function>
 
     <function name="TexCoord2hvNV" exec="dynamic" deprecated="3.1">
-        <param name="v" type="const GLhalfNV *" count="2"/>
+        <param name="v" type="const GLhalfNV *"/>
     </function>
 
     <function name="TexCoord3hNV" exec="dynamic" deprecated="3.1">
@@ -90,7 +90,7 @@
     </function>
 
     <function name="TexCoord3hvNV" exec="dynamic" deprecated="3.1">
-        <param name="v" type="const GLhalfNV *" count="3"/>
+        <param name="v" type="const GLhalfNV *"/>
     </function>
 
     <function name="TexCoord4hNV" exec="dynamic" deprecated="3.1">
@@ -101,7 +101,7 @@
     </function>
 
     <function name="TexCoord4hvNV" exec="dynamic" deprecated="3.1">
-        <param name="v" type="const GLhalfNV *" count="4"/>
+        <param name="v" type="const GLhalfNV *"/>
     </function>
 
     <function name="MultiTexCoord1hNV" exec="dynamic" deprecated="3.1">
@@ -111,7 +111,7 @@
 
     <function name="MultiTexCoord1hvNV" exec="dynamic" deprecated="3.1">
         <param name="target" type="GLenum"/>
-        <param name="v" type="const GLhalfNV *" count="1"/>
+        <param name="v" type="const GLhalfNV *"/>
     </function>
 
     <function name="MultiTexCoord2hNV" exec="dynamic" deprecated="3.1">
@@ -122,7 +122,7 @@
 
     <function name="MultiTexCoord2hvNV" exec="dynamic" deprecated="3.1">
         <param name="target" type="GLenum"/>
-        <param name="v" type="const GLhalfNV *" count="2"/>
+        <param name="v" type="const GLhalfNV *"/>
     </function>
 
     <function name="MultiTexCoord3hNV" exec="dynamic" deprecated="3.1">
@@ -134,7 +134,7 @@
 
     <function name="MultiTexCoord3hvNV" exec="dynamic" deprecated="3.1">
         <param name="target" type="GLenum"/>
-        <param name="v" type="const GLhalfNV *" count="3"/>
+        <param name="v" type="const GLhalfNV *"/>
     </function>
 
     <function name="MultiTexCoord4hNV" exec="dynamic" deprecated="3.1">
@@ -147,7 +147,7 @@
 
     <function name="MultiTexCoord4hvNV" exec="dynamic" deprecated="3.1">
         <param name="target" type="GLenum"/>
-        <param name="v" type="const GLhalfNV *" count="4"/>
+        <param name="v" type="const GLhalfNV *"/>
     </function>
 
     <function name="FogCoordhNV" exec="dynamic" deprecated="3.1">
@@ -155,7 +155,7 @@
     </function>
 
     <function name="FogCoordhvNV" exec="dynamic" deprecated="3.1">
-        <param name="v" type="const GLhalfNV *" count="1"/>
+        <param name="v" type="const GLhalfNV *"/>
     </function>
 
     <function name="SecondaryColor3hNV" exec="dynamic" deprecated="3.1">
@@ -165,7 +165,7 @@
     </function>
 
     <function name="SecondaryColor3hvNV" exec="dynamic" deprecated="3.1">
-        <param name="v" type="const GLhalfNV *" count="3"/>
+        <param name="v" type="const GLhalfNV *"/>
     </function>
 
 </category>
diff --git a/src/mapi/glapi/gen/es_EXT.xml b/src/mapi/glapi/gen/es_EXT.xml
index 929b40bbd4a..4386375940f 100644
--- a/src/mapi/glapi/gen/es_EXT.xml
+++ b/src/mapi/glapi/gen/es_EXT.xml
@@ -1076,7 +1076,7 @@
 <category name="GL_EXT_draw_elements_base_vertex" number="204">
 
     <function name="DrawElementsBaseVertexEXT" alias="DrawElementsBaseVertex"
-              es2="2.0">
+              es2="2.0" exec="dynamic">
         <param name="mode" type="GLenum"/>
         <param name="count" type="GLsizei"/>
         <param name="type" type="GLenum"/>
@@ -1085,7 +1085,7 @@
     </function>
 
     <function name="DrawRangeElementsBaseVertexEXT" alias="DrawRangeElementsBaseVertex"
-              es2="3.0">
+              es2="3.0" exec="dynamic">
         <param name="mode" type="GLenum"/>
         <param name="start" type="GLuint"/>
         <param name="end" type="GLuint"/>
@@ -1096,7 +1096,7 @@
     </function>
 
     <function name="MultiDrawElementsBaseVertexEXT" alias="MultiDrawElementsBaseVertex"
-              es2="2.0">
+              es2="2.0" exec="dynamic">
         <param name="mode" type="GLenum"/>
         <param name="count" type="const GLsizei *"/>
         <param name="type" type="GLenum"/>
@@ -1228,7 +1228,7 @@
 <category name="GL_OES_draw_elements_base_vertex" number="219">
 
     <function name="DrawElementsBaseVertexOES" alias="DrawElementsBaseVertex"
-              es2="2.0">
+              es2="2.0" exec="dynamic">
         <param name="mode" type="GLenum"/>
         <param name="count" type="GLsizei"/>
         <param name="type" type="GLenum"/>
@@ -1237,7 +1237,7 @@
     </function>
 
     <function name="DrawRangeElementsBaseVertexOES" alias="DrawRangeElementsBaseVertex"
-              es2="3.0">
+              es2="3.0" exec="dynamic">
         <param name="mode" type="GLenum"/>
         <param name="start" type="GLuint"/>
         <param name="end" type="GLuint"/>
diff --git a/src/mapi/glapi/gen/gl_API.dtd b/src/mapi/glapi/gen/gl_API.dtd
index 3d1801ad2b5..aceb69c07f6 100644
--- a/src/mapi/glapi/gen/gl_API.dtd
+++ b/src/mapi/glapi/gen/gl_API.dtd
@@ -124,12 +124,14 @@ param:
         offset data should be padded to the next even number of dimensions.
         For example, this will insert an empty "height" field after the
         "width" field in the protocol for TexImage1D.
-     marshal - One of "sync", "async", or "custom", defaulting to
+     marshal - One of "sync", "async", "draw", or "custom", defaulting to
         async unless one of the arguments is something we know we can't
         codegen for.  If "sync", we finish any queued glthread work and call
         the Mesa implementation directly.  If "async", we queue the function
         call to be performed by glthread.  If "custom", the prototype will be
         generated but a custom implementation will be present in marshal.c.
+        If "draw", it will follow the "async" rules except that "indices" are
+        ignored (since they may come from a VBO).
      marshal_sync - an expression that, if it evaluates true, causes glthread
         to sync and execute the call directly.
      marshal_count - same as count, but variable_param is ignored. Used by
diff --git a/src/mapi/glapi/gen/gl_API.xml b/src/mapi/glapi/gen/gl_API.xml
index f97aea70061..dcdbe80290c 100644
--- a/src/mapi/glapi/gen/gl_API.xml
+++ b/src/mapi/glapi/gen/gl_API.xml
@@ -1111,25 +1111,23 @@
     <type name="DEBUGPROC" size="4" pointer="true"/>
 
     <function name="NewList" deprecated="3.1"
-              marshal_call_after="_mesa_glthread_NewList(ctx, list, mode);">
+              marshal_call_after="if (COMPAT) ctx->GLThread.inside_dlist = true;">
         <param name="list" type="GLuint"/>
         <param name="mode" type="GLenum"/>
         <glx sop="101"/>
     </function>
 
     <function name="EndList" deprecated="3.1"
-              marshal_call_after="_mesa_glthread_EndList(ctx);">
+              marshal_call_after="if (COMPAT) ctx->GLThread.inside_dlist = false;">
         <glx sop="102"/>
     </function>
 
-    <function name="CallList" deprecated="3.1"
-              marshal_call_after="_mesa_glthread_CallList(ctx, list);">
+    <function name="CallList" deprecated="3.1">
         <param name="list" type="GLuint"/>
         <glx rop="1"/>
     </function>
 
-    <function name="CallLists" deprecated="3.1"
-              marshal_call_after="_mesa_glthread_CallLists(ctx, n, type, lists);">
+    <function name="CallLists" deprecated="3.1">
         <param name="n" type="GLsizei" counter="true"/>
         <param name="type" type="GLenum"/>
         <param name="lists" type="const GLvoid *" variable_param="type" count="n"
@@ -1137,8 +1135,7 @@
         <glx rop="2" large="true"/>
     </function>
 
-    <function name="DeleteLists" deprecated="3.1"
-              marshal_call_after="_mesa_glthread_DeleteLists(ctx, range);">
+    <function name="DeleteLists" deprecated="3.1">
         <param name="list" type="GLuint"/>
         <param name="range" type="GLsizei"/>
         <glx sop="103"/>
@@ -1150,8 +1147,7 @@
         <glx sop="104"/>
     </function>
 
-    <function name="ListBase" deprecated="3.1"
-              marshal_call_after="_mesa_glthread_ListBase(ctx, base);">
+    <function name="ListBase" deprecated="3.1">
         <param name="base" type="GLuint"/>
         <glx rop="3"/>
     </function>
@@ -1598,53 +1594,53 @@
         <glx rop="44"/>
     </function>
 
-    <function name="Rectd" vectorequiv="Rectdv" deprecated="3.1">
+    <function name="Rectd" vectorequiv="Rectdv" deprecated="3.1" exec="dynamic">
         <param name="x1" type="GLdouble"/>
         <param name="y1" type="GLdouble"/>
         <param name="x2" type="GLdouble"/>
         <param name="y2" type="GLdouble"/>
     </function>
 
-    <function name="Rectdv" deprecated="3.1">
+    <function name="Rectdv" deprecated="3.1" exec="dynamic">
         <param name="v1" type="const GLdouble *" count="2"/>
         <param name="v2" type="const GLdouble *" count="2"/>
         <glx rop="45"/>
     </function>
 
-    <function name="Rectf" vectorequiv="Rectfv" deprecated="3.1">
+    <function name="Rectf" vectorequiv="Rectfv" deprecated="3.1" exec="dynamic">
         <param name="x1" type="GLfloat"/>
         <param name="y1" type="GLfloat"/>
         <param name="x2" type="GLfloat"/>
         <param name="y2" type="GLfloat"/>
     </function>
 
-    <function name="Rectfv" deprecated="3.1">
+    <function name="Rectfv" deprecated="3.1" exec="dynamic">
         <param name="v1" type="const GLfloat *" count="2"/>
         <param name="v2" type="const GLfloat *" count="2"/>
         <glx rop="46"/>
     </function>
 
-    <function name="Recti" vectorequiv="Rectiv" deprecated="3.1">
+    <function name="Recti" vectorequiv="Rectiv" deprecated="3.1" exec="dynamic">
         <param name="x1" type="GLint"/>
         <param name="y1" type="GLint"/>
         <param name="x2" type="GLint"/>
         <param name="y2" type="GLint"/>
     </function>
 
-    <function name="Rectiv" deprecated="3.1">
+    <function name="Rectiv" deprecated="3.1" exec="dynamic">
         <param name="v1" type="const GLint *" count="2"/>
         <param name="v2" type="const GLint *" count="2"/>
         <glx rop="47"/>
     </function>
 
-    <function name="Rects" vectorequiv="Rectsv" deprecated="3.1">
+    <function name="Rects" vectorequiv="Rectsv" deprecated="3.1" exec="dynamic">
         <param name="x1" type="GLshort"/>
         <param name="y1" type="GLshort"/>
         <param name="x2" type="GLshort"/>
         <param name="y2" type="GLshort"/>
     </function>
 
-    <function name="Rectsv" deprecated="3.1">
+    <function name="Rectsv" deprecated="3.1" exec="dynamic">
         <param name="v1" type="const GLshort *" count="2"/>
         <param name="v2" type="const GLshort *" count="2"/>
         <glx rop="48"/>
@@ -2380,13 +2376,13 @@
     </function>
 
     <function name="Disable" es1="1.0" es2="2.0"
-              marshal_call_after="_mesa_glthread_Disable(ctx, cap);">
+              marshal_call_after="if (cap == GL_PRIMITIVE_RESTART || cap == GL_PRIMITIVE_RESTART_FIXED_INDEX) _mesa_glthread_set_prim_restart(ctx, cap, false);">
         <param name="cap" type="GLenum"/>
         <glx rop="138" handcode="client"/>
     </function>
 
     <function name="Enable" es1="1.0" es2="2.0"
-              marshal_call_after='_mesa_glthread_Enable(ctx, cap);'>
+              marshal_call_after='if (cap == GL_PRIMITIVE_RESTART || cap == GL_PRIMITIVE_RESTART_FIXED_INDEX) { _mesa_glthread_set_prim_restart(ctx, cap, true); } else if (cap == GL_DEBUG_OUTPUT_SYNCHRONOUS_ARB) { _mesa_glthread_disable(ctx, "Enable(DEBUG_OUTPUT_SYNCHRONOUS)"); }'>
         <param name="cap" type="GLenum"/>
         <glx rop="139" handcode="client"/>
     </function>
@@ -2406,13 +2402,11 @@
         <glx sop="142" handcode="true"/>
     </function>
 
-    <function name="PopAttrib" deprecated="3.1"
-              marshal_call_after="_mesa_glthread_PopAttrib(ctx);">
+    <function name="PopAttrib" deprecated="3.1">
         <glx rop="141"/>
     </function>
 
-    <function name="PushAttrib" deprecated="3.1"
-              marshal_call_after="_mesa_glthread_PushAttrib(ctx, mask);">
+    <function name="PushAttrib" deprecated="3.1">
         <param name="mask" type="GLbitfield"/>
         <glx rop="142"/>
     </function>
@@ -2716,7 +2710,7 @@
         <glx sop="116" handcode="client"/>
     </function>
 
-    <function name="GetIntegerv" es1="1.0" es2="2.0" marshal="custom">
+    <function name="GetIntegerv" es1="1.0" es2="2.0">
         <param name="pname" type="GLenum"/>
         <param name="params" type="GLint *" output="true" variable_param="pname"/>
         <glx sop="117" handcode="client"/>
@@ -2916,8 +2910,7 @@
         <glx rop="178"/>
     </function>
 
-    <function name="MatrixMode" es1="1.0" deprecated="3.1"
-              marshal_call_after="_mesa_glthread_MatrixMode(ctx, mode);">
+    <function name="MatrixMode" es1="1.0" deprecated="3.1">
         <param name="mode" type="GLenum"/>
         <glx rop="179"/>
     </function>
@@ -2942,13 +2935,11 @@
         <glx rop="182"/>
     </function>
 
-    <function name="PopMatrix" es1="1.0" deprecated="3.1"
-              marshal_call_after="_mesa_glthread_PopMatrix(ctx);">
+    <function name="PopMatrix" es1="1.0" deprecated="3.1">
         <glx rop="183"/>
     </function>
 
-    <function name="PushMatrix" es1="1.0" deprecated="3.1"
-              marshal_call_after="_mesa_glthread_PushMatrix(ctx);">
+    <function name="PushMatrix" es1="1.0" deprecated="3.1">
         <glx rop="184"/>
     </function>
 
@@ -3178,7 +3169,7 @@
     <enum name="CLIENT_VERTEX_ARRAY_BIT"                  value="0x00000002"/>
     <enum name="CLIENT_ALL_ATTRIB_BITS"                   value="0xFFFFFFFF"/>
 
-    <function name="ArrayElement" deprecated="3.1" exec="dynamic">
+    <function name="ArrayElement" deprecated="3.1" exec="dynamic" marshal="draw">
         <param name="i" type="GLint"/>
         <glx handcode="true"/>
     </function>
@@ -3199,14 +3190,14 @@
         <glx handcode="true"/>
     </function>
 
-    <function name="DrawArrays" es1="1.0" es2="2.0" marshal="custom">
+    <function name="DrawArrays" es1="1.0" es2="2.0" exec="dynamic" marshal="custom">
         <param name="mode" type="GLenum"/>
         <param name="first" type="GLint"/>
         <param name="count" type="GLsizei"/>
         <glx rop="193" handcode="true"/>
     </function>
 
-    <function name="DrawElements" es1="1.0" es2="2.0" marshal="custom">
+    <function name="DrawElements" es1="1.0" es2="2.0" exec="dynamic" marshal="custom">
         <param name="mode" type="GLenum"/>
         <param name="count" type="GLsizei"/>
         <param name="type" type="GLenum"/>
@@ -3406,12 +3397,12 @@
     </function>
 
     <function name="PopClientAttrib" deprecated="3.1"
-              marshal_call_after="_mesa_glthread_PopClientAttrib(ctx);">
+              marshal_call_after="if (COMPAT) _mesa_glthread_PopClientAttrib(ctx);">
         <glx handcode="true"/>
     </function>
 
     <function name="PushClientAttrib" deprecated="3.1"
-              marshal_call_after="_mesa_glthread_PushClientAttrib(ctx, mask, false);">
+              marshal_call_after="if (COMPAT) _mesa_glthread_PushClientAttrib(ctx, mask, false);">
         <param name="mask" type="GLbitfield"/>
         <glx handcode="true"/>
     </function>
@@ -3777,7 +3768,7 @@
         <glx rop="4097"/>
     </function>
 
-    <function name="DrawRangeElements" es2="3.0" marshal="custom">
+    <function name="DrawRangeElements" es2="3.0" exec="dynamic" marshal="custom">
         <param name="mode" type="GLenum"/>
         <param name="start" type="GLuint"/>
         <param name="end" type="GLuint"/>
@@ -4301,8 +4292,7 @@
     <enum name="DOT3_RGB"                                 value="0x86AE"/>
     <enum name="DOT3_RGBA"                                value="0x86AF"/>
 
-    <function name="ActiveTexture" es1="1.0" es2="2.0" no_error="true"
-              marshal_call_after="ctx->GLThread.ActiveTexture = texture - GL_TEXTURE0; if (ctx->GLThread.MatrixMode == GL_TEXTURE) ctx->GLThread.MatrixIndex = _mesa_get_matrix_index(ctx, texture);">
+    <function name="ActiveTexture" es1="1.0" es2="2.0" no_error="true">
         <param name="texture" type="GLenum"/>
         <glx rop="197"/>
     </function>
@@ -8166,7 +8156,7 @@
   <enum name="MAX_TRANSFORM_FEEDBACK_BUFFERS" value="0x8E70"/>
   <enum name="MAX_VERTEX_STREAMS"             value="0x8E71"/>
 
-  <function name="DrawTransformFeedbackStream">
+  <function name="DrawTransformFeedbackStream" marshal="draw">
     <param name="mode" type="GLenum"/>
     <param name="id" type="GLuint"/>
     <param name="stream" type="GLuint"/>
@@ -8214,13 +8204,13 @@
 <xi:include href="ARB_base_instance.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
 
 <category name="GL_ARB_transform_feedback_instanced" number="109">
-  <function name="DrawTransformFeedbackInstanced">
+  <function name="DrawTransformFeedbackInstanced" marshal="draw">
     <param name="mode" type="GLenum"/>
     <param name="id" type="GLuint"/>
     <param name="primcount" type="GLsizei"/>
   </function>
 
-  <function name="DrawTransformFeedbackStreamInstanced">
+  <function name="DrawTransformFeedbackStreamInstanced" marshal="draw">
     <param name="mode" type="GLenum"/>
     <param name="id" type="GLuint"/>
     <param name="stream" type="GLuint"/>
@@ -10298,14 +10288,14 @@
 </category>
 
 <category name="GL_EXT_multi_draw_arrays" number="148">
-    <function name="MultiDrawArraysEXT" es1="1.0" es2="2.0" alias="MultiDrawArrays">
+    <function name="MultiDrawArraysEXT" es1="1.0" es2="2.0" exec="dynamic" alias="MultiDrawArrays">
         <param name="mode" type="GLenum"/>
         <param name="first" type="const GLint *"/>
         <param name="count" type="const GLsizei *"/>
         <param name="primcount" type="GLsizei"/>
     </function>
 
-    <function name="MultiDrawElementsEXT" es1="1.0" es2="2.0" marshal="custom">
+    <function name="MultiDrawElementsEXT" es1="1.0" es2="2.0" exec="dynamic" marshal="custom">
         <param name="mode" type="GLenum"/>
         <param name="count" type="const GLsizei *" count="primcount"/>
         <param name="type" type="GLenum"/>
@@ -11419,7 +11409,7 @@
 </category>
 
 <category name="GL_IBM_multimode_draw_arrays" number="200">
-    <function name="MultiModeDrawArraysIBM"
+    <function name="MultiModeDrawArraysIBM" marshal="draw"
               marshal_sync="_mesa_glthread_has_non_vbo_vertices(ctx)">
         <param name="mode" type="const GLenum *" count="primcount"/>
         <param name="first" type="const GLint *" count="primcount"/>
@@ -11429,7 +11419,7 @@
         <glx handcode="true" ignore="true"/>
     </function>
 
-    <function name="MultiModeDrawElementsIBM"
+    <function name="MultiModeDrawElementsIBM" marshal="draw"
               marshal_sync="_mesa_glthread_has_non_vbo_vertices_or_indices(ctx)">
         <param name="mode" type="const GLenum *" count="primcount"/>
         <param name="count" type="const GLsizei *" count="primcount"/>
diff --git a/src/mapi/glapi/gen/gl_genexec.py b/src/mapi/glapi/gen/gl_genexec.py
index 8bca9ed6904..9f7c8ca8baa 100644
--- a/src/mapi/glapi/gen/gl_genexec.py
+++ b/src/mapi/glapi/gen/gl_genexec.py
@@ -149,6 +149,8 @@ _mesa_initialize_exec_table(struct gl_context *ctx)
    assert(exec != NULL);
 
    assert(ctx->Version > 0);
+
+   _mesa_initialize_exec_dispatch(ctx, exec);
 """
 
 
diff --git a/src/mapi/glapi/gen/marshal_XML.py b/src/mapi/glapi/gen/marshal_XML.py
index d1c0bca7938..42c82c15e68 100644
--- a/src/mapi/glapi/gen/marshal_XML.py
+++ b/src/mapi/glapi/gen/marshal_XML.py
@@ -65,7 +65,7 @@ class marshal_function(gl_XML.gl_function):
         client and server threads."""
         # If a "marshal" attribute was present, that overrides any
         # determination that would otherwise be made by this function.
-        if self.marshal is not None:
+        if self.marshal not in (None, 'draw'):
             return self.marshal
 
         if self.exec_flavor == 'skip':
@@ -78,7 +78,9 @@ class marshal_function(gl_XML.gl_function):
         for p in self.parameters:
             if p.is_output:
                 return 'sync'
-            if (p.is_pointer() and not (p.count or p.counter or p.marshal_count)):
+            if (p.is_pointer() and not (p.count or p.counter or p.marshal_count)
+                and not (self.marshal == 'draw'
+                         and (p.name == 'indices' or p.name == 'indirect'))):
                 return 'sync'
             if p.count_parameter_list and not p.marshal_count:
                 # Parameter size is determined by enums; haven't
diff --git a/src/mesa/Android.gen.mk b/src/mesa/Android.gen.mk
index ae79a7cc0d9..f326143d791 100644
--- a/src/mesa/Android.gen.mk
+++ b/src/mesa/Android.gen.mk
@@ -36,6 +36,7 @@ sources := \
 	main/dispatch.h \
 	main/format_fallback.c \
 	main/format_pack.c \
+	main/format_unpack.c \
 	main/format_info.h \
 	main/remap_helper.h \
 	main/get_hash.h \
@@ -187,3 +188,14 @@ $(intermediates)/main/format_pack.c: PRIVATE_SCRIPT := $(MESA_PYTHON2) $(FORMAT_
 $(intermediates)/main/format_pack.c: PRIVATE_XML :=
 $(intermediates)/main/format_pack.c: $(format_pack_deps)
 	$(call es-gen, $<)
+
+FORMAT_UNPACK := $(LOCAL_PATH)/main/format_unpack.py
+format_unpack_deps := \
+	$(LOCAL_PATH)/main/formats.csv \
+	$(LOCAL_PATH)/main/format_parser.py \
+	$(FORMAT_UNPACK)
+
+$(intermediates)/main/format_unpack.c: PRIVATE_SCRIPT := $(MESA_PYTHON2) $(FORMAT_UNPACK)
+$(intermediates)/main/format_unpack.c: PRIVATE_XML :=
+$(intermediates)/main/format_unpack.c: $(format_unpack_deps)
+	$(call es-gen, $<)
diff --git a/src/mesa/Makefile.sources b/src/mesa/Makefile.sources
index 23aa945fa47..3ec14d1607f 100644
--- a/src/mesa/Makefile.sources
+++ b/src/mesa/Makefile.sources
@@ -101,6 +101,7 @@ MAIN_FILES = \
 	main/format_pack.h \
 	main/format_pack.c \
 	main/format_unpack.h \
+	main/format_unpack.c \
 	main/formatquery.c \
 	main/formatquery.h \
 	main/formats.c \
@@ -123,7 +124,6 @@ MAIN_FILES = \
 	main/glthread.h \
 	main/glthread_bufferobj.c \
 	main/glthread_draw.c \
-        main/glthread_get.c \
 	main/glthread_marshal.h \
 	main/glthread_shaderobj.c \
 	main/glthread_varray.c \
diff --git a/src/mesa/SConscript b/src/mesa/SConscript
index e238e0916db..bba4101ea58 100644
--- a/src/mesa/SConscript
+++ b/src/mesa/SConscript
@@ -67,6 +67,13 @@ format_pack = env.CodeGenerate(
       command = python_cmd + ' $SCRIPT ' + ' $SOURCE > $TARGET'
 )
 
+format_unpack = env.CodeGenerate(
+      target = 'main/format_unpack.c',
+      script = 'main/format_unpack.py',
+      source = 'main/formats.csv',
+      command = python_cmd + ' $SCRIPT ' + ' $SOURCE > $TARGET'
+)
+
 format_fallback = env.CodeGenerate(
       target = 'main/format_fallback.c',
       script = 'main/format_fallback.py',
diff --git a/src/mesa/drivers/dri/common/utils.c b/src/mesa/drivers/dri/common/utils.c
index 58671d6935a..0fdca2d9d84 100644
--- a/src/mesa/drivers/dri/common/utils.c
+++ b/src/mesa/drivers/dri/common/utils.c
@@ -255,7 +255,6 @@ driCreateConfigs(mesa_format format,
       shifts = format_table[5].shifts;
       break;
    case MESA_FORMAT_R8G8B8X8_UNORM:
-   case MESA_FORMAT_R8G8B8X8_SRGB:
       masks = format_table[6].masks;
       shifts = format_table[6].shifts;
       break;
diff --git a/src/mesa/main/context.h b/src/mesa/main/context.h
index 9ffd75fbae3..b42fa40b748 100644
--- a/src/mesa/main/context.h
+++ b/src/mesa/main/context.h
@@ -356,6 +356,7 @@ static inline bool
 _mesa_has_half_float_textures(const struct gl_context *ctx)
 {
    return _mesa_has_ARB_texture_float(ctx) ||
+          _mesa_has_ARB_half_float_pixel(ctx) ||
           _mesa_has_OES_texture_half_float(ctx);
 }
 
diff --git a/src/mesa/main/dlist.c b/src/mesa/main/dlist.c
index 0f634af853b..b74790f01f9 100644
--- a/src/mesa/main/dlist.c
+++ b/src/mesa/main/dlist.c
@@ -63,7 +63,6 @@
 #include "varray.h"
 #include "arbprogram.h"
 #include "transformfeedback.h"
-#include "glthread_marshal.h"
 
 #include "math/m_matrix.h"
 
@@ -520,6 +519,7 @@ typedef enum
    OPCODE_MATERIAL,
    OPCODE_BEGIN,
    OPCODE_END,
+   OPCODE_RECTF,
    OPCODE_EVAL_C1,
    OPCODE_EVAL_C2,
    OPCODE_EVAL_P1,
@@ -1157,16 +1157,12 @@ void
 _mesa_delete_list(struct gl_context *ctx, struct gl_display_list *dlist)
 {
    Node *n, *block;
-
-   if (!dlist->Head) {
-      free(dlist->Label);
-      free(dlist);
-      return;
-   }
+   GLboolean done;
 
    n = block = dlist->Head;
 
-   while (1) {
+   done = block ? GL_FALSE : GL_TRUE;
+   while (!done) {
       const OpCode opcode = n[0].opcode;
 
       /* check for extension opcodes first */
@@ -1384,21 +1380,25 @@ _mesa_delete_list(struct gl_context *ctx, struct gl_display_list *dlist)
             n = (Node *) get_pointer(&n[1]);
             free(block);
             block = n;
-            continue;
+            break;
          case OPCODE_END_OF_LIST:
             free(block);
-            free(dlist->Label);
-            free(dlist);
-            return;
+            done = GL_TRUE;
+            break;
          default:
             /* just increment 'n' pointer, below */
             ;
          }
 
-         assert(InstSize[opcode] > 0);
-         n += InstSize[opcode];
+         if (opcode != OPCODE_CONTINUE) {
+            assert(InstSize[opcode] > 0);
+            n += InstSize[opcode];
+         }
       }
    }
+
+   free(dlist->Label);
+   free(dlist);
 }
 
 
@@ -1458,6 +1458,63 @@ destroy_list(struct gl_context *ctx, GLuint list)
 }
 
 
+/*
+ * Translate the nth element of list from <type> to GLint.
+ */
+static GLint
+translate_id(GLsizei n, GLenum type, const GLvoid * list)
+{
+   GLbyte *bptr;
+   GLubyte *ubptr;
+   GLshort *sptr;
+   GLushort *usptr;
+   GLint *iptr;
+   GLuint *uiptr;
+   GLfloat *fptr;
+
+   switch (type) {
+   case GL_BYTE:
+      bptr = (GLbyte *) list;
+      return (GLint) bptr[n];
+   case GL_UNSIGNED_BYTE:
+      ubptr = (GLubyte *) list;
+      return (GLint) ubptr[n];
+   case GL_SHORT:
+      sptr = (GLshort *) list;
+      return (GLint) sptr[n];
+   case GL_UNSIGNED_SHORT:
+      usptr = (GLushort *) list;
+      return (GLint) usptr[n];
+   case GL_INT:
+      iptr = (GLint *) list;
+      return iptr[n];
+   case GL_UNSIGNED_INT:
+      uiptr = (GLuint *) list;
+      return (GLint) uiptr[n];
+   case GL_FLOAT:
+      fptr = (GLfloat *) list;
+      return (GLint) floorf(fptr[n]);
+   case GL_2_BYTES:
+      ubptr = ((GLubyte *) list) + 2 * n;
+      return (GLint) ubptr[0] * 256
+           + (GLint) ubptr[1];
+   case GL_3_BYTES:
+      ubptr = ((GLubyte *) list) + 3 * n;
+      return (GLint) ubptr[0] * 65536
+           + (GLint) ubptr[1] * 256
+           + (GLint) ubptr[2];
+   case GL_4_BYTES:
+      ubptr = ((GLubyte *) list) + 4 * n;
+      return (GLint) ubptr[0] * 16777216
+           + (GLint) ubptr[1] * 65536
+           + (GLint) ubptr[2] * 256
+           + (GLint) ubptr[3];
+   default:
+      return 0;
+   }
+}
+
+
 /**
  * Wrapper for _mesa_unpack_image/bitmap() that handles pixel buffer objects.
  * If width < 0 or height < 0 or format or type are invalid we'll just
@@ -6044,6 +6101,66 @@ save_End(void)
    }
 }
 
+static void GLAPIENTRY
+save_Rectf(GLfloat a, GLfloat b, GLfloat c, GLfloat d)
+{
+   GET_CURRENT_CONTEXT(ctx);
+   Node *n;
+   ASSERT_OUTSIDE_SAVE_BEGIN_END_AND_FLUSH(ctx);
+   n = alloc_instruction(ctx, OPCODE_RECTF, 4);
+   if (n) {
+      n[1].f = a;
+      n[2].f = b;
+      n[3].f = c;
+      n[4].f = d;
+   }
+   if (ctx->ExecuteFlag) {
+      CALL_Rectf(ctx->Exec, (a, b, c, d));
+   }
+}
+
+static void GLAPIENTRY
+save_Rectd(GLdouble x1, GLdouble y1, GLdouble x2, GLdouble y2)
+{
+   save_Rectf((GLfloat) x1, (GLfloat) y1, (GLfloat) x2, (GLfloat) y2);
+}
+
+static void GLAPIENTRY
+save_Rectdv(const GLdouble *v1, const GLdouble *v2)
+{
+   save_Rectf((GLfloat) v1[0], (GLfloat) v1[1], (GLfloat) v2[0], (GLfloat) v2[1]);
+}
+
+static void GLAPIENTRY
+save_Rectfv(const GLfloat *v1, const GLfloat *v2)
+{
+   save_Rectf(v1[0], v1[1], v2[0], v2[1]);
+}
+
+static void GLAPIENTRY
+save_Recti(GLint x1, GLint y1, GLint x2, GLint y2)
+{
+   save_Rectf((GLfloat) x1, (GLfloat) y1, (GLfloat) x2, (GLfloat) y2);
+}
+
+static void GLAPIENTRY
+save_Rectiv(const GLint *v1, const GLint *v2)
+{
+   save_Rectf((GLfloat) v1[0], (GLfloat) v1[1], (GLfloat) v2[0], (GLfloat) v2[1]);
+}
+
+static void GLAPIENTRY
+save_Rects(GLshort x1, GLshort y1, GLshort x2, GLshort y2)
+{
+   save_Rectf((GLfloat) x1, (GLfloat) y1, (GLfloat) x2, (GLfloat) y2);
+}
+
+static void GLAPIENTRY
+save_Rectsv(const GLshort *v1, const GLshort *v2)
+{
+   save_Rectf((GLfloat) v1[0], (GLfloat) v1[1], (GLfloat) v2[0], (GLfloat) v2[1]);
+}
+
 static void GLAPIENTRY
 save_PrimitiveRestartNV(void)
 {
@@ -11234,9 +11351,9 @@ _mesa_compile_error(struct gl_context *ctx, GLenum error, const char *s)
 /**
  * Test if ID names a display list.
  */
-bool
-_mesa_get_list(struct gl_context *ctx, GLuint list,
-               struct gl_display_list **dlist)
+static GLboolean
+islist(struct gl_context *ctx, GLuint list,
+       struct gl_display_list ** dlist)
 {
    struct gl_display_list * dl =
       list > 0 ? _mesa_lookup_list(ctx, list) : NULL;
@@ -11265,8 +11382,9 @@ execute_list(struct gl_context *ctx, GLuint list)
 {
    struct gl_display_list *dlist;
    Node *n;
+   GLboolean done;
 
-   if (list == 0 || !_mesa_get_list(ctx, list, &dlist))
+   if (list == 0 || !islist(ctx, list, &dlist))
       return;
 
    if (ctx->ListState.CallDepth == MAX_LIST_NESTING) {
@@ -11280,7 +11398,8 @@ execute_list(struct gl_context *ctx, GLuint list)
 
    n = dlist->Head;
 
-   while (1) {
+   done = GL_FALSE;
+   while (!done) {
       const OpCode opcode = n[0].opcode;
 
       if (is_ext_opcode(opcode)) {
@@ -12842,6 +12961,9 @@ execute_list(struct gl_context *ctx, GLuint list)
          case OPCODE_END:
             CALL_End(ctx->Exec, ());
             break;
+         case OPCODE_RECTF:
+            CALL_Rectf(ctx->Exec, (n[1].f, n[2].f, n[3].f, n[4].f));
+            break;
          case OPCODE_EVAL_C1:
             CALL_EvalCoord1f(ctx->Exec, (n[1].f));
             break;
@@ -13457,10 +13579,13 @@ execute_list(struct gl_context *ctx, GLuint list)
 
          case OPCODE_CONTINUE:
             n = (Node *) get_pointer(&n[1]);
-            continue;
+            break;
          case OPCODE_NOP:
             /* no-op */
             break;
+         case OPCODE_END_OF_LIST:
+            done = GL_TRUE;
+            break;
          default:
             {
                char msg[1000];
@@ -13468,18 +13593,20 @@ execute_list(struct gl_context *ctx, GLuint list)
                              (int) opcode);
                _mesa_problem(ctx, "%s", msg);
             }
-            FALLTHROUGH;
-         case OPCODE_END_OF_LIST:
-            vbo_save_EndCallList(ctx);
-            ctx->ListState.CallDepth--;
-            return;
+            done = GL_TRUE;
          }
 
          /* increment n to point to next compiled command */
-         assert(InstSize[opcode] > 0);
-         n += InstSize[opcode];
+         if (opcode != OPCODE_CONTINUE) {
+            assert(InstSize[opcode] > 0);
+            n += InstSize[opcode];
+         }
       }
    }
+
+   vbo_save_EndCallList(ctx);
+
+   ctx->ListState.CallDepth--;
 }
 
 
@@ -13497,7 +13624,7 @@ _mesa_IsList(GLuint list)
    GET_CURRENT_CONTEXT(ctx);
    FLUSH_VERTICES(ctx, 0);      /* must be called before assert */
    ASSERT_OUTSIDE_BEGIN_END_WITH_RETVAL(ctx, GL_FALSE);
-   return _mesa_get_list(ctx, list, NULL);
+   return islist(ctx, list, NULL);
 }
 
 
@@ -13722,9 +13849,8 @@ _mesa_CallList(GLuint list)
    if (0)
       mesa_print_display_list( list );
 
-   /* Save the CompileFlag status, turn it off, execute the display list,
-    * and restore the CompileFlag. This is needed for GL_COMPILE_AND_EXECUTE
-    * because the call is already recorded and we just need to execute it.
+   /* VERY IMPORTANT:  Save the CompileFlag status, turn it off,
+    * execute the display list, and restore the CompileFlag.
     */
    save_compile_flag = ctx->CompileFlag;
    if (save_compile_flag) {
@@ -13811,12 +13937,26 @@ void GLAPIENTRY
 _mesa_CallLists(GLsizei n, GLenum type, const GLvoid * lists)
 {
    GET_CURRENT_CONTEXT(ctx);
+   GLint i;
    GLboolean save_compile_flag;
 
    if (MESA_VERBOSE & VERBOSE_API)
       _mesa_debug(ctx, "glCallLists %d\n", n);
 
-   if (type < GL_BYTE || type > GL_4_BYTES) {
+   switch (type) {
+   case GL_BYTE:
+   case GL_UNSIGNED_BYTE:
+   case GL_SHORT:
+   case GL_UNSIGNED_SHORT:
+   case GL_INT:
+   case GL_UNSIGNED_INT:
+   case GL_FLOAT:
+   case GL_2_BYTES:
+   case GL_3_BYTES:
+   case GL_4_BYTES:
+      /* OK */
+      break;
+   default:
       _mesa_error(ctx, GL_INVALID_ENUM, "glCallLists(type)");
       return;
    }
@@ -13833,87 +13973,15 @@ _mesa_CallLists(GLsizei n, GLenum type, const GLvoid * lists)
       return;
    }
 
-   /* Save the CompileFlag status, turn it off, execute the display lists,
-    * and restore the CompileFlag. This is needed for GL_COMPILE_AND_EXECUTE
-    * because the call is already recorded and we just need to execute it.
+   /* Save the CompileFlag status, turn it off, execute display list,
+    * and restore the CompileFlag.
     */
    save_compile_flag = ctx->CompileFlag;
    ctx->CompileFlag = GL_FALSE;
 
-   GLbyte *bptr;
-   GLubyte *ubptr;
-   GLshort *sptr;
-   GLushort *usptr;
-   GLint *iptr;
-   GLuint *uiptr;
-   GLfloat *fptr;
-
-   GLuint base = ctx->List.ListBase;
-
-   /* A loop inside a switch is faster than a switch inside a loop. */
-   switch (type) {
-   case GL_BYTE:
-      bptr = (GLbyte *) lists;
-      for (unsigned i = 0; i < n; i++)
-         execute_list(ctx, base + (int)bptr[i]);
-      break;
-   case GL_UNSIGNED_BYTE:
-      ubptr = (GLubyte *) lists;
-      for (unsigned i = 0; i < n; i++)
-         execute_list(ctx, base + (int)ubptr[i]);
-      break;
-   case GL_SHORT:
-      sptr = (GLshort *) lists;
-      for (unsigned i = 0; i < n; i++)
-         execute_list(ctx, base + (int)sptr[i]);
-      break;
-   case GL_UNSIGNED_SHORT:
-      usptr = (GLushort *) lists;
-      for (unsigned i = 0; i < n; i++)
-         execute_list(ctx, base + (int)usptr[i]);
-      break;
-   case GL_INT:
-      iptr = (GLint *) lists;
-      for (unsigned i = 0; i < n; i++)
-         execute_list(ctx, base + (int)iptr[i]);
-      break;
-   case GL_UNSIGNED_INT:
-      uiptr = (GLuint *) lists;
-      for (unsigned i = 0; i < n; i++)
-         execute_list(ctx, base + (int)uiptr[i]);
-      break;
-   case GL_FLOAT:
-      fptr = (GLfloat *) lists;
-      for (unsigned i = 0; i < n; i++)
-         execute_list(ctx, base + (int)fptr[i]);
-      break;
-   case GL_2_BYTES:
-      ubptr = (GLubyte *) lists;
-      for (unsigned i = 0; i < n; i++) {
-         execute_list(ctx, base +
-                      (int)ubptr[2 * i] * 256 +
-                      (int)ubptr[2 * i + 1]);
-      }
-      break;
-   case GL_3_BYTES:
-      ubptr = (GLubyte *) lists;
-      for (unsigned i = 0; i < n; i++) {
-         execute_list(ctx, base +
-                      (int)ubptr[3 * i] * 65536 +
-                      (int)ubptr[3 * i + 1] * 256 +
-                      (int)ubptr[3 * i + 2]);
-      }
-      break;
-   case GL_4_BYTES:
-      ubptr = (GLubyte *) lists;
-      for (unsigned i = 0; i < n; i++) {
-         execute_list(ctx, base +
-                      (int)ubptr[4 * i] * 16777216 +
-                      (int)ubptr[4 * i + 1] * 65536 +
-                      (int)ubptr[4 * i + 2] * 256 +
-                      (int)ubptr[4 * i + 3]);
-      }
-      break;
+   for (i = 0; i < n; i++) {
+      GLuint list = (GLuint) (ctx->List.ListBase + translate_id(i, type, lists));
+      execute_list(ctx, list);
    }
 
    ctx->CompileFlag = save_compile_flag;
@@ -14074,6 +14142,14 @@ _mesa_initialize_save_table(const struct gl_context *ctx)
    SET_RasterPos4s(table, save_RasterPos4s);
    SET_RasterPos4sv(table, save_RasterPos4sv);
    SET_ReadBuffer(table, save_ReadBuffer);
+   SET_Rectf(table, save_Rectf);
+   SET_Rectd(table, save_Rectd);
+   SET_Rectdv(table, save_Rectdv);
+   SET_Rectfv(table, save_Rectfv);
+   SET_Recti(table, save_Recti);
+   SET_Rectiv(table, save_Rectiv);
+   SET_Rects(table, save_Rects);
+   SET_Rectsv(table, save_Rectsv);
    SET_Rotated(table, save_Rotated);
    SET_Rotatef(table, save_Rotatef);
    SET_Scaled(table, save_Scaled);
@@ -14609,6 +14685,7 @@ print_list(struct gl_context *ctx, GLuint list, const char *fname)
 {
    struct gl_display_list *dlist;
    Node *n;
+   GLboolean done;
    FILE *f = stdout;
 
    if (fname) {
@@ -14617,19 +14694,17 @@ print_list(struct gl_context *ctx, GLuint list, const char *fname)
          return;
    }
 
-   if (!_mesa_get_list(ctx, list, &dlist)) {
+   if (!islist(ctx, list, &dlist)) {
       fprintf(f, "%u is not a display list ID\n", list);
-      fflush(f);
-      if (fname)
-         fclose(f);
-      return;
+      goto out;
    }
 
    n = dlist->Head;
 
    fprintf(f, "START-LIST %u, address %p\n", list, (void *) n);
 
-   while (1) {
+   done = n ? GL_FALSE : GL_TRUE;
+   while (!done) {
       const OpCode opcode = n[0].opcode;
 
       if (is_ext_opcode(opcode)) {
@@ -14848,6 +14923,10 @@ print_list(struct gl_context *ctx, GLuint list, const char *fname)
          case OPCODE_END:
             fprintf(f, "END\n");
             break;
+         case OPCODE_RECTF:
+            fprintf(f, "RECTF %f %f %f %f\n", n[1].f, n[2].f, n[3].f,
+                         n[4].f);
+            break;
          case OPCODE_EVAL_C1:
             fprintf(f, "EVAL_C1 %f\n", n[1].f);
             break;
@@ -14876,119 +14955,42 @@ print_list(struct gl_context *ctx, GLuint list, const char *fname)
          case OPCODE_CONTINUE:
             fprintf(f, "DISPLAY-LIST-CONTINUE\n");
             n = (Node *) get_pointer(&n[1]);
-            continue;
+            break;
          case OPCODE_NOP:
             fprintf(f, "NOP\n");
             break;
+         case OPCODE_END_OF_LIST:
+            fprintf(f, "END-LIST %u\n", list);
+            done = GL_TRUE;
+            break;
          default:
             if (opcode < 0 || opcode > OPCODE_END_OF_LIST) {
                printf
                   ("ERROR IN DISPLAY LIST: opcode = %d, address = %p\n",
                    opcode, (void *) n);
-            } else {
+               goto out;
+            }
+            else {
                fprintf(f, "command %d, %u operands\n", opcode,
                             InstSize[opcode]);
-               break;
             }
-            FALLTHROUGH;
-         case OPCODE_END_OF_LIST:
-            fprintf(f, "END-LIST %u\n", list);
-            fflush(f);
-            if (fname)
-               fclose(f);
-            return;
          }
-
          /* increment n to point to next compiled command */
-         assert(InstSize[opcode] > 0);
-         n += InstSize[opcode];
-      }
-   }
-}
-
-
-void
-_mesa_glthread_execute_list(struct gl_context *ctx, GLuint list)
-{
-   struct gl_display_list *dlist;
-
-   if (list == 0 ||
-       ctx->GLThread.ListCallDepth == MAX_LIST_NESTING ||
-       !_mesa_get_list(ctx, list, &dlist))
-      return;
-
-   ctx->GLThread.ListCallDepth++;
-
-   Node *n = dlist->Head;
-
-   while (1) {
-      const OpCode opcode = n[0].opcode;
-
-      if (is_ext_opcode(opcode)) {
-         n += ctx->ListExt->Opcode[n[0].opcode - OPCODE_EXT_0].Size;
-      } else {
-         switch (opcode) {
-         case OPCODE_CALL_LIST:
-            /* Generated by glCallList(), don't add ListBase */
-            if (ctx->GLThread.ListCallDepth < MAX_LIST_NESTING)
-               _mesa_glthread_execute_list(ctx, n[1].ui);
-            break;
-         case OPCODE_CALL_LISTS:
-            if (ctx->GLThread.ListCallDepth < MAX_LIST_NESTING)
-               _mesa_glthread_CallLists(ctx, n[1].i, n[2].e, get_pointer(&n[3]));
-            break;
-         case OPCODE_DISABLE:
-            _mesa_glthread_Disable(ctx, n[1].e);
-            break;
-         case OPCODE_ENABLE:
-            _mesa_glthread_Enable(ctx, n[1].e);
-            break;
-         case OPCODE_LIST_BASE:
-            _mesa_glthread_ListBase(ctx, n[1].ui);
-            break;
-         case OPCODE_MATRIX_MODE:
-            _mesa_glthread_MatrixMode(ctx, n[1].e);
-            break;
-         case OPCODE_POP_ATTRIB:
-            _mesa_glthread_PopAttrib(ctx);
-            break;
-         case OPCODE_POP_MATRIX:
-            _mesa_glthread_PopMatrix(ctx);
-            break;
-         case OPCODE_PUSH_ATTRIB:
-            _mesa_glthread_PushAttrib(ctx, n[1].bf);
-            break;
-         case OPCODE_PUSH_MATRIX:
-            _mesa_glthread_PushMatrix(ctx);
-            break;
-         case OPCODE_ACTIVE_TEXTURE:   /* GL_ARB_multitexture */
-            _mesa_glthread_ActiveTexture(ctx, n[1].e);
-            break;
-         case OPCODE_MATRIX_PUSH:
-            _mesa_glthread_MatrixPushEXT(ctx, n[1].e);
-            break;
-         case OPCODE_MATRIX_POP:
-            _mesa_glthread_MatrixPopEXT(ctx, n[1].e);
-            break;
-         case OPCODE_CONTINUE:
-            n = (Node *)get_pointer(&n[1]);
-            continue;
-         case OPCODE_END_OF_LIST:
-            ctx->GLThread.ListCallDepth--;
-            return;
-         default:
-            /* ignore */
-            break;
+         if (opcode != OPCODE_CONTINUE) {
+            assert(InstSize[opcode] > 0);
+            n += InstSize[opcode];
          }
-
-         /* increment n to point to next compiled command */
-         assert(InstSize[opcode] > 0);
-         n += InstSize[opcode];
       }
    }
+
+ out:
+   fflush(f);
+   if (fname)
+      fclose(f);
 }
 
 
+
 /**
  * Clients may call this function to help debug display list problems.
  * This function is _ONLY_FOR_DEBUGGING_PURPOSES_.  It may be removed,
diff --git a/src/mesa/main/dlist.h b/src/mesa/main/dlist.h
index 8fa09a7aec2..234f797ea1f 100644
--- a/src/mesa/main/dlist.h
+++ b/src/mesa/main/dlist.h
@@ -133,8 +133,5 @@ _mesa_init_display_list(struct gl_context * ctx);
 void
 _mesa_free_display_list_data(struct gl_context *ctx);
 
-bool
-_mesa_get_list(struct gl_context *ctx, GLuint list,
-               struct gl_display_list **dlist);
 
 #endif /* DLIST_H */
diff --git a/src/mesa/main/draw.c b/src/mesa/main/draw.c
index 2438091ec5d..eedf9c41167 100644
--- a/src/mesa/main/draw.c
+++ b/src/mesa/main/draw.c
@@ -91,26 +91,12 @@ _mesa_draw_gallium_fallback(struct gl_context *ctx,
 {
    struct _mesa_index_buffer ib;
    unsigned index_size = info->index_size;
-   unsigned min_index = 0, max_index = ~0u;
-   bool index_bounds_valid = false;
+   unsigned min_index = info->index_bounds_valid ? info->min_index : 0;
+   unsigned max_index = info->index_bounds_valid ? info->max_index : ~0;
 
    if (!info->instance_count)
       return;
 
-   if (index_size) {
-      if (info->index_bounds_valid) {
-         min_index = info->min_index;
-         max_index = info->max_index;
-         index_bounds_valid = true;
-      }
-   } else {
-      /* The index_bounds_valid field and min/max_index are not used for
-       * non-indexed draw calls (they are undefined), but classic drivers
-       * need the index bounds. They will be computed manually.
-       */
-      index_bounds_valid = true;
-   }
-
    ib.index_size_shift = util_logbase2(index_size);
 
    /* Single draw or a fallback for user indices. */
@@ -146,13 +132,8 @@ _mesa_draw_gallium_fallback(struct gl_context *ctx,
          prim.basevertex = index_size ? info->index_bias : 0;
          prim.draw_id = info->drawid + (info->increment_draw_id ? i : 0);
 
-         if (!index_size) {
-            min_index = draws[i].start;
-            max_index = draws[i].start + draws[i].count - 1;
-         }
-
          ctx->Driver.Draw(ctx, &prim, 1, index_size ? &ib : NULL,
-                          index_bounds_valid, info->primitive_restart,
+                          info->index_bounds_valid, info->primitive_restart,
                           info->restart_index, min_index, max_index,
                           info->instance_count, info->start_instance);
       }
@@ -165,9 +146,6 @@ _mesa_draw_gallium_fallback(struct gl_context *ctx,
 
    ALLOC_PRIMS(prim, num_draws, "DrawGallium");
 
-   min_index = ~0u;
-   max_index = 0;
-
    for (unsigned i = 0; i < num_draws; i++) {
       if (!draws[i].count)
          continue;
@@ -180,11 +158,6 @@ _mesa_draw_gallium_fallback(struct gl_context *ctx,
       prim[num_prims].basevertex = info->index_size ? info->index_bias : 0;
       prim[num_prims].draw_id = info->drawid + (info->increment_draw_id ? i : 0);
 
-      if (!index_size) {
-         min_index = MIN2(min_index, draws[i].start);
-         max_index = MAX2(max_index, draws[i].start + draws[i].count - 1);
-      }
-
       max_count = MAX2(max_count, prim[num_prims].count);
       num_prims++;
    }
@@ -203,7 +176,7 @@ _mesa_draw_gallium_fallback(struct gl_context *ctx,
    }
 
    ctx->Driver.Draw(ctx, prim, num_prims, index_size ? &ib : NULL,
-                    index_bounds_valid, info->primitive_restart,
+                    info->index_bounds_valid, info->primitive_restart,
                     info->restart_index, min_index, max_index,
                     info->instance_count, info->start_instance);
    FREE_PRIMS(prim, num_draws);
@@ -600,8 +573,8 @@ _mesa_draw_arrays(struct gl_context *ctx, GLenum mode, GLint start,
 /**
  * Execute a glRectf() function.
  */
-void GLAPIENTRY
-_mesa_Rectf(GLfloat x1, GLfloat y1, GLfloat x2, GLfloat y2)
+static void GLAPIENTRY
+_mesa_exec_Rectf(GLfloat x1, GLfloat y1, GLfloat x2, GLfloat y2)
 {
    GET_CURRENT_CONTEXT(ctx);
    ASSERT_OUTSIDE_BEGIN_END(ctx);
@@ -617,46 +590,46 @@ _mesa_Rectf(GLfloat x1, GLfloat y1, GLfloat x2, GLfloat y2)
 }
 
 
-void GLAPIENTRY
-_mesa_Rectd(GLdouble x1, GLdouble y1, GLdouble x2, GLdouble y2)
+static void GLAPIENTRY
+_mesa_exec_Rectd(GLdouble x1, GLdouble y1, GLdouble x2, GLdouble y2)
 {
-   _mesa_Rectf((GLfloat) x1, (GLfloat) y1, (GLfloat) x2, (GLfloat) y2);
+   _mesa_exec_Rectf((GLfloat) x1, (GLfloat) y1, (GLfloat) x2, (GLfloat) y2);
 }
 
-void GLAPIENTRY
-_mesa_Rectdv(const GLdouble *v1, const GLdouble *v2)
+static void GLAPIENTRY
+_mesa_exec_Rectdv(const GLdouble *v1, const GLdouble *v2)
 {
-   _mesa_Rectf((GLfloat) v1[0], (GLfloat) v1[1], (GLfloat) v2[0], (GLfloat) v2[1]);
+   _mesa_exec_Rectf((GLfloat) v1[0], (GLfloat) v1[1], (GLfloat) v2[0], (GLfloat) v2[1]);
 }
 
-void GLAPIENTRY
-_mesa_Rectfv(const GLfloat *v1, const GLfloat *v2)
+static void GLAPIENTRY
+_mesa_exec_Rectfv(const GLfloat *v1, const GLfloat *v2)
 {
-   _mesa_Rectf(v1[0], v1[1], v2[0], v2[1]);
+   _mesa_exec_Rectf(v1[0], v1[1], v2[0], v2[1]);
 }
 
-void GLAPIENTRY
-_mesa_Recti(GLint x1, GLint y1, GLint x2, GLint y2)
+static void GLAPIENTRY
+_mesa_exec_Recti(GLint x1, GLint y1, GLint x2, GLint y2)
 {
-   _mesa_Rectf((GLfloat) x1, (GLfloat) y1, (GLfloat) x2, (GLfloat) y2);
+   _mesa_exec_Rectf((GLfloat) x1, (GLfloat) y1, (GLfloat) x2, (GLfloat) y2);
 }
 
-void GLAPIENTRY
-_mesa_Rectiv(const GLint *v1, const GLint *v2)
+static void GLAPIENTRY
+_mesa_exec_Rectiv(const GLint *v1, const GLint *v2)
 {
-   _mesa_Rectf((GLfloat) v1[0], (GLfloat) v1[1], (GLfloat) v2[0], (GLfloat) v2[1]);
+   _mesa_exec_Rectf((GLfloat) v1[0], (GLfloat) v1[1], (GLfloat) v2[0], (GLfloat) v2[1]);
 }
 
-void GLAPIENTRY
-_mesa_Rects(GLshort x1, GLshort y1, GLshort x2, GLshort y2)
+static void GLAPIENTRY
+_mesa_exec_Rects(GLshort x1, GLshort y1, GLshort x2, GLshort y2)
 {
-   _mesa_Rectf((GLfloat) x1, (GLfloat) y1, (GLfloat) x2, (GLfloat) y2);
+   _mesa_exec_Rectf((GLfloat) x1, (GLfloat) y1, (GLfloat) x2, (GLfloat) y2);
 }
 
-void GLAPIENTRY
-_mesa_Rectsv(const GLshort *v1, const GLshort *v2)
+static void GLAPIENTRY
+_mesa_exec_Rectsv(const GLshort *v1, const GLshort *v2)
 {
-   _mesa_Rectf((GLfloat) v1[0], (GLfloat) v1[1], (GLfloat) v2[0], (GLfloat) v2[1]);
+   _mesa_exec_Rectf((GLfloat) v1[0], (GLfloat) v1[1], (GLfloat) v2[0], (GLfloat) v2[1]);
 }
 
 
@@ -889,9 +862,9 @@ _mesa_DrawArraysInstancedBaseInstance(GLenum mode, GLint first,
 /**
  * Called from glMultiDrawArrays when in immediate mode.
  */
-void GLAPIENTRY
-_mesa_MultiDrawArrays(GLenum mode, const GLint *first,
-                      const GLsizei *count, GLsizei primcount)
+static void GLAPIENTRY
+_mesa_exec_MultiDrawArrays(GLenum mode, const GLint *first,
+                           const GLsizei *count, GLsizei primcount)
 {
    GET_CURRENT_CONTEXT(ctx);
    GLint i;
@@ -1598,8 +1571,8 @@ _mesa_validated_multidrawelements(struct gl_context *ctx, GLenum mode,
 
 
 void GLAPIENTRY
-_mesa_MultiDrawElementsEXT(GLenum mode, const GLsizei *count, GLenum type,
-                           const GLvoid * const *indices, GLsizei primcount)
+_mesa_MultiDrawElements(GLenum mode, const GLsizei *count, GLenum type,
+                        const GLvoid * const *indices, GLsizei primcount)
 {
    GET_CURRENT_CONTEXT(ctx);
 
@@ -2171,6 +2144,49 @@ _mesa_MultiDrawElementsIndirectCountARB(GLenum mode, GLenum type,
 }
 
 
+/**
+ * Initialize the dispatch table with the VBO functions for drawing.
+ */
+void
+_mesa_initialize_exec_dispatch(const struct gl_context *ctx,
+                               struct _glapi_table *exec)
+{
+   SET_DrawArrays(exec, _mesa_DrawArrays);
+   SET_DrawElements(exec, _mesa_DrawElements);
+
+   if (_mesa_is_desktop_gl(ctx) || _mesa_is_gles3(ctx)) {
+      SET_DrawRangeElements(exec, _mesa_DrawRangeElements);
+   }
+
+   SET_MultiDrawArrays(exec, _mesa_exec_MultiDrawArrays);
+   SET_MultiDrawElementsEXT(exec, _mesa_MultiDrawElements);
+
+   if (ctx->API == API_OPENGL_COMPAT) {
+      SET_Rectf(exec, _mesa_exec_Rectf);
+      SET_Rectd(exec, _mesa_exec_Rectd);
+      SET_Rectdv(exec, _mesa_exec_Rectdv);
+      SET_Rectfv(exec, _mesa_exec_Rectfv);
+      SET_Recti(exec, _mesa_exec_Recti);
+      SET_Rectiv(exec, _mesa_exec_Rectiv);
+      SET_Rects(exec, _mesa_exec_Rects);
+      SET_Rectsv(exec, _mesa_exec_Rectsv);
+   }
+
+   if (ctx->API != API_OPENGLES &&
+       ctx->Extensions.ARB_draw_elements_base_vertex) {
+      SET_DrawElementsBaseVertex(exec, _mesa_DrawElementsBaseVertex);
+      SET_MultiDrawElementsBaseVertex(exec,
+                                      _mesa_MultiDrawElementsBaseVertex);
+
+      if (_mesa_is_desktop_gl(ctx) || _mesa_is_gles3(ctx)) {
+         SET_DrawRangeElementsBaseVertex(exec,
+                                         _mesa_DrawRangeElementsBaseVertex);
+      }
+   }
+}
+
+
+
 /* GL_IBM_multimode_draw_arrays */
 void GLAPIENTRY
 _mesa_MultiModeDrawArraysIBM( const GLenum * mode, const GLint * first,
diff --git a/src/mesa/main/draw.h b/src/mesa/main/draw.h
index f8dc1a451ed..2e2773e3134 100644
--- a/src/mesa/main/draw.h
+++ b/src/mesa/main/draw.h
@@ -77,6 +77,10 @@ struct _mesa_index_buffer
 };
 
 
+void
+_mesa_initialize_exec_dispatch(const struct gl_context *ctx,
+                               struct _glapi_table *exec);
+
 void
 _mesa_draw_gallium_fallback(struct gl_context *ctx,
                             struct pipe_draw_info *info,
@@ -207,8 +211,8 @@ _mesa_MultiDrawArrays(GLenum mode, const GLint *first,
 
 
 void GLAPIENTRY
-_mesa_MultiDrawElementsEXT(GLenum mode, const GLsizei *count, GLenum type,
-                           const GLvoid *const *indices, GLsizei primcount);
+_mesa_MultiDrawElements(GLenum mode, const GLsizei *count, GLenum type,
+                        const GLvoid *const *indices, GLsizei primcount);
 
 
 void GLAPIENTRY
@@ -229,29 +233,6 @@ _mesa_MultiModeDrawElementsIBM(const GLenum * mode, const GLsizei * count,
                                GLenum type, const GLvoid * const * indices,
                                GLsizei primcount, GLint modestride);
 
-void GLAPIENTRY
-_mesa_Rectf(GLfloat x1, GLfloat y1, GLfloat x2, GLfloat y2);
-
-void GLAPIENTRY
-_mesa_Rectd(GLdouble x1, GLdouble y1, GLdouble x2, GLdouble y2);
-
-void GLAPIENTRY
-_mesa_Rectdv(const GLdouble *v1, const GLdouble *v2);
-
-void GLAPIENTRY
-_mesa_Rectfv(const GLfloat *v1, const GLfloat *v2);
-
-void GLAPIENTRY
-_mesa_Recti(GLint x1, GLint y1, GLint x2, GLint y2);
-
-void GLAPIENTRY
-_mesa_Rectiv(const GLint *v1, const GLint *v2);
-
-void GLAPIENTRY
-_mesa_Rects(GLshort x1, GLshort y1, GLshort x2, GLshort y2);
-
-void GLAPIENTRY
-_mesa_Rectsv(const GLshort *v1, const GLshort *v2);
 
 #ifdef __cplusplus
 } // extern "C"
diff --git a/src/mesa/main/format_pack.h b/src/mesa/main/format_pack.h
index 2d628d20433..e54105aff51 100644
--- a/src/mesa/main/format_pack.h
+++ b/src/mesa/main/format_pack.h
@@ -27,13 +27,14 @@
 #define FORMAT_PACK_H
 
 
-#include "util/format/u_format.h"
 #include "formats.h"
 
-#ifdef __cplusplus
-extern "C" {
-#endif
 
+/** Pack a uint8_t rgba[4] color to dest address */
+typedef void (*mesa_pack_ubyte_rgba_func)(const uint8_t src[4], void *dst);
+
+/** Pack a float rgba[4] color to dest address */
+typedef void (*mesa_pack_float_rgba_func)(const float src[4], void *dst);
 
 /** Pack a float Z value to dest address */
 typedef void (*mesa_pack_float_z_func)(const float *src, void *dst);
@@ -47,6 +48,14 @@ typedef void (*mesa_pack_ubyte_stencil_func)(const uint8_t *src, void *dst);
 
 
 
+extern mesa_pack_ubyte_rgba_func
+_mesa_get_pack_ubyte_rgba_function(mesa_format format);
+
+
+extern mesa_pack_float_rgba_func
+_mesa_get_pack_float_rgba_function(mesa_format format);
+
+
 extern mesa_pack_float_z_func
 _mesa_get_pack_float_z_func(mesa_format format);
 
@@ -59,23 +68,17 @@ extern mesa_pack_ubyte_stencil_func
 _mesa_get_pack_ubyte_stencil_func(mesa_format format);
 
 
-static inline void
+extern void
 _mesa_pack_float_rgba_row(mesa_format format, uint32_t n,
-                          const float src[][4], void *dst)
-{
-   util_format_pack_rgba(format, dst, src, n);
-}
+                          const float src[][4], void *dst);
 
 extern void
 _mesa_pack_ubyte_rgba_row(mesa_format format, uint32_t n,
                           const uint8_t src[][4], void *dst);
 
-static inline void
+extern void
 _mesa_pack_uint_rgba_row(mesa_format format, uint32_t n,
-                         const uint32_t src[][4], void *dst)
-{
-   util_format_pack_rgba(format, dst, src, n);
-}
+                         const uint32_t src[][4], void *dst);
 
 extern void
 _mesa_pack_ubyte_rgba_rect(mesa_format format, uint32_t width, uint32_t height,
@@ -98,8 +101,4 @@ extern void
 _mesa_pack_uint_24_8_depth_stencil_row(mesa_format format, uint32_t n,
                                        const uint32_t *src, void *dst);
 
-#ifdef __cplusplus
-}
-#endif
-
 #endif
diff --git a/src/mesa/main/format_pack.py b/src/mesa/main/format_pack.py
index 2731ad111fe..05aee020297 100644
--- a/src/mesa/main/format_pack.py
+++ b/src/mesa/main/format_pack.py
@@ -160,6 +160,180 @@ pack_ubyte_r11g11b10_float(const uint8_t src[4], void *dst)
    *d = float3_to_r11g11b10f(rgb);
 }
 
+/* uint packing functions */
+
+%for f in rgb_formats:
+   %if not f.is_int():
+      <% continue %>
+   %elif f.is_normalized():
+      <% continue %>
+   %elif f.is_compressed():
+      <% continue %>
+   %endif
+
+static inline void
+pack_uint_${f.short_name()}(const uint32_t src[4], void *dst)
+{
+   %for (i, c) in enumerate(f.channels):
+      <% i = f.swizzle.inverse()[i] %>
+      %if c.type == 'x':
+         <% continue %>
+      %endif
+
+      ${c.datatype()} ${c.name} =
+      %if c.type == parser.SIGNED:
+         _mesa_signed_to_signed(src[${i}], ${c.size});
+      %elif c.type == parser.UNSIGNED:
+         _mesa_unsigned_to_unsigned(src[${i}], ${c.size});
+      %else:
+         assert(!"Invalid type: only integer types are allowed");
+      %endif
+   %endfor
+
+   %if f.layout == parser.ARRAY:
+      ${f.datatype()} *d = (${f.datatype()} *)dst;
+      %for (i, c) in enumerate(f.channels):
+         %if c.type == 'x':
+            <% continue %>
+         %endif
+         d[${i}] = ${c.name};
+      %endfor
+   %elif f.layout == parser.PACKED:
+      ${f.datatype()} d = 0;
+      %for (i, c) in enumerate(f.channels):
+         %if c.type == 'x':
+            <% continue %>
+         %endif
+         d |= PACK(${c.name}, ${c.shift}, ${c.size});
+      %endfor
+      (*(${f.datatype()} *)dst) = d;
+   %else:
+      <% assert False %>
+   %endif
+}
+%endfor
+
+/* float packing functions */
+
+%for f in rgb_formats:
+   %if f.name in ('MESA_FORMAT_R9G9B9E5_FLOAT', 'MESA_FORMAT_R11G11B10_FLOAT'):
+      <% continue %>
+   %elif f.is_int() and not f.is_normalized():
+      <% continue %>
+   %elif f.is_compressed():
+      <% continue %>
+   %endif
+
+static inline void
+pack_float_${f.short_name()}(const float src[4], void *dst)
+{
+   %for (i, c) in enumerate(f.channels):
+      <% i = f.swizzle.inverse()[i] %>
+      %if c.type == 'x':
+         <% continue %>
+      %endif
+
+      ${c.datatype()} ${c.name} =
+      %if c.type == parser.UNSIGNED:
+         %if f.colorspace == 'srgb' and c.name in 'rgb':
+            <% assert c.size == 8 %>
+            util_format_linear_float_to_srgb_8unorm(src[${i}]);
+         %else:
+            _mesa_float_to_unorm(src[${i}], ${c.size});
+         %endif
+      %elif c.type == parser.SIGNED:
+         _mesa_float_to_snorm(src[${i}], ${c.size});
+      %elif c.type == parser.FLOAT:
+         %if c.size == 32:
+            src[${i}];
+         %elif c.size == 16:
+            _mesa_float_to_half(src[${i}]);
+         %else:
+            <% assert False %>
+         %endif
+      %else:
+         <% assert False %>
+      %endif
+   %endfor
+
+   %if f.layout == parser.ARRAY:
+      ${f.datatype()} *d = (${f.datatype()} *)dst;
+      %for (i, c) in enumerate(f.channels):
+         %if c.type == 'x':
+            <% continue %>
+         %endif
+         d[${i}] = ${c.name};
+      %endfor
+   %elif f.layout == parser.PACKED:
+      ${f.datatype()} d = 0;
+      %for (i, c) in enumerate(f.channels):
+         %if c.type == 'x':
+            <% continue %>
+         %endif
+         d |= PACK(${c.name}, ${c.shift}, ${c.size});
+      %endfor
+      (*(${f.datatype()} *)dst) = d;
+   %else:
+      <% assert False %>
+   %endif
+}
+%endfor
+
+static inline void
+pack_float_r9g9b9e5_float(const float src[4], void *dst)
+{
+   uint32_t *d = (uint32_t *) dst;
+   *d = float3_to_rgb9e5(src);
+}
+
+static inline void
+pack_float_r11g11b10_float(const float src[4], void *dst)
+{
+   uint32_t *d = (uint32_t *) dst;
+   *d = float3_to_r11g11b10f(src);
+}
+
+/**
+ * Return a function that can pack a uint8_t rgba[4] color.
+ */
+mesa_pack_ubyte_rgba_func
+_mesa_get_pack_ubyte_rgba_function(mesa_format format)
+{
+   switch (format) {
+%for f in rgb_formats:
+   %if f.is_compressed():
+      <% continue %>
+   %endif
+
+   case ${f.name}:
+      return pack_ubyte_${f.short_name()};
+%endfor
+   default:
+      return NULL;
+   }
+}
+
+/**
+ * Return a function that can pack a float rgba[4] color.
+ */
+mesa_pack_float_rgba_func
+_mesa_get_pack_float_rgba_function(mesa_format format)
+{
+   switch (format) {
+%for f in rgb_formats:
+   %if f.is_compressed():
+      <% continue %>
+   %elif f.is_int() and not f.is_normalized():
+      <% continue %>
+   %endif
+
+   case ${f.name}:
+      return pack_float_${f.short_name()};
+%endfor
+   default:
+      return NULL;
+   }
+}
 
 /**
  * Pack a row of uint8_t rgba[4] values to the destination.
@@ -189,6 +363,67 @@ _mesa_pack_ubyte_rgba_row(mesa_format format, uint32_t n,
    }
 }
 
+/**
+ * Pack a row of uint32_t rgba[4] values to the destination.
+ */
+void
+_mesa_pack_uint_rgba_row(mesa_format format, uint32_t n,
+                          const uint32_t src[][4], void *dst)
+{
+   uint32_t i;
+   uint8_t *d = dst;
+
+   switch (format) {
+%for f in rgb_formats:
+   %if not f.is_int():
+      <% continue %>
+   %elif f.is_normalized():
+      <% continue %>
+   %elif f.is_compressed():
+      <% continue %>
+   %endif
+
+   case ${f.name}:
+      for (i = 0; i < n; ++i) {
+         pack_uint_${f.short_name()}(src[i], d);
+         d += ${f.block_size() // 8};
+      }
+      break;
+%endfor
+   default:
+      assert(!"Invalid format");
+   }
+}
+
+/**
+ * Pack a row of float rgba[4] values to the destination.
+ */
+void
+_mesa_pack_float_rgba_row(mesa_format format, uint32_t n,
+                          const float src[][4], void *dst)
+{
+   uint32_t i;
+   uint8_t *d = dst;
+
+   switch (format) {
+%for f in rgb_formats:
+   %if f.is_compressed():
+      <% continue %>
+   %elif f.is_int() and not f.is_normalized():
+      <% continue %>
+   %endif
+
+   case ${f.name}:
+      for (i = 0; i < n; ++i) {
+         pack_float_${f.short_name()}(src[i], d);
+         d += ${f.block_size() // 8};
+      }
+      break;
+%endfor
+   default:
+      assert(!"Invalid format");
+   }
+}
 
 /**
  * Pack a 2D image of ubyte RGBA pixels in the given format.
diff --git a/src/mesa/main/format_unpack.h b/src/mesa/main/format_unpack.h
index 06283c6bfa8..4de0cc267e2 100644
--- a/src/mesa/main/format_unpack.h
+++ b/src/mesa/main/format_unpack.h
@@ -25,30 +25,19 @@
 #ifndef FORMAT_UNPACK_H
 #define FORMAT_UNPACK_H
 
-#include "util/format/u_format.h"
 #include "formats.h"
 
-#ifdef __cplusplus
-extern "C" {
-#endif
-
-static inline void
+extern void
 _mesa_unpack_rgba_row(mesa_format format, uint32_t n,
-                      const void *src, float dst[][4])
-{
-   util_format_unpack_rgba(format, dst, src, n);
-}
+                      const void *src, float dst[][4]);
 
 extern void
 _mesa_unpack_ubyte_rgba_row(mesa_format format, uint32_t n,
                             const void *src, uint8_t dst[][4]);
 
-static inline void
+void
 _mesa_unpack_uint_rgba_row(mesa_format format, uint32_t n,
-                           const void *src, uint32_t dst[][4])
-{
-   util_format_unpack_rgba(format, dst, src, n);
-}
+                           const void *src, uint32_t dst[][4]);
 
 extern void
 _mesa_unpack_rgba_block(mesa_format format,
@@ -56,27 +45,18 @@ _mesa_unpack_rgba_block(mesa_format format,
                         float dst[][4], int32_t dstRowStride,
                         uint32_t x, uint32_t y, uint32_t width, uint32_t height);
 
-static inline void
+extern void
 _mesa_unpack_float_z_row(mesa_format format, uint32_t n,
-                         const void *src, float *dst)
-{
-   util_format_unpack_z_float((enum pipe_format)format, dst, src, n);
-}
+                         const void *src, float *dst);
 
 
-static inline void
+void
 _mesa_unpack_uint_z_row(mesa_format format, uint32_t n,
-                        const void *src, uint32_t *dst)
-{
-   util_format_unpack_z_32unorm((enum pipe_format)format, dst, src, n);
-}
+                        const void *src, uint32_t *dst);
 
-static inline void
+void
 _mesa_unpack_ubyte_stencil_row(mesa_format format, uint32_t n,
-                               const void *src, uint8_t *dst)
-{
-   util_format_unpack_s_8uint((enum pipe_format)format, dst, src, n);
-}
+                               const void *src, uint8_t *dst);
 
 void
 _mesa_unpack_uint_24_8_depth_stencil_row(mesa_format format, uint32_t n,
@@ -88,8 +68,4 @@ _mesa_unpack_float_32_uint_24_8_depth_stencil_row(mesa_format format,
                                                   const void *src,
                                                   uint32_t *dst);
 
-#ifdef __cplusplus
-}
-#endif
-
 #endif /* FORMAT_UNPACK_H */
diff --git a/src/mesa/main/format_unpack.py b/src/mesa/main/format_unpack.py
new file mode 100644
index 00000000000..161a4021567
--- /dev/null
+++ b/src/mesa/main/format_unpack.py
@@ -0,0 +1,852 @@
+from __future__ import print_function
+
+from mako.template import Template
+from sys import argv
+
+string = """/*
+ * Mesa 3-D graphics library
+ *
+ * Copyright (c) 2011 VMware, Inc.
+ * Copyright (c) 2014 Intel Corporation.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included
+ * in all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
+ * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+
+/**
+ * Color, depth, stencil packing functions.
+ * Used to pack basic color, depth and stencil formats to specific
+ * hardware formats.
+ *
+ * There are both per-pixel and per-row packing functions:
+ * - The former will be used by swrast to write values to the color, depth,
+ *   stencil buffers when drawing points, lines and masked spans.
+ * - The later will be used for image-oriented functions like glDrawPixels,
+ *   glAccum, and glTexImage.
+ */
+
+#include <stdint.h>
+#include <stdlib.h>
+
+#include "format_unpack.h"
+#include "format_utils.h"
+#include "macros.h"
+#include "util/format_rgb9e5.h"
+#include "util/format_r11g11b10f.h"
+#include "util/format_srgb.h"
+
+#define UNPACK(SRC, OFFSET, BITS) (((SRC) >> (OFFSET)) & MAX_UINT(BITS))
+
+<%
+import format_parser as parser
+
+formats = parser.parse(argv[1])
+
+rgb_formats = []
+for f in formats:
+   if f.name == 'MESA_FORMAT_NONE':
+      continue
+   if f.colorspace not in ('rgb', 'srgb'):
+      continue
+
+   rgb_formats.append(f)
+%>
+
+/* float unpacking functions */
+
+%for f in rgb_formats:
+   %if f.name in ('MESA_FORMAT_R9G9B9E5_FLOAT', 'MESA_FORMAT_R11G11B10_FLOAT'):
+      <% continue %>
+   %elif f.is_int() and not f.is_normalized():
+      <% continue %>
+   %elif f.is_compressed():
+      <% continue %>
+   %endif
+
+static inline void
+unpack_float_${f.short_name()}(const void *void_src, float dst[4])
+{
+   ${f.datatype()} *src = (${f.datatype()} *)void_src;
+   %if f.layout == parser.PACKED:
+      %for c in f.channels:
+         %if c.type != 'x':
+            ${c.datatype()} ${c.name} = UNPACK(*src, ${c.shift}, ${c.size});
+         %endif
+      %endfor
+   %elif f.layout == parser.ARRAY:
+      %for (i, c) in enumerate(f.channels):
+         %if c.type != 'x':
+            ${c.datatype()} ${c.name} = src[${i}];
+         %endif
+      %endfor
+   %else:
+      <% assert False %>
+   %endif
+
+   %for i in range(4):
+      <% s = f.swizzle[i] %>
+      %if 0 <= s and s <= parser.Swizzle.SWIZZLE_W:
+         <% c = f.channels[s] %>
+         %if c.type == parser.UNSIGNED:
+            %if f.colorspace == 'srgb' and c.name in 'rgb':
+               <% assert c.size == 8 %>
+               dst[${i}] = util_format_srgb_8unorm_to_linear_float(${c.name});
+            %else:
+               dst[${i}] = _mesa_unorm_to_float(${c.name}, ${c.size});
+            %endif
+         %elif c.type == parser.SIGNED:
+            dst[${i}] = _mesa_snorm_to_float(${c.name}, ${c.size});
+         %elif c.type == parser.FLOAT:
+            %if c.size == 32:
+               dst[${i}] = ${c.name};
+            %elif c.size == 16:
+               dst[${i}] = _mesa_half_to_float(${c.name});
+            %else:
+               <% assert False %>
+            %endif
+         %else:
+            <% assert False %>
+         %endif
+      %elif s == parser.Swizzle.SWIZZLE_ZERO:
+         dst[${i}] = 0.0f;
+      %elif s == parser.Swizzle.SWIZZLE_ONE:
+         dst[${i}] = 1.0f;
+      %else:
+         <% assert False %>
+      %endif
+   %endfor
+}
+%endfor
+
+static void
+unpack_float_r9g9b9e5_float(const void *src, float dst[4])
+{
+   rgb9e5_to_float3(*(const uint32_t *)src, dst);
+   dst[3] = 1.0f;
+}
+
+static void
+unpack_float_r11g11b10_float(const void *src, float dst[4])
+{
+   r11g11b10f_to_float3(*(const uint32_t *)src, dst);
+   dst[3] = 1.0f;
+}
+
+static void
+unpack_float_ycbcr(const void *src, float dst[][4], uint32_t n)
+{
+   uint32_t i;
+   for (i = 0; i < n; i++) {
+      const uint16_t *src0 = ((const uint16_t *) src) + i * 2; /* even */
+      const uint16_t *src1 = src0 + 1;         /* odd */
+      const uint8_t y0 = (*src0 >> 8) & 0xff;  /* luminance */
+      const uint8_t cb = *src0 & 0xff;         /* chroma U */
+      const uint8_t y1 = (*src1 >> 8) & 0xff;  /* luminance */
+      const uint8_t cr = *src1 & 0xff;         /* chroma V */
+      const uint8_t y = (i & 1) ? y1 : y0;     /* choose even/odd luminance */
+      float r = 1.164F * (y - 16) + 1.596F * (cr - 128);
+      float g = 1.164F * (y - 16) - 0.813F * (cr - 128) - 0.391F * (cb - 128);
+      float b = 1.164F * (y - 16) + 2.018F * (cb - 128);
+      r *= (1.0F / 255.0F);
+      g *= (1.0F / 255.0F);
+      b *= (1.0F / 255.0F);
+      dst[i][0] = CLAMP(r, 0.0F, 1.0F);
+      dst[i][1] = CLAMP(g, 0.0F, 1.0F);
+      dst[i][2] = CLAMP(b, 0.0F, 1.0F);
+      dst[i][3] = 1.0F;
+   }
+}
+
+static void
+unpack_float_ycbcr_rev(const void *src, float dst[][4], uint32_t n)
+{
+   uint32_t i;
+   for (i = 0; i < n; i++) {
+      const uint16_t *src0 = ((const uint16_t *) src) + i * 2; /* even */
+      const uint16_t *src1 = src0 + 1;         /* odd */
+      const uint8_t y0 = *src0 & 0xff;         /* luminance */
+      const uint8_t cr = (*src0 >> 8) & 0xff;  /* chroma V */
+      const uint8_t y1 = *src1 & 0xff;         /* luminance */
+      const uint8_t cb = (*src1 >> 8) & 0xff;  /* chroma U */
+      const uint8_t y = (i & 1) ? y1 : y0;     /* choose even/odd luminance */
+      float r = 1.164F * (y - 16) + 1.596F * (cr - 128);
+      float g = 1.164F * (y - 16) - 0.813F * (cr - 128) - 0.391F * (cb - 128);
+      float b = 1.164F * (y - 16) + 2.018F * (cb - 128);
+      r *= (1.0F / 255.0F);
+      g *= (1.0F / 255.0F);
+      b *= (1.0F / 255.0F);
+      dst[i][0] = CLAMP(r, 0.0F, 1.0F);
+      dst[i][1] = CLAMP(g, 0.0F, 1.0F);
+      dst[i][2] = CLAMP(b, 0.0F, 1.0F);
+      dst[i][3] = 1.0F;
+   }
+}
+
+/* ubyte packing functions */
+
+%for f in rgb_formats:
+   %if not f.is_normalized():
+      <% continue %>
+   %endif
+
+static inline void
+unpack_ubyte_${f.short_name()}(const void *void_src, uint8_t dst[4])
+{
+   ${f.datatype()} *src = (${f.datatype()} *)void_src;
+   %if f.layout == parser.PACKED:
+      %for c in f.channels:
+         %if c.type != 'x':
+            ${c.datatype()} ${c.name} = UNPACK(*src, ${c.shift}, ${c.size});
+         %endif
+      %endfor
+   %elif f.layout == parser.ARRAY:
+      %for (i, c) in enumerate(f.channels):
+         %if c.type != 'x':
+            ${c.datatype()} ${c.name} = src[${i}];
+         %endif
+      %endfor
+   %else:
+      <% assert False %>
+   %endif
+
+   %for i in range(4):
+      <% s = f.swizzle[i] %>
+      %if 0 <= s and s <= parser.Swizzle.SWIZZLE_W:
+         <% c = f.channels[s] %>
+         %if c.type == parser.UNSIGNED:
+            %if f.colorspace == 'srgb' and c.name in 'rgb':
+               <% assert c.size == 8 %>
+               dst[${i}] = util_format_srgb_to_linear_8unorm(${c.name});
+            %else:
+               dst[${i}] = _mesa_unorm_to_unorm(${c.name}, ${c.size}, 8);
+            %endif
+         %elif c.type == parser.SIGNED:
+            dst[${i}] = _mesa_snorm_to_unorm(${c.name}, ${c.size}, 8);
+         %elif c.type == parser.FLOAT:
+            %if c.size == 32:
+               dst[${i}] = _mesa_float_to_unorm(${c.name}, 8);
+            %elif c.size == 16:
+               dst[${i}] = _mesa_half_to_unorm(${c.name}, 8);
+            %else:
+               <% assert False %>
+            %endif
+         %else:
+            <% assert False %>
+         %endif
+      %elif s == parser.Swizzle.SWIZZLE_ZERO:
+         dst[${i}] = 0;
+      %elif s == parser.Swizzle.SWIZZLE_ONE:
+         dst[${i}] = 255;
+      %else:
+         <% assert False %>
+      %endif
+   %endfor
+}
+%endfor
+
+/* integer packing functions */
+
+%for f in rgb_formats:
+   %if not f.is_int():
+      <% continue %>
+   %elif f.is_normalized():
+      <% continue %>
+   %endif
+
+static inline void
+unpack_int_${f.short_name()}(const void *void_src, uint32_t dst[4])
+{
+   ${f.datatype()} *src = (${f.datatype()} *)void_src;
+   %if f.layout == parser.PACKED:
+      %for c in f.channels:
+         %if c.type != 'x':
+            ${c.datatype()} ${c.name} = UNPACK(*src, ${c.shift}, ${c.size});
+         %endif
+      %endfor
+   %elif f.layout == parser.ARRAY:
+      %for (i, c) in enumerate(f.channels):
+         %if c.type != 'x':
+            ${c.datatype()} ${c.name} = src[${i}];
+         %endif
+      %endfor
+   %else:
+      <% assert False %>
+   %endif
+
+   %for i in range(4):
+      <% s = f.swizzle[i] %>
+      %if 0 <= s and s <= parser.Swizzle.SWIZZLE_W:
+         dst[${i}] = ${f.channels[s].name};
+      %elif s == parser.Swizzle.SWIZZLE_ZERO:
+         dst[${i}] = 0;
+      %elif s == parser.Swizzle.SWIZZLE_ONE:
+         dst[${i}] = 1;
+      %else:
+         <% assert False %>
+      %endif
+   %endfor
+}
+%endfor
+
+
+void
+_mesa_unpack_rgba_row(mesa_format format, uint32_t n,
+                      const void *src, float dst[][4])
+{
+   uint8_t *s = (uint8_t *)src;
+   uint32_t i;
+
+   switch (format) {
+%for f in rgb_formats:
+   %if f.is_compressed():
+      <% continue %>
+   %elif f.is_int() and not f.is_normalized():
+      <% continue %>
+   %endif
+   case ${f.name}:
+      for (i = 0; i < n; ++i) {
+         unpack_float_${f.short_name()}(s, dst[i]);
+         s += ${f.block_size() // 8};
+      }
+      break;
+%endfor
+   case MESA_FORMAT_YCBCR:
+      unpack_float_ycbcr(src, dst, n);
+      break;
+   case MESA_FORMAT_YCBCR_REV:
+      unpack_float_ycbcr_rev(src, dst, n);
+      break;
+   default:
+      unreachable("bad format");
+   }
+}
+
+void
+_mesa_unpack_ubyte_rgba_row(mesa_format format, uint32_t n,
+                            const void *src, uint8_t dst[][4])
+{
+   uint8_t *s = (uint8_t *)src;
+   uint32_t i;
+
+   switch (format) {
+%for f in rgb_formats:
+   %if not f.is_normalized():
+      <% continue %>
+   %endif
+
+   case ${f.name}:
+      for (i = 0; i < n; ++i) {
+         unpack_ubyte_${f.short_name()}(s, dst[i]);
+         s += ${f.block_size() // 8};
+      }
+      break;
+%endfor
+   default:
+      /* get float values, convert to ubyte */
+      {
+         float *tmp = malloc(n * 4 * sizeof(float));
+         if (tmp) {
+            uint32_t i;
+            _mesa_unpack_rgba_row(format, n, src, (float (*)[4]) tmp);
+            for (i = 0; i < n; i++) {
+               dst[i][0] = _mesa_float_to_unorm(tmp[i*4+0], 8);
+               dst[i][1] = _mesa_float_to_unorm(tmp[i*4+1], 8);
+               dst[i][2] = _mesa_float_to_unorm(tmp[i*4+2], 8);
+               dst[i][3] = _mesa_float_to_unorm(tmp[i*4+3], 8);
+            }
+            free(tmp);
+         }
+      }
+      break;
+   }
+}
+
+void
+_mesa_unpack_uint_rgba_row(mesa_format format, uint32_t n,
+                           const void *src, uint32_t dst[][4])
+{
+   uint8_t *s = (uint8_t *)src;
+   uint32_t i;
+
+   switch (format) {
+%for f in rgb_formats:
+   %if not f.is_int():
+      <% continue %>
+   %elif f.is_normalized():
+      <% continue %>
+   %endif
+
+   case ${f.name}:
+      for (i = 0; i < n; ++i) {
+         unpack_int_${f.short_name()}(s, dst[i]);
+         s += ${f.block_size() // 8};
+      }
+      break;
+%endfor
+   default:
+      unreachable("bad format");
+   }
+}
+
+/**
+ * Unpack a 2D rect of pixels returning float RGBA colors.
+ * \param format  the source image format
+ * \param src  start address of the source image
+ * \param srcRowStride  source image row stride in bytes
+ * \param dst  start address of the dest image
+ * \param dstRowStride  dest image row stride in bytes
+ * \param x  source image start X pos
+ * \param y  source image start Y pos
+ * \param width  width of rect region to convert
+ * \param height  height of rect region to convert
+ */
+void
+_mesa_unpack_rgba_block(mesa_format format,
+                        const void *src, int32_t srcRowStride,
+                        float dst[][4], int32_t dstRowStride,
+                        uint32_t x, uint32_t y, uint32_t width, uint32_t height)
+{
+   const uint32_t srcPixStride = _mesa_get_format_bytes(format);
+   const uint32_t dstPixStride = 4 * sizeof(float);
+   const uint8_t *srcRow;
+   uint8_t *dstRow;
+   uint32_t i;
+
+   /* XXX needs to be fixed for compressed formats */
+
+   srcRow = ((const uint8_t *) src) + srcRowStride * y + srcPixStride * x;
+   dstRow = ((uint8_t *) dst) + dstRowStride * y + dstPixStride * x;
+
+   for (i = 0; i < height; i++) {
+      _mesa_unpack_rgba_row(format, width, srcRow, (float (*)[4]) dstRow);
+
+      dstRow += dstRowStride;
+      srcRow += srcRowStride;
+   }
+}
+
+/** Helper struct for MESA_FORMAT_Z32_FLOAT_S8X24_UINT */
+struct z32f_x24s8
+{
+   float z;
+   uint32_t x24s8;
+};
+
+typedef void (*unpack_float_z_func)(uint32_t n, const void *src, float *dst);
+
+static void
+unpack_float_z_X8_UINT_Z24_UNORM(uint32_t n, const void *src, float *dst)
+{
+   /* only return Z, not stencil data */
+   const uint32_t *s = ((const uint32_t *) src);
+   const double scale = 1.0 / (double) 0xffffff;
+   uint32_t i;
+   for (i = 0; i < n; i++) {
+      dst[i] = (float) ((s[i] >> 8) * scale);
+      assert(dst[i] >= 0.0F);
+      assert(dst[i] <= 1.0F);
+   }
+}
+
+static void
+unpack_float_z_Z24_UNORM_X8_UINT(uint32_t n, const void *src, float *dst)
+{
+   /* only return Z, not stencil data */
+   const uint32_t *s = ((const uint32_t *) src);
+   const double scale = 1.0 / (double) 0xffffff;
+   uint32_t i;
+   for (i = 0; i < n; i++) {
+      dst[i] = (float) ((s[i] & 0x00ffffff) * scale);
+      assert(dst[i] >= 0.0F);
+      assert(dst[i] <= 1.0F);
+   }
+}
+
+static void
+unpack_float_Z_UNORM16(uint32_t n, const void *src, float *dst)
+{
+   const uint16_t *s = ((const uint16_t *) src);
+   uint32_t i;
+   for (i = 0; i < n; i++) {
+      dst[i] = s[i] * (1.0F / 65535.0F);
+   }
+}
+
+static void
+unpack_float_Z_UNORM32(uint32_t n, const void *src, float *dst)
+{
+   const uint32_t *s = ((const uint32_t *) src);
+   uint32_t i;
+   for (i = 0; i < n; i++) {
+      dst[i] = s[i] * (1.0F / 0xffffffff);
+   }
+}
+
+static void
+unpack_float_Z_FLOAT32(uint32_t n, const void *src, float *dst)
+{
+   memcpy(dst, src, n * sizeof(float));
+}
+
+static void
+unpack_float_z_Z32X24S8(uint32_t n, const void *src, float *dst)
+{
+   const struct z32f_x24s8 *s = (const struct z32f_x24s8 *) src;
+   uint32_t i;
+   for (i = 0; i < n; i++) {
+      dst[i] = s[i].z;
+   }
+}
+
+
+
+/**
+ * Unpack Z values.
+ * The returned values will always be in the range [0.0, 1.0].
+ */
+void
+_mesa_unpack_float_z_row(mesa_format format, uint32_t n,
+                         const void *src, float *dst)
+{
+   unpack_float_z_func unpack;
+
+   switch (format) {
+   case MESA_FORMAT_S8_UINT_Z24_UNORM:
+   case MESA_FORMAT_X8_UINT_Z24_UNORM:
+      unpack = unpack_float_z_X8_UINT_Z24_UNORM;
+      break;
+   case MESA_FORMAT_Z24_UNORM_S8_UINT:
+   case MESA_FORMAT_Z24_UNORM_X8_UINT:
+      unpack = unpack_float_z_Z24_UNORM_X8_UINT;
+      break;
+   case MESA_FORMAT_Z_UNORM16:
+      unpack = unpack_float_Z_UNORM16;
+      break;
+   case MESA_FORMAT_Z_UNORM32:
+      unpack = unpack_float_Z_UNORM32;
+      break;
+   case MESA_FORMAT_Z_FLOAT32:
+      unpack = unpack_float_Z_FLOAT32;
+      break;
+   case MESA_FORMAT_Z32_FLOAT_S8X24_UINT:
+      unpack = unpack_float_z_Z32X24S8;
+      break;
+   default:
+      unreachable("bad format in _mesa_unpack_float_z_row");
+   }
+
+   unpack(n, src, dst);
+}
+
+
+
+typedef void (*unpack_uint_z_func)(const void *src, uint32_t *dst, uint32_t n);
+
+static void
+unpack_uint_z_X8_UINT_Z24_UNORM(const void *src, uint32_t *dst, uint32_t n)
+{
+   /* only return Z, not stencil data */
+   const uint32_t *s = ((const uint32_t *) src);
+   uint32_t i;
+   for (i = 0; i < n; i++) {
+      dst[i] = (s[i] & 0xffffff00) | (s[i] >> 24);
+   }
+}
+
+static void
+unpack_uint_z_Z24_UNORM_X8_UINT(const void *src, uint32_t *dst, uint32_t n)
+{
+   /* only return Z, not stencil data */
+   const uint32_t *s = ((const uint32_t *) src);
+   uint32_t i;
+   for (i = 0; i < n; i++) {
+      dst[i] = (s[i] << 8) | ((s[i] >> 16) & 0xff);
+   }
+}
+
+static void
+unpack_uint_Z_UNORM16(const void *src, uint32_t *dst, uint32_t n)
+{
+   const uint16_t *s = ((const uint16_t *)src);
+   uint32_t i;
+   for (i = 0; i < n; i++) {
+      dst[i] = (s[i] << 16) | s[i];
+   }
+}
+
+static void
+unpack_uint_Z_UNORM32(const void *src, uint32_t *dst, uint32_t n)
+{
+   memcpy(dst, src, n * sizeof(uint32_t));
+}
+
+static void
+unpack_uint_Z_FLOAT32(const void *src, uint32_t *dst, uint32_t n)
+{
+   const float *s = (const float *)src;
+   uint32_t i;
+   for (i = 0; i < n; i++) {
+      dst[i] = FLOAT_TO_UINT(CLAMP(s[i], 0.0F, 1.0F));
+   }
+}
+
+static void
+unpack_uint_Z_FLOAT32_X24S8(const void *src, uint32_t *dst, uint32_t n)
+{
+   const struct z32f_x24s8 *s = (const struct z32f_x24s8 *) src;
+   uint32_t i;
+
+   for (i = 0; i < n; i++) {
+      dst[i] = FLOAT_TO_UINT(CLAMP(s[i].z, 0.0F, 1.0F));
+   }
+}
+
+
+/**
+ * Unpack Z values.
+ * The returned values will always be in the range [0, 0xffffffff].
+ */
+void
+_mesa_unpack_uint_z_row(mesa_format format, uint32_t n,
+                        const void *src, uint32_t *dst)
+{
+   unpack_uint_z_func unpack;
+   const uint8_t *srcPtr = (uint8_t *) src;
+
+   switch (format) {
+   case MESA_FORMAT_S8_UINT_Z24_UNORM:
+   case MESA_FORMAT_X8_UINT_Z24_UNORM:
+      unpack = unpack_uint_z_X8_UINT_Z24_UNORM;
+      break;
+   case MESA_FORMAT_Z24_UNORM_S8_UINT:
+   case MESA_FORMAT_Z24_UNORM_X8_UINT:
+      unpack = unpack_uint_z_Z24_UNORM_X8_UINT;
+      break;
+   case MESA_FORMAT_Z_UNORM16:
+      unpack = unpack_uint_Z_UNORM16;
+      break;
+   case MESA_FORMAT_Z_UNORM32:
+      unpack = unpack_uint_Z_UNORM32;
+      break;
+   case MESA_FORMAT_Z_FLOAT32:
+      unpack = unpack_uint_Z_FLOAT32;
+      break;
+   case MESA_FORMAT_Z32_FLOAT_S8X24_UINT:
+      unpack = unpack_uint_Z_FLOAT32_X24S8;
+      break;
+   default:
+      unreachable("bad format %s in _mesa_unpack_uint_z_row");
+   }
+
+   unpack(srcPtr, dst, n);
+}
+
+
+static void
+unpack_ubyte_s_S_UINT8(const void *src, uint8_t *dst, uint32_t n)
+{
+   memcpy(dst, src, n);
+}
+
+static void
+unpack_ubyte_s_S8_UINT_Z24_UNORM(const void *src, uint8_t *dst, uint32_t n)
+{
+   uint32_t i;
+   const uint32_t *src32 = src;
+
+   for (i = 0; i < n; i++)
+      dst[i] = src32[i] & 0xff;
+}
+
+static void
+unpack_ubyte_s_Z24_UNORM_S8_UINT(const void *src, uint8_t *dst, uint32_t n)
+{
+   uint32_t i;
+   const uint32_t *src32 = src;
+
+   for (i = 0; i < n; i++)
+      dst[i] = src32[i] >> 24;
+}
+
+static void
+unpack_ubyte_s_Z32_FLOAT_S8X24_UINT(const void *src, uint8_t *dst, uint32_t n)
+{
+   uint32_t i;
+   const struct z32f_x24s8 *s = (const struct z32f_x24s8 *) src;
+
+   for (i = 0; i < n; i++)
+      dst[i] = s[i].x24s8 & 0xff;
+}
+
+void
+_mesa_unpack_ubyte_stencil_row(mesa_format format, uint32_t n,
+			       const void *src, uint8_t *dst)
+{
+   switch (format) {
+   case MESA_FORMAT_S_UINT8:
+      unpack_ubyte_s_S_UINT8(src, dst, n);
+      break;
+   case MESA_FORMAT_S8_UINT_Z24_UNORM:
+      unpack_ubyte_s_S8_UINT_Z24_UNORM(src, dst, n);
+      break;
+   case MESA_FORMAT_Z24_UNORM_S8_UINT:
+      unpack_ubyte_s_Z24_UNORM_S8_UINT(src, dst, n);
+      break;
+   case MESA_FORMAT_Z32_FLOAT_S8X24_UINT:
+      unpack_ubyte_s_Z32_FLOAT_S8X24_UINT(src, dst, n);
+      break;
+   default:
+      unreachable("bad format %s in _mesa_unpack_ubyte_s_row");
+   }
+}
+
+static void
+unpack_uint_24_8_depth_stencil_Z24_UNORM_S8_UINT(const uint32_t *src, uint32_t *dst, uint32_t n)
+{
+   uint32_t i;
+
+   for (i = 0; i < n; i++) {
+      uint32_t val = src[i];
+      dst[i] = val >> 24 | val << 8;
+   }
+}
+
+static void
+unpack_uint_24_8_depth_stencil_Z32_S8X24(const uint32_t *src,
+                                         uint32_t *dst, uint32_t n)
+{
+   uint32_t i;
+
+   for (i = 0; i < n; i++) {
+      /* 8 bytes per pixel (float + uint32) */
+      float zf = ((float *) src)[i * 2 + 0];
+      uint32_t z24 = (uint32_t) (zf * (float) 0xffffff);
+      uint32_t s = src[i * 2 + 1] & 0xff;
+      dst[i] = (z24 << 8) | s;
+   }
+}
+
+static void
+unpack_uint_24_8_depth_stencil_S8_UINT_Z24_UNORM(const uint32_t *src, uint32_t *dst, uint32_t n)
+{
+   memcpy(dst, src, n * 4);
+}
+
+/**
+ * Unpack depth/stencil returning as GL_UNSIGNED_INT_24_8.
+ * \param format  the source data format
+ */
+void
+_mesa_unpack_uint_24_8_depth_stencil_row(mesa_format format, uint32_t n,
+					 const void *src, uint32_t *dst)
+{
+   switch (format) {
+   case MESA_FORMAT_S8_UINT_Z24_UNORM:
+      unpack_uint_24_8_depth_stencil_S8_UINT_Z24_UNORM(src, dst, n);
+      break;
+   case MESA_FORMAT_Z24_UNORM_S8_UINT:
+      unpack_uint_24_8_depth_stencil_Z24_UNORM_S8_UINT(src, dst, n);
+      break;
+   case MESA_FORMAT_Z32_FLOAT_S8X24_UINT:
+      unpack_uint_24_8_depth_stencil_Z32_S8X24(src, dst, n);
+      break;
+   default:
+      unreachable("bad format %s in _mesa_unpack_uint_24_8_depth_stencil_row");
+   }
+}
+
+static void
+unpack_float_32_uint_24_8_Z24_UNORM_S8_UINT(const uint32_t *src,
+                                            uint32_t *dst, uint32_t n)
+{
+   uint32_t i;
+   struct z32f_x24s8 *d = (struct z32f_x24s8 *) dst;
+   const double scale = 1.0 / (double) 0xffffff;
+
+   for (i = 0; i < n; i++) {
+      const uint32_t z24 = src[i] & 0xffffff;
+      d[i].z = z24 * scale;
+      d[i].x24s8 = src[i] >> 24;
+      assert(d[i].z >= 0.0f);
+      assert(d[i].z <= 1.0f);
+   }
+}
+
+static void
+unpack_float_32_uint_24_8_Z32_FLOAT_S8X24_UINT(const uint32_t *src,
+                                               uint32_t *dst, uint32_t n)
+{
+   memcpy(dst, src, n * sizeof(struct z32f_x24s8));
+}
+
+static void
+unpack_float_32_uint_24_8_S8_UINT_Z24_UNORM(const uint32_t *src,
+                                            uint32_t *dst, uint32_t n)
+{
+   uint32_t i;
+   struct z32f_x24s8 *d = (struct z32f_x24s8 *) dst;
+   const double scale = 1.0 / (double) 0xffffff;
+
+   for (i = 0; i < n; i++) {
+      const uint32_t z24 = src[i] >> 8;
+      d[i].z = z24 * scale;
+      d[i].x24s8 = src[i] & 0xff;
+      assert(d[i].z >= 0.0f);
+      assert(d[i].z <= 1.0f);
+   }
+}
+
+/**
+ * Unpack depth/stencil returning as GL_FLOAT_32_UNSIGNED_INT_24_8_REV.
+ * \param format  the source data format
+ *
+ * In GL_FLOAT_32_UNSIGNED_INT_24_8_REV lower 4 bytes contain float
+ * component and higher 4 bytes contain packed 24-bit and 8-bit
+ * components.
+ *
+ *    31 30 29 28 ... 4 3 2 1 0    31 30 29 ... 9 8 7 6 5 ... 2 1 0
+ *    +-------------------------+  +--------------------------------+
+ *    |    Float Component      |  | Unused         | 8 bit stencil |
+ *    +-------------------------+  +--------------------------------+
+ *          lower 4 bytes                  higher 4 bytes
+ */
+void
+_mesa_unpack_float_32_uint_24_8_depth_stencil_row(mesa_format format, uint32_t n,
+			                          const void *src, uint32_t *dst)
+{
+   switch (format) {
+   case MESA_FORMAT_S8_UINT_Z24_UNORM:
+      unpack_float_32_uint_24_8_S8_UINT_Z24_UNORM(src, dst, n);
+      break;
+   case MESA_FORMAT_Z24_UNORM_S8_UINT:
+      unpack_float_32_uint_24_8_Z24_UNORM_S8_UINT(src, dst, n);
+      break;
+   case MESA_FORMAT_Z32_FLOAT_S8X24_UINT:
+      unpack_float_32_uint_24_8_Z32_FLOAT_S8X24_UINT(src, dst, n);
+      break;
+   default:
+      unreachable("bad format %s in _mesa_unpack_uint_24_8_depth_stencil_row");
+   }
+}
+
+"""
+
+template = Template(string, future_imports=['division']);
+
+print(template.render(argv = argv[0:]))
diff --git a/src/mesa/main/format_utils.h b/src/mesa/main/format_utils.h
index b4e8f99fe60..9b6d1c39be7 100644
--- a/src/mesa/main/format_utils.h
+++ b/src/mesa/main/format_utils.h
@@ -36,13 +36,187 @@
 #include "macros.h"
 #include "util/rounding.h"
 #include "util/half_float.h"
-#include "util/format/format_utils.h"
 
 extern const mesa_array_format RGBA32_FLOAT;
 extern const mesa_array_format RGBA8_UBYTE;
 extern const mesa_array_format RGBA32_UINT;
 extern const mesa_array_format RGBA32_INT;
 
+/* Only guaranteed to work for BITS <= 32 */
+#define MAX_UINT(BITS) ((BITS) == 32 ? UINT32_MAX : ((1u << (BITS)) - 1))
+#define MAX_INT(BITS) ((int)MAX_UINT((BITS) - 1))
+#define MIN_INT(BITS) ((BITS) == 32 ? INT32_MIN : (-(1 << (BITS - 1))))
+
+/* Extends an integer of size SRC_BITS to one of size DST_BITS linearly */
+#define EXTEND_NORMALIZED_INT(X, SRC_BITS, DST_BITS) \
+      (((X) * (int)(MAX_UINT(DST_BITS) / MAX_UINT(SRC_BITS))) + \
+       ((DST_BITS % SRC_BITS) ? ((X) >> (SRC_BITS - DST_BITS % SRC_BITS)) : 0))
+
+static inline float
+_mesa_unorm_to_float(unsigned x, unsigned src_bits)
+{
+   return x * (1.0f / (float)MAX_UINT(src_bits));
+}
+
+static inline float
+_mesa_snorm_to_float(int x, unsigned src_bits)
+{
+   if (x <= -MAX_INT(src_bits))
+      return -1.0f;
+   else
+      return x * (1.0f / (float)MAX_INT(src_bits));
+}
+
+static inline uint16_t
+_mesa_unorm_to_half(unsigned x, unsigned src_bits)
+{
+   return _mesa_float_to_half(_mesa_unorm_to_float(x, src_bits));
+}
+
+static inline uint16_t
+_mesa_snorm_to_half(int x, unsigned src_bits)
+{
+   return _mesa_float_to_half(_mesa_snorm_to_float(x, src_bits));
+}
+
+static inline unsigned
+_mesa_float_to_unorm(float x, unsigned dst_bits)
+{
+   if (x < 0.0f)
+      return 0;
+   else if (x > 1.0f)
+      return MAX_UINT(dst_bits);
+   else
+      return _mesa_i64roundevenf(x * MAX_UINT(dst_bits));
+}
+
+static inline unsigned
+_mesa_half_to_unorm(uint16_t x, unsigned dst_bits)
+{
+   return _mesa_float_to_unorm(_mesa_half_to_float(x), dst_bits);
+}
+
+static inline unsigned
+_mesa_unorm_to_unorm(unsigned x, unsigned src_bits, unsigned dst_bits)
+{
+   if (src_bits < dst_bits) {
+      return EXTEND_NORMALIZED_INT(x, src_bits, dst_bits);
+   } else if (src_bits > dst_bits) {
+      unsigned src_half = (1 << (src_bits - 1)) - 1;
+
+      if (src_bits + dst_bits > sizeof(x) * 8) {
+         assert(src_bits + dst_bits <= sizeof(uint64_t) * 8);
+         return (((uint64_t) x * MAX_UINT(dst_bits) + src_half) /
+                 MAX_UINT(src_bits));
+      } else {
+         return (x * MAX_UINT(dst_bits) + src_half) / MAX_UINT(src_bits);
+      }
+   } else {
+      return x;
+   }
+}
+
+static inline unsigned
+_mesa_snorm_to_unorm(int x, unsigned src_bits, unsigned dst_bits)
+{
+   if (x < 0)
+      return 0;
+   else
+      return _mesa_unorm_to_unorm(x, src_bits - 1, dst_bits);
+}
+
+static inline int
+_mesa_float_to_snorm(float x, unsigned dst_bits)
+{
+   if (x < -1.0f)
+      return -MAX_INT(dst_bits);
+   else if (x > 1.0f)
+      return MAX_INT(dst_bits);
+   else
+      return _mesa_lroundevenf(x * MAX_INT(dst_bits));
+}
+
+static inline int
+_mesa_half_to_snorm(uint16_t x, unsigned dst_bits)
+{
+   return _mesa_float_to_snorm(_mesa_half_to_float(x), dst_bits);
+}
+
+static inline int
+_mesa_unorm_to_snorm(unsigned x, unsigned src_bits, unsigned dst_bits)
+{
+   return _mesa_unorm_to_unorm(x, src_bits, dst_bits - 1);
+}
+
+static inline int
+_mesa_snorm_to_snorm(int x, unsigned src_bits, unsigned dst_bits)
+{
+   if (x < -MAX_INT(src_bits))
+      return -MAX_INT(dst_bits);
+   else if (src_bits < dst_bits)
+      return EXTEND_NORMALIZED_INT(x, src_bits - 1, dst_bits - 1);
+   else
+      return x >> (src_bits - dst_bits);
+}
+
+static inline unsigned
+_mesa_unsigned_to_unsigned(unsigned src, unsigned dst_size)
+{
+   return MIN2(src, MAX_UINT(dst_size));
+}
+
+static inline int
+_mesa_unsigned_to_signed(unsigned src, unsigned dst_size)
+{
+   return MIN2(src, (unsigned)MAX_INT(dst_size));
+}
+
+static inline int
+_mesa_signed_to_signed(int src, unsigned dst_size)
+{
+   return CLAMP(src, MIN_INT(dst_size), MAX_INT(dst_size));
+}
+
+static inline unsigned
+_mesa_signed_to_unsigned(int src, unsigned dst_size)
+{
+   return CLAMP(src, 0, MAX_UINT(dst_size));
+}
+
+static inline unsigned
+_mesa_float_to_unsigned(float src, unsigned dst_bits)
+{
+   if (src < 0.0f)
+      return 0;
+   if (src > (float)MAX_UINT(dst_bits))
+       return MAX_UINT(dst_bits);
+   return _mesa_signed_to_unsigned(src, dst_bits);
+}
+
+static inline unsigned
+_mesa_float_to_signed(float src, unsigned dst_bits)
+{
+   if (src < (float)(-MAX_INT(dst_bits)))
+      return -MAX_INT(dst_bits);
+   if (src > (float)MAX_INT(dst_bits))
+       return MAX_INT(dst_bits);
+   return _mesa_signed_to_signed(src, dst_bits);
+}
+
+static inline unsigned
+_mesa_half_to_unsigned(uint16_t src, unsigned dst_bits)
+{
+   if (_mesa_half_is_negative(src))
+      return 0;
+   return _mesa_unsigned_to_unsigned(_mesa_float_to_half(src), dst_bits);
+}
+
+static inline unsigned
+_mesa_half_to_signed(uint16_t src, unsigned dst_bits)
+{
+   return _mesa_float_to_signed(_mesa_half_to_float(src), dst_bits);
+}
+
 bool
 _mesa_format_to_array(mesa_format, GLenum *type, int *num_components,
                       uint8_t swizzle[4], bool *normalized);
diff --git a/src/mesa/main/glformats.c b/src/mesa/main/glformats.c
index 3d34a226fbd..34f930751bf 100644
--- a/src/mesa/main/glformats.c
+++ b/src/mesa/main/glformats.c
@@ -1074,8 +1074,6 @@ _mesa_is_color_format(GLenum format)
       case GL_COMPRESSED_RGBA_S3TC_DXT5_EXT:
       case GL_COMPRESSED_RGB_FXT1_3DFX:
       case GL_COMPRESSED_RGBA_FXT1_3DFX:
-      case GL_SR8_EXT:
-      case GL_SRG8_EXT:
       case GL_SRGB_EXT:
       case GL_SRGB8_EXT:
       case GL_SRGB_ALPHA_EXT:
@@ -1423,8 +1421,6 @@ GLboolean
 _mesa_is_srgb_format(GLenum format)
 {
    switch (format) {
-   case GL_SR8_EXT:
-   case GL_SRG8_EXT:
    case GL_SRGB:
    case GL_SRGB8:
    case GL_SRGB_ALPHA:
diff --git a/src/mesa/main/glthread.c b/src/mesa/main/glthread.c
index 883d6bcc283..6316cad4e32 100644
--- a/src/mesa/main/glthread.c
+++ b/src/mesa/main/glthread.c
@@ -76,7 +76,7 @@ glthread_unmarshal_batch(void *job, int thread_index)
    unsigned batch_index = batch - ctx->GLThread.batches;
    /* Atomically set this to -1 if it's equal to batch_index. */
    p_atomic_cmpxchg(&ctx->GLThread.LastProgramChangeBatch, batch_index, -1);
-   p_atomic_cmpxchg(&ctx->GLThread.LastDListChangeBatchIndex, batch_index, -1);
+
 }
 
 static void
@@ -139,8 +139,6 @@ _mesa_glthread_init(struct gl_context *ctx)
 
    ctx->CurrentClientDispatch = ctx->MarshalExec;
 
-   glthread->LastDListChangeBatchIndex = -1;
-
    /* Execute the thread initialization function in the thread. */
    struct util_queue_fence fence;
    util_queue_fence_init(&fence);
diff --git a/src/mesa/main/glthread.h b/src/mesa/main/glthread.h
index a7893af364e..177737f5be2 100644
--- a/src/mesa/main/glthread.h
+++ b/src/mesa/main/glthread.h
@@ -124,24 +124,6 @@ struct glthread_client_attrib {
    bool Valid;
 };
 
-/* For glPushAttrib / glPopAttrib. */
-struct glthread_attrib_node {
-   GLbitfield Mask;
-   int ActiveTexture;
-   GLenum MatrixMode;
-};
-
-typedef enum {
-   M_MODELVIEW,
-   M_PROJECTION,
-   M_PROGRAM0,
-   M_PROGRAM_LAST = M_PROGRAM0 + MAX_PROGRAM_MATRICES - 1,
-   M_TEXTURE0,
-   M_TEXTURE_LAST = M_TEXTURE0 + MAX_TEXTURE_UNITS - 1,
-   M_DUMMY, /* used instead of reporting errors */
-   M_NUM_MATRIX_STACKS,
-} gl_matrix_index;
-
 struct glthread_state
 {
    /** Multithreaded queue. */
@@ -153,10 +135,8 @@ struct glthread_state
    /** Whether GLThread is enabled. */
    bool enabled;
 
-   /** Display lists. */
-   GLenum ListMode; /**< Zero if not inside display list, else list mode. */
-   unsigned ListBase;
-   unsigned ListCallDepth;
+   /** Whether GLThread is inside a display list generation. */
+   bool inside_dlist;
 
    /** For L3 cache pinning. */
    unsigned pin_thread_counter;
@@ -211,20 +191,6 @@ struct glthread_state
     * glDeleteProgram or -1 if there is no such enqueued call.
     */
    int LastProgramChangeBatch;
-
-   /**
-    * The batch index of the last occurence of glEndList or
-    * glDeleteLists or -1 if there is no such enqueued call.
-    */
-   int LastDListChangeBatchIndex;
-
-   /** Basic matrix state tracking. */
-   int ActiveTexture;
-   GLenum MatrixMode;
-   gl_matrix_index MatrixIndex;
-   struct glthread_attrib_node AttribStack[MAX_ATTRIB_STACK_DEPTH];
-   int AttribStackDepth;
-   int MatrixStackDepth[M_NUM_MATRIX_STACKS];
 };
 
 void _mesa_glthread_init(struct gl_context *ctx);
@@ -242,7 +208,6 @@ void _mesa_glthread_upload(struct gl_context *ctx, const void *data,
 void _mesa_glthread_reset_vao(struct glthread_vao *vao);
 void _mesa_error_glthread_safe(struct gl_context *ctx, GLenum error,
                                bool glthread, const char *format, ...);
-void _mesa_glthread_execute_list(struct gl_context *ctx, GLuint list);
 
 void _mesa_glthread_BindBuffer(struct gl_context *ctx, GLenum target,
                                GLuint buffer);
diff --git a/src/mesa/main/glthread_draw.c b/src/mesa/main/glthread_draw.c
index a534c51ab84..1fd6828e1f6 100644
--- a/src/mesa/main/glthread_draw.c
+++ b/src/mesa/main/glthread_draw.c
@@ -253,53 +253,6 @@ upload_vertices(struct gl_context *ctx, unsigned user_buffer_mask,
    return true;
 }
 
-/* Generic DrawArrays structure NOT supporting user buffers. Ignore the name. */
-struct marshal_cmd_DrawArrays
-{
-   struct marshal_cmd_base cmd_base;
-   GLenum mode;
-   GLint first;
-   GLsizei count;
-   GLsizei instance_count;
-   GLuint baseinstance;
-};
-
-void
-_mesa_unmarshal_DrawArrays(struct gl_context *ctx,
-                           const struct marshal_cmd_DrawArrays *cmd)
-{
-   /* Ignore the function name. We use DISPATCH_CMD_DrawArrays
-    * for all DrawArrays variants without user buffers, and
-    * DISPATCH_CMD_DrawArraysInstancedBaseInstance for all DrawArrays
-    * variants with user buffrs.
-    */
-   const GLenum mode = cmd->mode;
-   const GLint first = cmd->first;
-   const GLsizei count = cmd->count;
-   const GLsizei instance_count = cmd->instance_count;
-   const GLuint baseinstance = cmd->baseinstance;
-
-   CALL_DrawArraysInstancedBaseInstance(ctx->CurrentServerDispatch,
-                                        (mode, first, count, instance_count,
-                                         baseinstance));
-}
-
-static ALWAYS_INLINE void
-draw_arrays_async(struct gl_context *ctx, GLenum mode, GLint first,
-                  GLsizei count, GLsizei instance_count, GLuint baseinstance)
-{
-   int cmd_size = sizeof(struct marshal_cmd_DrawArrays);
-   struct marshal_cmd_DrawArrays *cmd =
-      _mesa_glthread_allocate_command(ctx, DISPATCH_CMD_DrawArrays, cmd_size);
-
-   cmd->mode = mode;
-   cmd->first = first;
-   cmd->count = count;
-   cmd->instance_count = instance_count;
-   cmd->baseinstance = baseinstance;
-}
-
-/* Generic DrawArrays structure supporting user buffers. Ignore the name. */
 struct marshal_cmd_DrawArraysInstancedBaseInstance
 {
    struct marshal_cmd_base cmd_base;
@@ -315,11 +268,6 @@ void
 _mesa_unmarshal_DrawArraysInstancedBaseInstance(struct gl_context *ctx,
                                                 const struct marshal_cmd_DrawArraysInstancedBaseInstance *cmd)
 {
-   /* Ignore the function name. We use DISPATCH_CMD_DrawArrays
-    * for all DrawArrays variants without user buffers, and
-    * DISPATCH_CMD_DrawArraysInstancedBaseInstance for all DrawArrays
-    * variants with user buffrs.
-    */
    const GLenum mode = cmd->mode;
    const GLint first = cmd->first;
    const GLsizei count = cmd->count;
@@ -347,10 +295,10 @@ _mesa_unmarshal_DrawArraysInstancedBaseInstance(struct gl_context *ctx,
 }
 
 static ALWAYS_INLINE void
-draw_arrays_async_user(struct gl_context *ctx, GLenum mode, GLint first,
-                       GLsizei count, GLsizei instance_count, GLuint baseinstance,
-                       unsigned user_buffer_mask,
-                       const struct glthread_attrib_binding *buffers)
+draw_arrays_async(struct gl_context *ctx, GLenum mode, GLint first,
+                  GLsizei count, GLsizei instance_count, GLuint baseinstance,
+                  unsigned user_buffer_mask,
+                  const struct glthread_attrib_binding *buffers)
 {
    int buffers_size = util_bitcount(user_buffer_mask) * sizeof(buffers[0]);
    int cmd_size = sizeof(struct marshal_cmd_DrawArraysInstancedBaseInstance) +
@@ -379,7 +327,7 @@ draw_arrays(GLenum mode, GLint first, GLsizei count, GLsizei instance_count,
    struct glthread_vao *vao = ctx->GLThread.CurrentVAO;
    unsigned user_buffer_mask = vao->UserPointerMask & vao->BufferEnabled;
 
-   if (compiled_into_dlist && ctx->GLThread.ListMode) {
+   if (compiled_into_dlist && ctx->GLThread.inside_dlist) {
       _mesa_glthread_finish_before(ctx, "DrawArrays");
       /* Use the function that's compiled into a display list. */
       CALL_DrawArrays(ctx->CurrentServerDispatch, (mode, first, count));
@@ -393,7 +341,8 @@ draw_arrays(GLenum mode, GLint first, GLsizei count, GLsizei instance_count,
     */
    if (ctx->API == API_OPENGL_CORE || !user_buffer_mask ||
        count <= 0 || instance_count <= 0) {
-      draw_arrays_async(ctx, mode, first, count, instance_count, baseinstance);
+      draw_arrays_async(ctx, mode, first, count, instance_count, baseinstance,
+                        0, NULL);
       return;
    }
 
@@ -409,8 +358,8 @@ draw_arrays(GLenum mode, GLint first, GLsizei count, GLsizei instance_count,
       return;
    }
 
-   draw_arrays_async_user(ctx, mode, first, count, instance_count, baseinstance,
-                          user_buffer_mask, buffers);
+   draw_arrays_async(ctx, mode, first, count, instance_count, baseinstance,
+                     user_buffer_mask, buffers);
 }
 
 struct marshal_cmd_MultiDrawArrays
@@ -492,7 +441,7 @@ _mesa_marshal_MultiDrawArrays(GLenum mode, const GLint *first,
    struct glthread_vao *vao = ctx->GLThread.CurrentVAO;
    unsigned user_buffer_mask = vao->UserPointerMask & vao->BufferEnabled;
 
-   if (ctx->GLThread.ListMode)
+   if (ctx->GLThread.inside_dlist)
       goto sync;
 
    if (draw_count >= 0 &&
@@ -547,107 +496,6 @@ sync:
                         (mode, first, count, draw_count));
 }
 
-/* DrawElementsInstancedBaseVertexBaseInstance not supporting user buffers.
- * Ignore the name.
- */
-struct marshal_cmd_DrawElementsInstancedARB
-{
-   struct marshal_cmd_base cmd_base;
-   GLenum mode;
-   GLenum type;
-   GLsizei count;
-   GLsizei instance_count;
-   GLint basevertex;
-   GLuint baseinstance;
-   const GLvoid *indices;
-};
-
-void
-_mesa_unmarshal_DrawElementsInstancedARB(struct gl_context *ctx,
-                                         const struct marshal_cmd_DrawElementsInstancedARB *cmd)
-{
-   /* Ignore the function name. We use DISPATCH_CMD_DrawElementsInstanced-
-    * BaseVertexBaseInstance for all DrawElements variants with user buffers,
-    * and both DISPATCH_CMD_DrawElementsInstancedARB and DISPATCH_CMD_Draw-
-    * RangeElementsBaseVertex for all draw elements variants without user
-    * buffers.
-    */
-   const GLenum mode = cmd->mode;
-   const GLsizei count = cmd->count;
-   const GLenum type = cmd->type;
-   const GLvoid *indices = cmd->indices;
-   const GLsizei instance_count = cmd->instance_count;
-   const GLint basevertex = cmd->basevertex;
-   const GLuint baseinstance = cmd->baseinstance;
-
-   CALL_DrawElementsInstancedBaseVertexBaseInstance(ctx->CurrentServerDispatch,
-                                                    (mode, count, type, indices,
-                                                     instance_count, basevertex,
-                                                     baseinstance));
-}
-
-struct marshal_cmd_DrawRangeElementsBaseVertex
-{
-   struct marshal_cmd_base cmd_base;
-   GLenum mode;
-   GLenum type;
-   GLsizei count;
-   GLint basevertex;
-   GLuint min_index;
-   GLuint max_index;
-   const GLvoid *indices;
-};
-
-void
-_mesa_unmarshal_DrawRangeElementsBaseVertex(struct gl_context *ctx,
-                                            const struct marshal_cmd_DrawRangeElementsBaseVertex *cmd)
-{
-   const GLenum mode = cmd->mode;
-   const GLsizei count = cmd->count;
-   const GLenum type = cmd->type;
-   const GLvoid *indices = cmd->indices;
-   const GLint basevertex = cmd->basevertex;
-   const GLuint min_index = cmd->min_index;
-   const GLuint max_index = cmd->max_index;
-
-   CALL_DrawRangeElementsBaseVertex(ctx->CurrentServerDispatch,
-                                    (mode, min_index, max_index, count,
-                                     type, indices, basevertex));
-}
-
-static ALWAYS_INLINE void
-draw_elements_async(struct gl_context *ctx, GLenum mode, GLsizei count,
-                    GLenum type, const GLvoid *indices, GLsizei instance_count,
-                    GLint basevertex, GLuint baseinstance,
-                    bool index_bounds_valid, GLuint min_index, GLuint max_index)
-{
-   if (index_bounds_valid) {
-      int cmd_size = sizeof(struct marshal_cmd_DrawRangeElementsBaseVertex);
-      struct marshal_cmd_DrawRangeElementsBaseVertex *cmd =
-         _mesa_glthread_allocate_command(ctx, DISPATCH_CMD_DrawRangeElementsBaseVertex, cmd_size);
-
-      cmd->mode = mode;
-      cmd->count = count;
-      cmd->type = type;
-      cmd->indices = indices;
-      cmd->basevertex = basevertex;
-      cmd->min_index = min_index;
-      cmd->max_index = max_index;
-   } else {
-      int cmd_size = sizeof(struct marshal_cmd_DrawElementsInstancedARB);
-      struct marshal_cmd_DrawElementsInstancedARB *cmd =
-         _mesa_glthread_allocate_command(ctx, DISPATCH_CMD_DrawElementsInstancedARB, cmd_size);
-
-      cmd->mode = mode;
-      cmd->count = count;
-      cmd->type = type;
-      cmd->indices = indices;
-      cmd->instance_count = instance_count;
-      cmd->basevertex = basevertex;
-      cmd->baseinstance = baseinstance;
-   }
-}
-
 struct marshal_cmd_DrawElementsInstancedBaseVertexBaseInstance
 {
    struct marshal_cmd_base cmd_base;
@@ -669,12 +517,6 @@ void
 _mesa_unmarshal_DrawElementsInstancedBaseVertexBaseInstance(struct gl_context *ctx,
                                                             const struct marshal_cmd_DrawElementsInstancedBaseVertexBaseInstance *cmd)
 {
-   /* Ignore the function name. We use DISPATCH_CMD_DrawElementsInstanced-
-    * BaseVertexBaseInstance for all DrawElements variants with user buffers,
-    * and both DISPATCH_CMD_DrawElementsInstancedARB and DISPATCH_CMD_Draw-
-    * RangeElementsBaseVertex for all draw elements variants without user
-    * buffers.
-    */
    const GLenum mode = cmd->mode;
    const GLsizei count = cmd->count;
    const GLenum type = cmd->type;
@@ -721,13 +563,13 @@ _mesa_unmarshal_DrawElementsInstancedBaseVertexBaseInstance(struct gl_context *c
 }
 
 static ALWAYS_INLINE void
-draw_elements_async_user(struct gl_context *ctx, GLenum mode, GLsizei count,
-                         GLenum type, const GLvoid *indices, GLsizei instance_count,
-                         GLint basevertex, GLuint baseinstance,
-                         bool index_bounds_valid, GLuint min_index, GLuint max_index,
-                         struct gl_buffer_object *index_buffer,
-                         unsigned user_buffer_mask,
-                         const struct glthread_attrib_binding *buffers)
+draw_elements_async(struct gl_context *ctx, GLenum mode, GLsizei count,
+                    GLenum type, const GLvoid *indices, GLsizei instance_count,
+                    GLint basevertex, GLuint baseinstance,
+                    bool index_bounds_valid, GLuint min_index, GLuint max_index,
+                    struct gl_buffer_object *index_buffer,
+                    unsigned user_buffer_mask,
+                    const struct glthread_attrib_binding *buffers)
 {
    int buffers_size = util_bitcount(user_buffer_mask) * sizeof(buffers[0]);
    int cmd_size = sizeof(struct marshal_cmd_DrawElementsInstancedBaseVertexBaseInstance) +
@@ -764,7 +606,7 @@ draw_elements(GLenum mode, GLsizei count, GLenum type, const GLvoid *indices,
    unsigned user_buffer_mask = vao->UserPointerMask & vao->BufferEnabled;
    bool has_user_indices = vao->CurrentElementBufferName == 0;
 
-   if (compiled_into_dlist && ctx->GLThread.ListMode)
+   if (compiled_into_dlist && ctx->GLThread.inside_dlist)
       goto sync;
 
    /* Fast path when nothing needs to be done.
@@ -778,7 +620,7 @@ draw_elements(GLenum mode, GLsizei count, GLenum type, const GLvoid *indices,
        (!user_buffer_mask && !has_user_indices)) {
       draw_elements_async(ctx, mode, count, type, indices, instance_count,
                           basevertex, baseinstance, index_bounds_valid,
-                          min_index, max_index);
+                          min_index, max_index, 0, 0, NULL);
       return;
    }
 
@@ -828,16 +670,16 @@ draw_elements(GLenum mode, GLsizei count, GLenum type, const GLvoid *indices,
       index_buffer = upload_indices(ctx, count, index_size, &indices);
 
    /* Draw asynchronously. */
-   draw_elements_async_user(ctx, mode, count, type, indices, instance_count,
-                            basevertex, baseinstance, index_bounds_valid,
-                            min_index, max_index, index_buffer,
-                            user_buffer_mask, buffers);
+   draw_elements_async(ctx, mode, count, type, indices, instance_count,
+                       basevertex, baseinstance, index_bounds_valid,
+                       min_index, max_index, index_buffer,
+                       user_buffer_mask, buffers);
    return;
 
 sync:
    _mesa_glthread_finish_before(ctx, "DrawElements");
 
-   if (compiled_into_dlist && ctx->GLThread.ListMode) {
+   if (compiled_into_dlist && ctx->GLThread.inside_dlist) {
       /* Only use the ones that are compiled into display lists. */
       if (basevertex) {
          CALL_DrawElementsBaseVertex(ctx->CurrentServerDispatch,
@@ -977,7 +819,7 @@ _mesa_marshal_MultiDrawElementsBaseVertex(GLenum mode, const GLsizei *count,
    unsigned user_buffer_mask = vao->UserPointerMask & vao->BufferEnabled;
    bool has_user_indices = vao->CurrentElementBufferName == 0;
 
-   if (ctx->GLThread.ListMode)
+   if (ctx->GLThread.inside_dlist)
       goto sync;
 
    /* Fast path when nothing needs to be done. */
@@ -1207,6 +1049,12 @@ _mesa_marshal_MultiDrawElementsEXT(GLenum mode, const GLsizei *count,
                                              draw_count, NULL);
 }
 
+void
+_mesa_unmarshal_DrawArrays(struct gl_context *ctx, const struct marshal_cmd_DrawArrays *cmd)
+{
+   unreachable("never used - DrawArraysInstancedBaseInstance is used instead");
+}
+
 void
 _mesa_unmarshal_DrawArraysInstancedARB(struct gl_context *ctx, const struct marshal_cmd_DrawArraysInstancedARB *cmd)
 {
@@ -1225,12 +1073,24 @@ _mesa_unmarshal_DrawRangeElements(struct gl_context *ctx, const struct marshal_c
    unreachable("never used - DrawElementsInstancedBaseVertexBaseInstance is used instead");
 }
 
+void
+_mesa_unmarshal_DrawElementsInstancedARB(struct gl_context *ctx, const struct marshal_cmd_DrawElementsInstancedARB *cmd)
+{
+   unreachable("never used - DrawElementsInstancedBaseVertexBaseInstance is used instead");
+}
+
 void
 _mesa_unmarshal_DrawElementsBaseVertex(struct gl_context *ctx, const struct marshal_cmd_DrawElementsBaseVertex *cmd)
 {
    unreachable("never used - DrawElementsInstancedBaseVertexBaseInstance is used instead");
 }
 
+void
+_mesa_unmarshal_DrawRangeElementsBaseVertex(struct gl_context *ctx, const struct marshal_cmd_DrawRangeElementsBaseVertex *cmd)
+{
+   unreachable("never used - DrawElementsInstancedBaseVertexBaseInstance is used instead");
+}
+
 void
 _mesa_unmarshal_DrawElementsInstancedBaseVertex(struct gl_context *ctx, const struct marshal_cmd_DrawElementsInstancedBaseVertex *cmd)
 {
diff --git a/src/mesa/main/glthread_get.c b/src/mesa/main/glthread_get.c
deleted file mode 100644
index 34627395173..00000000000
--- a/src/mesa/main/glthread_get.c
+++ /dev/null
@@ -1,121 +0,0 @@
-/*
- * Copyright  2020 Advanced Micro Devices, Inc.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice (including the next
- * paragraph) shall be included in all copies or substantial portions of the
- * Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
- * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
- * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
- * IN THE SOFTWARE.
- */
-
-#include "main/glthread_marshal.h"
-#include "main/dispatch.h"
-
-void
-_mesa_unmarshal_GetIntegerv(struct gl_context *ctx,
-                            const struct marshal_cmd_GetIntegerv *cmd)
-{
-   unreachable("never executed");
-}
-
-void GLAPIENTRY
-_mesa_marshal_GetIntegerv(GLenum pname, GLint *p)
-{
-   GET_CURRENT_CONTEXT(ctx);
-
-   /* TODO: Use get_hash_params.py to return values for items containing:
-    * - CONST(
-    * - CONTEXT_[A-Z]*(Const
-    */
-
-   if (ctx->API != API_OPENGL_COMPAT) {
-      /* glthread only tracks these states for the compatibility profile. */
-      _mesa_glthread_finish_before(ctx, "GetIntegerv");
-      CALL_GetIntegerv(ctx->CurrentServerDispatch, (pname, p));
-      return;
-   }
-
-   switch (pname) {
-   case GL_ACTIVE_TEXTURE:
-      *p = GL_TEXTURE0 + ctx->GLThread.ActiveTexture;
-      return;
-   case GL_ARRAY_BUFFER_BINDING:
-      *p = ctx->GLThread.CurrentArrayBufferName;
-      return;
-   case GL_ATTRIB_STACK_DEPTH:
-      *p = ctx->GLThread.AttribStackDepth;
-      return;
-   case GL_CLIENT_ACTIVE_TEXTURE:
-      *p = ctx->GLThread.ClientActiveTexture;
-      return;
-   case GL_CLIENT_ATTRIB_STACK_DEPTH:
-      *p = ctx->GLThread.ClientAttribStackTop;
-      return;
-   case GL_DRAW_INDIRECT_BUFFER_BINDING:
-      *p = ctx->GLThread.CurrentDrawIndirectBufferName;
-      return;
-
-   case GL_MATRIX_MODE:
-      *p = ctx->GLThread.MatrixMode;
-      return;
-   case GL_CURRENT_MATRIX_STACK_DEPTH_ARB:
-      *p = ctx->GLThread.MatrixStackDepth[ctx->GLThread.MatrixIndex] + 1;
-      return;
-   case GL_MODELVIEW_STACK_DEPTH:
-      *p = ctx->GLThread.MatrixStackDepth[M_MODELVIEW] + 1;
-      return;
-   case GL_PROJECTION_STACK_DEPTH:
-      *p = ctx->GLThread.MatrixStackDepth[M_PROJECTION] + 1;
-      return;
-   case GL_TEXTURE_STACK_DEPTH:
-      *p = ctx->GLThread.MatrixStackDepth[M_TEXTURE0 + ctx->GLThread.ActiveTexture] + 1;
-      return;
-
-   case GL_VERTEX_ARRAY:
-      *p = (ctx->GLThread.CurrentVAO->UserEnabled & (1 << VERT_ATTRIB_POS)) != 0;
-      return;
-   case GL_NORMAL_ARRAY:
-      *p = (ctx->GLThread.CurrentVAO->UserEnabled & (1 << VERT_ATTRIB_NORMAL)) != 0;
-      return;
-   case GL_COLOR_ARRAY:
-      *p = (ctx->GLThread.CurrentVAO->UserEnabled & (1 << VERT_ATTRIB_COLOR0)) != 0;
-      return;
-   case GL_SECONDARY_COLOR_ARRAY:
-      *p = (ctx->GLThread.CurrentVAO->UserEnabled & (1 << VERT_ATTRIB_COLOR1)) != 0;
-      return;
-   case GL_FOG_COORD_ARRAY:
-      *p = (ctx->GLThread.CurrentVAO->UserEnabled & (1 << VERT_ATTRIB_FOG)) != 0;
-      return;
-   case GL_INDEX_ARRAY:
-      *p = (ctx->GLThread.CurrentVAO->UserEnabled & (1 << VERT_ATTRIB_COLOR_INDEX)) != 0;
-      return;
-   case GL_EDGE_FLAG_ARRAY:
-      *p = (ctx->GLThread.CurrentVAO->UserEnabled & (1 << VERT_ATTRIB_EDGEFLAG)) != 0;
-      return;
-   case GL_TEXTURE_COORD_ARRAY:
-      *p = (ctx->GLThread.CurrentVAO->UserEnabled &
-            (1 << (VERT_ATTRIB_TEX0 + ctx->GLThread.ClientActiveTexture))) != 0;
-      return;
-   case GL_POINT_SIZE_ARRAY_OES:
-      *p = (ctx->GLThread.CurrentVAO->UserEnabled & (1 << VERT_ATTRIB_POINT_SIZE)) != 0;
-      return;
-   }
-
-   _mesa_glthread_finish_before(ctx, "GetIntegerv");
-   CALL_GetIntegerv(ctx->CurrentServerDispatch, (pname, p));
-}
-
-/* TODO: Implement glGetBooleanv, glGetFloatv, etc. if needed */
diff --git a/src/mesa/main/glthread_marshal.h b/src/mesa/main/glthread_marshal.h
index 5d91095edeb..2680530a10f 100644
--- a/src/mesa/main/glthread_marshal.h
+++ b/src/mesa/main/glthread_marshal.h
@@ -405,307 +405,4 @@ _mesa_array_to_attrib(struct gl_context *ctx, GLenum array)
    }
 }
 
-static inline gl_matrix_index
-_mesa_get_matrix_index(struct gl_context *ctx, GLenum mode)
-{
-   if (mode == GL_MODELVIEW || mode == GL_PROJECTION)
-      return M_MODELVIEW + (mode - GL_MODELVIEW);
-
-   if (mode == GL_TEXTURE)
-      return M_TEXTURE0 + ctx->GLThread.ActiveTexture;
-
-   if (mode >= GL_TEXTURE0 && mode <= GL_TEXTURE0 + MAX_TEXTURE_UNITS - 1)
-      return M_TEXTURE0 + (mode - GL_TEXTURE0);
-
-   if (mode >= GL_MATRIX0_ARB && mode <= GL_MATRIX0_ARB + MAX_PROGRAM_MATRICES - 1)
-      return M_PROGRAM0 + (mode - GL_MATRIX0_ARB);
-
-   return M_DUMMY;
-}
-
-static inline void
-_mesa_glthread_Enable(struct gl_context *ctx, GLenum cap)
-{
-   if (ctx->GLThread.ListMode == GL_COMPILE)
-      return;
-
-   if (cap == GL_PRIMITIVE_RESTART ||
-       cap == GL_PRIMITIVE_RESTART_FIXED_INDEX)
-      _mesa_glthread_set_prim_restart(ctx, cap, true);
-   else if (cap == GL_DEBUG_OUTPUT_SYNCHRONOUS_ARB)
-      _mesa_glthread_disable(ctx, "Enable(DEBUG_OUTPUT_SYNCHRONOUS)");
-}
-
-static inline void
-_mesa_glthread_Disable(struct gl_context *ctx, GLenum cap)
-{
-   if (ctx->GLThread.ListMode == GL_COMPILE)
-      return;
-
-   if (cap == GL_PRIMITIVE_RESTART ||
-       cap == GL_PRIMITIVE_RESTART_FIXED_INDEX)
-      _mesa_glthread_set_prim_restart(ctx, cap, false);
-}
-
-static inline void
-_mesa_glthread_PushAttrib(struct gl_context *ctx, GLbitfield mask)
-{
-   if (ctx->GLThread.ListMode == GL_COMPILE)
-      return;
-
-   struct glthread_attrib_node *attr =
-      &ctx->GLThread.AttribStack[ctx->GLThread.AttribStackDepth++];
-
-   attr->Mask = mask;
-
-   if (mask & GL_TEXTURE_BIT)
-      attr->ActiveTexture = ctx->GLThread.ActiveTexture;
-
-   if (mask & GL_TRANSFORM_BIT)
-      attr->MatrixMode = ctx->GLThread.MatrixMode;
-}
-
-static inline void
-_mesa_glthread_PopAttrib(struct gl_context *ctx)
-{
-   if (ctx->GLThread.ListMode == GL_COMPILE)
-      return;
-
-   struct glthread_attrib_node *attr =
-      &ctx->GLThread.AttribStack[--ctx->GLThread.AttribStackDepth];
-   unsigned mask = attr->Mask;
-
-   if (mask & GL_TEXTURE_BIT)
-      ctx->GLThread.ActiveTexture = attr->ActiveTexture;
-
-   if (mask & GL_TRANSFORM_BIT) {
-      ctx->GLThread.MatrixMode = attr->MatrixMode;
-      ctx->GLThread.MatrixIndex = _mesa_get_matrix_index(ctx, attr->MatrixMode);
-   }
-}
-
-static inline void
-_mesa_glthread_MatrixPushEXT(struct gl_context *ctx, GLenum matrixMode)
-{
-   if (ctx->GLThread.ListMode == GL_COMPILE)
-      return;
-
-   ctx->GLThread.MatrixStackDepth[_mesa_get_matrix_index(ctx, matrixMode)]++;
-}
-
-static inline void
-_mesa_glthread_MatrixPopEXT(struct gl_context *ctx, GLenum matrixMode)
-{
-   if (ctx->GLThread.ListMode == GL_COMPILE)
-      return;
-
-   ctx->GLThread.MatrixStackDepth[_mesa_get_matrix_index(ctx, matrixMode)]--;
-}
-
-static inline void
-_mesa_glthread_ActiveTexture(struct gl_context *ctx, GLenum texture)
-{
-   if (ctx->GLThread.ListMode == GL_COMPILE)
-      return;
-
-   ctx->GLThread.ActiveTexture = texture - GL_TEXTURE0;
-   if (ctx->GLThread.MatrixMode == GL_TEXTURE)
-      ctx->GLThread.MatrixIndex = _mesa_get_matrix_index(ctx, texture);
-}
-
-static inline void
-_mesa_glthread_PushMatrix(struct gl_context *ctx)
-{
-   if (ctx->GLThread.ListMode == GL_COMPILE)
-      return;
-
-   ctx->GLThread.MatrixStackDepth[ctx->GLThread.MatrixIndex]++;
-}
-
-static inline void
-_mesa_glthread_PopMatrix(struct gl_context *ctx)
-{
-   if (ctx->GLThread.ListMode == GL_COMPILE)
-      return;
-
-   ctx->GLThread.MatrixStackDepth[ctx->GLThread.MatrixIndex]--;
-}
-
-static inline void
-_mesa_glthread_MatrixMode(struct gl_context *ctx, GLenum mode)
-{
-   if (ctx->GLThread.ListMode == GL_COMPILE)
-      return;
-
-   ctx->GLThread.MatrixIndex = _mesa_get_matrix_index(ctx, mode);
-   ctx->GLThread.MatrixMode = mode;
-}
-
-static inline void
-_mesa_glthread_ListBase(struct gl_context *ctx, GLuint base)
-{
-   if (ctx->GLThread.ListMode == GL_COMPILE)
-      return;
-
-   ctx->GLThread.ListBase = base;
-}
-
-static inline void
-_mesa_glthread_CallList(struct gl_context *ctx, GLuint list)
-{
-   if (ctx->GLThread.ListMode == GL_COMPILE)
-      return;
-
-   /* Wait for all glEndList and glDeleteLists calls to finish to ensure that
-    * all display lists are up to date and the driver thread is not
-    * modifiying them. We will be executing them in the application thread.
-    */
-   int batch = p_atomic_read(&ctx->GLThread.LastDListChangeBatchIndex);
-   if (batch != -1) {
-      util_queue_fence_wait(&ctx->GLThread.batches[batch].fence);
-      p_atomic_set(&ctx->GLThread.LastDListChangeBatchIndex, -1);
-   }
-
-   /* Clear GL_COMPILE_AND_EXECUTE if needed. We only execute here. */
-   unsigned saved_mode = ctx->GLThread.ListMode;
-   ctx->GLThread.ListMode = 0;
-
-   _mesa_glthread_execute_list(ctx, list);
-
-   ctx->GLThread.ListMode = saved_mode;
-}
-
-static inline void
-_mesa_glthread_CallLists(struct gl_context *ctx, GLsizei n, GLenum type,
-                         const GLvoid *lists)
-{
-   if (ctx->GLThread.ListMode == GL_COMPILE)
-      return;
-
-   if (n <= 0 || !lists)
-      return;
-
-   /* Wait for all glEndList and glDeleteLists calls to finish to ensure that
-    * all display lists are up to date and the driver thread is not
-    * modifiying them. We will be executing them in the application thread.
-    */
-   int batch = p_atomic_read(&ctx->GLThread.LastDListChangeBatchIndex);
-   if (batch != -1) {
-      util_queue_fence_wait(&ctx->GLThread.batches[batch].fence);
-      p_atomic_set(&ctx->GLThread.LastDListChangeBatchIndex, -1);
-   }
-
-   /* Clear GL_COMPILE_AND_EXECUTE if needed. We only execute here. */
-   unsigned saved_mode = ctx->GLThread.ListMode;
-   ctx->GLThread.ListMode = 0;
-
-   unsigned base = ctx->GLThread.ListBase;
-
-   GLbyte *bptr;
-   GLubyte *ubptr;
-   GLshort *sptr;
-   GLushort *usptr;
-   GLint *iptr;
-   GLuint *uiptr;
-   GLfloat *fptr;
-
-   switch (type) {
-   case GL_BYTE:
-      bptr = (GLbyte *) lists;
-      for (unsigned i = 0; i < n; i++)
-         _mesa_glthread_CallList(ctx, base + bptr[i]);
-      break;
-   case GL_UNSIGNED_BYTE:
-      ubptr = (GLubyte *) lists;
-      for (unsigned i = 0; i < n; i++)
-         _mesa_glthread_CallList(ctx, base + ubptr[i]);
-      break;
-   case GL_SHORT:
-      sptr = (GLshort *) lists;
-      for (unsigned i = 0; i < n; i++)
-         _mesa_glthread_CallList(ctx, base + sptr[i]);
-      break;
-   case GL_UNSIGNED_SHORT:
-      usptr = (GLushort *) lists;
-      for (unsigned i = 0; i < n; i++)
-         _mesa_glthread_CallList(ctx, base + usptr[i]);
-      break;
-   case GL_INT:
-      iptr = (GLint *) lists;
-      for (unsigned i = 0; i < n; i++)
-         _mesa_glthread_CallList(ctx, base + iptr[i]);
-      break;
-   case GL_UNSIGNED_INT:
-      uiptr = (GLuint *) lists;
-      for (unsigned i = 0; i < n; i++)
-         _mesa_glthread_CallList(ctx, base + uiptr[i]);
-      break;
-   case GL_FLOAT:
-      fptr = (GLfloat *) lists;
-      for (unsigned i = 0; i < n; i++)
-         _mesa_glthread_CallList(ctx, base + fptr[i]);
-      break;
-   case GL_2_BYTES:
-      ubptr = (GLubyte *) lists;
-      for (unsigned i = 0; i < n; i++) {
-         _mesa_glthread_CallList(ctx, base +
-                                 (GLint)ubptr[2 * i] * 256 +
-                                 (GLint)ubptr[2 * i + 1]);
-      }
-      break;
-   case GL_3_BYTES:
-      ubptr = (GLubyte *) lists;
-      for (unsigned i = 0; i < n; i++) {
-         _mesa_glthread_CallList(ctx, base +
-                                 (GLint)ubptr[3 * i] * 65536 +
-                                 (GLint)ubptr[3 * i + 1] * 256 +
-                                 (GLint)ubptr[3 * i + 2]);
-      }
-      break;
-   case GL_4_BYTES:
-      ubptr = (GLubyte *) lists;
-      for (unsigned i = 0; i < n; i++) {
-         _mesa_glthread_CallList(ctx, base +
-                                 (GLint)ubptr[4 * i] * 16777216 +
-                                 (GLint)ubptr[4 * i + 1] * 65536 +
-                                 (GLint)ubptr[4 * i + 2] * 256 +
-                                 (GLint)ubptr[4 * i + 3]);
-      }
-      break;
-   }
-
-   ctx->GLThread.ListMode = saved_mode;
-}
-
-static inline void
-_mesa_glthread_NewList(struct gl_context *ctx, GLuint list, GLuint mode)
-{
-   if (!ctx->GLThread.ListMode)
-      ctx->GLThread.ListMode = mode;
-}
-
-static inline void
-_mesa_glthread_EndList(struct gl_context *ctx)
-{
-   if (!ctx->GLThread.ListMode)
-      return;
-
-   ctx->GLThread.ListMode = 0;
-
-   /* Track the last display list change. */
-   p_atomic_set(&ctx->GLThread.LastDListChangeBatchIndex, ctx->GLThread.next);
-   _mesa_glthread_flush_batch(ctx);
-}
-
-static inline void
-_mesa_glthread_DeleteLists(struct gl_context *ctx, GLsizei range)
-{
-   if (range < 0)
-      return;
-
-   /* Track the last display list change. */
-   p_atomic_set(&ctx->GLThread.LastDListChangeBatchIndex, ctx->GLThread.next);
-   _mesa_glthread_flush_batch(ctx);
-}
-
 #endif /* MARSHAL_H */
diff --git a/src/mesa/main/pack.c b/src/mesa/main/pack.c
index 9d7d235d2e2..223bbb5adc1 100644
--- a/src/mesa/main/pack.c
+++ b/src/mesa/main/pack.c
@@ -55,8 +55,7 @@
 #include "glformats.h"
 #include "format_utils.h"
 #include "format_pack.h"
-#include "format_unpack.h"
-#include "util/format/u_format.h"
+
 
 /**
  * Flip the 8 bits in each byte of the given array.
@@ -1632,202 +1631,3 @@ _mesa_unpack_color_index_to_rgba_ubyte(struct gl_context *ctx, GLuint dims,
 
    return dst;
 }
-
-void
-_mesa_unpack_ubyte_rgba_row(mesa_format format, uint32_t n,
-                            const void *src, uint8_t dst[][4])
-{
-   const struct util_format_unpack_description *unpack =
-      util_format_unpack_description((enum pipe_format)format);
-
-   if (unpack->unpack_rgba_8unorm) {
-      unpack->unpack_rgba_8unorm((uint8_t *)dst, 0, src, 0, n, 1);
-   } else {
-      /* get float values, convert to ubyte */
-      {
-         float *tmp = malloc(n * 4 * sizeof(float));
-         if (tmp) {
-            uint32_t i;
-            _mesa_unpack_rgba_row(format, n, src, (float (*)[4]) tmp);
-            for (i = 0; i < n; i++) {
-               dst[i][0] = _mesa_float_to_unorm(tmp[i*4+0], 8);
-               dst[i][1] = _mesa_float_to_unorm(tmp[i*4+1], 8);
-               dst[i][2] = _mesa_float_to_unorm(tmp[i*4+2], 8);
-               dst[i][3] = _mesa_float_to_unorm(tmp[i*4+3], 8);
-            }
-            free(tmp);
-         }
-      }
-   }
-}
-
-/**
- * Unpack a 2D rect of pixels returning float RGBA colors.
- * \param format  the source image format
- * \param src  start address of the source image
- * \param srcRowStride  source image row stride in bytes
- * \param dst  start address of the dest image
- * \param dstRowStride  dest image row stride in bytes
- * \param x  source image start X pos
- * \param y  source image start Y pos
- * \param width  width of rect region to convert
- * \param height  height of rect region to convert
- */
-void
-_mesa_unpack_rgba_block(mesa_format format,
-                        const void *src, int32_t srcRowStride,
-                        float dst[][4], int32_t dstRowStride,
-                        uint32_t x, uint32_t y, uint32_t width, uint32_t height)
-{
-   const uint32_t srcPixStride = _mesa_get_format_bytes(format);
-   const uint32_t dstPixStride = 4 * sizeof(float);
-   const struct util_format_unpack_description *unpack =
-      util_format_unpack_description((enum pipe_format)format);
-   const uint8_t *srcRow;
-   uint8_t *dstRow;
-
-   /* XXX needs to be fixed for compressed formats */
-
-   srcRow = ((const uint8_t *) src) + srcRowStride * y + srcPixStride * x;
-   dstRow = ((uint8_t *) dst) + dstRowStride * y + dstPixStride * x;
-
-   unpack->unpack_rgba(dstRow, dstPixStride,
-                       srcRow, srcRowStride, width, height);
-}
-
-/** Helper struct for MESA_FORMAT_Z32_FLOAT_S8X24_UINT */
-struct z32f_x24s8
-{
-   float z;
-   uint32_t x24s8;
-};
-
-
-static void
-unpack_uint_24_8_depth_stencil_Z24_UNORM_S8_UINT(const uint32_t *src, uint32_t *dst, uint32_t n)
-{
-   uint32_t i;
-
-   for (i = 0; i < n; i++) {
-      uint32_t val = src[i];
-      dst[i] = val >> 24 | val << 8;
-   }
-}
-
-static void
-unpack_uint_24_8_depth_stencil_Z32_S8X24(const uint32_t *src,
-                                         uint32_t *dst, uint32_t n)
-{
-   uint32_t i;
-
-   for (i = 0; i < n; i++) {
-      /* 8 bytes per pixel (float + uint32) */
-      float zf = ((float *) src)[i * 2 + 0];
-      uint32_t z24 = (uint32_t) (zf * (float) 0xffffff);
-      uint32_t s = src[i * 2 + 1] & 0xff;
-      dst[i] = (z24 << 8) | s;
-   }
-}
-
-static void
-unpack_uint_24_8_depth_stencil_S8_UINT_Z24_UNORM(const uint32_t *src, uint32_t *dst, uint32_t n)
-{
-   memcpy(dst, src, n * 4);
-}
-
-/**
- * Unpack depth/stencil returning as GL_UNSIGNED_INT_24_8.
- * \param format  the source data format
- */
-void
-_mesa_unpack_uint_24_8_depth_stencil_row(mesa_format format, uint32_t n,
-                                         const void *src, uint32_t *dst)
-{
-   switch (format) {
-   case MESA_FORMAT_S8_UINT_Z24_UNORM:
-      unpack_uint_24_8_depth_stencil_S8_UINT_Z24_UNORM(src, dst, n);
-      break;
-   case MESA_FORMAT_Z24_UNORM_S8_UINT:
-      unpack_uint_24_8_depth_stencil_Z24_UNORM_S8_UINT(src, dst, n);
-      break;
-   case MESA_FORMAT_Z32_FLOAT_S8X24_UINT:
-      unpack_uint_24_8_depth_stencil_Z32_S8X24(src, dst, n);
-      break;
-   default:
-      unreachable("bad format %s in _mesa_unpack_uint_24_8_depth_stencil_row");
-   }
-}
-
-static void
-unpack_float_32_uint_24_8_Z24_UNORM_S8_UINT(const uint32_t *src,
-                                            uint32_t *dst, uint32_t n)
-{
-   uint32_t i;
-   struct z32f_x24s8 *d = (struct z32f_x24s8 *) dst;
-   const double scale = 1.0 / (double) 0xffffff;
-
-   for (i = 0; i < n; i++) {
-      const uint32_t z24 = src[i] & 0xffffff;
-      d[i].z = z24 * scale;
-      d[i].x24s8 = src[i] >> 24;
-      assert(d[i].z >= 0.0f);
-      assert(d[i].z <= 1.0f);
-   }
-}
-
-static void
-unpack_float_32_uint_24_8_Z32_FLOAT_S8X24_UINT(const uint32_t *src,
-                                               uint32_t *dst, uint32_t n)
-{
-   memcpy(dst, src, n * sizeof(struct z32f_x24s8));
-}
-
-static void
-unpack_float_32_uint_24_8_S8_UINT_Z24_UNORM(const uint32_t *src,
-                                            uint32_t *dst, uint32_t n)
-{
-   uint32_t i;
-   struct z32f_x24s8 *d = (struct z32f_x24s8 *) dst;
-   const double scale = 1.0 / (double) 0xffffff;
-
-   for (i = 0; i < n; i++) {
-      const uint32_t z24 = src[i] >> 8;
-      d[i].z = z24 * scale;
-      d[i].x24s8 = src[i] & 0xff;
-      assert(d[i].z >= 0.0f);
-      assert(d[i].z <= 1.0f);
-   }
-}
-
-/**
- * Unpack depth/stencil returning as GL_FLOAT_32_UNSIGNED_INT_24_8_REV.
- * \param format  the source data format
- *
- * In GL_FLOAT_32_UNSIGNED_INT_24_8_REV lower 4 bytes contain float
- * component and higher 4 bytes contain packed 24-bit and 8-bit
- * components.
- *
- *    31 30 29 28 ... 4 3 2 1 0    31 30 29 ... 9 8 7 6 5 ... 2 1 0
- *    +-------------------------+  +--------------------------------+
- *    |    Float Component      |  | Unused         | 8 bit stencil |
- *    +-------------------------+  +--------------------------------+
- *          lower 4 bytes                  higher 4 bytes
- */
-void
-_mesa_unpack_float_32_uint_24_8_depth_stencil_row(mesa_format format, uint32_t n,
-                                                  const void *src, uint32_t *dst)
-{
-   switch (format) {
-   case MESA_FORMAT_S8_UINT_Z24_UNORM:
-      unpack_float_32_uint_24_8_S8_UINT_Z24_UNORM(src, dst, n);
-      break;
-   case MESA_FORMAT_Z24_UNORM_S8_UINT:
-      unpack_float_32_uint_24_8_Z24_UNORM_S8_UINT(src, dst, n);
-      break;
-   case MESA_FORMAT_Z32_FLOAT_S8X24_UINT:
-      unpack_float_32_uint_24_8_Z32_FLOAT_S8X24_UINT(src, dst, n);
-      break;
-   default:
-      unreachable("bad format %s in _mesa_unpack_uint_24_8_depth_stencil_row");
-   }
-}
diff --git a/src/mesa/main/tests/dispatch_sanity.cpp b/src/mesa/main/tests/dispatch_sanity.cpp
index e499ea2f1b1..bbb3ee03791 100644
--- a/src/mesa/main/tests/dispatch_sanity.cpp
+++ b/src/mesa/main/tests/dispatch_sanity.cpp
@@ -2264,7 +2264,6 @@ const struct function gles2_functions_possible[] = {
    { "glDrawArrays", 20, _gloffset_DrawArrays },
    { "glDrawBuffersNV", 20, -1 },
    { "glDrawElements", 20, _gloffset_DrawElements },
-   { "glDrawElementsBaseVertex", 20, -1 },
    { "glEGLImageTargetRenderbufferStorageOES", 20, -1 },
    { "glEGLImageTargetTexture2DOES", 20, -1 },
    { "glEnable", 20, _gloffset_Enable },
@@ -2324,7 +2323,6 @@ const struct function gles2_functions_possible[] = {
    { "glMapBufferRangeEXT", 20, -1 },
    { "glMultiDrawArraysEXT", 20, -1 },
    { "glMultiDrawElementsEXT", 20, -1 },
-   { "glMultiDrawElementsBaseVertex", 20, -1 },
    { "glPixelStorei", 20, _gloffset_PixelStorei },
    { "glPolygonOffset", 20, _gloffset_PolygonOffset },
    { "glReadBufferNV", 20, _gloffset_ReadBuffer },
@@ -2558,7 +2556,6 @@ const struct function gles3_functions_possible[] = {
    // { "glDrawBuffers", 30, -1 },
    { "glDrawElementsInstanced", 30, -1 },
    { "glDrawRangeElements", 30, -1 },
-   { "glDrawRangeElementsBaseVertex", 30, -1 },
    // We check for the aliased -EXT version in GLES 2
    // { "glEndQuery", 30, -1 },
    { "glEndTransformFeedback", 30, -1 },
diff --git a/src/mesa/main/tests/mesa_formats.cpp b/src/mesa/main/tests/mesa_formats.cpp
index 6d81ce67c5b..6842d82e9cf 100644
--- a/src/mesa/main/tests/mesa_formats.cpp
+++ b/src/mesa/main/tests/mesa_formats.cpp
@@ -33,8 +33,6 @@
 
 #include "main/formats.h"
 #include "main/glformats.h"
-#include "main/format_unpack.h"
-#include "main/format_pack.h"
 
 /**
  * Debug/test: check that all uncompressed formats are handled in the
@@ -186,99 +184,3 @@ TEST(MesaFormatsTest, FormatMatchesFormatAndType)
                                                     GL_UNSIGNED_SHORT, false,
                                                     NULL));
 }
-
-static uint32_t
-test_unpack_r8i(int8_t val)
-{
-   uint32_t result[4];
-   _mesa_unpack_uint_rgba_row(MESA_FORMAT_R_SINT8, 1, &val, &result);
-   return result[0];
-}
-
-static uint32_t
-test_unpack_r32ui(uint32_t val)
-{
-   uint32_t result[4];
-   _mesa_unpack_uint_rgba_row(MESA_FORMAT_R_UINT32, 1, &val, &result);
-   return result[0];
-}
-
-TEST(MesaFormatsTest, UnpackRGBAUintRow)
-{
-   EXPECT_EQ(test_unpack_r8i(0), 0);
-   EXPECT_EQ(test_unpack_r8i(1), 1);
-   EXPECT_EQ(test_unpack_r8i(0xff), 0xffffffff);
-   EXPECT_EQ(test_unpack_r32ui(0), 0);
-   EXPECT_EQ(test_unpack_r32ui(0xffffffff), 0xffffffff);
-}
-
-TEST(MesaFormatsTest, UnpackRGBAUbyteRowRGBA32F)
-{
-   float val[4] = {0, 0.5, -1, 2};
-   uint8_t result[4];
-   _mesa_unpack_ubyte_rgba_row(MESA_FORMAT_RGBA_FLOAT32, 1, &val, &result);
-   EXPECT_EQ(result[0], 0);
-   EXPECT_EQ(result[1], 0x80);
-   EXPECT_EQ(result[2], 0);
-   EXPECT_EQ(result[3], 0xff);
-}
-
-TEST(MesaFormatsTest, UnpackRGBAUbyteRowRGBA4)
-{
-   uint16_t val = (1 << 0) | (0x3f << 5) | (0x10 << 11);
-   uint8_t result[4];
-   _mesa_unpack_ubyte_rgba_row(MESA_FORMAT_R5G6B5_UNORM, 1, &val, &result);
-   EXPECT_EQ(result[0], 0x08);
-   EXPECT_EQ(result[1], 0xff);
-   EXPECT_EQ(result[2], 0x84);
-   EXPECT_EQ(result[3], 0xff);
-}
-
-static float
-test_unpack_floatz_z32f(float val)
-{
-   float result;
-   _mesa_unpack_float_z_row(MESA_FORMAT_Z_FLOAT32, 1, &val, &result);
-   return result;
-}
-
-TEST(MesaFormatsTest, UnpackFloatZRow)
-{
-   EXPECT_EQ(test_unpack_floatz_z32f(0.5), 0.5);
-   EXPECT_EQ(test_unpack_floatz_z32f(-1.0), -1.0);
-   EXPECT_EQ(test_unpack_floatz_z32f(2.0), 2.0);
-}
-
-static uint32_t
-test_unpack_uintz_z32f(float val)
-{
-   uint32_t result;
-   _mesa_unpack_uint_z_row(MESA_FORMAT_Z_FLOAT32, 1, &val, &result);
-   return result;
-}
-
-TEST(MesaFormatsTest, UnpackUintZRow)
-{
-   EXPECT_EQ(test_unpack_uintz_z32f(0.5), 0x7fffffff);
-   EXPECT_EQ(test_unpack_uintz_z32f(-1.0), 0);
-   EXPECT_EQ(test_unpack_uintz_z32f(2.0), 0xffffffff);
-}
-
-/* It's easy to have precision issues packing 32-bit floats to unorm. */
-TEST(MesaFormatsTest, PackFloatZ)
-{
-   float val = 0.571428597f;
-   uint32_t result;
-   _mesa_pack_float_z_row(MESA_FORMAT_Z_UNORM32, 1, &val, &result);
-   EXPECT_EQ(result, 0x924924ff);
-}
-
-TEST(MesaFormatsTest, PackUbyteRGBARounding)
-{
-   for (int i = 0; i <= 255; i++) {
-      uint8_t val[4] = {(uint8_t)i, 0, 0, 0};
-      uint16_t result;
-      _mesa_pack_ubyte_rgba_row(MESA_FORMAT_R5G6B5_UNORM, 1, &val, &result);
-      EXPECT_EQ(result, (i * 31 + 127) / 255);
-   }
-}
diff --git a/src/mesa/main/tests/meson.build b/src/mesa/main/tests/meson.build
index 7e9e843cab4..0e95fbfbbd0 100644
--- a/src/mesa/main/tests/meson.build
+++ b/src/mesa/main/tests/meson.build
@@ -38,7 +38,7 @@ test(
   executable(
     'main_test',
     [files_main_test, main_dispatch_h],
-    include_directories : [inc_include, inc_src, inc_mapi, inc_mesa, inc_gallium],
+    include_directories : [inc_include, inc_src, inc_mapi, inc_mesa],
     dependencies : [idep_gtest, dep_clock, dep_dl, dep_thread, idep_mesautil],
     link_with : [libmesa_classic, link_main_test],
   ),
diff --git a/src/mesa/main/texcompress_etc.c b/src/mesa/main/texcompress_etc.c
index f14b1670728..32464f3ca9e 100644
--- a/src/mesa/main/texcompress_etc.c
+++ b/src/mesa/main/texcompress_etc.c
@@ -750,8 +750,8 @@ etc2_unpack_srgb8(uint8_t *dst_row,
 		  tmp = dst[0];
 		  dst[0] = dst[2];
 		  dst[2] = tmp;
+		  dst[3] = 255;
 	       }
-               dst[3] = 255;
 
                dst += comps;
             }
diff --git a/src/mesa/meson.build b/src/mesa/meson.build
index fc7e09f5fe1..7c10b0ac108 100644
--- a/src/mesa/meson.build
+++ b/src/mesa/meson.build
@@ -166,7 +166,6 @@ files_libmesa_common = files(
   'main/glthread.h',
   'main/glthread_bufferobj.c',
   'main/glthread_draw.c',
-  'main/glthread_get.c',
   'main/glthread_marshal.h',
   'main/glthread_shaderobj.c',
   'main/glthread_varray.c',
@@ -669,7 +668,8 @@ get_hash_h = custom_target(
 )
 
 foreach x : [['format_info.h', 'format_info.py'],
-             ['format_pack.c', 'format_pack.py']]
+             ['format_pack.c', 'format_pack.py'],
+             ['format_unpack.c', 'format_unpack.py']]
   files_libmesa_common += custom_target(
     x[0],
     input : ['main/@0@'.format(x[1]), 'main/formats.csv'],
diff --git a/src/mesa/state_tracker/st_atom_blend.c b/src/mesa/state_tracker/st_atom_blend.c
index 4b73856b493..0eb4f9552e2 100644
--- a/src/mesa/state_tracker/st_atom_blend.c
+++ b/src/mesa/state_tracker/st_atom_blend.c
@@ -283,7 +283,8 @@ st_update_blend( struct st_context *st )
       /* no blending / logicop */
    }
 
-   blend->dither = ctx->Color.DitherFlag;
+   if (!st->no_dithering)
+      blend->dither = ctx->Color.DitherFlag;
 
    if (_mesa_is_multisample_enabled(ctx) &&
        !(ctx->DrawBuffer->_IntegerBuffers & 0x1)) {
diff --git a/src/mesa/state_tracker/st_atom_constbuf.c b/src/mesa/state_tracker/st_atom_constbuf.c
index cfa625d528d..5907ed6cee4 100644
--- a/src/mesa/state_tracker/st_atom_constbuf.c
+++ b/src/mesa/state_tracker/st_atom_constbuf.c
@@ -102,13 +102,12 @@ st_upload_constants(struct st_context *st, struct gl_program *prog)
       cb.buffer_size = paramBytes;
 
       if (st->prefer_real_buffer_in_constbuf0) {
-         struct pipe_context *pipe = st->pipe;
          uint32_t *ptr;
          /* fetch_state always stores 4 components (16 bytes) per matrix row,
           * but matrix rows are sometimes allocated partially, so add 12
           * to compensate for the fetch_state defect.
           */
-         u_upload_alloc(pipe->const_uploader, 0, paramBytes + 12, 64,
+         u_upload_alloc(st->pipe->const_uploader, 0, paramBytes + 12, 64,
                         &cb.buffer_offset, &cb.buffer, (void**)&ptr);
 
          int uniform_bytes = params->UniformBytes;
@@ -121,8 +120,8 @@ st_upload_constants(struct st_context *st, struct gl_program *prog)
          if (params->StateFlags)
             _mesa_upload_state_parameters(st->ctx, params, ptr);
 
-         u_upload_unmap(pipe->const_uploader);
-         pipe->set_constant_buffer(pipe, shader_type, 0, &cb);
+         u_upload_unmap(st->pipe->const_uploader);
+         cso_set_constant_buffer(st->cso_context, shader_type, 0, &cb);
          pipe_resource_reference(&cb.buffer, NULL);
 
          /* Set inlinable constants. This is more involved because state
@@ -132,6 +131,7 @@ st_upload_constants(struct st_context *st, struct gl_program *prog)
           */
          unsigned num_inlinable_uniforms = prog->info.num_inlinable_uniforms;
          if (num_inlinable_uniforms) {
+            struct pipe_context *pipe = st->pipe;
             uint32_t values[MAX_INLINABLE_UNIFORMS];
             gl_constant_value *constbuf = params->ParameterValues;
             bool loaded_state_vars = false;
@@ -152,8 +152,6 @@ st_upload_constants(struct st_context *st, struct gl_program *prog)
                                           values);
          }
       } else {
-         struct pipe_context *pipe = st->pipe;
-
          cb.user_buffer = params->ParameterValues;
 
          /* Update the constants which come from fixed-function state, such as
@@ -162,11 +160,12 @@ st_upload_constants(struct st_context *st, struct gl_program *prog)
          if (params->StateFlags)
             _mesa_load_state_parameters(st->ctx, params);
 
-         pipe->set_constant_buffer(pipe, shader_type, 0, &cb);
+         cso_set_constant_buffer(st->cso_context, shader_type, 0, &cb);
 
          /* Set inlinable constants. */
          unsigned num_inlinable_uniforms = prog->info.num_inlinable_uniforms;
          if (num_inlinable_uniforms) {
+            struct pipe_context *pipe = st->pipe;
             uint32_t values[MAX_INLINABLE_UNIFORMS];
             gl_constant_value *constbuf = params->ParameterValues;
 
@@ -182,9 +181,7 @@ st_upload_constants(struct st_context *st, struct gl_program *prog)
       st->state.constbuf0_enabled_shader_mask |= 1 << shader_type;
    } else if (st->state.constbuf0_enabled_shader_mask & (1 << shader_type)) {
       /* Unbind. */
-      struct pipe_context *pipe = st->pipe;
-
-      pipe->set_constant_buffer(pipe, shader_type, 0, NULL);
+      cso_set_constant_buffer(st->cso_context, shader_type, 0, NULL);
       st->state.constbuf0_enabled_shader_mask &= ~(1 << shader_type);
    }
 }
@@ -263,8 +260,6 @@ st_bind_ubos(struct st_context *st, struct gl_program *prog,
    if (!prog)
       return;
 
-   struct pipe_context *pipe = st->pipe;
-
    for (i = 0; i < prog->sh.NumUniformBlocks; i++) {
       struct gl_buffer_binding *binding;
       struct st_buffer_object *st_obj;
@@ -290,7 +285,7 @@ st_bind_ubos(struct st_context *st, struct gl_program *prog,
          cb.buffer_size = 0;
       }
 
-      pipe->set_constant_buffer(pipe, shader_type, 1 + i, &cb);
+      cso_set_constant_buffer(st->cso_context, shader_type, 1 + i, &cb);
    }
 }
 
diff --git a/src/mesa/state_tracker/st_atom_image.c b/src/mesa/state_tracker/st_atom_image.c
index 37f12cf5989..a7fd17535a5 100644
--- a/src/mesa/state_tracker/st_atom_image.c
+++ b/src/mesa/state_tracker/st_atom_image.c
@@ -161,29 +161,26 @@ st_bind_images(struct st_context *st, struct gl_program *prog,
 {
    unsigned i;
    struct pipe_image_view images[MAX_IMAGE_UNIFORMS];
+   struct gl_program_constants *c;
 
    if (!prog || !st->pipe->set_shader_images)
       return;
 
-   unsigned num_images = prog->info.num_images;
+   c = &st->ctx->Const.Program[prog->info.stage];
 
-   for (i = 0; i < num_images; i++) {
+   for (i = 0; i < prog->info.num_images; i++) {
       struct pipe_image_view *img = &images[i];
 
       st_convert_image_from_unit(st, img, prog->sh.ImageUnits[i],
                                  prog->sh.ImageAccess[i]);
    }
-
-   struct pipe_context *pipe = st->pipe;
-   pipe->set_shader_images(pipe, shader_type, 0, num_images, images);
-
+   cso_set_shader_images(st->cso_context, shader_type, 0,
+                         prog->info.num_images, images);
    /* clear out any stale shader images */
-   unsigned last_num_images = st->state.num_images[shader_type];
-   if (num_images < last_num_images) {
-      pipe->set_shader_images(pipe, shader_type, num_images,
-                              last_num_images - num_images, NULL);
-   }
-   st->state.num_images[shader_type] = num_images;
+   if (prog->info.num_images < c->MaxImageUniforms)
+      cso_set_shader_images(
+            st->cso_context, shader_type, prog->info.num_images,
+            c->MaxImageUniforms - prog->info.num_images, NULL);
 }
 
 void st_bind_vs_images(struct st_context *st)
diff --git a/src/mesa/state_tracker/st_atom_texture.c b/src/mesa/state_tracker/st_atom_texture.c
index 2c8cf563033..b3010c8c02b 100644
--- a/src/mesa/state_tracker/st_atom_texture.c
+++ b/src/mesa/state_tracker/st_atom_texture.c
@@ -103,7 +103,6 @@ update_textures(struct st_context *st,
                 const struct gl_program *prog,
                 struct pipe_sampler_view **sampler_views)
 {
-   struct pipe_context *pipe = st->pipe;
    const GLuint old_max = st->state.num_sampler_views[shader_stage];
    GLbitfield samplers_used = prog->SamplersUsed;
    GLbitfield texel_fetch_samplers = prog->info.textures_used_by_txf;
@@ -198,7 +197,7 @@ update_textures(struct st_context *st,
          tmpl.swizzle_g = PIPE_SWIZZLE_Y;   /* tmpl from Y plane is R8 */
          extra = u_bit_scan(&free_slots);
          sampler_views[extra] =
-               pipe->create_sampler_view(pipe, stObj->pt->next, &tmpl);
+               st->pipe->create_sampler_view(st->pipe, stObj->pt->next, &tmpl);
          break;
       case PIPE_FORMAT_P010:
       case PIPE_FORMAT_P012:
@@ -208,17 +207,17 @@ update_textures(struct st_context *st,
          tmpl.swizzle_g = PIPE_SWIZZLE_Y;   /* tmpl from Y plane is R16 */
          extra = u_bit_scan(&free_slots);
          sampler_views[extra] =
-               pipe->create_sampler_view(pipe, stObj->pt->next, &tmpl);
+               st->pipe->create_sampler_view(st->pipe, stObj->pt->next, &tmpl);
          break;
       case PIPE_FORMAT_IYUV:
          /* we need two additional R8 views: */
          tmpl.format = PIPE_FORMAT_R8_UNORM;
          extra = u_bit_scan(&free_slots);
          sampler_views[extra] =
-               pipe->create_sampler_view(pipe, stObj->pt->next, &tmpl);
+               st->pipe->create_sampler_view(st->pipe, stObj->pt->next, &tmpl);
          extra = u_bit_scan(&free_slots);
          sampler_views[extra] =
-               pipe->create_sampler_view(pipe, stObj->pt->next->next, &tmpl);
+               st->pipe->create_sampler_view(st->pipe, stObj->pt->next->next, &tmpl);
          break;
       case PIPE_FORMAT_YUYV:
          /* we need one additional BGRA8888 view: */
@@ -227,7 +226,7 @@ update_textures(struct st_context *st,
          tmpl.swizzle_a = PIPE_SWIZZLE_W;
          extra = u_bit_scan(&free_slots);
          sampler_views[extra] =
-               pipe->create_sampler_view(pipe, stObj->pt->next, &tmpl);
+               st->pipe->create_sampler_view(st->pipe, stObj->pt->next, &tmpl);
          break;
       case PIPE_FORMAT_UYVY:
          /* we need one additional RGBA8888 view: */
@@ -236,7 +235,7 @@ update_textures(struct st_context *st,
          tmpl.swizzle_a = PIPE_SWIZZLE_W;
          extra = u_bit_scan(&free_slots);
          sampler_views[extra] =
-               pipe->create_sampler_view(pipe, stObj->pt->next, &tmpl);
+               st->pipe->create_sampler_view(st->pipe, stObj->pt->next, &tmpl);
          break;
       default:
          break;
@@ -245,15 +244,10 @@ update_textures(struct st_context *st,
       num_textures = MAX2(num_textures, extra + 1);
    }
 
-   /* Unbind old textures. */
-   unsigned old_num_textures = st->state.num_sampler_views[shader_stage];
-   unsigned num_unbind = old_num_textures > num_textures ?
-                            old_num_textures - num_textures : 0;
-   for (unsigned i = 0; i < num_unbind; i++)
-      pipe_sampler_view_reference(&sampler_views[num_textures + i], NULL);
-
-   pipe->set_sampler_views(pipe, shader_stage, 0, num_textures + num_unbind,
-                           sampler_views);
+   cso_set_sampler_views(st->cso_context,
+                         shader_stage,
+                         num_textures,
+                         sampler_views);
    st->state.num_sampler_views[shader_stage] = num_textures;
 }
 
diff --git a/src/mesa/state_tracker/st_cb_bitmap.c b/src/mesa/state_tracker/st_cb_bitmap.c
index 32fcd8a49df..1b372b0c229 100644
--- a/src/mesa/state_tracker/st_cb_bitmap.c
+++ b/src/mesa/state_tracker/st_cb_bitmap.c
@@ -170,7 +170,6 @@ setup_render_state(struct gl_context *ctx,
                    bool atlas)
 {
    struct st_context *st = st_context(ctx);
-   struct pipe_context *pipe = st->pipe;
    struct cso_context *cso = st->cso_context;
    struct st_fp_variant *fpv;
    struct st_fp_variant_key key;
@@ -201,9 +200,11 @@ setup_render_state(struct gl_context *ctx,
 
    cso_save_state(cso, (CSO_BIT_RASTERIZER |
                         CSO_BIT_FRAGMENT_SAMPLERS |
+                        CSO_BIT_FRAGMENT_SAMPLER_VIEWS |
                         CSO_BIT_VIEWPORT |
                         CSO_BIT_STREAM_OUTPUTS |
                         CSO_BIT_VERTEX_ELEMENTS |
+                        CSO_BIT_AUX_VERTEX_BUFFER_SLOT |
                         CSO_BITS_ALL_SHADERS));
 
 
@@ -247,9 +248,7 @@ setup_render_state(struct gl_context *ctx,
       memcpy(sampler_views, st->state.frag_sampler_views,
              sizeof(sampler_views));
       sampler_views[fpv->bitmap_sampler] = sv;
-      pipe->set_sampler_views(pipe, PIPE_SHADER_FRAGMENT, 0, num, sampler_views);
-      st->state.num_sampler_views[PIPE_SHADER_FRAGMENT] =
-         MAX2(st->state.num_sampler_views[PIPE_SHADER_FRAGMENT], num);
+      cso_set_sampler_views(cso, PIPE_SHADER_FRAGMENT, num, sampler_views);
    }
 
    /* viewport state: viewport matching window dims */
@@ -272,21 +271,8 @@ restore_render_state(struct gl_context *ctx)
 {
    struct st_context *st = st_context(ctx);
    struct cso_context *cso = st->cso_context;
-   struct pipe_context *pipe = st->pipe;
 
    cso_restore_state(cso);
-
-   /* Unbind all because st/mesa won't do it if the current shader doesn't
-    * use them.
-    */
-   static struct pipe_sampler_view *null[PIPE_MAX_SAMPLERS];
-   pipe->set_sampler_views(pipe, PIPE_SHADER_FRAGMENT, 0,
-                           st->state.num_sampler_views[PIPE_SHADER_FRAGMENT],
-                           null);
-   st->state.num_sampler_views[PIPE_SHADER_FRAGMENT] = 0;
-
-   st->dirty |= ST_NEW_VERTEX_ARRAYS |
-                ST_NEW_FS_SAMPLER_VIEWS;
 }
 
 
@@ -775,8 +761,6 @@ st_DrawAtlasBitmaps(struct gl_context *ctx,
    u_upload_unmap(pipe->stream_uploader);
 
    cso_set_vertex_buffers(st->cso_context, 0, 1, &vb);
-   st->last_num_vbuffers = MAX2(st->last_num_vbuffers, 1);
-
    cso_draw_arrays(st->cso_context, PIPE_PRIM_QUADS, 0, num_verts);
 
 out:
diff --git a/src/mesa/state_tracker/st_cb_clear.c b/src/mesa/state_tracker/st_cb_clear.c
index 4ff77a6b681..408d29aff74 100644
--- a/src/mesa/state_tracker/st_cb_clear.c
+++ b/src/mesa/state_tracker/st_cb_clear.c
@@ -266,6 +266,7 @@ clear_with_quad(struct gl_context *ctx, unsigned clear_buffers)
                         CSO_BIT_VIEWPORT |
                         CSO_BIT_STREAM_OUTPUTS |
                         CSO_BIT_VERTEX_ELEMENTS |
+                        CSO_BIT_AUX_VERTEX_BUFFER_SLOT |
                         (st->active_queries ? CSO_BIT_PAUSE_QUERIES : 0) |
                         CSO_BITS_ALL_SHADERS));
 
@@ -361,7 +362,6 @@ clear_with_quad(struct gl_context *ctx, unsigned clear_buffers)
 
    /* Restore pipe state */
    cso_restore_state(cso);
-   st->dirty |= ST_NEW_VERTEX_ARRAYS;
 }
 
 
diff --git a/src/mesa/state_tracker/st_cb_drawpixels.c b/src/mesa/state_tracker/st_cb_drawpixels.c
index 0564a694d44..18f82e00db4 100644
--- a/src/mesa/state_tracker/st_cb_drawpixels.c
+++ b/src/mesa/state_tracker/st_cb_drawpixels.c
@@ -745,7 +745,6 @@ draw_textured_quad(struct gl_context *ctx, GLint x, GLint y, GLfloat z,
                    GLboolean write_depth, GLboolean write_stencil)
 {
    struct st_context *st = st_context(ctx);
-   struct pipe_context *pipe = st->pipe;
    struct cso_context *cso = st->cso_context;
    const unsigned fb_width = _mesa_geometric_width(ctx->DrawBuffer);
    const unsigned fb_height = _mesa_geometric_height(ctx->DrawBuffer);
@@ -768,8 +767,10 @@ draw_textured_quad(struct gl_context *ctx, GLint x, GLint y, GLfloat z,
    cso_state_mask = (CSO_BIT_RASTERIZER |
                      CSO_BIT_VIEWPORT |
                      CSO_BIT_FRAGMENT_SAMPLERS |
+                     CSO_BIT_FRAGMENT_SAMPLER_VIEWS |
                      CSO_BIT_STREAM_OUTPUTS |
                      CSO_BIT_VERTEX_ELEMENTS |
+                     CSO_BIT_AUX_VERTEX_BUFFER_SLOT |
                      CSO_BITS_ALL_SHADERS);
    if (write_stencil) {
       cso_state_mask |= (CSO_BIT_DEPTH_STENCIL_ALPHA |
@@ -881,14 +882,10 @@ draw_textured_quad(struct gl_context *ctx, GLint x, GLint y, GLfloat z,
       sampler_views[fpv->drawpix_sampler] = sv[0];
       if (sv[1])
          sampler_views[fpv->pixelmap_sampler] = sv[1];
-      pipe->set_sampler_views(pipe, PIPE_SHADER_FRAGMENT, 0, num, sampler_views);
-      st->state.num_sampler_views[PIPE_SHADER_FRAGMENT] =
-         MAX2(st->state.num_sampler_views[PIPE_SHADER_FRAGMENT], num);
+      cso_set_sampler_views(cso, PIPE_SHADER_FRAGMENT, num, sampler_views);
    } else {
       /* drawing a depth/stencil image */
-      pipe->set_sampler_views(pipe, PIPE_SHADER_FRAGMENT, 0, num_sampler_view, sv);
-      st->state.num_sampler_views[PIPE_SHADER_FRAGMENT] =
-         MAX2(st->state.num_sampler_views[PIPE_SHADER_FRAGMENT], num_sampler_view);
+      cso_set_sampler_views(cso, PIPE_SHADER_FRAGMENT, num_sampler_view, sv);
    }
 
    /* viewport state: viewport matching window dims */
@@ -936,18 +933,6 @@ draw_textured_quad(struct gl_context *ctx, GLint x, GLint y, GLfloat z,
 
    /* restore state */
    cso_restore_state(cso);
-
-   /* Unbind all because st/mesa won't do it if the current shader doesn't
-    * use them.
-    */
-   static struct pipe_sampler_view *null[PIPE_MAX_SAMPLERS];
-   pipe->set_sampler_views(pipe, PIPE_SHADER_FRAGMENT, 0,
-                           st->state.num_sampler_views[PIPE_SHADER_FRAGMENT],
-                           null);
-   st->state.num_sampler_views[PIPE_SHADER_FRAGMENT] = 0;
-
-   st->dirty |= ST_NEW_VERTEX_ARRAYS |
-                ST_NEW_FS_SAMPLER_VIEWS;
 }
 
 
@@ -1397,6 +1382,11 @@ st_DrawPixels(struct gl_context *ctx, GLint x, GLint y,
       u_sampler_view_default_template(&templ, pt, pt->format);
       /* Set up the sampler view's swizzle */
       setup_sampler_swizzle(&templ, format, type);
+      if (st->screen->get_param(st->screen, PIPE_CAP_EMULATE_ARGB))
+         u_sampler_view_swizzle_argb(&templ,
+                                     format == GL_RGBA ?
+                                     PIPE_FORMAT_R8G8B8A8_UNORM :
+                                     PIPE_FORMAT_B8G8R8A8_UNORM);
 
       sv[0] = st->pipe->create_sampler_view(st->pipe, pt, &templ);
    }
diff --git a/src/mesa/state_tracker/st_cb_drawtex.c b/src/mesa/state_tracker/st_cb_drawtex.c
index 88865cd5e27..2ecdc4968d0 100644
--- a/src/mesa/state_tracker/st_cb_drawtex.c
+++ b/src/mesa/state_tracker/st_cb_drawtex.c
@@ -295,7 +295,8 @@ st_DrawTex(struct gl_context *ctx, GLfloat x, GLfloat y, GLfloat z,
                         CSO_BIT_TESSCTRL_SHADER |
                         CSO_BIT_TESSEVAL_SHADER |
                         CSO_BIT_GEOMETRY_SHADER |
-                        CSO_BIT_VERTEX_ELEMENTS));
+                        CSO_BIT_VERTEX_ELEMENTS |
+                        CSO_BIT_AUX_VERTEX_BUFFER_SLOT));
 
    {
       void *vs = lookup_shader(st, numAttribs,
@@ -338,13 +339,11 @@ st_DrawTex(struct gl_context *ctx, GLfloat x, GLfloat y, GLfloat z,
                            PIPE_PRIM_TRIANGLE_FAN,
                            4,  /* verts */
                            numAttribs); /* attribs/vert */
-   st->last_num_vbuffers = MAX2(st->last_num_vbuffers, 1);
 
    pipe_resource_reference(&vbuffer, NULL);
 
    /* restore state */
    cso_restore_state(cso);
-   st->dirty |= ST_NEW_VERTEX_ARRAYS;
 }
 
 
diff --git a/src/mesa/state_tracker/st_cb_readpixels.c b/src/mesa/state_tracker/st_cb_readpixels.c
index cc346117ecb..6be5a259d45 100644
--- a/src/mesa/state_tracker/st_cb_readpixels.c
+++ b/src/mesa/state_tracker/st_cb_readpixels.c
@@ -137,9 +137,12 @@ try_pbo_readpixels(struct st_context *st, struct st_renderbuffer *strb,
    if (!st_pbo_addresses_pixelstore(st, GL_TEXTURE_2D, false, pack, pixels, &addr))
       return false;
 
-   cso_save_state(cso, (CSO_BIT_FRAGMENT_SAMPLERS |
+   cso_save_state(cso, (CSO_BIT_FRAGMENT_SAMPLER_VIEWS |
+                        CSO_BIT_FRAGMENT_SAMPLERS |
+                        CSO_BIT_FRAGMENT_IMAGE0 |
                         CSO_BIT_BLEND |
                         CSO_BIT_VERTEX_ELEMENTS |
+                        CSO_BIT_AUX_VERTEX_BUFFER_SLOT |
                         CSO_BIT_FRAMEBUFFER |
                         CSO_BIT_VIEWPORT |
                         CSO_BIT_RASTERIZER |
@@ -150,6 +153,7 @@ try_pbo_readpixels(struct st_context *st, struct st_renderbuffer *strb,
                         CSO_BIT_MIN_SAMPLES |
                         CSO_BIT_RENDER_CONDITION |
                         CSO_BITS_ALL_SHADERS));
+   cso_save_constant_buffer_slot0(cso, PIPE_SHADER_FRAGMENT);
 
    cso_set_sample_mask(cso, ~0);
    cso_set_min_samples(cso, 1);
@@ -185,13 +189,13 @@ try_pbo_readpixels(struct st_context *st, struct st_renderbuffer *strb,
          addr.constants.layer_offset = surface->u.tex.first_layer;
       }
 
+      if (screen->get_param(screen, PIPE_CAP_EMULATE_ARGB))
+         u_sampler_view_swizzle_argb(&templ, dst_format);
       sampler_view = pipe->create_sampler_view(pipe, texture, &templ);
       if (sampler_view == NULL)
          goto fail;
 
-      pipe->set_sampler_views(pipe, PIPE_SHADER_FRAGMENT, 0, 1, &sampler_view);
-      st->state.num_sampler_views[PIPE_SHADER_FRAGMENT] =
-         MAX2(st->state.num_sampler_views[PIPE_SHADER_FRAGMENT], 1);
+      cso_set_sampler_views(cso, PIPE_SHADER_FRAGMENT, 1, &sampler_view);
 
       pipe_sampler_view_reference(&sampler_view, NULL);
 
@@ -211,7 +215,7 @@ try_pbo_readpixels(struct st_context *st, struct st_renderbuffer *strb,
       image.u.buf.size = (addr.last_element - addr.first_element + 1) *
                          addr.bytes_per_pixel;
 
-      pipe->set_shader_images(pipe, PIPE_SHADER_FRAGMENT, 0, 1, &image);
+      cso_set_shader_images(cso, PIPE_SHADER_FRAGMENT, 0, 1, &image);
    }
 
    /* Set up no-attachment framebuffer */
@@ -254,21 +258,7 @@ try_pbo_readpixels(struct st_context *st, struct st_renderbuffer *strb,
 
 fail:
    cso_restore_state(cso);
-
-   /* Unbind all because st/mesa won't do it if the current shader doesn't
-    * use them.
-    */
-   static struct pipe_sampler_view *null[PIPE_MAX_SAMPLERS];
-   pipe->set_sampler_views(pipe, PIPE_SHADER_FRAGMENT, 0,
-                           st->state.num_sampler_views[PIPE_SHADER_FRAGMENT],
-                           null);
-   st->state.num_sampler_views[PIPE_SHADER_FRAGMENT] = 0;
-   pipe->set_shader_images(pipe, PIPE_SHADER_FRAGMENT, 0, 1, NULL);
-
-   st->dirty |= ST_NEW_FS_CONSTANTS |
-                ST_NEW_FS_IMAGES |
-                ST_NEW_FS_SAMPLER_VIEWS |
-                ST_NEW_VERTEX_ARRAYS;
+   cso_restore_constant_buffer_slot0(cso, PIPE_SHADER_FRAGMENT);
 
    return success;
 }
diff --git a/src/mesa/state_tracker/st_cb_texture.c b/src/mesa/state_tracker/st_cb_texture.c
index f20afbacdb4..18d49613257 100644
--- a/src/mesa/state_tracker/st_cb_texture.c
+++ b/src/mesa/state_tracker/st_cb_texture.c
@@ -1275,7 +1275,9 @@ try_pbo_upload_common(struct gl_context *ctx,
    if (!fs)
       return false;
 
-   cso_save_state(cso, (CSO_BIT_VERTEX_ELEMENTS |
+   cso_save_state(cso, (CSO_BIT_FRAGMENT_SAMPLER_VIEWS |
+                        CSO_BIT_VERTEX_ELEMENTS |
+                        CSO_BIT_AUX_VERTEX_BUFFER_SLOT |
                         CSO_BIT_FRAMEBUFFER |
                         CSO_BIT_VIEWPORT |
                         CSO_BIT_BLEND |
@@ -1287,6 +1289,7 @@ try_pbo_upload_common(struct gl_context *ctx,
                         CSO_BIT_MIN_SAMPLES |
                         CSO_BIT_RENDER_CONDITION |
                         CSO_BITS_ALL_SHADERS));
+   cso_save_constant_buffer_slot0(cso, PIPE_SHADER_FRAGMENT);
 
    cso_set_sample_mask(cso, ~0);
    cso_set_min_samples(cso, 1);
@@ -1307,14 +1310,14 @@ try_pbo_upload_common(struct gl_context *ctx,
       templ.swizzle_g = PIPE_SWIZZLE_Y;
       templ.swizzle_b = PIPE_SWIZZLE_Z;
       templ.swizzle_a = PIPE_SWIZZLE_W;
+      if (st->screen->get_param(st->screen, PIPE_CAP_EMULATE_ARGB))
+         u_sampler_view_swizzle_argb(&templ, surface->format);
 
       sampler_view = pipe->create_sampler_view(pipe, addr->buffer, &templ);
       if (sampler_view == NULL)
          goto fail;
 
-      pipe->set_sampler_views(pipe, PIPE_SHADER_FRAGMENT, 0, 1, &sampler_view);
-      st->state.num_sampler_views[PIPE_SHADER_FRAGMENT] =
-         MAX2(st->state.num_sampler_views[PIPE_SHADER_FRAGMENT], 1);
+      cso_set_sampler_views(cso, PIPE_SHADER_FRAGMENT, 1, &sampler_view);
 
       pipe_sampler_view_reference(&sampler_view, NULL);
    }
@@ -1350,19 +1353,7 @@ try_pbo_upload_common(struct gl_context *ctx,
 
 fail:
    cso_restore_state(cso);
-
-   /* Unbind all because st/mesa won't do it if the current shader doesn't
-    * use them.
-    */
-   static struct pipe_sampler_view *null[PIPE_MAX_SAMPLERS];
-   pipe->set_sampler_views(pipe, PIPE_SHADER_FRAGMENT, 0,
-                           st->state.num_sampler_views[PIPE_SHADER_FRAGMENT],
-                           null);
-   st->state.num_sampler_views[PIPE_SHADER_FRAGMENT] = 0;
-
-   st->dirty |= ST_NEW_VERTEX_ARRAYS |
-                ST_NEW_FS_CONSTANTS |
-                ST_NEW_FS_SAMPLER_VIEWS;
+   cso_restore_constant_buffer_slot0(cso, PIPE_SHADER_FRAGMENT);
 
    return success;
 }
diff --git a/src/mesa/state_tracker/st_context.c b/src/mesa/state_tracker/st_context.c
index a035d792ce0..5bb68b98408 100644
--- a/src/mesa/state_tracker/st_context.c
+++ b/src/mesa/state_tracker/st_context.c
@@ -699,6 +699,8 @@ st_create_context_priv(struct gl_context *ctx, struct pipe_context *pipe,
       screen->get_param(screen, PIPE_CAP_INDEP_BLEND_FUNC);
    st->needs_rgb_dst_alpha_override =
       screen->get_param(screen, PIPE_CAP_RGB_OVERRIDE_DST_ALPHA_BLEND);
+   st->no_dithering =
+      screen->get_param(screen, PIPE_CAP_NO_DITHERING);
    st->lower_flatshade =
       !screen->get_param(screen, PIPE_CAP_FLATSHADE);
    st->lower_alpha_test =
diff --git a/src/mesa/state_tracker/st_context.h b/src/mesa/state_tracker/st_context.h
index b8246ddb5dc..659db8274f4 100644
--- a/src/mesa/state_tracker/st_context.h
+++ b/src/mesa/state_tracker/st_context.h
@@ -152,6 +152,7 @@ struct st_context
    boolean has_single_pipe_stat;
    boolean has_indep_blend_func;
    boolean needs_rgb_dst_alpha_override;
+   boolean no_dithering;
    boolean can_bind_const_buffer_as_vertex;
    boolean lower_flatshade;
    boolean lower_alpha_test;
@@ -208,7 +209,6 @@ struct st_context
       struct pipe_sampler_view *vert_sampler_views[PIPE_MAX_SAMPLERS];
       struct pipe_sampler_view *frag_sampler_views[PIPE_MAX_SAMPLERS];
       GLuint num_sampler_views[PIPE_SHADER_TYPES];
-      unsigned num_images[PIPE_SHADER_TYPES];
       struct pipe_clip_state clip;
       unsigned constbuf0_enabled_shader_mask;
       unsigned fb_width;
diff --git a/src/mesa/state_tracker/st_draw.c b/src/mesa/state_tracker/st_draw.c
index c95fbd411ad..c11934c0a39 100644
--- a/src/mesa/state_tracker/st_draw.c
+++ b/src/mesa/state_tracker/st_draw.c
@@ -543,7 +543,6 @@ st_draw_quad(struct st_context *st,
    u_upload_unmap(st->pipe->stream_uploader);
 
    cso_set_vertex_buffers(st->cso_context, 0, 1, &vb);
-   st->last_num_vbuffers = MAX2(st->last_num_vbuffers, 1);
 
    if (num_instances > 1) {
       cso_draw_arrays_instanced(st->cso_context, PIPE_PRIM_TRIANGLE_FAN, 0, 4,
diff --git a/src/mesa/state_tracker/st_glsl_to_tgsi.cpp b/src/mesa/state_tracker/st_glsl_to_tgsi.cpp
index 9e9289b41cb..740fb927f47 100644
--- a/src/mesa/state_tracker/st_glsl_to_tgsi.cpp
+++ b/src/mesa/state_tracker/st_glsl_to_tgsi.cpp
@@ -3263,7 +3263,6 @@ glsl_to_tgsi_visitor::visit(ir_assignment *ir)
               ir->rhs == ((glsl_to_tgsi_instruction *)this->instructions.get_tail())->ir &&
               !((glsl_to_tgsi_instruction *)this->instructions.get_tail())->is_64bit_expanded &&
               type_size(ir->lhs->type) == 1 &&
-              !ir->lhs->type->is_64bit() &&
               l.writemask == ((glsl_to_tgsi_instruction *)this->instructions.get_tail())->dst[0].writemask) {
       /* To avoid emitting an extra MOV when assigning an expression to a
        * variable, emit the last instruction of the expression again, but
diff --git a/src/mesa/state_tracker/st_manager.c b/src/mesa/state_tracker/st_manager.c
index 42c3947b7af..ea3d5207ffd 100644
--- a/src/mesa/state_tracker/st_manager.c
+++ b/src/mesa/state_tracker/st_manager.c
@@ -833,23 +833,6 @@ st_thread_finish(struct st_context_iface *stctxi)
 }
 
 
-static void
-st_context_invalidate_state(struct st_context_iface *stctxi,
-                            unsigned flags)
-{
-   struct st_context *st = (struct st_context *) stctxi;
-
-   if (flags & ST_INVALIDATE_FS_SAMPLER_VIEWS)
-      st->dirty |= ST_NEW_FS_SAMPLER_VIEWS;
-   if (flags & ST_INVALIDATE_FS_CONSTBUF0)
-      st->dirty |= ST_NEW_FS_CONSTANTS;
-   if (flags & ST_INVALIDATE_VS_CONSTBUF0)
-      st->dirty |= ST_NEW_VS_CONSTANTS;
-   if (flags & ST_INVALIDATE_VERTEX_BUFFERS)
-      st->dirty |= ST_NEW_VERTEX_ARRAYS;
-}
-
-
 static void
 st_manager_destroy(struct st_manager *smapi)
 {
@@ -1002,7 +985,6 @@ st_api_create_context(struct st_api *stapi, struct st_manager *smapi,
    st->iface.share = st_context_share;
    st->iface.start_thread = st_start_thread;
    st->iface.thread_finish = st_thread_finish;
-   st->iface.invalidate_state = st_context_invalidate_state;
    st->iface.st_context_private = (void *) smapi;
    st->iface.cso_context = st->cso_context;
    st->iface.pipe = st->pipe;
diff --git a/src/mesa/state_tracker/st_nir_builtins.c b/src/mesa/state_tracker/st_nir_builtins.c
index 6e1989a9889..a5279dbc2af 100644
--- a/src/mesa/state_tracker/st_nir_builtins.c
+++ b/src/mesa/state_tracker/st_nir_builtins.c
@@ -22,18 +22,20 @@
 
 #include "tgsi/tgsi_from_mesa.h"
 #include "st_nir.h"
-#include "st_program.h"
 
 #include "compiler/nir/nir_builder.h"
 #include "compiler/glsl/gl_nir.h"
+#include "nir/nir_to_tgsi.h"
 #include "tgsi/tgsi_parse.h"
 
 struct pipe_shader_state *
 st_nir_finish_builtin_shader(struct st_context *st,
                              nir_shader *nir)
 {
+   struct pipe_context *pipe = st->pipe;
    struct pipe_screen *screen = st->screen;
    gl_shader_stage stage = nir->info.stage;
+   enum pipe_shader_type sh = pipe_shader_type_from_mesa(stage);
 
    nir->info.separate_shader = true;
    if (stage == MESA_SHADER_FRAGMENT)
@@ -73,7 +75,38 @@ st_nir_finish_builtin_shader(struct st_context *st,
       .ir.nir = nir,
    };
 
-   return st_create_nir_shader(st, &state);
+   if (PIPE_SHADER_IR_NIR !=
+       screen->get_shader_param(screen, sh, PIPE_SHADER_CAP_PREFERRED_IR)) {
+      state.type = PIPE_SHADER_IR_TGSI;
+      state.tokens = nir_to_tgsi(nir, screen);
+   }
+
+   struct pipe_shader_state *shader;
+   switch (stage) {
+   case MESA_SHADER_VERTEX:
+      shader = pipe->create_vs_state(pipe, &state);
+      break;
+   case MESA_SHADER_TESS_CTRL:
+      shader = pipe->create_tcs_state(pipe, &state);
+      break;
+   case MESA_SHADER_TESS_EVAL:
+      shader = pipe->create_tes_state(pipe, &state);
+      break;
+   case MESA_SHADER_GEOMETRY:
+      shader = pipe->create_gs_state(pipe, &state);
+      break;
+   case MESA_SHADER_FRAGMENT:
+      shader = pipe->create_fs_state(pipe, &state);
+      break;
+   default:
+      unreachable("unsupported shader stage");
+      return NULL;
+   }
+
+   if (state.type == PIPE_SHADER_IR_TGSI)
+      tgsi_free_tokens(state.tokens);
+
+   return shader;
 }
 
 /**
diff --git a/src/mesa/state_tracker/st_pbo.c b/src/mesa/state_tracker/st_pbo.c
index 0b5c90d30c7..70dbf554a34 100644
--- a/src/mesa/state_tracker/st_pbo.c
+++ b/src/mesa/state_tracker/st_pbo.c
@@ -194,7 +194,6 @@ st_pbo_draw(struct st_context *st, const struct st_pbo_addresses *addr,
             unsigned surface_width, unsigned surface_height)
 {
    struct cso_context *cso = st->cso_context;
-   struct pipe_context *pipe = st->pipe;
 
    /* Setup vertex and geometry shaders */
    if (!st->pbo.vs) {
@@ -256,7 +255,6 @@ st_pbo_draw(struct st_context *st, const struct st_pbo_addresses *addr,
       cso_set_vertex_elements(cso, &velem);
 
       cso_set_vertex_buffers(cso, 0, 1, &vbo);
-      st->last_num_vbuffers = MAX2(st->last_num_vbuffers, 1);
 
       pipe_resource_reference(&vbo.buffer.resource, NULL);
    }
@@ -270,7 +268,7 @@ st_pbo_draw(struct st_context *st, const struct st_pbo_addresses *addr,
       cb.buffer_offset = 0;
       cb.buffer_size = sizeof(addr->constants);
 
-      pipe->set_constant_buffer(pipe, PIPE_SHADER_FRAGMENT, 0, &cb);
+      cso_set_constant_buffer(cso, PIPE_SHADER_FRAGMENT, 0, &cb);
 
       pipe_resource_reference(&cb.buffer, NULL);
    }
@@ -435,21 +433,16 @@ create_fs(struct st_context *st, bool download,
    nir_ssa_def *coord = nir_load_var(&b, fragcoord);
 
    nir_ssa_def *layer = NULL;
-   if (st->pbo.layers && (!download || target == PIPE_TEXTURE_1D_ARRAY ||
-                                       target == PIPE_TEXTURE_2D_ARRAY ||
-                                       target == PIPE_TEXTURE_3D ||
-                                       target == PIPE_TEXTURE_CUBE ||
-                                       target == PIPE_TEXTURE_CUBE_ARRAY)) {
-      if (need_layer) {
-         nir_variable *var = nir_variable_create(b.shader, nir_var_shader_in,
-                                                glsl_int_type(), "gl_Layer");
-         var->data.location = VARYING_SLOT_LAYER;
-         var->data.interpolation = INTERP_MODE_FLAT;
-         layer = nir_load_var(&b, var);
-      }
-      else {
-         layer = zero;
-      }
+   if (st->pbo.layers && need_layer && (!download || target == PIPE_TEXTURE_1D_ARRAY ||
+                                                     target == PIPE_TEXTURE_2D_ARRAY ||
+                                                     target == PIPE_TEXTURE_3D ||
+                                                     target == PIPE_TEXTURE_CUBE ||
+                                                     target == PIPE_TEXTURE_CUBE_ARRAY)) {
+      nir_variable *var = nir_variable_create(b.shader, nir_var_shader_in,
+                                              glsl_int_type(), "gl_Layer");
+      var->data.location = VARYING_SLOT_LAYER;
+      var->data.interpolation = INTERP_MODE_FLAT;
+      layer = nir_load_var(&b, var);
    }
 
    /* offset_pos = param.xy + f2i(coord.xy) */
diff --git a/src/mesa/state_tracker/st_program.c b/src/mesa/state_tracker/st_program.c
index e85d0f58a61..07bb8d8436e 100644
--- a/src/mesa/state_tracker/st_program.c
+++ b/src/mesa/state_tracker/st_program.c
@@ -40,7 +40,6 @@
 #include "program/prog_to_nir.h"
 #include "program/programopt.h"
 
-#include "compiler/glsl/gl_nir.h"
 #include "compiler/nir/nir.h"
 #include "compiler/nir/nir_serialize.h"
 #include "draw/draw_context.h"
@@ -490,72 +489,6 @@ st_translate_stream_output_info(struct gl_program *prog)
    so_info->num_outputs = info->NumOutputs;
 }
 
-/**
- * Creates a driver shader from a NIR shader.  Takes ownership of the
- * passed nir_shader.
- */
-struct pipe_shader_state *
-st_create_nir_shader(struct st_context *st, struct pipe_shader_state *state)
-{
-   struct pipe_context *pipe = st->pipe;
-   struct pipe_screen *screen = st->screen;
-
-   assert(state->type == PIPE_SHADER_IR_NIR);
-   nir_shader *nir = state->ir.nir;
-   gl_shader_stage stage = nir->info.stage;
-   enum pipe_shader_type sh = pipe_shader_type_from_mesa(stage);
-
-   if (ST_DEBUG & DEBUG_PRINT_IR) {
-      fprintf(stderr, "NIR before handing off to driver:\n");
-      nir_print_shader(nir, stderr);
-   }
-
-   if (PIPE_SHADER_IR_NIR !=
-       screen->get_shader_param(screen, sh, PIPE_SHADER_CAP_PREFERRED_IR)) {
-      /* u_screen.c defaults to images as deref enabled for some reason (which
-       * is what radeonsi wants), but nir-to-tgsi requires lowered images.
-       */
-      if (screen->get_param(screen, PIPE_CAP_NIR_IMAGES_AS_DEREF))
-         NIR_PASS_V(nir, gl_nir_lower_images, false);
-
-      state->type = PIPE_SHADER_IR_TGSI;
-      state->tokens = nir_to_tgsi(nir, screen);
-
-      if (ST_DEBUG & DEBUG_PRINT_IR) {
-         fprintf(stderr, "TGSI for driver after nir-to-tgsi:\n");
-         tgsi_dump(state->tokens, 0);
-         fprintf(stderr, "\n");
-      }
-   }
-
-   struct pipe_shader_state *shader;
-   switch (stage) {
-   case MESA_SHADER_VERTEX:
-      shader = pipe->create_vs_state(pipe, state);
-      break;
-   case MESA_SHADER_TESS_CTRL:
-      shader = pipe->create_tcs_state(pipe, state);
-      break;
-   case MESA_SHADER_TESS_EVAL:
-      shader = pipe->create_tes_state(pipe, state);
-      break;
-   case MESA_SHADER_GEOMETRY:
-      shader = pipe->create_gs_state(pipe, state);
-      break;
-   case MESA_SHADER_FRAGMENT:
-      shader = pipe->create_fs_state(pipe, state);
-      break;
-   default:
-      unreachable("unsupported shader stage");
-      return NULL;
-   }
-
-   if (state->type == PIPE_SHADER_IR_TGSI)
-      tgsi_free_tokens(state->tokens);
-
-   return shader;
-}
-
 /**
  * Translate a vertex program.
  */
@@ -793,10 +726,27 @@ st_create_vp_variant(struct st_context *st,
                                 nir_shader_get_entrypoint(state.ir.nir));
       }
 
+      if (ST_DEBUG & DEBUG_PRINT_IR)
+         nir_print_shader(state.ir.nir, stderr);
+
+      /* If the driver wants TGSI, then translate before handing off. */
+
+      if (st->pipe->screen->get_shader_param(st->pipe->screen,
+                                             PIPE_SHADER_VERTEX,
+                                             PIPE_SHADER_CAP_PREFERRED_IR) !=
+          PIPE_SHADER_IR_NIR) {
+         nir_shader *s = state.ir.nir;
+         state.tokens = nir_to_tgsi(s, st->pipe->screen);
+         state.type = PIPE_SHADER_IR_TGSI;
+      }
+
       if (key->is_draw_shader)
          vpv->base.driver_shader = draw_create_vertex_shader(st->draw, &state);
       else
-         vpv->base.driver_shader = st_create_nir_shader(st, &state);
+         vpv->base.driver_shader = pipe->create_vs_state(pipe, &state);
+
+      if (state.type == PIPE_SHADER_IR_TGSI)
+         tgsi_free_tokens(state.tokens);
 
       return vpv;
    }
@@ -1387,9 +1337,25 @@ st_create_fp_variant(struct st_context *st,
             screen->finalize_nir(screen, state.ir.nir, false);
       }
 
-      variant->base.driver_shader = st_create_nir_shader(st, &state);
+      if (ST_DEBUG & DEBUG_PRINT_IR)
+         nir_print_shader(state.ir.nir, stderr);
+
+      /* If the driver wants TGSI, then translate before handing off. */
+      if (st->pipe->screen->get_shader_param(st->pipe->screen,
+                                             PIPE_SHADER_FRAGMENT,
+                                             PIPE_SHADER_CAP_PREFERRED_IR) !=
+          PIPE_SHADER_IR_NIR) {
+         nir_shader *s = state.ir.nir;
+         state.tokens = nir_to_tgsi(s, st->pipe->screen);
+         state.type = PIPE_SHADER_IR_TGSI;
+      }
+
+      variant->base.driver_shader = pipe->create_fs_state(pipe, &state);
       variant->key = *key;
 
+      if (state.type == PIPE_SHADER_IR_TGSI)
+         tgsi_free_tokens(state.tokens);
+
       return variant;
    }
 
diff --git a/src/mesa/state_tracker/st_program.h b/src/mesa/state_tracker/st_program.h
index 157339e34a3..e1bdd1fbb79 100644
--- a/src/mesa/state_tracker/st_program.h
+++ b/src/mesa/state_tracker/st_program.h
@@ -357,9 +357,6 @@ st_serialize_nir(struct st_program *stp);
 extern void
 st_finalize_program(struct st_context *st, struct gl_program *prog);
 
-struct pipe_shader_state *
-st_create_nir_shader(struct st_context *st, struct pipe_shader_state *state);
-
 #ifdef __cplusplus
 }
 #endif
diff --git a/src/mesa/state_tracker/st_sampler_view.c b/src/mesa/state_tracker/st_sampler_view.c
index 1516d0deb6b..60488e1670f 100644
--- a/src/mesa/state_tracker/st_sampler_view.c
+++ b/src/mesa/state_tracker/st_sampler_view.c
@@ -559,6 +559,10 @@ st_create_texture_sampler_view_from_stobj(struct st_context *st,
    templ.swizzle_b = GET_SWZ(swizzle, 2);
    templ.swizzle_a = GET_SWZ(swizzle, 3);
 
+   /* TODO: this is broken for ARGB/ABGR -> ARGB/ABGR, but we don't have the dst format yet... */
+   if (st->screen->get_param(st->screen, PIPE_CAP_EMULATE_ARGB))
+      u_sampler_view_swizzle_argb(&templ, 0);
+
    return st->pipe->create_sampler_view(st->pipe, stObj->pt, &templ);
 }
 
diff --git a/src/mesa/swrast/s_span.c b/src/mesa/swrast/s_span.c
index d20cea5eaa3..d1933e32929 100644
--- a/src/mesa/swrast/s_span.c
+++ b/src/mesa/swrast/s_span.c
@@ -1041,23 +1041,25 @@ put_values(struct gl_context *ctx, struct gl_renderbuffer *rb,
            GLuint count, const GLint x[], const GLint y[],
            const void *values, const GLubyte *mask)
 {
-   struct swrast_renderbuffer *srb = swrast_renderbuffer(rb);
+   mesa_pack_ubyte_rgba_func pack_ubyte = NULL;
+   mesa_pack_float_rgba_func pack_float = NULL;
    GLuint i;
 
+   if (datatype == GL_UNSIGNED_BYTE)
+      pack_ubyte = _mesa_get_pack_ubyte_rgba_function(rb->Format);
+   else
+      pack_float = _mesa_get_pack_float_rgba_function(rb->Format);
+
    for (i = 0; i < count; i++) {
       if (mask[i]) {
+         GLubyte *dst = _swrast_pixel_address(rb, x[i], y[i]);
+
          if (datatype == GL_UNSIGNED_BYTE) {
-            util_format_write_4ub(rb->Format,
-                                  (uint8_t *)values + 4 * i, 0,
-                                  srb->Map, srb->RowStride,
-                                  x[i], y[i], 1, 1);
+            pack_ubyte((const GLubyte *) values + 4 * i, dst);
          }
          else {
             assert(datatype == GL_FLOAT);
-            util_format_write_4(rb->Format,
-                                (float *)values + 4 * i, 0,
-                                srb->Map, srb->RowStride,
-                                x[i], y[i], 1, 1);
+            pack_float((const GLfloat *) values + 4 * i, dst);
          }
       }
    }
diff --git a/src/mesa/tnl/t_draw.c b/src/mesa/tnl/t_draw.c
index 1370af691c2..c28378cb148 100644
--- a/src/mesa/tnl/t_draw.c
+++ b/src/mesa/tnl/t_draw.c
@@ -341,7 +341,6 @@ static void bind_inputs(struct gl_context *ctx,
 /* Translate indices to GLuints and store in VB->Elts.
  */
 static void bind_indices(struct gl_context *ctx,
-                         unsigned start,
                          const struct _mesa_index_buffer *ib,
                          struct gl_buffer_object **bo,
                          GLuint *nr_bo)
@@ -377,23 +376,21 @@ static void bind_indices(struct gl_context *ctx,
       VB->Elts = (GLuint *) ptr;
    }
    else {
-      GLuint *elts = (GLuint *)get_space(ctx, (start + ib->count) * sizeof(GLuint));
+      GLuint *elts = (GLuint *)get_space(ctx, ib->count * sizeof(GLuint));
       VB->Elts = elts;
 
-      elts += start;
-
       if (ib->index_size_shift == 2) {
-         const GLuint *in = (GLuint *)ptr + start;
+         const GLuint *in = (GLuint *)ptr;
          for (i = 0; i < ib->count; i++)
             *elts++ = (GLuint)(*in++) + VB->Primitive[0].basevertex;
       }
       else if (ib->index_size_shift == 1) {
-         const GLushort *in = (GLushort *)ptr + start;
+         const GLushort *in = (GLushort *)ptr;
          for (i = 0; i < ib->count; i++)
             *elts++ = (GLuint)(*in++) + VB->Primitive[0].basevertex;
       }
       else {
-         const GLubyte *in = (GLubyte *)ptr + start;
+         const GLubyte *in = (GLubyte *)ptr;
          for (i = 0; i < ib->count; i++)
             *elts++ = (GLuint)(*in++) + VB->Primitive[0].basevertex;
       }
@@ -494,7 +491,7 @@ void _tnl_draw_prims(struct gl_context *ctx,
        * one for the index buffer.
        */
       struct gl_buffer_object *bo[VERT_ATTRIB_MAX + 1];
-      GLuint nr_bo;
+      GLuint nr_bo = 0;
       GLuint inst;
 
       assert(num_instances > 0);
@@ -508,8 +505,7 @@ void _tnl_draw_prims(struct gl_context *ctx,
           */
          for (this_nr_prims = 1; i + this_nr_prims < nr_prims;
               this_nr_prims++) {
-            if (prim[i].basevertex != prim[i + this_nr_prims].basevertex ||
-                prim[i].start != prim[i + this_nr_prims].start)
+            if (prim[i].basevertex != prim[i + this_nr_prims].basevertex)
                break;
          }
 
@@ -517,12 +513,11 @@ void _tnl_draw_prims(struct gl_context *ctx,
           * They will need to be unmapped below.
           */
          for (inst = 0; inst < num_instances; inst++) {
-            nr_bo = 0;
 
             bind_prims(ctx, &prim[i], this_nr_prims);
             bind_inputs(ctx, arrays, max_index + prim[i].basevertex + 1,
                         bo, &nr_bo);
-            bind_indices(ctx, prim[i].start, ib, bo, &nr_bo);
+            bind_indices(ctx, ib, bo, &nr_bo);
 
             tnl->CurInstance = inst;
             TNL_CONTEXT(ctx)->Driver.RunPipeline(ctx);
diff --git a/src/mesa/tnl/t_rebase.c b/src/mesa/tnl/t_rebase.c
index 0471895641e..6f600e39068 100644
--- a/src/mesa/tnl/t_rebase.c
+++ b/src/mesa/tnl/t_rebase.c
@@ -59,12 +59,11 @@
 
 #define REBASE(TYPE) 						\
 static void *rebase_##TYPE(const void *ptr,			\
-                           unsigned start, 			\
                            unsigned count, 			\
                            TYPE min_index)			\
 {								\
    const TYPE *in = (TYPE *)ptr;				\
-   TYPE *tmp_indices = malloc((start + count) * sizeof(TYPE));	\
+   TYPE *tmp_indices = malloc(count * sizeof(TYPE));		\
                                                                 \
    if (tmp_indices == NULL) {                                   \
       _mesa_error_no_memory(__func__);                          \
@@ -72,7 +71,7 @@ static void *rebase_##TYPE(const void *ptr,			\
    }                                                            \
                                                                 \
    for (unsigned i = 0; i < count; i++)                         \
-      tmp_indices[start + i] = in[start + i] - min_index;	\
+      tmp_indices[i] = in[i] - min_index;			\
                                                                 \
    return (void *)tmp_indices;					\
 }
@@ -146,23 +145,6 @@ void t_rebase_prims(struct gl_context *ctx,
 
       prim = tmp_prims;
    } else if (ib) {
-      unsigned start = prim[0].start;
-      for (i = 1; i < nr_prims; i++) {
-         if (prim[i].start != start) {
-            if (0) {
-               printf("%s recursing due to mismatched start "
-                      "(prim[0].start = %u vs. prim[%u].start = %u)\n",
-                      __func__, start, i, prim[i].start);
-            }
-
-            t_rebase_prims(ctx, arrays, &prim[0], i, ib, min_index,
-                           max_index, num_instances, base_instance, draw);
-            t_rebase_prims(ctx, arrays, &prim[i], nr_prims - i, ib, min_index,
-                           max_index, num_instances, base_instance, draw);
-            return;
-         }
-      }
-
       /* Unfortunately need to adjust each index individually.
        */
       bool map_ib = false;
@@ -184,13 +166,13 @@ void t_rebase_prims(struct gl_context *ctx,
        */
       switch (ib->index_size_shift) {
       case 2:
-         tmp_indices = rebase_GLuint(ptr, start, ib->count, min_index);
+         tmp_indices = rebase_GLuint( ptr, ib->count, min_index );
          break;
       case 1:
-         tmp_indices = rebase_GLushort(ptr, start, ib->count, min_index);
+         tmp_indices = rebase_GLushort( ptr, ib->count, min_index );
          break;
       case 0:
-         tmp_indices = rebase_GLubyte(ptr, start, ib->count, min_index);
+         tmp_indices = rebase_GLubyte( ptr, ib->count, min_index );
          break;
       }
 
diff --git a/src/mesa/vbo/vbo_save_api.c b/src/mesa/vbo/vbo_save_api.c
index b5d7d670f91..b8edee7cf5b 100644
--- a/src/mesa/vbo/vbo_save_api.c
+++ b/src/mesa/vbo/vbo_save_api.c
@@ -670,6 +670,7 @@ compile_vertex_list(struct gl_context *ctx)
       int available = save->previous_ib ? (save->previous_ib->Size / 4 - save->ib_first_free_index) : 0;
       if (available >= max_indices_count) {
          indices_offset = save->ib_first_free_index;
+         node->min_index = node->max_index = indices_offset;
       }
       int size = max_indices_count * sizeof(uint32_t);
       uint32_t* indices = (uint32_t*) malloc(size);
@@ -688,7 +689,7 @@ compile_vertex_list(struct gl_context *ctx)
             continue;
          }
 
-         /* Line strips may get converted to lines */
+         /* Line strips get converted to lines */
          if (mode == GL_LINE_STRIP)
             mode = GL_LINES;
 
@@ -739,9 +740,6 @@ compile_vertex_list(struct gl_context *ctx)
                }
             }
          } else {
-            /* We didn't convert to LINES, so restore the original mode */
-            mode = original_prims[i].mode;
-
             for (unsigned j = 0; j < vertex_count; j++) {
                indices[idx++] = original_prims[i].start + j;
             }
diff --git a/src/panfrost/bifrost/bi_packer.c.py b/src/panfrost/bifrost/bi_packer.c.py
index 28669ebfa13..85af8619329 100644
--- a/src/panfrost/bifrost/bi_packer.c.py
+++ b/src/panfrost/bifrost/bi_packer.c.py
@@ -89,7 +89,7 @@ def pack_modifier(mod, width, default, opts, body, pack_exprs):
     # Swizzles need to be packed "specially"
     SWIZZLE_BUCKETS = [
             set(['h00', 'h0']),
-            set(['h01', 'none', 'b0123', 'w0']), # Identity
+            set(['h01', 'none', 'b0123']), # Identity
             set(['h10']),
             set(['h11', 'h1']),
             set(['b0000', 'b00', 'b0']),
diff --git a/src/panfrost/bifrost/bi_printer.c.py b/src/panfrost/bifrost/bi_printer.c.py
index 7b7005688b7..542e48c1f88 100644
--- a/src/panfrost/bifrost/bi_printer.c.py
+++ b/src/panfrost/bifrost/bi_printer.c.py
@@ -88,7 +88,7 @@ bi_print_index(FILE *fp, bi_index index)
     else if (index.type == BI_INDEX_NORMAL && index.reg)
         fprintf(fp, "r%u", index.value);
     else if (index.type == BI_INDEX_NORMAL)
-        fprintf(fp, "%u", index.value);
+        fprintf(fp, "%u", index.value - 1);
     else
         unreachable("Invalid index");
 
diff --git a/src/panfrost/bifrost/bi_ra.c b/src/panfrost/bifrost/bi_ra.c
index 572c39026de..c7c2003bfd3 100644
--- a/src/panfrost/bifrost/bi_ra.c
+++ b/src/panfrost/bifrost/bi_ra.c
@@ -48,7 +48,7 @@ bi_compute_interference(bi_context *ctx, struct lcra_state *l)
                                 if (bi_get_node(ins->dest[d]) >= node_count)
                                         continue;
 
-                                for (unsigned i = 0; i < node_count; ++i) {
+                                for (unsigned i = 1; i < node_count; ++i) {
                                         if (live[i])
                                                 lcra_add_node_interference(l, bi_get_node(ins->dest[d]), bi_writemask(ins), i, live[i]);
                                 }
@@ -57,7 +57,7 @@ bi_compute_interference(bi_context *ctx, struct lcra_state *l)
                         if (!ctx->is_blend && ins->op == BI_OPCODE_BLEND) {
                                 /* Add blend shader interference: blend shaders might
                                  * clobber r0-r15. */
-                                for (unsigned i = 0; i < node_count; ++i) {
+                                for (unsigned i = 1; i < node_count; ++i) {
                                         if (!live[i])
                                                 continue;
 
@@ -116,7 +116,7 @@ bi_allocate_registers(bi_context *ctx, bool *success)
                         l->solutions[node] = 0;
                 }
 
-                if (dest >= node_count)
+                if (!dest || (dest >= node_count))
                         continue;
 
                 l->class[dest] = BI_REG_CLASS_WORK;
@@ -311,7 +311,7 @@ bi_register_allocate(bi_context *ctx)
         unsigned iter_count = 1000; /* max iterations */
 
         /* Number of bytes of memory we've spilled into */
-        unsigned spill_count = ctx->tls_size;
+        unsigned spill_count = 0;
 
         do {
                 if (l) {
diff --git a/src/panfrost/bifrost/bifrost_compile.c b/src/panfrost/bifrost/bifrost_compile.c
index f4e15dcf07b..2e9c42a3efb 100644
--- a/src/panfrost/bifrost/bifrost_compile.c
+++ b/src/panfrost/bifrost/bifrost_compile.c
@@ -488,23 +488,18 @@ bi_emit_load_ubo(bi_builder *b, nir_intrinsic_instr *instr)
          */
         assert(b->shader->nir->info.first_ubo_is_default_ubo);
 
-        nir_src *offset = nir_get_io_offset_src(instr);
-
-        bool offset_is_const = nir_src_is_const(*offset);
-        bi_index dyn_offset = bi_src_index(offset);
+        bool offset_is_const = nir_src_is_const(instr->src[1]);
+        bi_index dyn_offset = bi_src_index(&instr->src[1]);
         uint32_t const_offset = 0;
 
-        bool kernel_input = (instr->intrinsic == nir_intrinsic_load_kernel_input);
-
         /* We may need to offset UBO loads by however many sysvals we have */
         unsigned sysval_offset = 16 * b->shader->sysvals.sysval_count;
 
-        if (nir_src_is_const(*offset))
-                const_offset = nir_src_as_uint(*offset);
+        if (nir_src_is_const(instr->src[1]))
+                const_offset = nir_src_as_uint(instr->src[1]);
 
-        if ((kernel_input ||
-             (nir_src_is_const(instr->src[0]) &&
-              nir_src_as_uint(instr->src[0]) == 0)) &&
+        if (nir_src_is_const(instr->src[0]) &&
+            nir_src_as_uint(instr->src[0]) == 0 &&
             b->shader->sysvals.sysval_count) {
                 if (offset_is_const) {
                         const_offset += sysval_offset;
@@ -517,36 +512,10 @@ bi_emit_load_ubo(bi_builder *b, nir_intrinsic_instr *instr)
         bi_load_to(b, instr->num_components * 32,
                         bi_dest_index(&instr->dest), offset_is_const ?
                         bi_imm_u32(const_offset) : dyn_offset,
-                        kernel_input ? bi_zero() : bi_src_index(&instr->src[0]),
+                        bi_src_index(&instr->src[0]),
                         BI_SEG_UBO);
 }
 
-static bi_index
-bi_addr_high(nir_src *src)
-{
-	return (nir_src_bit_size(*src) == 64) ?
-		bi_word(bi_src_index(src), 1) : bi_zero();
-}
-
-static void
-bi_emit_load(bi_builder *b, nir_intrinsic_instr *instr, enum bi_seg seg)
-{
-        bi_load_to(b, instr->num_components * nir_dest_bit_size(instr->dest),
-                   bi_dest_index(&instr->dest),
-                   bi_src_index(&instr->src[0]), bi_addr_high(&instr->src[0]),
-                   seg);
-}
-
-static void
-bi_emit_store(bi_builder *b, nir_intrinsic_instr *instr, enum bi_seg seg)
-{
-        bi_store_to(b, instr->num_components * nir_src_bit_size(instr->src[0]),
-                    bi_null(),
-                    bi_src_index(&instr->src[0]),
-                    bi_src_index(&instr->src[1]), bi_addr_high(&instr->src[1]),
-                    seg);
-}
-
 static void
 bi_load_sysval(bi_builder *b, nir_instr *instr,
                 unsigned nr_components, unsigned offset)
@@ -633,35 +602,9 @@ bi_emit_intrinsic(bi_builder *b, nir_intrinsic_instr *instr)
                 break;
 
         case nir_intrinsic_load_ubo:
-        case nir_intrinsic_load_kernel_input:
                 bi_emit_load_ubo(b, instr);
                 break;
 
-        case nir_intrinsic_load_global:
-        case nir_intrinsic_load_global_constant:
-                bi_emit_load(b, instr, BI_SEG_NONE);
-                break;
-
-        case nir_intrinsic_store_global:
-                bi_emit_store(b, instr, BI_SEG_NONE);
-                break;
-
-        case nir_intrinsic_load_scratch:
-                bi_emit_load(b, instr, BI_SEG_TL);
-                break;
-
-        case nir_intrinsic_store_scratch:
-                bi_emit_store(b, instr, BI_SEG_TL);
-                break;
-
-        case nir_intrinsic_load_shared:
-                bi_emit_load(b, instr, BI_SEG_WLS);
-                break;
-
-        case nir_intrinsic_store_shared:
-                bi_emit_store(b, instr, BI_SEG_WLS);
-                break;
-
         case nir_intrinsic_load_frag_coord:
                 bi_emit_load_frag_coord(b, instr);
                 break;
@@ -689,10 +632,6 @@ bi_emit_intrinsic(bi_builder *b, nir_intrinsic_instr *instr)
                 break;
 
         case nir_intrinsic_load_ssbo_address:
-                bi_load_sysval(b, &instr->instr, 2, 0);
-                break;
-
-        case nir_intrinsic_load_work_dim:
                 bi_load_sysval(b, &instr->instr, 1, 0);
                 break;
 
@@ -704,7 +643,6 @@ bi_emit_intrinsic(bi_builder *b, nir_intrinsic_instr *instr)
         case nir_intrinsic_load_viewport_offset:
         case nir_intrinsic_load_num_work_groups:
         case nir_intrinsic_load_sampler_lod_parameters_pan:
-        case nir_intrinsic_load_local_group_size:
                 bi_load_sysval(b, &instr->instr, 3, 0);
                 break;
         case nir_intrinsic_load_blend_const_color_r_float:
@@ -755,26 +693,8 @@ bi_emit_intrinsic(bi_builder *b, nir_intrinsic_instr *instr)
                 bi_mov_i32_to(b, dst, bi_register(62));
                 break;
 
-        case nir_intrinsic_load_local_invocation_id:
-                for (unsigned i = 0; i < 3; ++i)
-                        bi_u16_to_u32_to(b, bi_word(dst, i),
-                                         bi_half(bi_register(55 + i / 2), i % 2));
-                break;
-
-        case nir_intrinsic_load_work_group_id:
-                for (unsigned i = 0; i < 3; ++i)
-                        bi_mov_i32_to(b, bi_word(dst, i), bi_register(57 + i));
-                break;
-
-        case nir_intrinsic_load_global_invocation_id:
-        case nir_intrinsic_load_global_invocation_id_zero_base:
-                for (unsigned i = 0; i < 3; ++i)
-                        bi_mov_i32_to(b, bi_word(dst, i), bi_register(60 + i));
-                break;
-
         default:
-                fprintf(stderr, "Unhandled intrinsic %s\n", nir_intrinsic_infos[instr->intrinsic].name);
-                assert(0);
+                unreachable("Unknown intrinsic");
         }
 }
 
@@ -903,7 +823,6 @@ bi_emit_alu(bi_builder *b, nir_alu_instr *instr)
          * are the exceptions that need to handle swizzles specially. */
 
         switch (instr->op) {
-        case nir_op_pack_32_2x16:
         case nir_op_vec2:
         case nir_op_vec3:
         case nir_op_vec4: {
@@ -929,25 +848,6 @@ bi_emit_alu(bi_builder *b, nir_alu_instr *instr)
         case nir_op_vec16:
                 unreachable("should've been lowered");
 
-        case nir_op_unpack_32_2x16:
-        case nir_op_unpack_64_2x32_split_x:
-                bi_mov_i32_to(b, dst, bi_src_index(&instr->src[0].src));
-                return;
-
-        case nir_op_unpack_64_2x32_split_y:
-                bi_mov_i32_to(b, dst, bi_word(bi_src_index(&instr->src[0].src), 1));
-                return;
-
-        case nir_op_pack_64_2x32_split:
-                bi_mov_i32_to(b, bi_word(dst, 0), bi_src_index(&instr->src[0].src));
-                bi_mov_i32_to(b, bi_word(dst, 1), bi_src_index(&instr->src[1].src));
-                return;
-
-        case nir_op_pack_64_2x32:
-                bi_mov_i32_to(b, bi_word(dst, 0), bi_word(bi_src_index(&instr->src[0].src), 0));
-                bi_mov_i32_to(b, bi_word(dst, 1), bi_word(bi_src_index(&instr->src[0].src), 1));
-                return;
-
         case nir_op_mov: {
                 bi_index idx = bi_src_index(&instr->src[0].src);
                 bi_index unoffset_srcs[4] = { idx, idx, idx, idx };
@@ -1261,26 +1161,10 @@ bi_emit_alu(bi_builder *b, nir_alu_instr *instr)
                 bi_iadd_to(b, sz, dst, s0, s1, false);
                 break;
 
-        case nir_op_iadd_sat:
-                bi_iadd_to(b, sz, dst, s0, s1, true);
-                break;
-
-        case nir_op_ihadd:
-                bi_hadd_to(b, sz, dst, s0, s1, BI_ROUND_RTN);
-                break;
-
-        case nir_op_irhadd:
-                bi_hadd_to(b, sz, dst, s0, s1, BI_ROUND_RTP);
-                break;
-
         case nir_op_isub:
                 bi_isub_to(b, sz, dst, s0, s1, false);
                 break;
 
-        case nir_op_isub_sat:
-                bi_isub_to(b, sz, dst, s0, s1, true);
-                break;
-
         case nir_op_imul:
                 bi_imul_to(b, sz, dst, s0, s1);
                 break;
@@ -2160,10 +2044,6 @@ bi_optimize_nir(nir_shader *nir)
                 .lower_txd = true,
         };
 
-        NIR_PASS(progress, nir, pan_nir_lower_64bit_intrin);
-
-        NIR_PASS(progress, nir, nir_lower_int64);
-
         NIR_PASS(progress, nir, nir_lower_tex, &lower_tex_options);
         NIR_PASS(progress, nir, nir_lower_alu_to_scalar, NULL, NULL);
         NIR_PASS(progress, nir, nir_lower_load_const_to_scalar);
@@ -2183,8 +2063,6 @@ bi_optimize_nir(nir_shader *nir)
                 NIR_PASS(progress, nir, nir_opt_algebraic);
                 NIR_PASS(progress, nir, nir_opt_constant_folding);
 
-                NIR_PASS(progress, nir, nir_lower_alu);
-
                 if (lower_flrp != 0) {
                         bool lower_flrp_progress = false;
                         NIR_PASS(lower_flrp_progress,
@@ -2257,7 +2135,7 @@ bifrost_nir_lower_i8_fragout_impl(struct nir_builder *b,
         nir_alu_type type =
                 nir_alu_type_get_base_type(nir_intrinsic_src_type(intr));
 
-        assert(type == nir_type_int || type == nir_type_uint);
+        assert(type == nir_type_int || nir_type_uint);
 
         b->cursor = nir_before_instr(instr);
         nir_ssa_def *cast = type == nir_type_int ?
@@ -2335,7 +2213,6 @@ bifrost_compile_shader_nir(void *mem_ctx, nir_shader *nir,
         program->sysval_count = ctx->sysvals.sysval_count;
         memcpy(program->sysvals, ctx->sysvals.sysvals, sizeof(ctx->sysvals.sysvals[0]) * ctx->sysvals.sysval_count);
         ctx->blend_types = program->blend_types;
-        ctx->tls_size = nir->scratch_size;
 
         nir_foreach_function(func, nir) {
                 if (!func->impl)
diff --git a/src/panfrost/bifrost/bifrost_compile.h b/src/panfrost/bifrost/bifrost_compile.h
index e424a23a91e..2c6d6217d93 100644
--- a/src/panfrost/bifrost/bifrost_compile.h
+++ b/src/panfrost/bifrost/bifrost_compile.h
@@ -67,11 +67,6 @@ static const nir_shader_compiler_options bifrost_nir_options = {
         .lower_pack_split = true,
 
         .lower_doubles_options = nir_lower_dmod,
-        /* TODO: Don't lower supported 64-bit operations */
-        .lower_int64_options = ~0,
-        /* TODO: Use IMULD on v7 */
-        .lower_mul_high = true,
-        .lower_uadd_carry = true,
 
         .lower_bitfield_extract_to_shifts = true,
         .has_fsub = true,
@@ -83,9 +78,6 @@ static const nir_shader_compiler_options bifrost_nir_options = {
         .use_interpolated_input_intrinsics = true,
 
         .lower_uniforms_to_ubo = true,
-
-        .has_cs_global_id = true,
-        .lower_cs_local_index_from_id = true,
 };
 
 #endif
diff --git a/src/panfrost/bifrost/cmdline.c b/src/panfrost/bifrost/cmdline.c
index c0223745983..9eb749d1d4a 100644
--- a/src/panfrost/bifrost/cmdline.c
+++ b/src/panfrost/bifrost/cmdline.c
@@ -79,10 +79,6 @@ compile_shader(char **argv, bool vertex_only)
         return compiled;
 }
 
-#define BI_FOURCC(ch0, ch1, ch2, ch3) ( \
-  (uint32_t)(ch0)        | (uint32_t)(ch1) << 8 | \
-  (uint32_t)(ch2) << 16  | (uint32_t)(ch3) << 24)
-
 static void
 disassemble(const char *filename, bool verbose)
 {
@@ -93,27 +89,14 @@ disassemble(const char *filename, bool verbose)
         unsigned filesize = ftell(fp);
         rewind(fp);
 
-        uint32_t *code = malloc(filesize);
+        unsigned char *code = malloc(filesize);
         unsigned res = fread(code, 1, filesize, fp);
         if (res != filesize) {
                 printf("Couldn't read full file\n");
         }
         fclose(fp);
 
-        if (filesize && code[0] == BI_FOURCC('M', 'B', 'S', '2')) {
-                for (int i = 0; i < filesize / 4; ++i) {
-                        if (code[i] != BI_FOURCC('O', 'B', 'J', 'C'))
-                                continue;
-
-                        unsigned size = code[i + 1];
-                        unsigned offset = i + 2;
-
-                        disassemble_bifrost(stdout, (uint8_t*)(code + offset), size, verbose);
-                }
-        } else {
-                disassemble_bifrost(stdout, (uint8_t*)code, filesize, verbose);
-        }
-
+        disassemble_bifrost(stdout, code, filesize, verbose);
         free(code);
 }
 
diff --git a/src/panfrost/lib/decode_common.c b/src/panfrost/lib/decode_common.c
index 72fba5a7ff3..53f2a50de7d 100644
--- a/src/panfrost/lib/decode_common.c
+++ b/src/panfrost/lib/decode_common.c
@@ -117,24 +117,6 @@ pandecode_inject_mmap(uint64_t gpu_va, void *cpu, unsigned sz, const char *name)
                 _mesa_hash_table_u64_insert(mmap_table, gpu_va + i, mapped_mem);
 }
 
-void
-pandecode_inject_free(uint64_t gpu_va, unsigned sz)
-{
-        struct pandecode_mapped_memory *mem =
-                pandecode_find_mapped_gpu_mem_containing_rw(gpu_va);
-
-        if (!mem)
-                return;
-
-        assert(mem->gpu_va == gpu_va);
-        assert(mem->length == sz);
-
-        free(mem);
-
-        for (unsigned i = 0; i < sz; i += 4096)
-                _mesa_hash_table_u64_remove(mmap_table, gpu_va + i);
-}
-
 char *
 pointer_as_memory_reference(uint64_t ptr)
 {
diff --git a/src/panfrost/lib/pan_bo.c b/src/panfrost/lib/pan_bo.c
index 0852ac63c7c..f64a787e828 100644
--- a/src/panfrost/lib/pan_bo.c
+++ b/src/panfrost/lib/pan_bo.c
@@ -436,9 +436,6 @@ panfrost_bo_unreference(struct panfrost_bo *bo)
                 /* When the reference count goes to zero, we need to cleanup */
                 panfrost_bo_munmap(bo);
 
-                if (dev->debug & (PAN_DBG_TRACE | PAN_DBG_SYNC))
-                        pandecode_inject_free(bo->ptr.gpu, bo->size);
-
                 /* Rather than freeing the BO now, we'll cache the BO for later
                  * allocations if we're allowed to.
                  */
diff --git a/src/panfrost/lib/pan_texture.h b/src/panfrost/lib/pan_texture.h
index ba7d26f93fc..1b2f40646ed 100644
--- a/src/panfrost/lib/pan_texture.h
+++ b/src/panfrost/lib/pan_texture.h
@@ -72,9 +72,6 @@ struct panfrost_slice {
 
         /* Has anything been written to this slice? */
         bool initialized;
-
-        /* Is the checksum for this slice valid? */
-        bool checksum_valid;
 };
 
 struct pan_image_layout {
diff --git a/src/panfrost/lib/pan_util.h b/src/panfrost/lib/pan_util.h
index 72a815897b7..f893cf62c57 100644
--- a/src/panfrost/lib/pan_util.h
+++ b/src/panfrost/lib/pan_util.h
@@ -35,7 +35,7 @@
 #define PAN_DBG_SYNC            0x0010
 #define PAN_DBG_PRECOMPILE      0x0020
 #define PAN_DBG_NOFP16          0x0040
-#define PAN_DBG_NO_CRC          0x0080
+/* 0x80 unused */
 #define PAN_DBG_GL3             0x0100
 #define PAN_DBG_NO_AFBC         0x0200
 #define PAN_DBG_FP16            0x0400
diff --git a/src/panfrost/lib/wrap.h b/src/panfrost/lib/wrap.h
index 6167c1e9470..abb18c8750b 100644
--- a/src/panfrost/lib/wrap.h
+++ b/src/panfrost/lib/wrap.h
@@ -51,8 +51,6 @@ void pandecode_close(void);
 void
 pandecode_inject_mmap(uint64_t gpu_va, void *cpu, unsigned sz, const char *name);
 
-void pandecode_inject_free(uint64_t gpu_va, unsigned sz);
-
 void pandecode_jc(uint64_t jc_gpu_va, bool bifrost, unsigned gpu_id, bool minimal);
 
 #endif /* __MMAP_TRACE_H__ */
diff --git a/src/panfrost/midgard/midgard_compile.c b/src/panfrost/midgard/midgard_compile.c
index 19b5ccdec2f..253cdedc53f 100644
--- a/src/panfrost/midgard/midgard_compile.c
+++ b/src/panfrost/midgard/midgard_compile.c
@@ -229,29 +229,6 @@ mdg_is_64(const nir_instr *instr, const void *_unused)
         }
 }
 
-/* Only vectorize int64 up to vec2 */
-static bool
-midgard_vectorize_filter(const nir_instr *instr, void *data)
-{
-        if (instr->type != nir_instr_type_alu)
-                return true;
-
-        const nir_alu_instr *alu = nir_instr_as_alu(instr);
-
-        unsigned num_components = alu->dest.dest.ssa.num_components;
-
-        int src_bit_size = nir_src_bit_size(alu->src[0].src);
-        int dst_bit_size = nir_dest_bit_size(alu->dest.dest);
-
-        if (src_bit_size == 64 || dst_bit_size == 64) {
-                if (num_components > 1)
-                        return false;
-        }
-
-        return true;
-}
-
-
 /* Flushes undefined values to zero */
 
 static void
@@ -331,8 +308,7 @@ optimise_nir(nir_shader *nir, unsigned quirks, bool is_blend)
                          nir_var_shader_out |
                          nir_var_function_temp);
 
-                NIR_PASS(progress, nir, nir_opt_vectorize,
-                         midgard_vectorize_filter, NULL);
+                NIR_PASS(progress, nir, nir_opt_vectorize, NULL, NULL);
         } while (progress);
 
         NIR_PASS_V(nir, nir_lower_alu_to_scalar, mdg_is_64, NULL);
@@ -463,7 +439,7 @@ nir_is_non_scalar_swizzle(nir_alu_src *src, unsigned nr_components)
 		break;
 
 #define ALU_CHECK_CMP() \
-                assert(src_bitsize == 16 || src_bitsize == 32 || src_bitsize == 64); \
+                assert(src_bitsize == 16 || src_bitsize == 32); \
                 assert(dst_bitsize == 16 || dst_bitsize == 32); \
 
 #define ALU_CASE_BCAST(nir, _op, count) \
@@ -1822,7 +1798,6 @@ emit_intrinsic(compiler_context *ctx, nir_intrinsic_instr *instr)
                 break;
 
         case nir_intrinsic_load_ssbo_address:
-        case nir_intrinsic_load_work_dim:
                 emit_sysval_read(ctx, &instr->instr, 1, 0);
                 break;
 
@@ -1834,7 +1809,6 @@ emit_intrinsic(compiler_context *ctx, nir_intrinsic_instr *instr)
         case nir_intrinsic_load_viewport_offset:
         case nir_intrinsic_load_num_work_groups:
         case nir_intrinsic_load_sampler_lod_parameters_pan:
-        case nir_intrinsic_load_local_group_size:
                 emit_sysval_read(ctx, &instr->instr, 3, 0);
                 break;
 
diff --git a/src/panfrost/midgard/midgard_ra.c b/src/panfrost/midgard/midgard_ra.c
index 72c06ade683..04be27da50e 100644
--- a/src/panfrost/midgard/midgard_ra.c
+++ b/src/panfrost/midgard/midgard_ra.c
@@ -1011,7 +1011,7 @@ mir_ra(compiler_context *ctx)
         int iter_count = 1000; /* max iterations */
 
         /* Number of 128-bit slots in memory we've spilled into */
-        unsigned spill_count = DIV_ROUND_UP(ctx->tls_size, 16);
+        unsigned spill_count = 0;
 
 
         mir_create_pipeline_registers(ctx);
@@ -1054,7 +1054,7 @@ mir_ra(compiler_context *ctx)
         /* Report spilling information. spill_count is in 128-bit slots (vec4 x
          * fp32), but tls_size is in bytes, so multiply by 16 */
 
-        ctx->tls_size = spill_count * 16;
+        ctx->tls_size += spill_count * 16;
 
         install_registers(ctx, l);
 
diff --git a/src/panfrost/midgard/midgard_schedule.c b/src/panfrost/midgard/midgard_schedule.c
index 7bd8229fe33..9b0f276f30b 100644
--- a/src/panfrost/midgard/midgard_schedule.c
+++ b/src/panfrost/midgard/midgard_schedule.c
@@ -116,8 +116,6 @@ mir_create_dependency_graph(midgard_instruction **instructions, unsigned count,
                 instructions[i]->nr_dependencies = 0;
         }
 
-        unsigned prev_ldst[3] = {~0, ~0, ~0};
-
         /* Populate dependency graph */
         for (signed i = count - 1; i >= 0; --i) {
                 if (instructions[i]->compact_branch)
@@ -135,34 +133,6 @@ mir_create_dependency_graph(midgard_instruction **instructions, unsigned count,
                         }
                 }
 
-                /* Create a list of dependencies for each type of load/store
-                 * instruction to prevent reordering. */
-                if (instructions[i]->type == TAG_LOAD_STORE_4 &&
-                    load_store_opcode_props[instructions[i]->op].props & LDST_ADDRESS) {
-
-                        unsigned type;
-                        switch (instructions[i]->load_store.arg_1 & 0x3E) {
-                        case LDST_SHARED: type = 0; break;
-                        case LDST_SCRATCH: type = 1; break;
-                        default: type = 2; break;
-                        }
-
-                        unsigned prev = prev_ldst[type];
-
-                        if (prev != ~0) {
-                                BITSET_WORD *dependents = instructions[prev]->dependents;
-
-                                /* Already have the dependency */
-                                if (BITSET_TEST(dependents, i))
-                                        continue;
-
-                                BITSET_SET(dependents, i);
-                                instructions[i]->nr_dependencies++;
-                        }
-
-                        prev_ldst[type] = i;
-                }
-
                 if (dest < node_count) {
                         add_dependency(last_read, dest, mask, instructions, i);
                         add_dependency(last_write, dest, mask, instructions, i);
diff --git a/src/panfrost/util/pan_ir.h b/src/panfrost/util/pan_ir.h
index 4c90587f35d..6de184eec1f 100644
--- a/src/panfrost/util/pan_ir.h
+++ b/src/panfrost/util/pan_ir.h
@@ -50,8 +50,6 @@ enum {
         PAN_SYSVAL_SSBO = 4,
         PAN_SYSVAL_NUM_WORK_GROUPS = 5,
         PAN_SYSVAL_SAMPLER = 7,
-        PAN_SYSVAL_LOCAL_GROUP_SIZE = 8,
-        PAN_SYSVAL_WORK_DIM = 9,
 };
 
 #define PAN_TXS_SYSVAL_ID(texidx, dim, is_array)          \
diff --git a/src/panfrost/util/pan_sysval.c b/src/panfrost/util/pan_sysval.c
index 80e4469f754..cbc957e50a2 100644
--- a/src/panfrost/util/pan_sysval.c
+++ b/src/panfrost/util/pan_sysval.c
@@ -58,10 +58,6 @@ panfrost_nir_sysval_for_intrinsic(nir_intrinsic_instr *instr)
                 return PAN_SYSVAL_VIEWPORT_OFFSET;
         case nir_intrinsic_load_num_work_groups:
                 return PAN_SYSVAL_NUM_WORK_GROUPS;
-        case nir_intrinsic_load_local_group_size:
-                return PAN_SYSVAL_LOCAL_GROUP_SIZE;
-        case nir_intrinsic_load_work_dim:
-                return PAN_SYSVAL_WORK_DIM;
         case nir_intrinsic_load_ssbo_address: 
         case nir_intrinsic_get_ssbo_size: 
                 return panfrost_sysval_for_ssbo(instr);
@@ -123,7 +119,6 @@ panfrost_nir_assign_sysval_body(struct panfrost_sysvals *ctx, nir_instr *instr)
         /* It hasn't -- so assign it now! */
 
         unsigned id = ctx->sysval_count++;
-        assert(id < MAX_SYSVAL_COUNT);
         _mesa_hash_table_u64_insert(ctx->sysval_to_id, sysval, (void *) ((uintptr_t) id + 1));
         ctx->sysvals[id] = sysval;
 }
diff --git a/src/util/format/format_utils.h b/src/util/format/format_utils.h
deleted file mode 100644
index fa1d30060d9..00000000000
--- a/src/util/format/format_utils.h
+++ /dev/null
@@ -1,212 +0,0 @@
-/**
- * \file format_utils.h
- * A collection of format conversion utility functions.
- */
-
-/*
- * Mesa 3-D graphics library
- *
- * Copyright (C) 1999-2006  Brian Paul  All Rights Reserved.
- * Copyright (C) 2014  Intel Corporation  All Rights Reserved.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice shall be included
- * in all copies or substantial portions of the Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
- * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR
- * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
- * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
- * OTHER DEALINGS IN THE SOFTWARE.
- */
-
-#ifndef UTIL_FORMAT_UTILS_H
-#define UTIL_FORMAT_UTILS_H
-
-#include "util/half_float.h"
-#include "util/rounding.h"
-
-/* Only guaranteed to work for BITS <= 32 */
-#define MAX_UINT(BITS) ((BITS) == 32 ? UINT32_MAX : ((1u << (BITS)) - 1))
-#define MAX_INT(BITS) ((int)MAX_UINT((BITS) - 1))
-#define MIN_INT(BITS) ((BITS) == 32 ? INT32_MIN : (-(1 << (BITS - 1))))
-
-/* Extends an integer of size SRC_BITS to one of size DST_BITS linearly */
-#define EXTEND_NORMALIZED_INT(X, SRC_BITS, DST_BITS) \
-      (((X) * (int)(MAX_UINT(DST_BITS) / MAX_UINT(SRC_BITS))) + \
-       ((DST_BITS % SRC_BITS) ? ((X) >> (SRC_BITS - DST_BITS % SRC_BITS)) : 0))
-
-static inline float
-_mesa_unorm_to_float(unsigned x, unsigned src_bits)
-{
-   return x * (1.0f / (float)MAX_UINT(src_bits));
-}
-
-static inline float
-_mesa_snorm_to_float(int x, unsigned src_bits)
-{
-   if (x <= -MAX_INT(src_bits))
-      return -1.0f;
-   else
-      return x * (1.0f / (float)MAX_INT(src_bits));
-}
-
-static inline uint16_t
-_mesa_unorm_to_half(unsigned x, unsigned src_bits)
-{
-   return _mesa_float_to_half(_mesa_unorm_to_float(x, src_bits));
-}
-
-static inline uint16_t
-_mesa_snorm_to_half(int x, unsigned src_bits)
-{
-   return _mesa_float_to_half(_mesa_snorm_to_float(x, src_bits));
-}
-
-static inline unsigned
-_mesa_float_to_unorm(float x, unsigned dst_bits)
-{
-   if (x < 0.0f)
-      return 0;
-   else if (x > 1.0f)
-      return MAX_UINT(dst_bits);
-   else
-      return _mesa_i64roundevenf(x * MAX_UINT(dst_bits));
-}
-
-static inline unsigned
-_mesa_half_to_unorm(uint16_t x, unsigned dst_bits)
-{
-   return _mesa_float_to_unorm(_mesa_half_to_float(x), dst_bits);
-}
-
-static inline unsigned
-_mesa_unorm_to_unorm(unsigned x, unsigned src_bits, unsigned dst_bits)
-{
-   if (src_bits < dst_bits) {
-      return EXTEND_NORMALIZED_INT(x, src_bits, dst_bits);
-   } else if (src_bits > dst_bits) {
-      unsigned src_half = (1u << (src_bits - 1)) - 1;
-
-      if (src_bits + dst_bits > sizeof(x) * 8) {
-         assert(src_bits + dst_bits <= sizeof(uint64_t) * 8);
-         return (((uint64_t) x * MAX_UINT(dst_bits) + src_half) /
-                 MAX_UINT(src_bits));
-      } else {
-         return (x * MAX_UINT(dst_bits) + src_half) / MAX_UINT(src_bits);
-      }
-   } else {
-      return x;
-   }
-}
-
-static inline unsigned
-_mesa_snorm_to_unorm(int x, unsigned src_bits, unsigned dst_bits)
-{
-   if (x < 0)
-      return 0;
-   else
-      return _mesa_unorm_to_unorm(x, src_bits - 1, dst_bits);
-}
-
-static inline int
-_mesa_float_to_snorm(float x, unsigned dst_bits)
-{
-   if (x < -1.0f)
-      return -MAX_INT(dst_bits);
-   else if (x > 1.0f)
-      return MAX_INT(dst_bits);
-   else
-      return _mesa_lroundevenf(x * MAX_INT(dst_bits));
-}
-
-static inline int
-_mesa_half_to_snorm(uint16_t x, unsigned dst_bits)
-{
-   return _mesa_float_to_snorm(_mesa_half_to_float(x), dst_bits);
-}
-
-static inline int
-_mesa_unorm_to_snorm(unsigned x, unsigned src_bits, unsigned dst_bits)
-{
-   return _mesa_unorm_to_unorm(x, src_bits, dst_bits - 1);
-}
-
-static inline int
-_mesa_snorm_to_snorm(int x, unsigned src_bits, unsigned dst_bits)
-{
-   if (x < -MAX_INT(src_bits))
-      return -MAX_INT(dst_bits);
-   else if (src_bits < dst_bits)
-      return EXTEND_NORMALIZED_INT(x, src_bits - 1, dst_bits - 1);
-   else
-      return x >> (src_bits - dst_bits);
-}
-
-static inline unsigned
-_mesa_unsigned_to_unsigned(unsigned src, unsigned dst_size)
-{
-   return MIN2(src, MAX_UINT(dst_size));
-}
-
-static inline int
-_mesa_unsigned_to_signed(unsigned src, unsigned dst_size)
-{
-   return MIN2(src, (unsigned)MAX_INT(dst_size));
-}
-
-static inline int
-_mesa_signed_to_signed(int src, unsigned dst_size)
-{
-   return CLAMP(src, MIN_INT(dst_size), MAX_INT(dst_size));
-}
-
-static inline unsigned
-_mesa_signed_to_unsigned(int src, unsigned dst_size)
-{
-   return CLAMP(src, 0, MAX_UINT(dst_size));
-}
-
-static inline unsigned
-_mesa_float_to_unsigned(float src, unsigned dst_bits)
-{
-   if (src < 0.0f)
-      return 0;
-   if (src > (float)MAX_UINT(dst_bits))
-       return MAX_UINT(dst_bits);
-   return _mesa_signed_to_unsigned(src, dst_bits);
-}
-
-static inline unsigned
-_mesa_float_to_signed(float src, unsigned dst_bits)
-{
-   if (src < (float)(-MAX_INT(dst_bits)))
-      return -MAX_INT(dst_bits);
-   if (src > (float)MAX_INT(dst_bits))
-       return MAX_INT(dst_bits);
-   return _mesa_signed_to_signed(src, dst_bits);
-}
-
-static inline unsigned
-_mesa_half_to_unsigned(uint16_t src, unsigned dst_bits)
-{
-   if (_mesa_half_is_negative(src))
-      return 0;
-   return _mesa_unsigned_to_unsigned(_mesa_float_to_half(src), dst_bits);
-}
-
-static inline unsigned
-_mesa_half_to_signed(uint16_t src, unsigned dst_bits)
-{
-   return _mesa_float_to_signed(_mesa_half_to_float(src), dst_bits);
-}
-
-#endif /* UTIL_FORMAT_UTILS_H */
diff --git a/src/util/format/u_format.c b/src/util/format/u_format.c
index b5cb27de3d9..5a33b187f0b 100644
--- a/src/util/format/u_format.c
+++ b/src/util/format/u_format.c
@@ -32,6 +32,7 @@
  * @author Jose Fonseca <jfonseca@vmware.com>
  */
 
+#include "drm-uapi/drm_fourcc.h"
 #include "util/format/u_format.h"
 #include "util/format/u_format_s3tc.h"
 #include "util/u_math.h"
@@ -957,3 +958,44 @@ util_format_snorm8_to_sint8(enum pipe_format format)
       return format;
    }
 }
+
+unsigned
+util_format_get_modifier_num_planes(uint64_t modifier, enum pipe_format format)
+{
+   switch (modifier) {
+   case I915_FORMAT_MOD_Y_TILED_GEN12_MC_CCS:
+   case I915_FORMAT_MOD_Y_TILED_GEN12_RC_CCS:
+   case I915_FORMAT_MOD_Y_TILED_CCS:
+      return 2 * util_format_get_num_planes(format);
+   case DRM_FORMAT_MOD_ARM_16X16_BLOCK_U_INTERLEAVED:
+   case DRM_FORMAT_MOD_ARM_AFBC(
+                        AFBC_FORMAT_MOD_BLOCK_SIZE_16x16 |
+                        AFBC_FORMAT_MOD_SPARSE |
+                        AFBC_FORMAT_MOD_YTR):
+   case DRM_FORMAT_MOD_ARM_AFBC(
+                        AFBC_FORMAT_MOD_BLOCK_SIZE_16x16 |
+                        AFBC_FORMAT_MOD_SPARSE):
+   case DRM_FORMAT_MOD_BROADCOM_UIF:
+   case DRM_FORMAT_MOD_BROADCOM_VC4_T_TILED:
+   case DRM_FORMAT_MOD_LINEAR:
+   /* DRM_FORMAT_MOD_NONE is the same as LINEAR */
+   case DRM_FORMAT_MOD_NVIDIA_16BX2_BLOCK_EIGHT_GOB:
+   case DRM_FORMAT_MOD_NVIDIA_16BX2_BLOCK_FOUR_GOB:
+   case DRM_FORMAT_MOD_NVIDIA_16BX2_BLOCK_ONE_GOB:
+   case DRM_FORMAT_MOD_NVIDIA_16BX2_BLOCK_SIXTEEN_GOB:
+   case DRM_FORMAT_MOD_NVIDIA_16BX2_BLOCK_THIRTYTWO_GOB:
+   case DRM_FORMAT_MOD_NVIDIA_16BX2_BLOCK_TWO_GOB:
+   case DRM_FORMAT_MOD_QCOM_COMPRESSED:
+   case DRM_FORMAT_MOD_VIVANTE_SPLIT_SUPER_TILED:
+   case DRM_FORMAT_MOD_VIVANTE_SPLIT_TILED:
+   case DRM_FORMAT_MOD_VIVANTE_SUPER_TILED:
+   case DRM_FORMAT_MOD_VIVANTE_TILED:
+   /* FD_FORMAT_MOD_QCOM_TILED is not in drm_fourcc.h */
+   case I915_FORMAT_MOD_X_TILED:
+   case I915_FORMAT_MOD_Y_TILED:
+   case DRM_FORMAT_MOD_INVALID:
+      return util_format_get_num_planes(format);
+   default:
+      return 0;
+   }
+}
diff --git a/src/util/format/u_format.csv b/src/util/format/u_format.csv
index 237c4c95475..8acfb869bdb 100644
--- a/src/util/format/u_format.csv
+++ b/src/util/format/u_format.csv
@@ -500,7 +500,7 @@ PIPE_FORMAT_R4G4B4A4_UINT           , plain, 1, 1, 1, up4 , up4 , up4 , up4 , xy
 PIPE_FORMAT_B4G4R4A4_UINT           , plain, 1, 1, 1, up4 , up4 , up4 , up4 , zyxw, rgb, up4 , up4 , up4 , up4 , yzwx
 PIPE_FORMAT_A4R4G4B4_UINT           , plain, 1, 1, 1, up4 , up4 , up4 , up4 , yzwx, rgb, up4 , up4 , up4 , up4 , zyxw
 PIPE_FORMAT_A4B4G4R4_UINT           , plain, 1, 1, 1, up4 , up4 , up4 , up4 , wzyx, rgb, up4 , up4 , up4 , up4 , xyzw
-PIPE_FORMAT_A1R5G5B5_UINT           , plain, 1, 1, 1, up1 , up5 , up5 , up5 , yzwx, rgb, up5 , up5 , up5 , up1 , zyxw
+PIPE_FORMAT_A1R5G5B5_UINT           , plain, 1, 1, 1, up1 , up5 , up5 , up5 , wzyx, rgb, up5 , up5 , up5 , up1 , zyxw
 PIPE_FORMAT_A1B5G5R5_UINT           , plain, 1, 1, 1, up1 , up5 , up5 , up5 , wzyx, rgb, up5 , up5 , up5 , up1 , xyzw
 PIPE_FORMAT_R5G5B5A1_UINT           , plain, 1, 1, 1, up5 , up5 , up5 , up1 , xyzw, rgb, up5 , up5 , up5 , up1 , wzyx
 PIPE_FORMAT_B5G5R5A1_UINT           , plain, 1, 1, 1, up5 , up5 , up5 , up1 , zyxw, rgb, up1 , up5 , up5 , up5 , yzwx
diff --git a/src/util/format/u_format.h b/src/util/format/u_format.h
index d0fa84194eb..6f6ee7f4243 100644
--- a/src/util/format/u_format.h
+++ b/src/util/format/u_format.h
@@ -437,6 +437,38 @@ util_format_short_name(enum pipe_format format)
    return desc->short_name;
 }
 
+
+static inline bool
+util_format_is_argb(enum pipe_format format)
+{
+   switch (format) {
+      case PIPE_FORMAT_A8R8G8B8_UNORM:
+      case PIPE_FORMAT_A8R8G8B8_UINT:
+      case PIPE_FORMAT_A8R8G8B8_SRGB:
+         return true;
+      default:
+         break;
+   }
+   return false;
+}
+
+static inline bool
+util_format_is_abgr(enum pipe_format format)
+{
+   switch (format) {
+      case PIPE_FORMAT_A8B8G8R8_UNORM:
+      case PIPE_FORMAT_A8B8G8R8_UINT:
+      case PIPE_FORMAT_A8B8G8R8_SRGB:
+      case PIPE_FORMAT_A8B8G8R8_SINT:
+      case PIPE_FORMAT_A8B8G8R8_SSCALED:
+      case PIPE_FORMAT_A8B8G8R8_USCALED:
+         return true;
+      default:
+         break;
+   }
+   return false;
+}
+
 /**
  * Whether this format is plain, see UTIL_FORMAT_LAYOUT_PLAIN for more info.
  */
@@ -1309,6 +1341,10 @@ util_format_get_num_planes(enum pipe_format format)
    }
 }
 
+
+unsigned
+util_format_get_modifier_num_planes(uint64_t modifier, enum pipe_format format);
+
 static inline enum pipe_format
 util_format_get_plane_format(enum pipe_format format, unsigned plane)
 {
diff --git a/src/util/format/u_format_pack.py b/src/util/format/u_format_pack.py
index c65548256a9..3545f9a0eae 100644
--- a/src/util/format/u_format_pack.py
+++ b/src/util/format/u_format_pack.py
@@ -379,19 +379,21 @@ def conversion_expr(src_channel,
             # neither is normalized -- just cast
             return '(%s)%s' % (dst_native_type, value)
 
-        if src_norm and dst_channel.norm:
-            return "_mesa_%snorm_to_%snorm(%s, %d, %d)" % ("s" if src_type == SIGNED else "u",
-                                                           "s" if dst_channel.type == SIGNED else "u",
-                                                           value, src_channel.size, dst_channel.size)
+        src_one = get_one(src_channel)
+        dst_one = get_one(dst_channel)
+
+        if src_one > dst_one and src_norm and dst_channel.norm:
+            # We can just bitshift
+            src_shift = get_one_shift(src_channel)
+            dst_shift = get_one_shift(dst_channel)
+            value = '(%s >> %s)' % (value, src_shift - dst_shift)
         else:
             # We need to rescale using an intermediate type big enough to hold the multiplication of both
-            src_one = get_one(src_channel)
-            dst_one = get_one(dst_channel)
             tmp_native_type = intermediate_native_type(src_size + dst_channel.size, src_channel.sign and dst_channel.sign)
             value = '((%s)%s)' % (tmp_native_type, value)
-            value = '(%s)(%s * 0x%x / 0x%x)' % (dst_native_type, value, dst_one, src_one)
-            return value
-
+            value = '(%s * 0x%x / 0x%x)' % (value, dst_one, src_one)
+        value = '(%s)%s' % (dst_native_type, value)
+        return value
 
     # Promote to either float or double
     if src_type != FLOAT:
@@ -452,6 +454,14 @@ def generate_unpack_kernel(format, dst_channel, dst_native_type):
         depth = format.block_size()
         print('         uint%u_t value = *(const uint%u_t *)src;' % (depth, depth)) 
 
+        # Declare the intermediate variables
+        for i in range(format.nr_channels()):
+            src_channel = channels[i]
+            if src_channel.type == UNSIGNED:
+                print('         uint%u_t %s;' % (depth, src_channel.name))
+            elif src_channel.type == SIGNED:
+                print('         int%u_t %s;' % (depth, src_channel.name))
+
         # Compute the intermediate unshifted values 
         for i in range(format.nr_channels()):
             src_channel = channels[i]
@@ -462,7 +472,6 @@ def generate_unpack_kernel(format, dst_channel, dst_native_type):
                     value = '%s >> %u' % (value, shift)
                 if shift + src_channel.size < depth:
                     value = '(%s) & 0x%x' % (value, (1 << src_channel.size) - 1)
-                print('         uint%u_t %s = %s;' % (depth, src_channel.name, value))
             elif src_channel.type == SIGNED:
                 if shift + src_channel.size < depth:
                     # Align the sign bit
@@ -474,10 +483,12 @@ def generate_unpack_kernel(format, dst_channel, dst_native_type):
                     # Align the LSB bit
                     rshift = depth - src_channel.size
                     value = '(%s) >> %u' % (value, rshift)
-                print('         int%u_t %s = %s;' % (depth, src_channel.name, value))
             else:
                 value = None
-
+                
+            if value is not None:
+                print('         %s = %s;' % (src_channel.name, value))
+                
         # Convert, swizzle, and store final values
         for i in range(4):
             swizzle = swizzles[i]
@@ -708,7 +719,6 @@ def generate(formats):
     print('#include "u_format.h"')
     print('#include "u_format_other.h"')
     print('#include "util/format_srgb.h"')
-    print('#include "format_utils.h"')
     print('#include "u_format_yuv.h"')
     print('#include "u_format_zs.h"')
     print('#include "u_format_pack.h"')
diff --git a/src/util/format/u_format_zs.c b/src/util/format/u_format_zs.c
index d941c37d22d..197e881dbc0 100644
--- a/src/util/format/u_format_zs.c
+++ b/src/util/format/u_format_zs.c
@@ -309,8 +309,7 @@ util_format_z32_float_unpack_z_32unorm(uint32_t *dst_row, unsigned dst_stride,
       uint32_t *dst = dst_row;
       const float *src = (const float *)src_row;
       for(x = 0; x < width; ++x) {
-         float z = *src++;
-         *dst++ = z32_float_to_z32_unorm(CLAMP(z, 0.0f, 1.0f));
+         *dst++ = z32_float_to_z32_unorm(*src++);
       }
       src_row += src_stride/sizeof(*src_row);
       dst_row += dst_stride/sizeof(*dst_row);
@@ -866,7 +865,7 @@ util_format_z32_float_s8x24_uint_unpack_z_32unorm(uint32_t *dst_row, unsigned ds
       uint32_t *dst = dst_row;
       const float *src = (const float *)src_row;
       for(x = 0; x < width; ++x) {
-         *dst = z32_float_to_z32_unorm(CLAMP(*src, 0.0f, 1.0f));
+         *dst = z32_float_to_z32_unorm(*src);
          src += 2;
          dst += 1;
       }
diff --git a/src/util/hash_table.c b/src/util/hash_table.c
index 92897c1bc83..0cce8c5f0f0 100644
--- a/src/util/hash_table.c
+++ b/src/util/hash_table.c
@@ -234,13 +234,6 @@ _mesa_hash_table_destroy(struct hash_table *ht,
    ralloc_free(ht);
 }
 
-static void
-hash_table_clear_fast(struct hash_table *ht)
-{
-   memset(ht->table, 0, sizeof(struct hash_entry) * hash_sizes[ht->size_index].size);
-   ht->entries = ht->deleted_entries = 0;
-}
-
 /**
  * Deletes all entries of the given hash table without deleting the table
  * itself or changing its structure.
@@ -256,17 +249,15 @@ _mesa_hash_table_clear(struct hash_table *ht,
 
    struct hash_entry *entry;
 
-   if (delete_function) {
-      for (entry = ht->table; entry != ht->table + ht->size; entry++) {
-         if (entry_is_present(ht, entry))
-            delete_function(entry);
+   for (entry = ht->table; entry != ht->table + ht->size; entry++) {
+      if (entry_is_present(ht, entry) && delete_function != NULL)
+         delete_function(entry);
 
-         entry->key = NULL;
-      }
-      ht->entries = 0;
-      ht->deleted_entries = 0;
-   } else
-      hash_table_clear_fast(ht);
+      entry->key = NULL;
+   }
+
+   ht->entries = 0;
+   ht->deleted_entries = 0;
 }
 
 /** Sets the value of the key pointer used for deleted entries in the table.
@@ -372,8 +363,8 @@ _mesa_hash_table_rehash(struct hash_table *ht, unsigned new_size_index)
    struct hash_entry *table;
 
    if (ht->size_index == new_size_index && ht->deleted_entries == ht->max_entries) {
-      hash_table_clear_fast(ht);
-      assert(!ht->entries);
+      memset(ht->table, 0, sizeof(struct hash_entry) * hash_sizes[ht->size_index].size);
+      ht->deleted_entries = 0;
       return;
    }
 
diff --git a/src/util/set.c b/src/util/set.c
index 2e3cb92cb00..7b2da59633f 100644
--- a/src/util/set.c
+++ b/src/util/set.c
@@ -220,14 +220,6 @@ _mesa_set_destroy(struct set *ht, void (*delete_function)(struct set_entry *entr
    ralloc_free(ht);
 }
 
-
-static void
-set_clear_fast(struct set *ht)
-{
-   memset(ht->table, 0, sizeof(struct set_entry) * hash_sizes[ht->size_index].size);
-   ht->entries = ht->deleted_entries = 0;
-}
-
 /**
  * Clears all values from the given set.
  *
@@ -242,17 +234,15 @@ _mesa_set_clear(struct set *set, void (*delete_function)(struct set_entry *entry
 
    struct set_entry *entry;
 
-   if (delete_function) {
-      for (entry = set->table; entry != set->table + set->size; entry++) {
-         if (entry_is_present(entry))
-            delete_function(entry);
+   for (entry = set->table; entry != set->table + set->size; entry++) {
+      if (entry_is_present(entry) && delete_function != NULL)
+         delete_function(entry);
 
-         entry->key = NULL;
-      }
-      set->entries = 0;
-      set->deleted_entries = 0;
-   } else
-      set_clear_fast(set);
+      entry->key = NULL;
+   }
+
+   set->entries = 0;
+   set->deleted_entries = 0;
 }
 
 /**
@@ -334,8 +324,8 @@ set_rehash(struct set *ht, unsigned new_size_index)
    struct set_entry *table;
 
    if (ht->size_index == new_size_index && ht->deleted_entries == ht->max_entries) {
-      set_clear_fast(ht);
-      assert(!ht->entries);
+      memset(ht->table, 0, sizeof(struct set_entry) * hash_sizes[ht->size_index].size);
+      ht->deleted_entries = 0;
       return;
    }
 
@@ -382,6 +372,62 @@ _mesa_set_resize(struct set *set, uint32_t entries)
    set_rehash(set, size_index);
 }
 
+/**
+ * This is the same as set_search_or_add(), but optimized for a table with no deleted entries.
+ */
+static struct set_entry *
+set_search_or_add_fast(struct set *ht, uint32_t hash, const void *key, bool *found)
+{
+   struct set_entry *available_entry = NULL;
+
+   assert(!ht->deleted_entries);
+   assert(!key_pointer_is_reserved(key));
+
+   if (ht->entries >= ht->max_entries) {
+      set_rehash(ht, ht->size_index + 1);
+   }
+
+   uint32_t size = ht->size;
+   uint32_t start_address = util_fast_urem32(hash, size, ht->size_magic);
+   uint32_t double_hash = util_fast_urem32(hash, ht->rehash,
+                                           ht->rehash_magic) + 1;
+   uint32_t hash_address = start_address;
+   do {
+      struct set_entry *entry = ht->table + hash_address;
+
+      if (!entry->key) {
+         /* Stash the first available entry we find */
+         if (available_entry == NULL)
+            available_entry = entry;
+         break;
+      }
+
+      if (entry->hash == hash &&
+          ht->key_equals_function(key, entry->key)) {
+         if (found)
+            *found = true;
+         return entry;
+      }
+
+      hash_address = hash_address + double_hash;
+      if (hash_address >= size)
+         hash_address -= size;
+   } while (hash_address != start_address);
+
+   if (available_entry) {
+      available_entry->hash = hash;
+      available_entry->key = key;
+      ht->entries++;
+      if (found)
+         *found = false;
+      return available_entry;
+   }
+
+   /* We could hit here if a required resize failed. An unchecked-malloc
+    * application could ignore this result.
+    */
+   return NULL;
+}
 /**
  * Find a matching entry for the given key, or insert it if it doesn't already
  * exist.
@@ -394,6 +440,9 @@ set_search_or_add(struct set *ht, uint32_t hash, const void *key, bool *found)
 {
    struct set_entry *available_entry = NULL;
 
+   if (!ht->deleted_entries)
+      return set_search_or_add_fast(ht, hash, key, found);
+
    assert(!key_pointer_is_reserved(key));
 
    if (ht->entries >= ht->max_entries) {
@@ -560,6 +609,27 @@ _mesa_set_remove_key(struct set *set, const void *key)
    _mesa_set_remove(set, _mesa_set_search(set, key));
 }
 
+/**
+ * This function is an iterator over the set when no deleted entries are present.
+ *
+ * Pass in NULL for the first entry, as in the start of a for loop.
+ */
+struct set_entry *
+_mesa_set_next_entry_fast(const struct set *ht, struct set_entry *entry)
+{
+   assert(!ht->deleted_entries);
+   if (!ht->entries)
+      return NULL;
+   if (entry == NULL)
+      entry = ht->table;
+   else
+      entry = entry + 1;
+   if (entry != ht->table + ht->size)
+      return entry->key ? entry : _mesa_set_next_entry_fast(ht, entry);
+
+   return NULL;
+}
+
 /**
  * This function is an iterator over the hash table.
  *
diff --git a/src/util/set.h b/src/util/set.h
index 54983138477..9aeb6edab14 100644
--- a/src/util/set.h
+++ b/src/util/set.h
@@ -111,6 +111,8 @@ _mesa_set_remove_key(struct set *set, const void *key);
 
 struct set_entry *
 _mesa_set_next_entry(const struct set *set, struct set_entry *entry);
+struct set_entry *
+_mesa_set_next_entry_fast(const struct set *set, struct set_entry *entry);
 
 struct set_entry *
 _mesa_set_random_entry(struct set *set,
@@ -132,6 +134,15 @@ _mesa_set_intersects(struct set *a, struct set *b);
         entry != NULL;                                              \
         entry = _mesa_set_next_entry(set, entry))
 
+/**
+ * This foreach function destroys the table as it iterates.
+ * It is not safe to use when inserting or removing entries.
+ */
+#define set_foreach_remove(set, entry)                              \
+   for (struct set_entry *entry = _mesa_set_next_entry_fast(set, NULL);  \
+        (set)->entries;                                              \
+        entry->key = 0, entry->key = (void*)NULL, (set)->entries--, entry = _mesa_set_next_entry_fast(set, entry))
+
 #ifdef __cplusplus
 } /* extern C */
 #endif
diff --git a/src/util/tests/hash_table/clear.c b/src/util/tests/hash_table/clear.c
index 90ca30e3a1c..e61f60ece1b 100644
--- a/src/util/tests/hash_table/clear.c
+++ b/src/util/tests/hash_table/clear.c
@@ -86,12 +86,6 @@ int main()
    hash_table_foreach(ht, entry) {
       assert(key_id(entry->key) < SIZE);
    }
-   _mesa_hash_table_clear(ht, NULL);
-   assert(!ht->entries);
-   assert(!ht->deleted_entries);
-   hash_table_foreach(ht, entry) {
-      assert(0);
-   }
 
    _mesa_hash_table_destroy(ht, NULL);
 
diff --git a/src/util/tests/set/set_test.cpp b/src/util/tests/set/set_test.cpp
index 6b9c993e3fd..8048b9270ff 100644
--- a/src/util/tests/set/set_test.cpp
+++ b/src/util/tests/set/set_test.cpp
@@ -51,13 +51,6 @@ TEST(set, basic)
    entry = _mesa_set_search(s, a);
    EXPECT_FALSE(entry);
 
-   _mesa_set_clear(s, NULL);
-   EXPECT_EQ(s->entries, 0);
-   EXPECT_EQ(s->deleted_entries, 0);
-   set_foreach(s, he) {
-      GTEST_FAIL();
-   }
-
    _mesa_set_destroy(s, NULL);
 }
 
diff --git a/src/util/u_debug_stack.c b/src/util/u_debug_stack.c
index 01f69e14442..21f0371d3c1 100644
--- a/src/util/u_debug_stack.c
+++ b/src/util/u_debug_stack.c
@@ -94,7 +94,7 @@ debug_backtrace_capture(struct debug_stack_frame *backtrace,
    unw_proc_info_t pip;
    unsigned i = 0;
 
-   pip.unwind_info = 0;
+   pip.unwind_info = NULL;
 
    unw_getcontext(&context);
    unw_init_local(&cursor, &context);
diff --git a/src/util/u_queue.c b/src/util/u_queue.c
index cae4cc316d2..a33f892775f 100644
--- a/src/util/u_queue.c
+++ b/src/util/u_queue.c
@@ -183,11 +183,7 @@ _util_queue_fence_wait_timeout(struct util_queue_fence *fence,
    if (rel > 0) {
       struct timespec ts;
 
-#ifdef HAVE_TIMESPEC_GET
       timespec_get(&ts, TIME_UTC);
-#else
-      clock_gettime(CLOCK_REALTIME, &ts);
-#endif
 
       ts.tv_sec += abs_timeout / (1000*1000*1000);
       ts.tv_nsec += abs_timeout % (1000*1000*1000);
@@ -305,7 +301,7 @@ util_queue_thread_func(void *input)
          queue->total_jobs_size -= job.job_size;
       mtx_unlock(&queue->lock);
 
-      if (job.job) {
+      if (job.execute) {
          job.execute(job.job, thread_index);
          util_queue_fence_signal(job.fence);
          if (job.cleanup)
diff --git a/src/vulkan/Makefile.sources b/src/vulkan/Makefile.sources
index e979d1a2450..640c36eff22 100644
--- a/src/vulkan/Makefile.sources
+++ b/src/vulkan/Makefile.sources
@@ -27,8 +27,6 @@ VULKAN_UTIL_FILES := \
 	util/vk_alloc.h \
 	util/vk_debug_report.c \
 	util/vk_debug_report.h \
-	util/vk_deferred_operation.c \
-	util/vk_deferred_operation.h \
 	util/vk_format.c \
 	util/vk_object.c \
 	util/vk_object.h \
diff --git a/src/vulkan/device-select-layer/VkLayer_MESA_device_select.json b/src/vulkan/device-select-layer/VkLayer_MESA_device_select.json
index 361ae9fe74e..1d5fffd0135 100644
--- a/src/vulkan/device-select-layer/VkLayer_MESA_device_select.json
+++ b/src/vulkan/device-select-layer/VkLayer_MESA_device_select.json
@@ -4,7 +4,7 @@
     "name": "VK_LAYER_MESA_device_select",
     "type": "GLOBAL",
     "library_path": "libVkLayer_MESA_device_select.so",
-    "api_version": "1.2.73",
+    "api_version": "1.1.73",
     "implementation_version": "1",
     "description": "Linux device selection layer",
     "functions": {
diff --git a/src/vulkan/util/meson.build b/src/vulkan/util/meson.build
index f7d8a3df502..687a7d48d10 100644
--- a/src/vulkan/util/meson.build
+++ b/src/vulkan/util/meson.build
@@ -22,8 +22,6 @@ files_vulkan_util = files(
   'vk_alloc.h',
   'vk_debug_report.c',
   'vk_debug_report.h',
-  'vk_deferred_operation.c',
-  'vk_deferred_operation.h',
   'vk_format.c',
   'vk_object.c',
   'vk_object.h',
diff --git a/src/vulkan/util/vk_deferred_operation.c b/src/vulkan/util/vk_deferred_operation.c
deleted file mode 100644
index 765a8b3d4c2..00000000000
--- a/src/vulkan/util/vk_deferred_operation.c
+++ /dev/null
@@ -1,80 +0,0 @@
-/*
- * Copyright  2020 Intel Corporation
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice (including the next
- * paragraph) shall be included in all copies or substantial portions of the
- * Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
- * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
- * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
- * IN THE SOFTWARE.
- */
-
-#include "vk_deferred_operation.h"
-
-#include "vk_alloc.h"
-
-VkResult
-vk_create_deferred_operation(struct vk_device *device,
-                             const VkAllocationCallbacks *pAllocator,
-                             VkDeferredOperationKHR *pDeferredOperation)
-{
-   struct vk_deferred_operation *op =
-      vk_alloc2(&device->alloc, pAllocator, sizeof(*op), 8,
-                       VK_SYSTEM_ALLOCATION_SCOPE_OBJECT);
-   if (op == NULL)
-      return VK_ERROR_OUT_OF_HOST_MEMORY;
-
-   vk_object_base_init(device, &op->base,
-                       VK_OBJECT_TYPE_DEFERRED_OPERATION_KHR);
-
-   *pDeferredOperation = vk_deferred_operation_to_handle(op);
-
-   return VK_SUCCESS;
-}
-
-void
-vk_destroy_deferred_operation(struct vk_device *device,
-                              VkDeferredOperationKHR operation,
-                              const VkAllocationCallbacks *pAllocator)
-{
-   if (operation == VK_NULL_HANDLE)
-      return;
-
-   VK_FROM_HANDLE(vk_deferred_operation, op, operation);
-
-   vk_object_base_finish(&op->base);
-   vk_free2(&device->alloc, pAllocator, op);
-}
-
-uint32_t
-vk_get_deferred_operation_max_concurrency(UNUSED struct vk_device *device,
-                                          UNUSED VkDeferredOperationKHR operation)
-{
-   return 1;
-}
-
-VkResult
-vk_get_deferred_operation_result(UNUSED struct vk_device *device,
-                                 UNUSED VkDeferredOperationKHR operation)
-{
-   return VK_SUCCESS;
-}
-
-VkResult
-vk_deferred_operation_join(UNUSED struct vk_device *device,
-                           UNUSED VkDeferredOperationKHR operation)
-{
-   return VK_SUCCESS;
-}
diff --git a/src/vulkan/util/vk_deferred_operation.h b/src/vulkan/util/vk_deferred_operation.h
deleted file mode 100644
index 78b1c450bbe..00000000000
--- a/src/vulkan/util/vk_deferred_operation.h
+++ /dev/null
@@ -1,69 +0,0 @@
-/*
- * Copyright  2020 Intel Corporation
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice (including the next
- * paragraph) shall be included in all copies or substantial portions of the
- * Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
- * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
- * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
- * IN THE SOFTWARE.
- */
-#ifndef VK_DEFERRED_OPERATION_H
-#define VK_DEFERRED_OPERATION_H
-
-#include "vk_object.h"
-
-#include "c11/threads.h"
-#include "util/list.h"
-
-#ifdef __cplusplus
-extern "C" {
-#endif
-
-struct vk_deferred_operation {
-   struct vk_object_base base;
-};
-
-VK_DEFINE_NONDISP_HANDLE_CASTS(vk_deferred_operation, base,
-                               VkDeferredOperationKHR,
-                               VK_OBJECT_TYPE_DEFERRED_OPERATION_KHR)
-
-VkResult
-vk_create_deferred_operation(struct vk_device *device,
-                             const VkAllocationCallbacks *pAllocator,
-                             VkDeferredOperationKHR *pDeferredOperation);
-
-void
-vk_destroy_deferred_operation(struct vk_device *device,
-                              VkDeferredOperationKHR operation,
-                              const VkAllocationCallbacks *pAllocator);
-
-uint32_t
-vk_get_deferred_operation_max_concurrency(struct vk_device *device,
-                                          VkDeferredOperationKHR operation);
-
-VkResult
-vk_get_deferred_operation_result(struct vk_device *device,
-                                 VkDeferredOperationKHR operation);
-
-VkResult
-vk_deferred_operation_join(struct vk_device *device,
-                           VkDeferredOperationKHR operation);
-
-#ifdef __cplusplus
-}
-#endif
-
-#endif /* VK_DEFERRED_OPERATION_H */
diff --git a/src/vulkan/wsi/wsi_common_x11.c b/src/vulkan/wsi/wsi_common_x11.c
index 165b366e2df..124377dd6cb 100644
--- a/src/vulkan/wsi/wsi_common_x11.c
+++ b/src/vulkan/wsi/wsi_common_x11.c
@@ -120,31 +120,30 @@ static bool
 wsi_x11_detect_xwayland(xcb_connection_t *conn)
 {
    xcb_randr_query_version_cookie_t ver_cookie =
-      xcb_randr_query_version_unchecked(conn, 1, 3);
+      xcb_randr_query_version_unchecked(conn, 1, 2);
    xcb_randr_query_version_reply_t *ver_reply =
       xcb_randr_query_version_reply(conn, ver_cookie, NULL);
-   bool has_randr_v1_3 = ver_reply && (ver_reply->major_version > 1 ||
-                                       ver_reply->minor_version >= 3);
+   bool has_randr_v1_2 = ver_reply && (ver_reply->major_version > 1 ||
+                                       ver_reply->minor_version >= 2);
    free(ver_reply);
 
-   if (!has_randr_v1_3)
+   if (!has_randr_v1_2)
       return false;
 
    const xcb_setup_t *setup = xcb_get_setup(conn);
    xcb_screen_iterator_t iter = xcb_setup_roots_iterator(setup);
 
-   xcb_randr_get_screen_resources_current_cookie_t gsr_cookie =
-      xcb_randr_get_screen_resources_current_unchecked(conn, iter.data->root);
-   xcb_randr_get_screen_resources_current_reply_t *gsr_reply =
-      xcb_randr_get_screen_resources_current_reply(conn, gsr_cookie, NULL);
+   xcb_randr_get_screen_resources_cookie_t gsr_cookie =
+      xcb_randr_get_screen_resources_unchecked(conn, iter.data->root);
+   xcb_randr_get_screen_resources_reply_t *gsr_reply =
+      xcb_randr_get_screen_resources_reply(conn, gsr_cookie, NULL);
 
    if (!gsr_reply || gsr_reply->num_outputs == 0) {
       free(gsr_reply);
       return false;
    }
 
-   xcb_randr_output_t *randr_outputs =
-      xcb_randr_get_screen_resources_current_outputs(gsr_reply);
+   xcb_randr_output_t *randr_outputs = xcb_randr_get_screen_resources_outputs(gsr_reply);
    xcb_randr_get_output_info_cookie_t goi_cookie =
       xcb_randr_get_output_info(conn, randr_outputs[0], gsr_reply->config_timestamp);
    free(gsr_reply);
